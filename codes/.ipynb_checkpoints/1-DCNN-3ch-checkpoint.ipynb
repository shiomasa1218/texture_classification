{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import Input,Model,optimizers\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Conv1D, UpSampling1D,Activation\n",
    "from tensorflow.keras.layers import MaxPooling1D, BatchNormalization, Flatten,GlobalMaxPooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "from tensorflow.keras.callbacks import Callback, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory growth: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, classification_report,plot_confusion_matrix\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print('memory growth:', tf.config.experimental.get_memory_growth(physical_devices[0]))\n",
    "else:\n",
    "    print(\"Not enough GPU hardware devices available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_RATE = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define\n",
    "EPOCH = 3000\n",
    "BATCH_SIZE = 128\n",
    "MINIBATCH = 64\n",
    "DROP_RATE = 0.5\n",
    "\n",
    "FC_SIZE = 50\n",
    "FILTER_SIZE = 64\n",
    "\n",
    "DATA_START = 250\n",
    "DATA_LEN = 4096\n",
    "\n",
    "IN_DIR_PATH = \"3ch_normalized\"\n",
    "\n",
    "os.makedirs('../logs/'+IN_DIR_PATH+'/events',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/976 [00:00<?, ?it/s]/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/ipykernel_launcher.py:25: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "100%|██████████| 976/976 [01:01<00:00, 15.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# input_dir and out_dir\n",
    "input_dir_path = \"../preprocessed/train_\"+IN_DIR_PATH+\"/*/*.csv\"\n",
    "input_dir = glob.glob(input_dir_path)\n",
    "input_num = len(input_dir)\n",
    "\n",
    "class_list = np.array([])\n",
    "all_data = np.array([])\n",
    "all_label = np.array([])\n",
    "\n",
    "init_flg = True\n",
    "\n",
    "for d in tqdm(input_dir):\n",
    "    # print(d)\n",
    "    # delete DS_Sore \n",
    "    if \".DS_Store\" in d:\n",
    "        os.remove(d)\n",
    "        input_dir.remove(d)\n",
    "        continue\n",
    "    \n",
    "    # extract texture naem from path\n",
    "    end_index = d.rfind('/')\n",
    "    start_index = d[:end_index].rfind('/')\n",
    "    class_name = d[start_index+1:end_index]\n",
    "    # create classname list\n",
    "    if class_name not in class_list:\n",
    "        class_list = np.append(class_list,class_name)\n",
    "        # print(class_name)\n",
    "    # label classname list index\n",
    "    label = np.where(class_list == class_name)\n",
    "    all_label = np.append(all_label,label)\n",
    "    \n",
    "    data = np.loadtxt(d, delimiter=\",\")\n",
    "    \n",
    "    # axis_time = np.vstack(data[DATA_START:DATA_START+DATA_LEN,0])\n",
    "    axis_value = np.hstack([data[DATA_START:DATA_START+DATA_LEN, 1:]])\n",
    "    \n",
    "    # preprocessed = np.hstack([axis_time,axis_value])\n",
    "    if init_flg:\n",
    "        all_data = axis_value\n",
    "        init_flg = False\n",
    "    else:\n",
    "        all_data = np.dstack([all_data,axis_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(976, 4096, 3)\n"
     ]
    }
   ],
   "source": [
    "all_data_trans = np.transpose(all_data, (2,0,1))\n",
    "print(all_data_trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_label = to_categorical(all_label,class_list.shape[0])\n",
    "\n",
    "p = np.random.permutation(input_num)\n",
    "shuffled_data = all_data_trans[p]\n",
    "shuffled_label = one_hot_label[p]\n",
    "\n",
    "trainX = shuffled_data[:int(input_num*VAL_RATE)]\n",
    "valX = shuffled_data[int(input_num*VAL_RATE):]\n",
    "trainY = shuffled_label[:int(input_num*VAL_RATE)]\n",
    "valY = shuffled_label[int(input_num*VAL_RATE):]\n",
    "\n",
    "trainX = np.reshape(trainX,(int(input_num*VAL_RATE),DATA_LEN,3)).astype(np.float32)\n",
    "valX = np.reshape(valX,(input_num-int(input_num*VAL_RATE),DATA_LEN,3)).astype(np.float32)\n",
    "# trainY = np.reshape(trainY,(int(input_num*VAL_RATE),class_list.shape[0],1))\n",
    "# valY = np.reshape(valY,(input_num-int(input_num*VAL_RATE),class_list.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878, 4096, 3)\n",
      "(878, 9)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mynet():\n",
    "    inputs = Input(shape=(DATA_LEN,3))\n",
    "    # Due to memory limitation, images will resized on-the-fly.\n",
    "    x = Conv1D(FILTER_SIZE, 5, padding='same', input_shape=(DATA_LEN,3), activation=None)(inputs)\n",
    "    x = Conv1D(FILTER_SIZE, 5, padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Dropout(DROP_RATE)(x)\n",
    "\n",
    "    x = Conv1D(FILTER_SIZE*2, 5, padding='same')(x)\n",
    "    x = Conv1D(FILTER_SIZE*2, 5, padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Dropout(DROP_RATE)(x)\n",
    "    \n",
    "    x = Conv1D(FILTER_SIZE*4, 5, padding='same')(x)\n",
    "    x = Conv1D(FILTER_SIZE*4, 5, padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Dropout(DROP_RATE)(x)\n",
    "    \n",
    "    fc = Flatten()(x)\n",
    "    fc = Dense(FC_SIZE*2, activation='relu')(fc)\n",
    "    fc = BatchNormalization()(fc)\n",
    "    fc = Dropout(DROP_RATE)(fc)\n",
    "    \n",
    "    fc = Dense(FC_SIZE, activation='relu')(fc)\n",
    "    fc = BatchNormalization()(fc)\n",
    "    fc = Dropout(DROP_RATE)(fc)\n",
    "    \n",
    "    fc = Dense(class_list.shape[0])(fc)\n",
    "    softmax = Activation('softmax')(fc)\n",
    "    model = Model(inputs=inputs, outputs=softmax)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mynet_squeeze():\n",
    "    inputs = Input(shape=(DATA_LEN,3))\n",
    "    # Due to memory limitation, images will resized on-the-fly.\n",
    "    x = Conv1D(FILTER_SIZE, 5, padding='same', input_shape=(DATA_LEN,3), activation='relu')(inputs)\n",
    "    x = Conv1D(FILTER_SIZE, 5, padding='same',activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Conv1D(int(FILTER_SIZE*2), 5, padding='same',activation='relu')(x)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Dropout(DROP_RATE)(x)\n",
    "    x = Conv1D(int(FILTER_SIZE), 5, padding='same', activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "    \n",
    "    fc = GlobalMaxPooling1D()(x)\n",
    "    fc = Dropout(DROP_RATE)(fc)\n",
    "    fc = Flatten()(fc)\n",
    "    fc = Dense(class_list.shape[0])(fc)\n",
    "    softmax = Activation('softmax')(fc)\n",
    "    model = Model(inputs=inputs, outputs=softmax)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=False):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig('../logs/'+IN_DIR_PATH+'/cm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4096, 3)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 4096, 64)          1024      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 4096, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 2048, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 2048, 128)         41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1024, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1024, 64)          41024     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 9)                 585       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 104,265\n",
      "Trainable params: 104,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 702 samples, validate on 176 samples\n",
      "Epoch 1/3000\n",
      "128/702 [====>.........................] - ETA: 5s - loss: 2.1989 - acc: 0.1172WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.114273). Check your callbacks.\n",
      "702/702 [==============================] - 3s 4ms/sample - loss: 2.1749 - acc: 0.1538 - val_loss: 2.1264 - val_acc: 0.3409\n",
      "Epoch 2/3000\n",
      "702/702 [==============================] - 0s 602us/sample - loss: 2.0949 - acc: 0.2365 - val_loss: 2.0407 - val_acc: 0.3295\n",
      "Epoch 3/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 1.9299 - acc: 0.3319 - val_loss: 1.8725 - val_acc: 0.3693\n",
      "Epoch 4/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 1.7620 - acc: 0.3533 - val_loss: 1.7578 - val_acc: 0.4432\n",
      "Epoch 5/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 1.6315 - acc: 0.4017 - val_loss: 1.6458 - val_acc: 0.4773\n",
      "Epoch 6/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 1.5366 - acc: 0.4530 - val_loss: 1.5321 - val_acc: 0.5455\n",
      "Epoch 7/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 1.4352 - acc: 0.4915 - val_loss: 1.4135 - val_acc: 0.5739\n",
      "Epoch 8/3000\n",
      "702/702 [==============================] - 0s 534us/sample - loss: 1.3897 - acc: 0.5043 - val_loss: 1.3563 - val_acc: 0.6080\n",
      "Epoch 9/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 1.3213 - acc: 0.5128 - val_loss: 1.3685 - val_acc: 0.6307\n",
      "Epoch 10/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 1.2798 - acc: 0.5413 - val_loss: 1.3459 - val_acc: 0.6023\n",
      "Epoch 11/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 1.2685 - acc: 0.5584 - val_loss: 1.3345 - val_acc: 0.6250\n",
      "Epoch 12/3000\n",
      "702/702 [==============================] - 0s 619us/sample - loss: 1.2535 - acc: 0.5470 - val_loss: 1.3001 - val_acc: 0.6307\n",
      "Epoch 13/3000\n",
      "702/702 [==============================] - 0s 545us/sample - loss: 1.2073 - acc: 0.5613 - val_loss: 1.2698 - val_acc: 0.6364\n",
      "Epoch 14/3000\n",
      "702/702 [==============================] - 0s 542us/sample - loss: 1.1667 - acc: 0.5869 - val_loss: 1.2602 - val_acc: 0.6307\n",
      "Epoch 15/3000\n",
      "702/702 [==============================] - 0s 559us/sample - loss: 1.1573 - acc: 0.5883 - val_loss: 1.2884 - val_acc: 0.6364\n",
      "Epoch 16/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 1.1923 - acc: 0.5769 - val_loss: 1.2549 - val_acc: 0.6534\n",
      "Epoch 17/3000\n",
      "702/702 [==============================] - 0s 564us/sample - loss: 1.1354 - acc: 0.5869 - val_loss: 1.2279 - val_acc: 0.6534\n",
      "Epoch 18/3000\n",
      "702/702 [==============================] - 0s 554us/sample - loss: 1.1304 - acc: 0.5969 - val_loss: 1.2392 - val_acc: 0.6364\n",
      "Epoch 19/3000\n",
      "702/702 [==============================] - 0s 554us/sample - loss: 1.1594 - acc: 0.6054 - val_loss: 1.2291 - val_acc: 0.6364\n",
      "Epoch 20/3000\n",
      "702/702 [==============================] - 0s 566us/sample - loss: 1.1150 - acc: 0.6225 - val_loss: 1.1885 - val_acc: 0.6818\n",
      "Epoch 21/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 1.0894 - acc: 0.6239 - val_loss: 1.1833 - val_acc: 0.6875\n",
      "Epoch 22/3000\n",
      "702/702 [==============================] - 0s 604us/sample - loss: 1.0456 - acc: 0.6311 - val_loss: 1.1867 - val_acc: 0.6420\n",
      "Epoch 23/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 1.0200 - acc: 0.6268 - val_loss: 1.1399 - val_acc: 0.6875\n",
      "Epoch 24/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 1.0185 - acc: 0.6211 - val_loss: 1.1330 - val_acc: 0.6761\n",
      "Epoch 25/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.9860 - acc: 0.6695 - val_loss: 1.1351 - val_acc: 0.6818\n",
      "Epoch 26/3000\n",
      "702/702 [==============================] - 0s 540us/sample - loss: 1.0033 - acc: 0.6467 - val_loss: 1.0927 - val_acc: 0.6875\n",
      "Epoch 27/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.9257 - acc: 0.6524 - val_loss: 1.1324 - val_acc: 0.6080\n",
      "Epoch 28/3000\n",
      "702/702 [==============================] - 0s 546us/sample - loss: 0.9530 - acc: 0.6538 - val_loss: 1.0802 - val_acc: 0.7102\n",
      "Epoch 29/3000\n",
      "702/702 [==============================] - 0s 539us/sample - loss: 0.9310 - acc: 0.6638 - val_loss: 1.0902 - val_acc: 0.6136\n",
      "Epoch 30/3000\n",
      "702/702 [==============================] - 0s 543us/sample - loss: 0.9053 - acc: 0.6681 - val_loss: 1.0396 - val_acc: 0.7102\n",
      "Epoch 31/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.8689 - acc: 0.6652 - val_loss: 1.0316 - val_acc: 0.6534\n",
      "Epoch 32/3000\n",
      "702/702 [==============================] - 0s 602us/sample - loss: 0.8175 - acc: 0.7208 - val_loss: 1.0095 - val_acc: 0.7216\n",
      "Epoch 33/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.8066 - acc: 0.7066 - val_loss: 0.9817 - val_acc: 0.7330\n",
      "Epoch 34/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.8623 - acc: 0.6738 - val_loss: 0.9581 - val_acc: 0.7216\n",
      "Epoch 35/3000\n",
      "702/702 [==============================] - 0s 534us/sample - loss: 0.7982 - acc: 0.7151 - val_loss: 0.9931 - val_acc: 0.7045\n",
      "Epoch 36/3000\n",
      "702/702 [==============================] - 0s 543us/sample - loss: 0.8288 - acc: 0.6895 - val_loss: 0.9623 - val_acc: 0.7102\n",
      "Epoch 37/3000\n",
      "702/702 [==============================] - 0s 555us/sample - loss: 0.8116 - acc: 0.6895 - val_loss: 0.9573 - val_acc: 0.7045\n",
      "Epoch 38/3000\n",
      "702/702 [==============================] - 0s 551us/sample - loss: 0.8142 - acc: 0.7194 - val_loss: 0.9787 - val_acc: 0.7557\n",
      "Epoch 39/3000\n",
      "702/702 [==============================] - 0s 544us/sample - loss: 0.7586 - acc: 0.7350 - val_loss: 0.9486 - val_acc: 0.7330\n",
      "Epoch 40/3000\n",
      "702/702 [==============================] - 0s 543us/sample - loss: 0.7687 - acc: 0.7293 - val_loss: 0.9330 - val_acc: 0.7216\n",
      "Epoch 41/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.7451 - acc: 0.7279 - val_loss: 0.8728 - val_acc: 0.7670\n",
      "Epoch 42/3000\n",
      "702/702 [==============================] - 0s 624us/sample - loss: 0.7631 - acc: 0.7165 - val_loss: 0.8922 - val_acc: 0.7273\n",
      "Epoch 43/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 0.7023 - acc: 0.7521 - val_loss: 0.8800 - val_acc: 0.7500\n",
      "Epoch 44/3000\n",
      "702/702 [==============================] - 0s 548us/sample - loss: 0.7076 - acc: 0.7407 - val_loss: 0.8702 - val_acc: 0.7443\n",
      "Epoch 45/3000\n",
      "702/702 [==============================] - 0s 544us/sample - loss: 0.7301 - acc: 0.7151 - val_loss: 0.8888 - val_acc: 0.7670\n",
      "Epoch 46/3000\n",
      "702/702 [==============================] - 0s 548us/sample - loss: 0.7107 - acc: 0.7436 - val_loss: 0.8471 - val_acc: 0.7727\n",
      "Epoch 47/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 0.7089 - acc: 0.7379 - val_loss: 0.8252 - val_acc: 0.7330\n",
      "Epoch 48/3000\n",
      "702/702 [==============================] - 0s 543us/sample - loss: 0.6752 - acc: 0.7621 - val_loss: 0.8464 - val_acc: 0.7330\n",
      "Epoch 49/3000\n",
      "702/702 [==============================] - 0s 545us/sample - loss: 0.6666 - acc: 0.7521 - val_loss: 0.8490 - val_acc: 0.7955\n",
      "Epoch 50/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 0.6635 - acc: 0.7407 - val_loss: 0.8256 - val_acc: 0.7557\n",
      "Epoch 51/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.6267 - acc: 0.7664 - val_loss: 0.8159 - val_acc: 0.7727\n",
      "Epoch 52/3000\n",
      "702/702 [==============================] - 0s 616us/sample - loss: 0.6523 - acc: 0.7664 - val_loss: 0.8322 - val_acc: 0.7443\n",
      "Epoch 53/3000\n",
      "702/702 [==============================] - 0s 544us/sample - loss: 0.6291 - acc: 0.7692 - val_loss: 0.7954 - val_acc: 0.7955\n",
      "Epoch 54/3000\n",
      "702/702 [==============================] - 0s 543us/sample - loss: 0.6387 - acc: 0.7593 - val_loss: 0.8214 - val_acc: 0.7670\n",
      "Epoch 55/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.5773 - acc: 0.7934 - val_loss: 0.7974 - val_acc: 0.7670\n",
      "Epoch 56/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.6027 - acc: 0.7835 - val_loss: 0.8104 - val_acc: 0.7670\n",
      "Epoch 57/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.5838 - acc: 0.7835 - val_loss: 0.7933 - val_acc: 0.7784\n",
      "Epoch 58/3000\n",
      "702/702 [==============================] - 0s 570us/sample - loss: 0.5945 - acc: 0.7920 - val_loss: 0.7572 - val_acc: 0.7955\n",
      "Epoch 59/3000\n",
      "702/702 [==============================] - 0s 560us/sample - loss: 0.5739 - acc: 0.7877 - val_loss: 0.7741 - val_acc: 0.7841\n",
      "Epoch 60/3000\n",
      "702/702 [==============================] - 0s 570us/sample - loss: 0.5603 - acc: 0.8063 - val_loss: 0.7668 - val_acc: 0.7898\n",
      "Epoch 61/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.5846 - acc: 0.7835 - val_loss: 0.7844 - val_acc: 0.7784\n",
      "Epoch 62/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.5784 - acc: 0.7749 - val_loss: 0.7796 - val_acc: 0.7841\n",
      "Epoch 63/3000\n",
      "702/702 [==============================] - 0s 559us/sample - loss: 0.5963 - acc: 0.7877 - val_loss: 0.7802 - val_acc: 0.7841\n",
      "Epoch 64/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.5830 - acc: 0.7920 - val_loss: 0.7636 - val_acc: 0.7784\n",
      "Epoch 65/3000\n",
      "702/702 [==============================] - 0s 550us/sample - loss: 0.5819 - acc: 0.7721 - val_loss: 0.7533 - val_acc: 0.7841\n",
      "Epoch 66/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.5534 - acc: 0.7949 - val_loss: 0.7421 - val_acc: 0.7841\n",
      "Epoch 67/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.5721 - acc: 0.7849 - val_loss: 0.7527 - val_acc: 0.7955\n",
      "Epoch 68/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.5474 - acc: 0.7920 - val_loss: 0.7670 - val_acc: 0.7841\n",
      "Epoch 69/3000\n",
      "702/702 [==============================] - 0s 556us/sample - loss: 0.5525 - acc: 0.7963 - val_loss: 0.7405 - val_acc: 0.8068\n",
      "Epoch 70/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 0.5418 - acc: 0.7835 - val_loss: 0.7237 - val_acc: 0.7898\n",
      "Epoch 71/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.5709 - acc: 0.8020 - val_loss: 0.7407 - val_acc: 0.7955\n",
      "Epoch 72/3000\n",
      "702/702 [==============================] - 0s 587us/sample - loss: 0.5184 - acc: 0.8134 - val_loss: 0.7231 - val_acc: 0.7841\n",
      "Epoch 73/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.4806 - acc: 0.8191 - val_loss: 0.7061 - val_acc: 0.8125\n",
      "Epoch 74/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.4770 - acc: 0.8262 - val_loss: 0.7190 - val_acc: 0.7898\n",
      "Epoch 75/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.4982 - acc: 0.8134 - val_loss: 0.7117 - val_acc: 0.8125\n",
      "Epoch 76/3000\n",
      "702/702 [==============================] - 0s 549us/sample - loss: 0.5045 - acc: 0.7963 - val_loss: 0.7186 - val_acc: 0.7898\n",
      "Epoch 77/3000\n",
      "702/702 [==============================] - 0s 535us/sample - loss: 0.5082 - acc: 0.8134 - val_loss: 0.6976 - val_acc: 0.8011\n",
      "Epoch 78/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.5120 - acc: 0.8162 - val_loss: 0.7469 - val_acc: 0.7443\n",
      "Epoch 79/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.5057 - acc: 0.8006 - val_loss: 0.7597 - val_acc: 0.6761\n",
      "Epoch 80/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.5417 - acc: 0.7863 - val_loss: 0.7225 - val_acc: 0.7898\n",
      "Epoch 81/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.5351 - acc: 0.8091 - val_loss: 0.7945 - val_acc: 0.7784\n",
      "Epoch 82/3000\n",
      "702/702 [==============================] - 0s 584us/sample - loss: 0.5401 - acc: 0.7821 - val_loss: 0.6884 - val_acc: 0.8011\n",
      "Epoch 83/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.5171 - acc: 0.8148 - val_loss: 0.7342 - val_acc: 0.8068\n",
      "Epoch 84/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.5810 - acc: 0.7821 - val_loss: 0.7930 - val_acc: 0.6193\n",
      "Epoch 85/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.5386 - acc: 0.8048 - val_loss: 0.7355 - val_acc: 0.7898\n",
      "Epoch 86/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.5141 - acc: 0.8162 - val_loss: 0.7205 - val_acc: 0.8068\n",
      "Epoch 87/3000\n",
      "702/702 [==============================] - 0s 549us/sample - loss: 0.4725 - acc: 0.8262 - val_loss: 0.6845 - val_acc: 0.8068\n",
      "Epoch 88/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.4690 - acc: 0.8234 - val_loss: 0.6989 - val_acc: 0.8068\n",
      "Epoch 89/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.4721 - acc: 0.8219 - val_loss: 0.6857 - val_acc: 0.8068\n",
      "Epoch 90/3000\n",
      "702/702 [==============================] - 0s 538us/sample - loss: 0.4836 - acc: 0.8162 - val_loss: 0.7085 - val_acc: 0.8182\n",
      "Epoch 91/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.4545 - acc: 0.8234 - val_loss: 0.6559 - val_acc: 0.8352\n",
      "Epoch 92/3000\n",
      "702/702 [==============================] - 0s 606us/sample - loss: 0.4699 - acc: 0.8476 - val_loss: 0.6687 - val_acc: 0.8182\n",
      "Epoch 93/3000\n",
      "702/702 [==============================] - 0s 539us/sample - loss: 0.4628 - acc: 0.8333 - val_loss: 0.6730 - val_acc: 0.8125\n",
      "Epoch 94/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.4284 - acc: 0.8547 - val_loss: 0.6402 - val_acc: 0.8409\n",
      "Epoch 95/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.4782 - acc: 0.8234 - val_loss: 0.6348 - val_acc: 0.8352\n",
      "Epoch 96/3000\n",
      "702/702 [==============================] - 0s 542us/sample - loss: 0.4283 - acc: 0.8447 - val_loss: 0.6326 - val_acc: 0.8409\n",
      "Epoch 97/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.4010 - acc: 0.8490 - val_loss: 0.6269 - val_acc: 0.8239\n",
      "Epoch 98/3000\n",
      "702/702 [==============================] - 0s 538us/sample - loss: 0.4198 - acc: 0.8433 - val_loss: 0.6283 - val_acc: 0.8295\n",
      "Epoch 99/3000\n",
      "702/702 [==============================] - 0s 539us/sample - loss: 0.3956 - acc: 0.8647 - val_loss: 0.6148 - val_acc: 0.8466\n",
      "Epoch 100/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.4233 - acc: 0.8291 - val_loss: 0.6332 - val_acc: 0.8409\n",
      "Epoch 101/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.3964 - acc: 0.8632 - val_loss: 0.5980 - val_acc: 0.8182\n",
      "Epoch 102/3000\n",
      "702/702 [==============================] - 0s 611us/sample - loss: 0.4252 - acc: 0.8390 - val_loss: 0.6049 - val_acc: 0.8409\n",
      "Epoch 103/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.4104 - acc: 0.8533 - val_loss: 0.6063 - val_acc: 0.8466\n",
      "Epoch 104/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.4169 - acc: 0.8390 - val_loss: 0.6380 - val_acc: 0.8182\n",
      "Epoch 105/3000\n",
      "702/702 [==============================] - 0s 534us/sample - loss: 0.4255 - acc: 0.8490 - val_loss: 0.6228 - val_acc: 0.8352\n",
      "Epoch 106/3000\n",
      "702/702 [==============================] - 0s 546us/sample - loss: 0.4159 - acc: 0.8490 - val_loss: 0.6162 - val_acc: 0.8239\n",
      "Epoch 107/3000\n",
      "702/702 [==============================] - 0s 542us/sample - loss: 0.3906 - acc: 0.8590 - val_loss: 0.5990 - val_acc: 0.8466\n",
      "Epoch 108/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.3920 - acc: 0.8504 - val_loss: 0.5863 - val_acc: 0.8295\n",
      "Epoch 109/3000\n",
      "702/702 [==============================] - 0s 537us/sample - loss: 0.4294 - acc: 0.8462 - val_loss: 0.6117 - val_acc: 0.8352\n",
      "Epoch 110/3000\n",
      "702/702 [==============================] - 0s 544us/sample - loss: 0.4046 - acc: 0.8533 - val_loss: 0.5983 - val_acc: 0.8523\n",
      "Epoch 111/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.3641 - acc: 0.8789 - val_loss: 0.5937 - val_acc: 0.8523\n",
      "Epoch 112/3000\n",
      "702/702 [==============================] - 0s 606us/sample - loss: 0.4061 - acc: 0.8590 - val_loss: 0.6320 - val_acc: 0.8239\n",
      "Epoch 113/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.3978 - acc: 0.8519 - val_loss: 0.6174 - val_acc: 0.8523\n",
      "Epoch 114/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.4085 - acc: 0.8462 - val_loss: 0.6261 - val_acc: 0.8068\n",
      "Epoch 115/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 0.4565 - acc: 0.8462 - val_loss: 0.5869 - val_acc: 0.8466\n",
      "Epoch 116/3000\n",
      "702/702 [==============================] - 0s 537us/sample - loss: 0.4051 - acc: 0.8533 - val_loss: 0.6017 - val_acc: 0.8295\n",
      "Epoch 117/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.4136 - acc: 0.8433 - val_loss: 0.5811 - val_acc: 0.8352\n",
      "Epoch 118/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.4073 - acc: 0.8447 - val_loss: 0.5862 - val_acc: 0.8409\n",
      "Epoch 119/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.3729 - acc: 0.8732 - val_loss: 0.5365 - val_acc: 0.8580\n",
      "Epoch 120/3000\n",
      "702/702 [==============================] - 0s 538us/sample - loss: 0.4024 - acc: 0.8604 - val_loss: 0.6123 - val_acc: 0.8466\n",
      "Epoch 121/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.3889 - acc: 0.8533 - val_loss: 0.5909 - val_acc: 0.8295\n",
      "Epoch 122/3000\n",
      "702/702 [==============================] - 0s 598us/sample - loss: 0.3786 - acc: 0.8575 - val_loss: 0.5358 - val_acc: 0.8523\n",
      "Epoch 123/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.3800 - acc: 0.8504 - val_loss: 0.5467 - val_acc: 0.8466\n",
      "Epoch 124/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.3800 - acc: 0.8732 - val_loss: 0.5468 - val_acc: 0.8523\n",
      "Epoch 125/3000\n",
      "702/702 [==============================] - 0s 537us/sample - loss: 0.4113 - acc: 0.8405 - val_loss: 0.5656 - val_acc: 0.8693\n",
      "Epoch 126/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.3382 - acc: 0.8803 - val_loss: 0.5231 - val_acc: 0.8636\n",
      "Epoch 127/3000\n",
      "702/702 [==============================] - 0s 538us/sample - loss: 0.3879 - acc: 0.8647 - val_loss: 0.5279 - val_acc: 0.8409\n",
      "Epoch 128/3000\n",
      "702/702 [==============================] - 0s 551us/sample - loss: 0.3473 - acc: 0.8746 - val_loss: 0.5292 - val_acc: 0.8636\n",
      "Epoch 129/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.3529 - acc: 0.8561 - val_loss: 0.5203 - val_acc: 0.8523\n",
      "Epoch 130/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.3445 - acc: 0.8746 - val_loss: 0.5333 - val_acc: 0.8693\n",
      "Epoch 131/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.3384 - acc: 0.8689 - val_loss: 0.4903 - val_acc: 0.8523\n",
      "Epoch 132/3000\n",
      "702/702 [==============================] - 0s 594us/sample - loss: 0.3405 - acc: 0.8718 - val_loss: 0.5133 - val_acc: 0.8523\n",
      "Epoch 133/3000\n",
      "702/702 [==============================] - 0s 535us/sample - loss: 0.3280 - acc: 0.8704 - val_loss: 0.5085 - val_acc: 0.8295\n",
      "Epoch 134/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.3526 - acc: 0.8661 - val_loss: 0.5357 - val_acc: 0.8295\n",
      "Epoch 135/3000\n",
      "702/702 [==============================] - 0s 542us/sample - loss: 0.3742 - acc: 0.8675 - val_loss: 0.5402 - val_acc: 0.8295\n",
      "Epoch 136/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.3765 - acc: 0.8604 - val_loss: 0.5537 - val_acc: 0.8409\n",
      "Epoch 137/3000\n",
      "702/702 [==============================] - 0s 543us/sample - loss: 0.3353 - acc: 0.8917 - val_loss: 0.5291 - val_acc: 0.8466\n",
      "Epoch 138/3000\n",
      "702/702 [==============================] - 0s 557us/sample - loss: 0.3487 - acc: 0.8689 - val_loss: 0.5309 - val_acc: 0.8409\n",
      "Epoch 139/3000\n",
      "702/702 [==============================] - 0s 537us/sample - loss: 0.3818 - acc: 0.8476 - val_loss: 0.5021 - val_acc: 0.8523\n",
      "Epoch 140/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 0.3252 - acc: 0.8746 - val_loss: 0.5593 - val_acc: 0.8466\n",
      "Epoch 141/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.3327 - acc: 0.8832 - val_loss: 0.4885 - val_acc: 0.8636\n",
      "Epoch 142/3000\n",
      "702/702 [==============================] - 0s 599us/sample - loss: 0.3533 - acc: 0.8732 - val_loss: 0.5048 - val_acc: 0.8636\n",
      "Epoch 143/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 0.3230 - acc: 0.8746 - val_loss: 0.4892 - val_acc: 0.8693\n",
      "Epoch 144/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.3094 - acc: 0.8875 - val_loss: 0.4778 - val_acc: 0.8750\n",
      "Epoch 145/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.3240 - acc: 0.8761 - val_loss: 0.5169 - val_acc: 0.8580\n",
      "Epoch 146/3000\n",
      "702/702 [==============================] - 0s 538us/sample - loss: 0.3118 - acc: 0.8846 - val_loss: 0.4847 - val_acc: 0.8750\n",
      "Epoch 147/3000\n",
      "702/702 [==============================] - 0s 547us/sample - loss: 0.2984 - acc: 0.8974 - val_loss: 0.4906 - val_acc: 0.8523\n",
      "Epoch 148/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.2947 - acc: 0.8932 - val_loss: 0.5065 - val_acc: 0.8580\n",
      "Epoch 149/3000\n",
      "702/702 [==============================] - 0s 537us/sample - loss: 0.2996 - acc: 0.8889 - val_loss: 0.4693 - val_acc: 0.8750\n",
      "Epoch 150/3000\n",
      "702/702 [==============================] - 0s 543us/sample - loss: 0.3325 - acc: 0.8704 - val_loss: 0.5138 - val_acc: 0.8523\n",
      "Epoch 151/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.3207 - acc: 0.8860 - val_loss: 0.4651 - val_acc: 0.8693\n",
      "Epoch 152/3000\n",
      "702/702 [==============================] - 0s 604us/sample - loss: 0.3222 - acc: 0.8832 - val_loss: 0.5132 - val_acc: 0.8466\n",
      "Epoch 153/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.3214 - acc: 0.8875 - val_loss: 0.5026 - val_acc: 0.8409\n",
      "Epoch 154/3000\n",
      "702/702 [==============================] - 0s 534us/sample - loss: 0.3104 - acc: 0.8974 - val_loss: 0.4435 - val_acc: 0.8636\n",
      "Epoch 155/3000\n",
      "702/702 [==============================] - 0s 542us/sample - loss: 0.3301 - acc: 0.8675 - val_loss: 0.5152 - val_acc: 0.8523\n",
      "Epoch 156/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.3075 - acc: 0.8803 - val_loss: 0.4760 - val_acc: 0.8807\n",
      "Epoch 157/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.2985 - acc: 0.8860 - val_loss: 0.4748 - val_acc: 0.8636\n",
      "Epoch 158/3000\n",
      "702/702 [==============================] - 0s 542us/sample - loss: 0.3156 - acc: 0.8818 - val_loss: 0.4710 - val_acc: 0.8636\n",
      "Epoch 159/3000\n",
      "702/702 [==============================] - 0s 544us/sample - loss: 0.3159 - acc: 0.8846 - val_loss: 0.4741 - val_acc: 0.8636\n",
      "Epoch 160/3000\n",
      "702/702 [==============================] - 0s 535us/sample - loss: 0.3022 - acc: 0.8960 - val_loss: 0.4928 - val_acc: 0.8466\n",
      "Epoch 161/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.3129 - acc: 0.8746 - val_loss: 0.4872 - val_acc: 0.8523\n",
      "Epoch 162/3000\n",
      "702/702 [==============================] - 0s 607us/sample - loss: 0.3257 - acc: 0.8789 - val_loss: 0.4767 - val_acc: 0.8693\n",
      "Epoch 163/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.2914 - acc: 0.8989 - val_loss: 0.4851 - val_acc: 0.8636\n",
      "Epoch 164/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.3088 - acc: 0.8903 - val_loss: 0.5131 - val_acc: 0.8636\n",
      "Epoch 165/3000\n",
      "702/702 [==============================] - 0s 534us/sample - loss: 0.2797 - acc: 0.9003 - val_loss: 0.4762 - val_acc: 0.8693\n",
      "Epoch 166/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.2876 - acc: 0.8889 - val_loss: 0.4721 - val_acc: 0.8580\n",
      "Epoch 167/3000\n",
      "702/702 [==============================] - 0s 544us/sample - loss: 0.2981 - acc: 0.9046 - val_loss: 0.4917 - val_acc: 0.8523\n",
      "Epoch 168/3000\n",
      "702/702 [==============================] - 0s 542us/sample - loss: 0.3003 - acc: 0.9003 - val_loss: 0.4820 - val_acc: 0.8580\n",
      "Epoch 169/3000\n",
      "702/702 [==============================] - 0s 549us/sample - loss: 0.2686 - acc: 0.9074 - val_loss: 0.4940 - val_acc: 0.8523\n",
      "Epoch 170/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.2866 - acc: 0.8889 - val_loss: 0.5230 - val_acc: 0.8523\n",
      "Epoch 171/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.2682 - acc: 0.9046 - val_loss: 0.4664 - val_acc: 0.8523\n",
      "Epoch 172/3000\n",
      "702/702 [==============================] - 0s 606us/sample - loss: 0.2780 - acc: 0.8917 - val_loss: 0.4916 - val_acc: 0.8523\n",
      "Epoch 173/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.2816 - acc: 0.9003 - val_loss: 0.4825 - val_acc: 0.8466\n",
      "Epoch 174/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.2812 - acc: 0.8917 - val_loss: 0.5039 - val_acc: 0.8750\n",
      "Epoch 175/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.2949 - acc: 0.8875 - val_loss: 0.4989 - val_acc: 0.8750\n",
      "Epoch 176/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.2506 - acc: 0.9160 - val_loss: 0.4484 - val_acc: 0.8636\n",
      "Epoch 177/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.2932 - acc: 0.8846 - val_loss: 0.4803 - val_acc: 0.8636\n",
      "Epoch 178/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.3367 - acc: 0.8846 - val_loss: 0.4665 - val_acc: 0.8580\n",
      "Epoch 179/3000\n",
      "702/702 [==============================] - 0s 539us/sample - loss: 0.2949 - acc: 0.8989 - val_loss: 0.4370 - val_acc: 0.8636\n",
      "Epoch 180/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.2757 - acc: 0.8946 - val_loss: 0.4662 - val_acc: 0.8636\n",
      "Epoch 181/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.2714 - acc: 0.8889 - val_loss: 0.4518 - val_acc: 0.8750\n",
      "Epoch 182/3000\n",
      "702/702 [==============================] - 0s 599us/sample - loss: 0.2414 - acc: 0.9145 - val_loss: 0.4597 - val_acc: 0.8580\n",
      "Epoch 183/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.2358 - acc: 0.9117 - val_loss: 0.5157 - val_acc: 0.8636\n",
      "Epoch 184/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.2638 - acc: 0.9060 - val_loss: 0.4275 - val_acc: 0.8807\n",
      "Epoch 185/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.2620 - acc: 0.8989 - val_loss: 0.4936 - val_acc: 0.8466\n",
      "Epoch 186/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.2502 - acc: 0.9160 - val_loss: 0.4107 - val_acc: 0.8807\n",
      "Epoch 187/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.2432 - acc: 0.9231 - val_loss: 0.4707 - val_acc: 0.8693\n",
      "Epoch 188/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.2595 - acc: 0.9017 - val_loss: 0.4237 - val_acc: 0.8693\n",
      "Epoch 189/3000\n",
      "702/702 [==============================] - 0s 538us/sample - loss: 0.2817 - acc: 0.9046 - val_loss: 0.4890 - val_acc: 0.8580\n",
      "Epoch 190/3000\n",
      "702/702 [==============================] - 0s 534us/sample - loss: 0.3015 - acc: 0.8946 - val_loss: 0.4590 - val_acc: 0.8636\n",
      "Epoch 191/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2581 - acc: 0.9103 - val_loss: 0.4440 - val_acc: 0.8750\n",
      "Epoch 192/3000\n",
      "702/702 [==============================] - 0s 599us/sample - loss: 0.2599 - acc: 0.9074 - val_loss: 0.4720 - val_acc: 0.8580\n",
      "Epoch 193/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.2374 - acc: 0.9088 - val_loss: 0.4505 - val_acc: 0.8693\n",
      "Epoch 194/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.2228 - acc: 0.9174 - val_loss: 0.4527 - val_acc: 0.8750\n",
      "Epoch 195/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.2489 - acc: 0.9060 - val_loss: 0.4678 - val_acc: 0.8750\n",
      "Epoch 196/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.2346 - acc: 0.9145 - val_loss: 0.4079 - val_acc: 0.8807\n",
      "Epoch 197/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.2856 - acc: 0.8875 - val_loss: 0.4939 - val_acc: 0.8352\n",
      "Epoch 198/3000\n",
      "702/702 [==============================] - 0s 549us/sample - loss: 0.2668 - acc: 0.9060 - val_loss: 0.4322 - val_acc: 0.8750\n",
      "Epoch 199/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.2395 - acc: 0.9074 - val_loss: 0.4405 - val_acc: 0.8807\n",
      "Epoch 200/3000\n",
      "702/702 [==============================] - 0s 546us/sample - loss: 0.2302 - acc: 0.9174 - val_loss: 0.4329 - val_acc: 0.8750\n",
      "Epoch 201/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.2350 - acc: 0.9160 - val_loss: 0.4126 - val_acc: 0.8807\n",
      "Epoch 202/3000\n",
      "702/702 [==============================] - 0s 598us/sample - loss: 0.2122 - acc: 0.9217 - val_loss: 0.4448 - val_acc: 0.8693\n",
      "Epoch 203/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.2320 - acc: 0.9174 - val_loss: 0.4311 - val_acc: 0.8807\n",
      "Epoch 204/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.2354 - acc: 0.9117 - val_loss: 0.4414 - val_acc: 0.8864\n",
      "Epoch 205/3000\n",
      "702/702 [==============================] - 0s 535us/sample - loss: 0.2233 - acc: 0.9231 - val_loss: 0.4644 - val_acc: 0.8864\n",
      "Epoch 206/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 0.2383 - acc: 0.9060 - val_loss: 0.4528 - val_acc: 0.8693\n",
      "Epoch 207/3000\n",
      "702/702 [==============================] - 0s 537us/sample - loss: 0.2444 - acc: 0.9117 - val_loss: 0.4542 - val_acc: 0.8807\n",
      "Epoch 208/3000\n",
      "702/702 [==============================] - 0s 535us/sample - loss: 0.2529 - acc: 0.9259 - val_loss: 0.4461 - val_acc: 0.8466\n",
      "Epoch 209/3000\n",
      "702/702 [==============================] - 0s 557us/sample - loss: 0.2236 - acc: 0.9202 - val_loss: 0.4621 - val_acc: 0.8807\n",
      "Epoch 210/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.2413 - acc: 0.9003 - val_loss: 0.4640 - val_acc: 0.8523\n",
      "Epoch 211/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2136 - acc: 0.9145 - val_loss: 0.4700 - val_acc: 0.8750\n",
      "Epoch 212/3000\n",
      "702/702 [==============================] - 0s 598us/sample - loss: 0.2408 - acc: 0.9117 - val_loss: 0.4915 - val_acc: 0.8352\n",
      "Epoch 213/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.2231 - acc: 0.9245 - val_loss: 0.4315 - val_acc: 0.8636\n",
      "Epoch 214/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.2300 - acc: 0.9245 - val_loss: 0.4296 - val_acc: 0.8693\n",
      "Epoch 215/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.2187 - acc: 0.9231 - val_loss: 0.4218 - val_acc: 0.8750\n",
      "Epoch 216/3000\n",
      "702/702 [==============================] - 0s 534us/sample - loss: 0.2159 - acc: 0.9231 - val_loss: 0.4475 - val_acc: 0.8693\n",
      "Epoch 217/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.2059 - acc: 0.9259 - val_loss: 0.4088 - val_acc: 0.8807\n",
      "Epoch 218/3000\n",
      "702/702 [==============================] - 0s 535us/sample - loss: 0.2632 - acc: 0.9017 - val_loss: 0.4459 - val_acc: 0.8807\n",
      "Epoch 219/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.2078 - acc: 0.9202 - val_loss: 0.4172 - val_acc: 0.8807\n",
      "Epoch 220/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.2096 - acc: 0.9145 - val_loss: 0.4093 - val_acc: 0.8864\n",
      "Epoch 221/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.2106 - acc: 0.9274 - val_loss: 0.4841 - val_acc: 0.8750\n",
      "Epoch 222/3000\n",
      "702/702 [==============================] - 0s 599us/sample - loss: 0.2116 - acc: 0.9274 - val_loss: 0.4118 - val_acc: 0.8977\n",
      "Epoch 223/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.2363 - acc: 0.9174 - val_loss: 0.4341 - val_acc: 0.8750\n",
      "Epoch 224/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.2274 - acc: 0.9217 - val_loss: 0.4704 - val_acc: 0.8750\n",
      "Epoch 225/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.2130 - acc: 0.9202 - val_loss: 0.3649 - val_acc: 0.8920\n",
      "Epoch 226/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 0.2002 - acc: 0.9330 - val_loss: 0.4044 - val_acc: 0.8864\n",
      "Epoch 227/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.2000 - acc: 0.9245 - val_loss: 0.4134 - val_acc: 0.8750\n",
      "Epoch 228/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.1854 - acc: 0.9302 - val_loss: 0.4253 - val_acc: 0.8750\n",
      "Epoch 229/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.1788 - acc: 0.9302 - val_loss: 0.4083 - val_acc: 0.8750\n",
      "Epoch 230/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.2030 - acc: 0.9259 - val_loss: 0.4297 - val_acc: 0.8920\n",
      "Epoch 231/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.2096 - acc: 0.9259 - val_loss: 0.3930 - val_acc: 0.8977\n",
      "Epoch 232/3000\n",
      "702/702 [==============================] - 0s 602us/sample - loss: 0.1951 - acc: 0.9259 - val_loss: 0.4510 - val_acc: 0.8750\n",
      "Epoch 233/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.2048 - acc: 0.9202 - val_loss: 0.3707 - val_acc: 0.8864\n",
      "Epoch 234/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.2162 - acc: 0.9231 - val_loss: 0.4370 - val_acc: 0.8693\n",
      "Epoch 235/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.1982 - acc: 0.9373 - val_loss: 0.3937 - val_acc: 0.8920\n",
      "Epoch 236/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.2064 - acc: 0.9330 - val_loss: 0.4115 - val_acc: 0.8864\n",
      "Epoch 237/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.1969 - acc: 0.9288 - val_loss: 0.4432 - val_acc: 0.8636\n",
      "Epoch 238/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.1888 - acc: 0.9288 - val_loss: 0.3971 - val_acc: 0.8864\n",
      "Epoch 239/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.1941 - acc: 0.9345 - val_loss: 0.4277 - val_acc: 0.8693\n",
      "Epoch 240/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.2123 - acc: 0.9202 - val_loss: 0.4139 - val_acc: 0.8807\n",
      "Epoch 241/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1748 - acc: 0.9231 - val_loss: 0.4181 - val_acc: 0.8750\n",
      "Epoch 242/3000\n",
      "702/702 [==============================] - 0s 595us/sample - loss: 0.1799 - acc: 0.9402 - val_loss: 0.4439 - val_acc: 0.8750\n",
      "Epoch 243/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.1887 - acc: 0.9330 - val_loss: 0.3875 - val_acc: 0.8750\n",
      "Epoch 244/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.1891 - acc: 0.9316 - val_loss: 0.4360 - val_acc: 0.8750\n",
      "Epoch 245/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.2083 - acc: 0.9302 - val_loss: 0.3786 - val_acc: 0.8750\n",
      "Epoch 246/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.1611 - acc: 0.9516 - val_loss: 0.3726 - val_acc: 0.8864\n",
      "Epoch 247/3000\n",
      "702/702 [==============================] - 0s 542us/sample - loss: 0.1886 - acc: 0.9416 - val_loss: 0.4194 - val_acc: 0.8750\n",
      "Epoch 248/3000\n",
      "702/702 [==============================] - 0s 539us/sample - loss: 0.1882 - acc: 0.9274 - val_loss: 0.3965 - val_acc: 0.8580\n",
      "Epoch 249/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.1892 - acc: 0.9345 - val_loss: 0.4311 - val_acc: 0.8807\n",
      "Epoch 250/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.1642 - acc: 0.9416 - val_loss: 0.4542 - val_acc: 0.8636\n",
      "Epoch 251/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.2002 - acc: 0.9245 - val_loss: 0.3774 - val_acc: 0.8920\n",
      "Epoch 252/3000\n",
      "702/702 [==============================] - 0s 597us/sample - loss: 0.1981 - acc: 0.9274 - val_loss: 0.4335 - val_acc: 0.8523\n",
      "Epoch 253/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.1927 - acc: 0.9202 - val_loss: 0.3706 - val_acc: 0.8977\n",
      "Epoch 254/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.1939 - acc: 0.9330 - val_loss: 0.3877 - val_acc: 0.8636\n",
      "Epoch 255/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.1890 - acc: 0.9274 - val_loss: 0.4724 - val_acc: 0.8693\n",
      "Epoch 256/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.1710 - acc: 0.9316 - val_loss: 0.3873 - val_acc: 0.8864\n",
      "Epoch 257/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.1649 - acc: 0.9430 - val_loss: 0.3947 - val_acc: 0.8807\n",
      "Epoch 258/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.2122 - acc: 0.9160 - val_loss: 0.4448 - val_acc: 0.8864\n",
      "Epoch 259/3000\n",
      "702/702 [==============================] - 0s 539us/sample - loss: 0.1675 - acc: 0.9359 - val_loss: 0.3966 - val_acc: 0.8807\n",
      "Epoch 260/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.1940 - acc: 0.9302 - val_loss: 0.3851 - val_acc: 0.8977\n",
      "Epoch 261/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1803 - acc: 0.9302 - val_loss: 0.4001 - val_acc: 0.8920\n",
      "Epoch 262/3000\n",
      "702/702 [==============================] - 0s 597us/sample - loss: 0.1807 - acc: 0.9330 - val_loss: 0.3406 - val_acc: 0.9034\n",
      "Epoch 263/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.1435 - acc: 0.9459 - val_loss: 0.4029 - val_acc: 0.8636\n",
      "Epoch 264/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.2038 - acc: 0.9274 - val_loss: 0.4014 - val_acc: 0.8580\n",
      "Epoch 265/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.1523 - acc: 0.9402 - val_loss: 0.4255 - val_acc: 0.8693\n",
      "Epoch 266/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.1681 - acc: 0.9387 - val_loss: 0.4219 - val_acc: 0.8693\n",
      "Epoch 267/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 0.1862 - acc: 0.9274 - val_loss: 0.3786 - val_acc: 0.8920\n",
      "Epoch 268/3000\n",
      "702/702 [==============================] - 0s 535us/sample - loss: 0.1818 - acc: 0.9302 - val_loss: 0.4338 - val_acc: 0.8807\n",
      "Epoch 269/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.1720 - acc: 0.9416 - val_loss: 0.3870 - val_acc: 0.8864\n",
      "Epoch 270/3000\n",
      "702/702 [==============================] - 0s 540us/sample - loss: 0.1581 - acc: 0.9430 - val_loss: 0.4274 - val_acc: 0.8693\n",
      "Epoch 271/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.1702 - acc: 0.9373 - val_loss: 0.3894 - val_acc: 0.8920\n",
      "Epoch 272/3000\n",
      "702/702 [==============================] - 0s 597us/sample - loss: 0.1821 - acc: 0.9387 - val_loss: 0.4117 - val_acc: 0.8750\n",
      "Epoch 273/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.1562 - acc: 0.9444 - val_loss: 0.3766 - val_acc: 0.8920\n",
      "Epoch 274/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.1356 - acc: 0.9544 - val_loss: 0.3846 - val_acc: 0.8864\n",
      "Epoch 275/3000\n",
      "702/702 [==============================] - 0s 553us/sample - loss: 0.1505 - acc: 0.9544 - val_loss: 0.4452 - val_acc: 0.8807\n",
      "Epoch 276/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.1632 - acc: 0.9402 - val_loss: 0.3728 - val_acc: 0.8864\n",
      "Epoch 277/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.1537 - acc: 0.9387 - val_loss: 0.3827 - val_acc: 0.8864\n",
      "Epoch 278/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.1481 - acc: 0.9288 - val_loss: 0.3624 - val_acc: 0.8864\n",
      "Epoch 279/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.1601 - acc: 0.9359 - val_loss: 0.4041 - val_acc: 0.8750\n",
      "Epoch 280/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.1577 - acc: 0.9516 - val_loss: 0.4130 - val_acc: 0.8693\n",
      "Epoch 281/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.1579 - acc: 0.9459 - val_loss: 0.4117 - val_acc: 0.8750\n",
      "Epoch 282/3000\n",
      "702/702 [==============================] - 0s 601us/sample - loss: 0.1609 - acc: 0.9501 - val_loss: 0.3870 - val_acc: 0.8864\n",
      "Epoch 283/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.1563 - acc: 0.9402 - val_loss: 0.4107 - val_acc: 0.8693\n",
      "Epoch 284/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.1669 - acc: 0.9359 - val_loss: 0.4334 - val_acc: 0.8636\n",
      "Epoch 285/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.1819 - acc: 0.9345 - val_loss: 0.3644 - val_acc: 0.8807\n",
      "Epoch 286/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.1494 - acc: 0.9501 - val_loss: 0.4028 - val_acc: 0.8750\n",
      "Epoch 287/3000\n",
      "702/702 [==============================] - 0s 538us/sample - loss: 0.1608 - acc: 0.9444 - val_loss: 0.3636 - val_acc: 0.8807\n",
      "Epoch 288/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 0.1416 - acc: 0.9459 - val_loss: 0.3761 - val_acc: 0.8977\n",
      "Epoch 289/3000\n",
      "702/702 [==============================] - 0s 546us/sample - loss: 0.1466 - acc: 0.9459 - val_loss: 0.3945 - val_acc: 0.8807\n",
      "Epoch 290/3000\n",
      "702/702 [==============================] - 0s 539us/sample - loss: 0.1456 - acc: 0.9444 - val_loss: 0.3698 - val_acc: 0.8750\n",
      "Epoch 291/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.1840 - acc: 0.9402 - val_loss: 0.3691 - val_acc: 0.8750\n",
      "Epoch 292/3000\n",
      "702/702 [==============================] - 0s 585us/sample - loss: 0.1744 - acc: 0.9459 - val_loss: 0.4010 - val_acc: 0.8693\n",
      "Epoch 293/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1438 - acc: 0.9516 - val_loss: 0.3525 - val_acc: 0.9034\n",
      "Epoch 294/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.1528 - acc: 0.9459 - val_loss: 0.4229 - val_acc: 0.8920\n",
      "Epoch 295/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1418 - acc: 0.9459 - val_loss: 0.3904 - val_acc: 0.9034\n",
      "Epoch 296/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.1404 - acc: 0.9416 - val_loss: 0.4320 - val_acc: 0.8920\n",
      "Epoch 297/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.1549 - acc: 0.9459 - val_loss: 0.4294 - val_acc: 0.8864\n",
      "Epoch 298/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.1361 - acc: 0.9444 - val_loss: 0.3617 - val_acc: 0.8977\n",
      "Epoch 299/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.1503 - acc: 0.9487 - val_loss: 0.3745 - val_acc: 0.8920\n",
      "Epoch 300/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.1591 - acc: 0.9430 - val_loss: 0.4017 - val_acc: 0.8523\n",
      "Epoch 301/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.1441 - acc: 0.9459 - val_loss: 0.3359 - val_acc: 0.8977\n",
      "Epoch 302/3000\n",
      "702/702 [==============================] - 0s 588us/sample - loss: 0.1279 - acc: 0.9573 - val_loss: 0.3971 - val_acc: 0.8750\n",
      "Epoch 303/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.1497 - acc: 0.9473 - val_loss: 0.4049 - val_acc: 0.8864\n",
      "Epoch 304/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.1360 - acc: 0.9487 - val_loss: 0.3676 - val_acc: 0.8750\n",
      "Epoch 305/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.1352 - acc: 0.9473 - val_loss: 0.3948 - val_acc: 0.8807\n",
      "Epoch 306/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.1397 - acc: 0.9430 - val_loss: 0.3645 - val_acc: 0.8807\n",
      "Epoch 307/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.1217 - acc: 0.9573 - val_loss: 0.3824 - val_acc: 0.8750\n",
      "Epoch 308/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.1723 - acc: 0.9402 - val_loss: 0.3444 - val_acc: 0.8807\n",
      "Epoch 309/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.1468 - acc: 0.9444 - val_loss: 0.3792 - val_acc: 0.8864\n",
      "Epoch 310/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.1556 - acc: 0.9430 - val_loss: 0.4508 - val_acc: 0.8636\n",
      "Epoch 311/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.1469 - acc: 0.9387 - val_loss: 0.3900 - val_acc: 0.8580\n",
      "Epoch 312/3000\n",
      "702/702 [==============================] - 0s 602us/sample - loss: 0.1482 - acc: 0.9459 - val_loss: 0.3768 - val_acc: 0.8750\n",
      "Epoch 313/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1316 - acc: 0.9544 - val_loss: 0.3659 - val_acc: 0.8807\n",
      "Epoch 314/3000\n",
      "702/702 [==============================] - 0s 548us/sample - loss: 0.1168 - acc: 0.9558 - val_loss: 0.3980 - val_acc: 0.8807\n",
      "Epoch 315/3000\n",
      "702/702 [==============================] - 0s 537us/sample - loss: 0.1378 - acc: 0.9459 - val_loss: 0.3721 - val_acc: 0.8807\n",
      "Epoch 316/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.1392 - acc: 0.9402 - val_loss: 0.4127 - val_acc: 0.8693\n",
      "Epoch 317/3000\n",
      "702/702 [==============================] - 0s 537us/sample - loss: 0.1591 - acc: 0.9487 - val_loss: 0.3944 - val_acc: 0.8807\n",
      "Epoch 318/3000\n",
      "702/702 [==============================] - 0s 537us/sample - loss: 0.1347 - acc: 0.9516 - val_loss: 0.3786 - val_acc: 0.8750\n",
      "Epoch 319/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 0.1354 - acc: 0.9444 - val_loss: 0.3779 - val_acc: 0.8864\n",
      "Epoch 320/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.1462 - acc: 0.9473 - val_loss: 0.4213 - val_acc: 0.8750\n",
      "Epoch 321/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.1513 - acc: 0.9416 - val_loss: 0.3993 - val_acc: 0.8864\n",
      "Epoch 322/3000\n",
      "702/702 [==============================] - 0s 611us/sample - loss: 0.1339 - acc: 0.9473 - val_loss: 0.3967 - val_acc: 0.8864\n",
      "Epoch 323/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.1160 - acc: 0.9544 - val_loss: 0.3821 - val_acc: 0.8920\n",
      "Epoch 324/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.1593 - acc: 0.9402 - val_loss: 0.3844 - val_acc: 0.8807\n",
      "Epoch 325/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.1272 - acc: 0.9544 - val_loss: 0.3979 - val_acc: 0.8977\n",
      "Epoch 326/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.1135 - acc: 0.9558 - val_loss: 0.4070 - val_acc: 0.8636\n",
      "Epoch 327/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 0.1390 - acc: 0.9544 - val_loss: 0.3595 - val_acc: 0.8920\n",
      "Epoch 328/3000\n",
      "702/702 [==============================] - 0s 554us/sample - loss: 0.1333 - acc: 0.9530 - val_loss: 0.3316 - val_acc: 0.9034\n",
      "Epoch 329/3000\n",
      "702/702 [==============================] - 0s 553us/sample - loss: 0.1429 - acc: 0.9430 - val_loss: 0.3566 - val_acc: 0.8920\n",
      "Epoch 330/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.1103 - acc: 0.9615 - val_loss: 0.3491 - val_acc: 0.8693\n",
      "Epoch 331/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.1270 - acc: 0.9530 - val_loss: 0.3686 - val_acc: 0.8636\n",
      "Epoch 332/3000\n",
      "702/702 [==============================] - 0s 596us/sample - loss: 0.1420 - acc: 0.9501 - val_loss: 0.3219 - val_acc: 0.8750\n",
      "Epoch 333/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.1144 - acc: 0.9615 - val_loss: 0.3534 - val_acc: 0.8693\n",
      "Epoch 334/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.1144 - acc: 0.9687 - val_loss: 0.3146 - val_acc: 0.8864\n",
      "Epoch 335/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.1229 - acc: 0.9487 - val_loss: 0.3644 - val_acc: 0.8807\n",
      "Epoch 336/3000\n",
      "702/702 [==============================] - 0s 548us/sample - loss: 0.1141 - acc: 0.9630 - val_loss: 0.3415 - val_acc: 0.8920\n",
      "Epoch 337/3000\n",
      "702/702 [==============================] - 0s 542us/sample - loss: 0.1239 - acc: 0.9544 - val_loss: 0.3507 - val_acc: 0.8807\n",
      "Epoch 338/3000\n",
      "702/702 [==============================] - 0s 538us/sample - loss: 0.1188 - acc: 0.9530 - val_loss: 0.3930 - val_acc: 0.8864\n",
      "Epoch 339/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.1119 - acc: 0.9601 - val_loss: 0.3652 - val_acc: 0.8920\n",
      "Epoch 340/3000\n",
      "702/702 [==============================] - 0s 542us/sample - loss: 0.1321 - acc: 0.9473 - val_loss: 0.3544 - val_acc: 0.8920\n",
      "Epoch 341/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.1148 - acc: 0.9558 - val_loss: 0.3658 - val_acc: 0.8920\n",
      "Epoch 342/3000\n",
      "702/702 [==============================] - 0s 615us/sample - loss: 0.1053 - acc: 0.9630 - val_loss: 0.3983 - val_acc: 0.8807\n",
      "Epoch 343/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.1290 - acc: 0.9530 - val_loss: 0.3805 - val_acc: 0.8977\n",
      "Epoch 344/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.1313 - acc: 0.9416 - val_loss: 0.3436 - val_acc: 0.9091\n",
      "Epoch 345/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.1298 - acc: 0.9459 - val_loss: 0.4391 - val_acc: 0.8750\n",
      "Epoch 346/3000\n",
      "702/702 [==============================] - 0s 544us/sample - loss: 0.1456 - acc: 0.9459 - val_loss: 0.3940 - val_acc: 0.8750\n",
      "Epoch 347/3000\n",
      "702/702 [==============================] - 0s 535us/sample - loss: 0.1245 - acc: 0.9601 - val_loss: 0.3326 - val_acc: 0.9034\n",
      "Epoch 348/3000\n",
      "702/702 [==============================] - 0s 534us/sample - loss: 0.1168 - acc: 0.9587 - val_loss: 0.4253 - val_acc: 0.8750\n",
      "Epoch 349/3000\n",
      "702/702 [==============================] - 0s 543us/sample - loss: 0.1131 - acc: 0.9601 - val_loss: 0.4050 - val_acc: 0.8750\n",
      "Epoch 350/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.1426 - acc: 0.9516 - val_loss: 0.3593 - val_acc: 0.8977\n",
      "Epoch 351/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.1014 - acc: 0.9644 - val_loss: 0.3429 - val_acc: 0.8977\n",
      "Epoch 352/3000\n",
      "702/702 [==============================] - 0s 588us/sample - loss: 0.1156 - acc: 0.9644 - val_loss: 0.3273 - val_acc: 0.8977\n",
      "Epoch 353/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.1190 - acc: 0.9601 - val_loss: 0.3627 - val_acc: 0.8807\n",
      "Epoch 354/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.1165 - acc: 0.9544 - val_loss: 0.4131 - val_acc: 0.8864\n",
      "Epoch 355/3000\n",
      "702/702 [==============================] - 0s 545us/sample - loss: 0.1094 - acc: 0.9601 - val_loss: 0.3581 - val_acc: 0.8977\n",
      "Epoch 356/3000\n",
      "702/702 [==============================] - 0s 551us/sample - loss: 0.1377 - acc: 0.9459 - val_loss: 0.3996 - val_acc: 0.9034\n",
      "Epoch 357/3000\n",
      "702/702 [==============================] - 0s 556us/sample - loss: 0.1512 - acc: 0.9459 - val_loss: 0.3917 - val_acc: 0.8750\n",
      "Epoch 358/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.1222 - acc: 0.9530 - val_loss: 0.3349 - val_acc: 0.8920\n",
      "Epoch 359/3000\n",
      "702/702 [==============================] - 0s 543us/sample - loss: 0.0954 - acc: 0.9672 - val_loss: 0.3468 - val_acc: 0.8920\n",
      "Epoch 360/3000\n",
      "702/702 [==============================] - 0s 543us/sample - loss: 0.1319 - acc: 0.9530 - val_loss: 0.3615 - val_acc: 0.8977\n",
      "Epoch 361/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.1228 - acc: 0.9615 - val_loss: 0.3519 - val_acc: 0.8864\n",
      "Epoch 362/3000\n",
      "702/702 [==============================] - 0s 603us/sample - loss: 0.1177 - acc: 0.9615 - val_loss: 0.3939 - val_acc: 0.8807\n",
      "Epoch 363/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 0.1444 - acc: 0.9402 - val_loss: 0.3365 - val_acc: 0.8977\n",
      "Epoch 364/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1254 - acc: 0.9544 - val_loss: 0.3234 - val_acc: 0.8750\n",
      "Epoch 365/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1575 - acc: 0.9487 - val_loss: 0.4127 - val_acc: 0.8636\n",
      "Epoch 366/3000\n",
      "702/702 [==============================] - 0s 561us/sample - loss: 0.1373 - acc: 0.9601 - val_loss: 0.3089 - val_acc: 0.9091\n",
      "Epoch 367/3000\n",
      "702/702 [==============================] - 0s 552us/sample - loss: 0.1279 - acc: 0.9615 - val_loss: 0.4459 - val_acc: 0.8636\n",
      "Epoch 368/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.1347 - acc: 0.9530 - val_loss: 0.3899 - val_acc: 0.8636\n",
      "Epoch 369/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.1579 - acc: 0.9373 - val_loss: 0.3343 - val_acc: 0.9034\n",
      "Epoch 370/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.1381 - acc: 0.9444 - val_loss: 0.4130 - val_acc: 0.8807\n",
      "Epoch 371/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1667 - acc: 0.9544 - val_loss: 0.3742 - val_acc: 0.8693\n",
      "Epoch 372/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.1236 - acc: 0.9558 - val_loss: 0.3543 - val_acc: 0.8920\n",
      "Epoch 373/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.1065 - acc: 0.9587 - val_loss: 0.4874 - val_acc: 0.8636\n",
      "Epoch 374/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.1198 - acc: 0.9544 - val_loss: 0.3562 - val_acc: 0.8807\n",
      "Epoch 375/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.1103 - acc: 0.9587 - val_loss: 0.3353 - val_acc: 0.8920\n",
      "Epoch 376/3000\n",
      "702/702 [==============================] - 0s 535us/sample - loss: 0.1336 - acc: 0.9430 - val_loss: 0.3709 - val_acc: 0.8750\n",
      "Epoch 377/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.1085 - acc: 0.9601 - val_loss: 0.3225 - val_acc: 0.8807\n",
      "Epoch 378/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.1064 - acc: 0.9729 - val_loss: 0.3377 - val_acc: 0.8750\n",
      "Epoch 379/3000\n",
      "702/702 [==============================] - 0s 534us/sample - loss: 0.0943 - acc: 0.9701 - val_loss: 0.3861 - val_acc: 0.8864\n",
      "Epoch 380/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.1053 - acc: 0.9615 - val_loss: 0.3732 - val_acc: 0.8864\n",
      "Epoch 381/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.1144 - acc: 0.9630 - val_loss: 0.3445 - val_acc: 0.8864\n",
      "Epoch 382/3000\n",
      "702/702 [==============================] - 0s 561us/sample - loss: 0.1250 - acc: 0.9573 - val_loss: 0.3723 - val_acc: 0.8693\n",
      "Epoch 383/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.1039 - acc: 0.9630 - val_loss: 0.3944 - val_acc: 0.8750\n",
      "Epoch 384/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.1029 - acc: 0.9744 - val_loss: 0.3025 - val_acc: 0.8750\n",
      "Epoch 385/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1125 - acc: 0.9601 - val_loss: 0.3475 - val_acc: 0.8807\n",
      "Epoch 386/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0960 - acc: 0.9644 - val_loss: 0.3637 - val_acc: 0.8693\n",
      "Epoch 387/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.1373 - acc: 0.9430 - val_loss: 0.4312 - val_acc: 0.8864\n",
      "Epoch 388/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.1427 - acc: 0.9487 - val_loss: 0.3289 - val_acc: 0.8864\n",
      "Epoch 389/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.1373 - acc: 0.9530 - val_loss: 0.3200 - val_acc: 0.9034\n",
      "Epoch 390/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.1307 - acc: 0.9473 - val_loss: 0.3721 - val_acc: 0.8920\n",
      "Epoch 391/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1067 - acc: 0.9672 - val_loss: 0.3503 - val_acc: 0.8977\n",
      "Epoch 392/3000\n",
      "702/702 [==============================] - 0s 581us/sample - loss: 0.1203 - acc: 0.9573 - val_loss: 0.3704 - val_acc: 0.8864\n",
      "Epoch 393/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.1069 - acc: 0.9601 - val_loss: 0.3230 - val_acc: 0.9034\n",
      "Epoch 394/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0963 - acc: 0.9573 - val_loss: 0.3495 - val_acc: 0.8920\n",
      "Epoch 395/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0973 - acc: 0.9630 - val_loss: 0.3804 - val_acc: 0.8693\n",
      "Epoch 396/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.1005 - acc: 0.9630 - val_loss: 0.3615 - val_acc: 0.8920\n",
      "Epoch 397/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.1122 - acc: 0.9729 - val_loss: 0.3249 - val_acc: 0.8864\n",
      "Epoch 398/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0931 - acc: 0.9715 - val_loss: 0.3563 - val_acc: 0.8807\n",
      "Epoch 399/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0893 - acc: 0.9701 - val_loss: 0.3881 - val_acc: 0.8977\n",
      "Epoch 400/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.1007 - acc: 0.9587 - val_loss: 0.3477 - val_acc: 0.8920\n",
      "Epoch 401/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1231 - acc: 0.9516 - val_loss: 0.3499 - val_acc: 0.8920\n",
      "Epoch 402/3000\n",
      "702/702 [==============================] - 0s 570us/sample - loss: 0.0850 - acc: 0.9658 - val_loss: 0.3129 - val_acc: 0.8864\n",
      "Epoch 403/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0983 - acc: 0.9601 - val_loss: 0.3622 - val_acc: 0.8864\n",
      "Epoch 404/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0843 - acc: 0.9701 - val_loss: 0.3693 - val_acc: 0.8920\n",
      "Epoch 405/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.1041 - acc: 0.9587 - val_loss: 0.3907 - val_acc: 0.8920\n",
      "Epoch 406/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 0.0903 - acc: 0.9672 - val_loss: 0.3975 - val_acc: 0.8864\n",
      "Epoch 407/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0947 - acc: 0.9644 - val_loss: 0.3426 - val_acc: 0.8864\n",
      "Epoch 408/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0786 - acc: 0.9786 - val_loss: 0.3664 - val_acc: 0.8750\n",
      "Epoch 409/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.0787 - acc: 0.9729 - val_loss: 0.3400 - val_acc: 0.8864\n",
      "Epoch 410/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0801 - acc: 0.9758 - val_loss: 0.3561 - val_acc: 0.8920\n",
      "Epoch 411/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0808 - acc: 0.9687 - val_loss: 0.3989 - val_acc: 0.8807\n",
      "Epoch 412/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.0905 - acc: 0.9672 - val_loss: 0.3449 - val_acc: 0.8920\n",
      "Epoch 413/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0835 - acc: 0.9729 - val_loss: 0.4055 - val_acc: 0.8864\n",
      "Epoch 414/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0742 - acc: 0.9729 - val_loss: 0.3943 - val_acc: 0.8977\n",
      "Epoch 415/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0940 - acc: 0.9672 - val_loss: 0.3471 - val_acc: 0.8977\n",
      "Epoch 416/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.1150 - acc: 0.9587 - val_loss: 0.3184 - val_acc: 0.8807\n",
      "Epoch 417/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.1038 - acc: 0.9615 - val_loss: 0.3968 - val_acc: 0.8750\n",
      "Epoch 418/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0898 - acc: 0.9715 - val_loss: 0.3957 - val_acc: 0.8750\n",
      "Epoch 419/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0942 - acc: 0.9687 - val_loss: 0.3979 - val_acc: 0.8580\n",
      "Epoch 420/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0880 - acc: 0.9687 - val_loss: 0.3265 - val_acc: 0.8920\n",
      "Epoch 421/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0781 - acc: 0.9772 - val_loss: 0.3633 - val_acc: 0.8864\n",
      "Epoch 422/3000\n",
      "702/702 [==============================] - 0s 587us/sample - loss: 0.0933 - acc: 0.9658 - val_loss: 0.3529 - val_acc: 0.8977\n",
      "Epoch 423/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0691 - acc: 0.9801 - val_loss: 0.3610 - val_acc: 0.8864\n",
      "Epoch 424/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.1062 - acc: 0.9701 - val_loss: 0.3137 - val_acc: 0.8920\n",
      "Epoch 425/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0769 - acc: 0.9715 - val_loss: 0.3500 - val_acc: 0.8920\n",
      "Epoch 426/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0790 - acc: 0.9758 - val_loss: 0.3557 - val_acc: 0.8977\n",
      "Epoch 427/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0801 - acc: 0.9729 - val_loss: 0.3408 - val_acc: 0.8750\n",
      "Epoch 428/3000\n",
      "702/702 [==============================] - 0s 539us/sample - loss: 0.0848 - acc: 0.9658 - val_loss: 0.3847 - val_acc: 0.8807\n",
      "Epoch 429/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0852 - acc: 0.9615 - val_loss: 0.3733 - val_acc: 0.8750\n",
      "Epoch 430/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0741 - acc: 0.9687 - val_loss: 0.3468 - val_acc: 0.9034\n",
      "Epoch 431/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0873 - acc: 0.9658 - val_loss: 0.3235 - val_acc: 0.8977\n",
      "Epoch 432/3000\n",
      "702/702 [==============================] - 0s 587us/sample - loss: 0.0877 - acc: 0.9658 - val_loss: 0.3135 - val_acc: 0.9034\n",
      "Epoch 433/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0758 - acc: 0.9786 - val_loss: 0.3607 - val_acc: 0.9034\n",
      "Epoch 434/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0810 - acc: 0.9744 - val_loss: 0.3293 - val_acc: 0.8920\n",
      "Epoch 435/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0942 - acc: 0.9687 - val_loss: 0.3377 - val_acc: 0.9034\n",
      "Epoch 436/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0637 - acc: 0.9801 - val_loss: 0.3190 - val_acc: 0.9148\n",
      "Epoch 437/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0801 - acc: 0.9687 - val_loss: 0.3397 - val_acc: 0.8807\n",
      "Epoch 438/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.1073 - acc: 0.9558 - val_loss: 0.3837 - val_acc: 0.9034\n",
      "Epoch 439/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0793 - acc: 0.9687 - val_loss: 0.2951 - val_acc: 0.9034\n",
      "Epoch 440/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0961 - acc: 0.9687 - val_loss: 0.3027 - val_acc: 0.8977\n",
      "Epoch 441/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0884 - acc: 0.9687 - val_loss: 0.3316 - val_acc: 0.9034\n",
      "Epoch 442/3000\n",
      "702/702 [==============================] - 0s 580us/sample - loss: 0.0945 - acc: 0.9672 - val_loss: 0.3600 - val_acc: 0.8977\n",
      "Epoch 443/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0881 - acc: 0.9701 - val_loss: 0.3344 - val_acc: 0.9091\n",
      "Epoch 444/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0646 - acc: 0.9758 - val_loss: 0.3281 - val_acc: 0.8977\n",
      "Epoch 445/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0843 - acc: 0.9744 - val_loss: 0.3618 - val_acc: 0.8977\n",
      "Epoch 446/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.1151 - acc: 0.9601 - val_loss: 0.3413 - val_acc: 0.9091\n",
      "Epoch 447/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.1024 - acc: 0.9701 - val_loss: 0.3066 - val_acc: 0.8920\n",
      "Epoch 448/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0787 - acc: 0.9744 - val_loss: 0.3665 - val_acc: 0.8864\n",
      "Epoch 449/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0992 - acc: 0.9672 - val_loss: 0.2887 - val_acc: 0.9034\n",
      "Epoch 450/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.1009 - acc: 0.9672 - val_loss: 0.3793 - val_acc: 0.8864\n",
      "Epoch 451/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0720 - acc: 0.9701 - val_loss: 0.3659 - val_acc: 0.9034\n",
      "Epoch 452/3000\n",
      "702/702 [==============================] - 0s 636us/sample - loss: 0.0849 - acc: 0.9715 - val_loss: 0.3216 - val_acc: 0.9034\n",
      "Epoch 453/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 0.0726 - acc: 0.9786 - val_loss: 0.3746 - val_acc: 0.8977\n",
      "Epoch 454/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.1089 - acc: 0.9658 - val_loss: 0.2955 - val_acc: 0.8864\n",
      "Epoch 455/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0702 - acc: 0.9801 - val_loss: 0.3517 - val_acc: 0.8977\n",
      "Epoch 456/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0906 - acc: 0.9687 - val_loss: 0.3437 - val_acc: 0.9034\n",
      "Epoch 457/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0915 - acc: 0.9658 - val_loss: 0.3313 - val_acc: 0.8977\n",
      "Epoch 458/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.1082 - acc: 0.9601 - val_loss: 0.3559 - val_acc: 0.8977\n",
      "Epoch 459/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0913 - acc: 0.9715 - val_loss: 0.4025 - val_acc: 0.8920\n",
      "Epoch 460/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0893 - acc: 0.9672 - val_loss: 0.3544 - val_acc: 0.9091\n",
      "Epoch 461/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0770 - acc: 0.9786 - val_loss: 0.2964 - val_acc: 0.9034\n",
      "Epoch 462/3000\n",
      "702/702 [==============================] - 0s 584us/sample - loss: 0.0911 - acc: 0.9729 - val_loss: 0.3398 - val_acc: 0.9034\n",
      "Epoch 463/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0715 - acc: 0.9729 - val_loss: 0.2945 - val_acc: 0.9091\n",
      "Epoch 464/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0949 - acc: 0.9672 - val_loss: 0.2854 - val_acc: 0.9148\n",
      "Epoch 465/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0702 - acc: 0.9758 - val_loss: 0.3445 - val_acc: 0.9091\n",
      "Epoch 466/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0680 - acc: 0.9772 - val_loss: 0.3143 - val_acc: 0.8864\n",
      "Epoch 467/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0807 - acc: 0.9801 - val_loss: 0.3455 - val_acc: 0.8977\n",
      "Epoch 468/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0901 - acc: 0.9658 - val_loss: 0.3285 - val_acc: 0.8977\n",
      "Epoch 469/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0716 - acc: 0.9758 - val_loss: 0.3257 - val_acc: 0.9034\n",
      "Epoch 470/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0698 - acc: 0.9729 - val_loss: 0.3366 - val_acc: 0.9091\n",
      "Epoch 471/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0610 - acc: 0.9843 - val_loss: 0.3433 - val_acc: 0.9034\n",
      "Epoch 472/3000\n",
      "702/702 [==============================] - 0s 577us/sample - loss: 0.0666 - acc: 0.9801 - val_loss: 0.3463 - val_acc: 0.8977\n",
      "Epoch 473/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0708 - acc: 0.9658 - val_loss: 0.3142 - val_acc: 0.9091\n",
      "Epoch 474/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0719 - acc: 0.9744 - val_loss: 0.3475 - val_acc: 0.9091\n",
      "Epoch 475/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0608 - acc: 0.9772 - val_loss: 0.3675 - val_acc: 0.8977\n",
      "Epoch 476/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0757 - acc: 0.9715 - val_loss: 0.3604 - val_acc: 0.8920\n",
      "Epoch 477/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0685 - acc: 0.9786 - val_loss: 0.3221 - val_acc: 0.9091\n",
      "Epoch 478/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0693 - acc: 0.9772 - val_loss: 0.3725 - val_acc: 0.9034\n",
      "Epoch 479/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0675 - acc: 0.9815 - val_loss: 0.3795 - val_acc: 0.8977\n",
      "Epoch 480/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0421 - acc: 0.9900 - val_loss: 0.3409 - val_acc: 0.8920\n",
      "Epoch 481/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0768 - acc: 0.9758 - val_loss: 0.3185 - val_acc: 0.9034\n",
      "Epoch 482/3000\n",
      "702/702 [==============================] - 0s 584us/sample - loss: 0.0766 - acc: 0.9715 - val_loss: 0.3651 - val_acc: 0.8807\n",
      "Epoch 483/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0930 - acc: 0.9687 - val_loss: 0.4479 - val_acc: 0.8864\n",
      "Epoch 484/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0845 - acc: 0.9701 - val_loss: 0.3646 - val_acc: 0.8636\n",
      "Epoch 485/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0931 - acc: 0.9644 - val_loss: 0.3489 - val_acc: 0.8920\n",
      "Epoch 486/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.1061 - acc: 0.9587 - val_loss: 0.4221 - val_acc: 0.8580\n",
      "Epoch 487/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0960 - acc: 0.9672 - val_loss: 0.3456 - val_acc: 0.8977\n",
      "Epoch 488/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0844 - acc: 0.9758 - val_loss: 0.3067 - val_acc: 0.8977\n",
      "Epoch 489/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0858 - acc: 0.9715 - val_loss: 0.3654 - val_acc: 0.8977\n",
      "Epoch 490/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.0790 - acc: 0.9729 - val_loss: 0.4219 - val_acc: 0.8636\n",
      "Epoch 491/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0951 - acc: 0.9672 - val_loss: 0.3429 - val_acc: 0.9091\n",
      "Epoch 492/3000\n",
      "702/702 [==============================] - 0s 578us/sample - loss: 0.0785 - acc: 0.9687 - val_loss: 0.3708 - val_acc: 0.8920\n",
      "Epoch 493/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0784 - acc: 0.9658 - val_loss: 0.3788 - val_acc: 0.8977\n",
      "Epoch 494/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.1406 - acc: 0.9530 - val_loss: 0.3049 - val_acc: 0.8750\n",
      "Epoch 495/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.1172 - acc: 0.9601 - val_loss: 0.4093 - val_acc: 0.8636\n",
      "Epoch 496/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0744 - acc: 0.9815 - val_loss: 0.3818 - val_acc: 0.8807\n",
      "Epoch 497/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0818 - acc: 0.9701 - val_loss: 0.3649 - val_acc: 0.8693\n",
      "Epoch 498/3000\n",
      "702/702 [==============================] - 0s 538us/sample - loss: 0.1005 - acc: 0.9587 - val_loss: 0.3660 - val_acc: 0.8864\n",
      "Epoch 499/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0902 - acc: 0.9687 - val_loss: 0.3534 - val_acc: 0.8807\n",
      "Epoch 500/3000\n",
      "702/702 [==============================] - 0s 538us/sample - loss: 0.0589 - acc: 0.9786 - val_loss: 0.3875 - val_acc: 0.8864\n",
      "Epoch 501/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0865 - acc: 0.9729 - val_loss: 0.3395 - val_acc: 0.8864\n",
      "Epoch 502/3000\n",
      "702/702 [==============================] - 0s 604us/sample - loss: 0.0956 - acc: 0.9672 - val_loss: 0.3191 - val_acc: 0.8750\n",
      "Epoch 503/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0983 - acc: 0.9601 - val_loss: 0.3408 - val_acc: 0.9034\n",
      "Epoch 504/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0886 - acc: 0.9729 - val_loss: 0.3241 - val_acc: 0.8807\n",
      "Epoch 505/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.1047 - acc: 0.9573 - val_loss: 0.3626 - val_acc: 0.8807\n",
      "Epoch 506/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.1000 - acc: 0.9630 - val_loss: 0.2762 - val_acc: 0.9091\n",
      "Epoch 507/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0939 - acc: 0.9701 - val_loss: 0.3113 - val_acc: 0.8977\n",
      "Epoch 508/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.0675 - acc: 0.9786 - val_loss: 0.3799 - val_acc: 0.8864\n",
      "Epoch 509/3000\n",
      "702/702 [==============================] - 0s 538us/sample - loss: 0.0676 - acc: 0.9744 - val_loss: 0.3914 - val_acc: 0.8864\n",
      "Epoch 510/3000\n",
      "702/702 [==============================] - 0s 537us/sample - loss: 0.0712 - acc: 0.9744 - val_loss: 0.4216 - val_acc: 0.8864\n",
      "Epoch 511/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0597 - acc: 0.9815 - val_loss: 0.3355 - val_acc: 0.8920\n",
      "Epoch 512/3000\n",
      "702/702 [==============================] - 0s 608us/sample - loss: 0.0690 - acc: 0.9786 - val_loss: 0.3050 - val_acc: 0.8977\n",
      "Epoch 513/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0662 - acc: 0.9715 - val_loss: 0.2928 - val_acc: 0.8977\n",
      "Epoch 514/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0777 - acc: 0.9715 - val_loss: 0.3193 - val_acc: 0.8864\n",
      "Epoch 515/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.0634 - acc: 0.9829 - val_loss: 0.3933 - val_acc: 0.8977\n",
      "Epoch 516/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0705 - acc: 0.9786 - val_loss: 0.3294 - val_acc: 0.9091\n",
      "Epoch 517/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0721 - acc: 0.9772 - val_loss: 0.2938 - val_acc: 0.8977\n",
      "Epoch 518/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.0612 - acc: 0.9786 - val_loss: 0.3307 - val_acc: 0.8977\n",
      "Epoch 519/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0937 - acc: 0.9672 - val_loss: 0.3200 - val_acc: 0.8920\n",
      "Epoch 520/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.0566 - acc: 0.9858 - val_loss: 0.3049 - val_acc: 0.8920\n",
      "Epoch 521/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0763 - acc: 0.9758 - val_loss: 0.3684 - val_acc: 0.9034\n",
      "Epoch 522/3000\n",
      "702/702 [==============================] - 0s 634us/sample - loss: 0.0860 - acc: 0.9772 - val_loss: 0.3805 - val_acc: 0.8920\n",
      "Epoch 523/3000\n",
      "702/702 [==============================] - 0s 542us/sample - loss: 0.0675 - acc: 0.9744 - val_loss: 0.3788 - val_acc: 0.8864\n",
      "Epoch 524/3000\n",
      "702/702 [==============================] - 0s 548us/sample - loss: 0.0518 - acc: 0.9843 - val_loss: 0.4072 - val_acc: 0.9034\n",
      "Epoch 525/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.0730 - acc: 0.9715 - val_loss: 0.2945 - val_acc: 0.8920\n",
      "Epoch 526/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.0727 - acc: 0.9701 - val_loss: 0.3063 - val_acc: 0.9091\n",
      "Epoch 527/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0606 - acc: 0.9772 - val_loss: 0.3089 - val_acc: 0.9148\n",
      "Epoch 528/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0551 - acc: 0.9815 - val_loss: 0.3493 - val_acc: 0.8977\n",
      "Epoch 529/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0635 - acc: 0.9772 - val_loss: 0.3635 - val_acc: 0.8864\n",
      "Epoch 530/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0566 - acc: 0.9801 - val_loss: 0.3716 - val_acc: 0.9034\n",
      "Epoch 531/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0511 - acc: 0.9843 - val_loss: 0.3616 - val_acc: 0.9148\n",
      "Epoch 532/3000\n",
      "702/702 [==============================] - 0s 585us/sample - loss: 0.0543 - acc: 0.9786 - val_loss: 0.3394 - val_acc: 0.9034\n",
      "Epoch 533/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0654 - acc: 0.9786 - val_loss: 0.3511 - val_acc: 0.8807\n",
      "Epoch 534/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0722 - acc: 0.9772 - val_loss: 0.3548 - val_acc: 0.8977\n",
      "Epoch 535/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.0719 - acc: 0.9701 - val_loss: 0.3263 - val_acc: 0.9148\n",
      "Epoch 536/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0589 - acc: 0.9815 - val_loss: 0.3579 - val_acc: 0.9034\n",
      "Epoch 537/3000\n",
      "702/702 [==============================] - 0s 558us/sample - loss: 0.0661 - acc: 0.9772 - val_loss: 0.3471 - val_acc: 0.8977\n",
      "Epoch 538/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0588 - acc: 0.9801 - val_loss: 0.3745 - val_acc: 0.9034\n",
      "Epoch 539/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0516 - acc: 0.9843 - val_loss: 0.3581 - val_acc: 0.8977\n",
      "Epoch 540/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0771 - acc: 0.9801 - val_loss: 0.3684 - val_acc: 0.8920\n",
      "Epoch 541/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.1078 - acc: 0.9644 - val_loss: 0.3726 - val_acc: 0.9091\n",
      "Epoch 542/3000\n",
      "702/702 [==============================] - 0s 584us/sample - loss: 0.0896 - acc: 0.9715 - val_loss: 0.3611 - val_acc: 0.9148\n",
      "Epoch 543/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0605 - acc: 0.9815 - val_loss: 0.3455 - val_acc: 0.9148\n",
      "Epoch 544/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0599 - acc: 0.9772 - val_loss: 0.3528 - val_acc: 0.9034\n",
      "Epoch 545/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0498 - acc: 0.9843 - val_loss: 0.3664 - val_acc: 0.8977\n",
      "Epoch 546/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0606 - acc: 0.9772 - val_loss: 0.3382 - val_acc: 0.8920\n",
      "Epoch 547/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0751 - acc: 0.9772 - val_loss: 0.3491 - val_acc: 0.8977\n",
      "Epoch 548/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0464 - acc: 0.9872 - val_loss: 0.4180 - val_acc: 0.8920\n",
      "Epoch 549/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0644 - acc: 0.9801 - val_loss: 0.4390 - val_acc: 0.8807\n",
      "Epoch 550/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0900 - acc: 0.9601 - val_loss: 0.3302 - val_acc: 0.8920\n",
      "Epoch 551/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0707 - acc: 0.9729 - val_loss: 0.2916 - val_acc: 0.9148\n",
      "Epoch 552/3000\n",
      "702/702 [==============================] - 0s 581us/sample - loss: 0.0807 - acc: 0.9715 - val_loss: 0.2953 - val_acc: 0.8920\n",
      "Epoch 553/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0786 - acc: 0.9701 - val_loss: 0.3684 - val_acc: 0.9034\n",
      "Epoch 554/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0655 - acc: 0.9758 - val_loss: 0.4206 - val_acc: 0.9091\n",
      "Epoch 555/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0818 - acc: 0.9744 - val_loss: 0.3510 - val_acc: 0.8920\n",
      "Epoch 556/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0616 - acc: 0.9815 - val_loss: 0.3669 - val_acc: 0.8807\n",
      "Epoch 557/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0724 - acc: 0.9758 - val_loss: 0.3855 - val_acc: 0.8864\n",
      "Epoch 558/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0657 - acc: 0.9715 - val_loss: 0.3665 - val_acc: 0.9034\n",
      "Epoch 559/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0564 - acc: 0.9843 - val_loss: 0.3228 - val_acc: 0.9091\n",
      "Epoch 560/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0748 - acc: 0.9729 - val_loss: 0.3602 - val_acc: 0.8977\n",
      "Epoch 561/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0676 - acc: 0.9772 - val_loss: 0.3403 - val_acc: 0.8977\n",
      "Epoch 562/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.0688 - acc: 0.9715 - val_loss: 0.2866 - val_acc: 0.9034\n",
      "Epoch 563/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0554 - acc: 0.9829 - val_loss: 0.3507 - val_acc: 0.8920\n",
      "Epoch 564/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0659 - acc: 0.9772 - val_loss: 0.3410 - val_acc: 0.8920\n",
      "Epoch 565/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0668 - acc: 0.9815 - val_loss: 0.4479 - val_acc: 0.8864\n",
      "Epoch 566/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0739 - acc: 0.9744 - val_loss: 0.4258 - val_acc: 0.8920\n",
      "Epoch 567/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0478 - acc: 0.9843 - val_loss: 0.3470 - val_acc: 0.9091\n",
      "Epoch 568/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0509 - acc: 0.9801 - val_loss: 0.3692 - val_acc: 0.9034\n",
      "Epoch 569/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0557 - acc: 0.9786 - val_loss: 0.2980 - val_acc: 0.8920\n",
      "Epoch 570/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0680 - acc: 0.9772 - val_loss: 0.3054 - val_acc: 0.9205\n",
      "Epoch 571/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0740 - acc: 0.9729 - val_loss: 0.3664 - val_acc: 0.8750\n",
      "Epoch 572/3000\n",
      "702/702 [==============================] - 0s 593us/sample - loss: 0.0698 - acc: 0.9744 - val_loss: 0.3993 - val_acc: 0.8920\n",
      "Epoch 573/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0695 - acc: 0.9801 - val_loss: 0.4253 - val_acc: 0.8636\n",
      "Epoch 574/3000\n",
      "702/702 [==============================] - 0s 557us/sample - loss: 0.0663 - acc: 0.9758 - val_loss: 0.4595 - val_acc: 0.8920\n",
      "Epoch 575/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0702 - acc: 0.9772 - val_loss: 0.3790 - val_acc: 0.8920\n",
      "Epoch 576/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.0462 - acc: 0.9886 - val_loss: 0.3821 - val_acc: 0.8864\n",
      "Epoch 577/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.0554 - acc: 0.9858 - val_loss: 0.3266 - val_acc: 0.9034\n",
      "Epoch 578/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0681 - acc: 0.9744 - val_loss: 0.3367 - val_acc: 0.9091\n",
      "Epoch 579/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0535 - acc: 0.9744 - val_loss: 0.3877 - val_acc: 0.8920\n",
      "Epoch 580/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.0549 - acc: 0.9786 - val_loss: 0.3227 - val_acc: 0.9091\n",
      "Epoch 581/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0922 - acc: 0.9701 - val_loss: 0.3978 - val_acc: 0.8977\n",
      "Epoch 582/3000\n",
      "702/702 [==============================] - 0s 600us/sample - loss: 0.0765 - acc: 0.9801 - val_loss: 0.3710 - val_acc: 0.9091\n",
      "Epoch 583/3000\n",
      "702/702 [==============================] - 0s 545us/sample - loss: 0.0693 - acc: 0.9729 - val_loss: 0.3983 - val_acc: 0.8977\n",
      "Epoch 584/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0626 - acc: 0.9786 - val_loss: 0.3923 - val_acc: 0.8920\n",
      "Epoch 585/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0890 - acc: 0.9672 - val_loss: 0.4239 - val_acc: 0.8864\n",
      "Epoch 586/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0514 - acc: 0.9843 - val_loss: 0.4427 - val_acc: 0.8977\n",
      "Epoch 587/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0579 - acc: 0.9801 - val_loss: 0.3986 - val_acc: 0.8920\n",
      "Epoch 588/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0709 - acc: 0.9772 - val_loss: 0.4803 - val_acc: 0.8864\n",
      "Epoch 589/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.1317 - acc: 0.9615 - val_loss: 0.3657 - val_acc: 0.8920\n",
      "Epoch 590/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.1045 - acc: 0.9615 - val_loss: 0.4826 - val_acc: 0.8920\n",
      "Epoch 591/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0875 - acc: 0.9630 - val_loss: 0.3742 - val_acc: 0.8977\n",
      "Epoch 592/3000\n",
      "702/702 [==============================] - 0s 641us/sample - loss: 0.0817 - acc: 0.9744 - val_loss: 0.3341 - val_acc: 0.8920\n",
      "Epoch 593/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0916 - acc: 0.9701 - val_loss: 0.4079 - val_acc: 0.8977\n",
      "Epoch 594/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0603 - acc: 0.9801 - val_loss: 0.4438 - val_acc: 0.8807\n",
      "Epoch 595/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.0864 - acc: 0.9715 - val_loss: 0.3464 - val_acc: 0.9091\n",
      "Epoch 596/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.0748 - acc: 0.9701 - val_loss: 0.3579 - val_acc: 0.9091\n",
      "Epoch 597/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0623 - acc: 0.9801 - val_loss: 0.3609 - val_acc: 0.9091\n",
      "Epoch 598/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0580 - acc: 0.9843 - val_loss: 0.3179 - val_acc: 0.9091\n",
      "Epoch 599/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.0429 - acc: 0.9843 - val_loss: 0.4301 - val_acc: 0.9034\n",
      "Epoch 600/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0597 - acc: 0.9801 - val_loss: 0.4388 - val_acc: 0.9034\n",
      "Epoch 601/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0556 - acc: 0.9758 - val_loss: 0.3646 - val_acc: 0.8977\n",
      "Epoch 602/3000\n",
      "702/702 [==============================] - 0s 583us/sample - loss: 0.0670 - acc: 0.9672 - val_loss: 0.3636 - val_acc: 0.9034\n",
      "Epoch 603/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0692 - acc: 0.9758 - val_loss: 0.4084 - val_acc: 0.9034\n",
      "Epoch 604/3000\n",
      "702/702 [==============================] - 0s 554us/sample - loss: 0.0663 - acc: 0.9715 - val_loss: 0.3628 - val_acc: 0.8977\n",
      "Epoch 605/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 0.0466 - acc: 0.9872 - val_loss: 0.3277 - val_acc: 0.9091\n",
      "Epoch 606/3000\n",
      "702/702 [==============================] - 0s 564us/sample - loss: 0.0388 - acc: 0.9915 - val_loss: 0.3406 - val_acc: 0.9148\n",
      "Epoch 607/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0475 - acc: 0.9858 - val_loss: 0.3841 - val_acc: 0.9034\n",
      "Epoch 608/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0420 - acc: 0.9872 - val_loss: 0.3651 - val_acc: 0.9034\n",
      "Epoch 609/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0380 - acc: 0.9815 - val_loss: 0.3432 - val_acc: 0.9091\n",
      "Epoch 610/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0364 - acc: 0.9915 - val_loss: 0.3767 - val_acc: 0.9148\n",
      "Epoch 611/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0462 - acc: 0.9858 - val_loss: 0.3880 - val_acc: 0.9034\n",
      "Epoch 612/3000\n",
      "702/702 [==============================] - 0s 609us/sample - loss: 0.0548 - acc: 0.9872 - val_loss: 0.3837 - val_acc: 0.9091\n",
      "Epoch 613/3000\n",
      "702/702 [==============================] - 0s 534us/sample - loss: 0.0424 - acc: 0.9829 - val_loss: 0.3833 - val_acc: 0.9034\n",
      "Epoch 614/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0786 - acc: 0.9701 - val_loss: 0.3984 - val_acc: 0.8920\n",
      "Epoch 615/3000\n",
      "702/702 [==============================] - 0s 548us/sample - loss: 0.0615 - acc: 0.9801 - val_loss: 0.3529 - val_acc: 0.8864\n",
      "Epoch 616/3000\n",
      "702/702 [==============================] - 0s 550us/sample - loss: 0.0502 - acc: 0.9815 - val_loss: 0.3510 - val_acc: 0.9091\n",
      "Epoch 617/3000\n",
      "702/702 [==============================] - 0s 539us/sample - loss: 0.0607 - acc: 0.9729 - val_loss: 0.3595 - val_acc: 0.9034\n",
      "Epoch 618/3000\n",
      "702/702 [==============================] - 0s 550us/sample - loss: 0.0771 - acc: 0.9744 - val_loss: 0.4248 - val_acc: 0.8977\n",
      "Epoch 619/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0591 - acc: 0.9815 - val_loss: 0.3711 - val_acc: 0.8920\n",
      "Epoch 620/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0670 - acc: 0.9744 - val_loss: 0.3893 - val_acc: 0.8864\n",
      "Epoch 621/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0807 - acc: 0.9801 - val_loss: 0.4774 - val_acc: 0.8807\n",
      "Epoch 622/3000\n",
      "702/702 [==============================] - 0s 635us/sample - loss: 0.0873 - acc: 0.9729 - val_loss: 0.3756 - val_acc: 0.8750\n",
      "Epoch 623/3000\n",
      "702/702 [==============================] - 0s 555us/sample - loss: 0.0669 - acc: 0.9801 - val_loss: 0.3973 - val_acc: 0.8807\n",
      "Epoch 624/3000\n",
      "702/702 [==============================] - 0s 556us/sample - loss: 0.0541 - acc: 0.9801 - val_loss: 0.4180 - val_acc: 0.8864\n",
      "Epoch 625/3000\n",
      "702/702 [==============================] - 0s 566us/sample - loss: 0.0586 - acc: 0.9772 - val_loss: 0.3855 - val_acc: 0.8977\n",
      "Epoch 626/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 0.0689 - acc: 0.9801 - val_loss: 0.3384 - val_acc: 0.9034\n",
      "Epoch 627/3000\n",
      "702/702 [==============================] - 0s 568us/sample - loss: 0.0603 - acc: 0.9829 - val_loss: 0.3603 - val_acc: 0.8750\n",
      "Epoch 628/3000\n",
      "702/702 [==============================] - 0s 549us/sample - loss: 0.0513 - acc: 0.9829 - val_loss: 0.3664 - val_acc: 0.8807\n",
      "Epoch 629/3000\n",
      "702/702 [==============================] - 0s 554us/sample - loss: 0.0538 - acc: 0.9843 - val_loss: 0.3364 - val_acc: 0.8864\n",
      "Epoch 630/3000\n",
      "702/702 [==============================] - 0s 535us/sample - loss: 0.0596 - acc: 0.9786 - val_loss: 0.3272 - val_acc: 0.8807\n",
      "Epoch 631/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0438 - acc: 0.9843 - val_loss: 0.4106 - val_acc: 0.8864\n",
      "Epoch 632/3000\n",
      "702/702 [==============================] - 0s 583us/sample - loss: 0.0390 - acc: 0.9858 - val_loss: 0.4259 - val_acc: 0.8864\n",
      "Epoch 633/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0514 - acc: 0.9858 - val_loss: 0.3529 - val_acc: 0.8920\n",
      "Epoch 634/3000\n",
      "702/702 [==============================] - 0s 548us/sample - loss: 0.0332 - acc: 0.9943 - val_loss: 0.4049 - val_acc: 0.8864\n",
      "Epoch 635/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 0.0484 - acc: 0.9786 - val_loss: 0.3883 - val_acc: 0.8864\n",
      "Epoch 636/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 0.0698 - acc: 0.9772 - val_loss: 0.4278 - val_acc: 0.8920\n",
      "Epoch 637/3000\n",
      "702/702 [==============================] - 0s 558us/sample - loss: 0.0667 - acc: 0.9843 - val_loss: 0.3564 - val_acc: 0.8977\n",
      "Epoch 638/3000\n",
      "702/702 [==============================] - 0s 553us/sample - loss: 0.0671 - acc: 0.9786 - val_loss: 0.3572 - val_acc: 0.9091\n",
      "Epoch 639/3000\n",
      "702/702 [==============================] - 0s 537us/sample - loss: 0.0623 - acc: 0.9786 - val_loss: 0.3564 - val_acc: 0.8807\n",
      "Epoch 640/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 0.0647 - acc: 0.9786 - val_loss: 0.3548 - val_acc: 0.8807\n",
      "Epoch 641/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0580 - acc: 0.9829 - val_loss: 0.3309 - val_acc: 0.9091\n",
      "Epoch 642/3000\n",
      "702/702 [==============================] - 0s 584us/sample - loss: 0.0686 - acc: 0.9772 - val_loss: 0.3526 - val_acc: 0.8920\n",
      "Epoch 643/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0632 - acc: 0.9829 - val_loss: 0.3174 - val_acc: 0.8977\n",
      "Epoch 644/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0694 - acc: 0.9786 - val_loss: 0.2919 - val_acc: 0.9148\n",
      "Epoch 645/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0675 - acc: 0.9772 - val_loss: 0.3158 - val_acc: 0.8920\n",
      "Epoch 646/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.0750 - acc: 0.9758 - val_loss: 0.3506 - val_acc: 0.8864\n",
      "Epoch 647/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.0686 - acc: 0.9772 - val_loss: 0.3534 - val_acc: 0.8977\n",
      "Epoch 648/3000\n",
      "702/702 [==============================] - 0s 534us/sample - loss: 0.0627 - acc: 0.9715 - val_loss: 0.3212 - val_acc: 0.8977\n",
      "Epoch 649/3000\n",
      "702/702 [==============================] - 0s 570us/sample - loss: 0.0814 - acc: 0.9729 - val_loss: 0.2819 - val_acc: 0.9148\n",
      "Epoch 650/3000\n",
      "702/702 [==============================] - 0s 564us/sample - loss: 0.0717 - acc: 0.9829 - val_loss: 0.3330 - val_acc: 0.9034\n",
      "Epoch 651/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0708 - acc: 0.9744 - val_loss: 0.3028 - val_acc: 0.9091\n",
      "Epoch 652/3000\n",
      "702/702 [==============================] - 0s 578us/sample - loss: 0.0594 - acc: 0.9815 - val_loss: 0.2717 - val_acc: 0.9034\n",
      "Epoch 653/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0616 - acc: 0.9886 - val_loss: 0.2959 - val_acc: 0.8977\n",
      "Epoch 654/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0544 - acc: 0.9829 - val_loss: 0.2640 - val_acc: 0.9205\n",
      "Epoch 655/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0695 - acc: 0.9801 - val_loss: 0.3614 - val_acc: 0.8977\n",
      "Epoch 656/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0820 - acc: 0.9744 - val_loss: 0.3691 - val_acc: 0.8807\n",
      "Epoch 657/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0690 - acc: 0.9701 - val_loss: 0.3400 - val_acc: 0.8977\n",
      "Epoch 658/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0680 - acc: 0.9801 - val_loss: 0.3443 - val_acc: 0.8977\n",
      "Epoch 659/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0691 - acc: 0.9758 - val_loss: 0.2679 - val_acc: 0.9034\n",
      "Epoch 660/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0703 - acc: 0.9744 - val_loss: 0.3279 - val_acc: 0.9091\n",
      "Epoch 661/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0432 - acc: 0.9858 - val_loss: 0.3934 - val_acc: 0.9034\n",
      "Epoch 662/3000\n",
      "702/702 [==============================] - 0s 594us/sample - loss: 0.0651 - acc: 0.9729 - val_loss: 0.3468 - val_acc: 0.8864\n",
      "Epoch 663/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.0709 - acc: 0.9758 - val_loss: 0.3264 - val_acc: 0.9148\n",
      "Epoch 664/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0720 - acc: 0.9758 - val_loss: 0.3027 - val_acc: 0.9034\n",
      "Epoch 665/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0563 - acc: 0.9786 - val_loss: 0.3464 - val_acc: 0.8864\n",
      "Epoch 666/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0588 - acc: 0.9758 - val_loss: 0.3272 - val_acc: 0.9034\n",
      "Epoch 667/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0685 - acc: 0.9758 - val_loss: 0.3248 - val_acc: 0.8920\n",
      "Epoch 668/3000\n",
      "702/702 [==============================] - 0s 539us/sample - loss: 0.0554 - acc: 0.9829 - val_loss: 0.3456 - val_acc: 0.9034\n",
      "Epoch 669/3000\n",
      "702/702 [==============================] - 0s 534us/sample - loss: 0.0483 - acc: 0.9843 - val_loss: 0.3277 - val_acc: 0.9034\n",
      "Epoch 670/3000\n",
      "702/702 [==============================] - 0s 538us/sample - loss: 0.0515 - acc: 0.9772 - val_loss: 0.2983 - val_acc: 0.8920\n",
      "Epoch 671/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0651 - acc: 0.9786 - val_loss: 0.3825 - val_acc: 0.8807\n",
      "Epoch 672/3000\n",
      "702/702 [==============================] - 0s 597us/sample - loss: 0.0363 - acc: 0.9872 - val_loss: 0.3837 - val_acc: 0.8807\n",
      "Epoch 673/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0432 - acc: 0.9843 - val_loss: 0.3324 - val_acc: 0.9091\n",
      "Epoch 674/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0647 - acc: 0.9815 - val_loss: 0.3241 - val_acc: 0.9034\n",
      "Epoch 675/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 0.0462 - acc: 0.9801 - val_loss: 0.3769 - val_acc: 0.8920\n",
      "Epoch 676/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0479 - acc: 0.9843 - val_loss: 0.3759 - val_acc: 0.8864\n",
      "Epoch 677/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0617 - acc: 0.9729 - val_loss: 0.3332 - val_acc: 0.9148\n",
      "Epoch 678/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.0660 - acc: 0.9786 - val_loss: 0.3591 - val_acc: 0.8977\n",
      "Epoch 679/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0691 - acc: 0.9801 - val_loss: 0.2920 - val_acc: 0.9091\n",
      "Epoch 680/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0588 - acc: 0.9801 - val_loss: 0.3735 - val_acc: 0.8864\n",
      "Epoch 681/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0653 - acc: 0.9744 - val_loss: 0.3897 - val_acc: 0.8693\n",
      "Epoch 682/3000\n",
      "702/702 [==============================] - 0s 567us/sample - loss: 0.0619 - acc: 0.9815 - val_loss: 0.3560 - val_acc: 0.8920\n",
      "Epoch 683/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0547 - acc: 0.9758 - val_loss: 0.3587 - val_acc: 0.9091\n",
      "Epoch 684/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0435 - acc: 0.9772 - val_loss: 0.3788 - val_acc: 0.9091\n",
      "Epoch 685/3000\n",
      "702/702 [==============================] - 0s 538us/sample - loss: 0.0554 - acc: 0.9801 - val_loss: 0.3531 - val_acc: 0.9034\n",
      "Epoch 686/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0293 - acc: 0.9929 - val_loss: 0.4259 - val_acc: 0.9034\n",
      "Epoch 687/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.0453 - acc: 0.9886 - val_loss: 0.3508 - val_acc: 0.9034\n",
      "Epoch 688/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.0317 - acc: 0.9915 - val_loss: 0.3475 - val_acc: 0.9148\n",
      "Epoch 689/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.0538 - acc: 0.9843 - val_loss: 0.3430 - val_acc: 0.9091\n",
      "Epoch 690/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.0663 - acc: 0.9786 - val_loss: 0.3014 - val_acc: 0.9034\n",
      "Epoch 691/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0462 - acc: 0.9858 - val_loss: 0.2991 - val_acc: 0.9091\n",
      "Epoch 692/3000\n",
      "702/702 [==============================] - 0s 583us/sample - loss: 0.0386 - acc: 0.9886 - val_loss: 0.2974 - val_acc: 0.8977\n",
      "Epoch 693/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0737 - acc: 0.9744 - val_loss: 0.3323 - val_acc: 0.8977\n",
      "Epoch 694/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0463 - acc: 0.9815 - val_loss: 0.3390 - val_acc: 0.9091\n",
      "Epoch 695/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0594 - acc: 0.9772 - val_loss: 0.3534 - val_acc: 0.8977\n",
      "Epoch 696/3000\n",
      "702/702 [==============================] - 0s 551us/sample - loss: 0.0473 - acc: 0.9829 - val_loss: 0.4050 - val_acc: 0.8920\n",
      "Epoch 697/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.0516 - acc: 0.9786 - val_loss: 0.3307 - val_acc: 0.8977\n",
      "Epoch 698/3000\n",
      "702/702 [==============================] - 0s 548us/sample - loss: 0.0565 - acc: 0.9843 - val_loss: 0.3465 - val_acc: 0.9148\n",
      "Epoch 699/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0594 - acc: 0.9815 - val_loss: 0.3635 - val_acc: 0.9148\n",
      "Epoch 700/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0665 - acc: 0.9744 - val_loss: 0.3472 - val_acc: 0.9034\n",
      "Epoch 701/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0571 - acc: 0.9786 - val_loss: 0.3703 - val_acc: 0.9034\n",
      "Epoch 702/3000\n",
      "702/702 [==============================] - 0s 627us/sample - loss: 0.0679 - acc: 0.9701 - val_loss: 0.3387 - val_acc: 0.9034\n",
      "Epoch 703/3000\n",
      "702/702 [==============================] - 0s 555us/sample - loss: 0.0499 - acc: 0.9858 - val_loss: 0.2969 - val_acc: 0.8977\n",
      "Epoch 704/3000\n",
      "702/702 [==============================] - 0s 539us/sample - loss: 0.0374 - acc: 0.9886 - val_loss: 0.3222 - val_acc: 0.8864\n",
      "Epoch 705/3000\n",
      "702/702 [==============================] - 0s 560us/sample - loss: 0.0482 - acc: 0.9858 - val_loss: 0.3902 - val_acc: 0.8977\n",
      "Epoch 706/3000\n",
      "702/702 [==============================] - 0s 543us/sample - loss: 0.0718 - acc: 0.9829 - val_loss: 0.3790 - val_acc: 0.9148\n",
      "Epoch 707/3000\n",
      "702/702 [==============================] - 0s 569us/sample - loss: 0.0423 - acc: 0.9886 - val_loss: 0.3136 - val_acc: 0.8977\n",
      "Epoch 708/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.0669 - acc: 0.9815 - val_loss: 0.3325 - val_acc: 0.8920\n",
      "Epoch 709/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.0564 - acc: 0.9843 - val_loss: 0.4760 - val_acc: 0.8920\n",
      "Epoch 710/3000\n",
      "702/702 [==============================] - 0s 549us/sample - loss: 0.0968 - acc: 0.9658 - val_loss: 0.3248 - val_acc: 0.8977\n",
      "Epoch 711/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0616 - acc: 0.9772 - val_loss: 0.2985 - val_acc: 0.9091\n",
      "Epoch 712/3000\n",
      "702/702 [==============================] - 0s 633us/sample - loss: 0.0554 - acc: 0.9815 - val_loss: 0.3503 - val_acc: 0.9148\n",
      "Epoch 713/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0546 - acc: 0.9786 - val_loss: 0.3704 - val_acc: 0.8920\n",
      "Epoch 714/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 0.0592 - acc: 0.9815 - val_loss: 0.3533 - val_acc: 0.8920\n",
      "Epoch 715/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0418 - acc: 0.9801 - val_loss: 0.3121 - val_acc: 0.9148\n",
      "Epoch 716/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0390 - acc: 0.9900 - val_loss: 0.3346 - val_acc: 0.9205\n",
      "Epoch 717/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0265 - acc: 0.9929 - val_loss: 0.3412 - val_acc: 0.9034\n",
      "Epoch 718/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0324 - acc: 0.9886 - val_loss: 0.3143 - val_acc: 0.9091\n",
      "Epoch 719/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0427 - acc: 0.9801 - val_loss: 0.3425 - val_acc: 0.8977\n",
      "Epoch 720/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0606 - acc: 0.9772 - val_loss: 0.3656 - val_acc: 0.8920\n",
      "Epoch 721/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0390 - acc: 0.9900 - val_loss: 0.2837 - val_acc: 0.9261\n",
      "Epoch 722/3000\n",
      "702/702 [==============================] - 0s 585us/sample - loss: 0.0491 - acc: 0.9829 - val_loss: 0.3461 - val_acc: 0.9091\n",
      "Epoch 723/3000\n",
      "702/702 [==============================] - 0s 558us/sample - loss: 0.0506 - acc: 0.9829 - val_loss: 0.3980 - val_acc: 0.9205\n",
      "Epoch 724/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0614 - acc: 0.9786 - val_loss: 0.3666 - val_acc: 0.8977\n",
      "Epoch 725/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0613 - acc: 0.9701 - val_loss: 0.3749 - val_acc: 0.8920\n",
      "Epoch 726/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0558 - acc: 0.9758 - val_loss: 0.3966 - val_acc: 0.8977\n",
      "Epoch 727/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0707 - acc: 0.9786 - val_loss: 0.3304 - val_acc: 0.9091\n",
      "Epoch 728/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0573 - acc: 0.9772 - val_loss: 0.3634 - val_acc: 0.9091\n",
      "Epoch 729/3000\n",
      "702/702 [==============================] - 0s 550us/sample - loss: 0.0605 - acc: 0.9786 - val_loss: 0.3723 - val_acc: 0.8864\n",
      "Epoch 730/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0684 - acc: 0.9758 - val_loss: 0.3664 - val_acc: 0.9091\n",
      "Epoch 731/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0521 - acc: 0.9786 - val_loss: 0.3534 - val_acc: 0.9034\n",
      "Epoch 732/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.0726 - acc: 0.9772 - val_loss: 0.3767 - val_acc: 0.8920\n",
      "Epoch 733/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0563 - acc: 0.9858 - val_loss: 0.3677 - val_acc: 0.9091\n",
      "Epoch 734/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0555 - acc: 0.9772 - val_loss: 0.4039 - val_acc: 0.8977\n",
      "Epoch 735/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0514 - acc: 0.9815 - val_loss: 0.3206 - val_acc: 0.8977\n",
      "Epoch 736/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0362 - acc: 0.9872 - val_loss: 0.3421 - val_acc: 0.9205\n",
      "Epoch 737/3000\n",
      "702/702 [==============================] - 0s 557us/sample - loss: 0.0352 - acc: 0.9900 - val_loss: 0.3625 - val_acc: 0.9148\n",
      "Epoch 738/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0541 - acc: 0.9815 - val_loss: 0.3431 - val_acc: 0.9148\n",
      "Epoch 739/3000\n",
      "702/702 [==============================] - 0s 540us/sample - loss: 0.0402 - acc: 0.9843 - val_loss: 0.3812 - val_acc: 0.9205\n",
      "Epoch 740/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.0421 - acc: 0.9858 - val_loss: 0.3688 - val_acc: 0.9148\n",
      "Epoch 741/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0430 - acc: 0.9858 - val_loss: 0.2940 - val_acc: 0.9091\n",
      "Epoch 742/3000\n",
      "702/702 [==============================] - 0s 588us/sample - loss: 0.0514 - acc: 0.9858 - val_loss: 0.3376 - val_acc: 0.9091\n",
      "Epoch 743/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0334 - acc: 0.9900 - val_loss: 0.3852 - val_acc: 0.9148\n",
      "Epoch 744/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0341 - acc: 0.9929 - val_loss: 0.3568 - val_acc: 0.9091\n",
      "Epoch 745/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0344 - acc: 0.9900 - val_loss: 0.2995 - val_acc: 0.9091\n",
      "Epoch 746/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0419 - acc: 0.9886 - val_loss: 0.3399 - val_acc: 0.9034\n",
      "Epoch 747/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0355 - acc: 0.9929 - val_loss: 0.3472 - val_acc: 0.9148\n",
      "Epoch 748/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0607 - acc: 0.9786 - val_loss: 0.3767 - val_acc: 0.9091\n",
      "Epoch 749/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0644 - acc: 0.9829 - val_loss: 0.3786 - val_acc: 0.9034\n",
      "Epoch 750/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0494 - acc: 0.9858 - val_loss: 0.3235 - val_acc: 0.9091\n",
      "Epoch 751/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0897 - acc: 0.9687 - val_loss: 0.4404 - val_acc: 0.8864\n",
      "Epoch 752/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.1293 - acc: 0.9715 - val_loss: 0.4344 - val_acc: 0.8920\n",
      "Epoch 753/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0880 - acc: 0.9744 - val_loss: 0.3823 - val_acc: 0.9091\n",
      "Epoch 754/3000\n",
      "702/702 [==============================] - 0s 556us/sample - loss: 0.0597 - acc: 0.9829 - val_loss: 0.3537 - val_acc: 0.9034\n",
      "Epoch 755/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0611 - acc: 0.9801 - val_loss: 0.3833 - val_acc: 0.9091\n",
      "Epoch 756/3000\n",
      "702/702 [==============================] - 0s 555us/sample - loss: 0.0530 - acc: 0.9801 - val_loss: 0.4180 - val_acc: 0.9148\n",
      "Epoch 757/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0425 - acc: 0.9815 - val_loss: 0.3578 - val_acc: 0.9148\n",
      "Epoch 758/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.0442 - acc: 0.9858 - val_loss: 0.3578 - val_acc: 0.9091\n",
      "Epoch 759/3000\n",
      "702/702 [==============================] - 0s 568us/sample - loss: 0.0370 - acc: 0.9886 - val_loss: 0.4035 - val_acc: 0.9091\n",
      "Epoch 760/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.0429 - acc: 0.9815 - val_loss: 0.3689 - val_acc: 0.9034\n",
      "Epoch 761/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0542 - acc: 0.9801 - val_loss: 0.3321 - val_acc: 0.9034\n",
      "Epoch 762/3000\n",
      "702/702 [==============================] - 0s 601us/sample - loss: 0.0353 - acc: 0.9872 - val_loss: 0.4106 - val_acc: 0.9034\n",
      "Epoch 763/3000\n",
      "702/702 [==============================] - 0s 534us/sample - loss: 0.0343 - acc: 0.9872 - val_loss: 0.3467 - val_acc: 0.8977\n",
      "Epoch 764/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0550 - acc: 0.9843 - val_loss: 0.3160 - val_acc: 0.9091\n",
      "Epoch 765/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0518 - acc: 0.9829 - val_loss: 0.3202 - val_acc: 0.9034\n",
      "Epoch 766/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0328 - acc: 0.9886 - val_loss: 0.3630 - val_acc: 0.8977\n",
      "Epoch 767/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0437 - acc: 0.9915 - val_loss: 0.3428 - val_acc: 0.9034\n",
      "Epoch 768/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0774 - acc: 0.9786 - val_loss: 0.2646 - val_acc: 0.9091\n",
      "Epoch 769/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0519 - acc: 0.9929 - val_loss: 0.3701 - val_acc: 0.9034\n",
      "Epoch 770/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0349 - acc: 0.9900 - val_loss: 0.4116 - val_acc: 0.8920\n",
      "Epoch 771/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0412 - acc: 0.9900 - val_loss: 0.3373 - val_acc: 0.8920\n",
      "Epoch 772/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.0562 - acc: 0.9829 - val_loss: 0.3218 - val_acc: 0.9091\n",
      "Epoch 773/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0412 - acc: 0.9900 - val_loss: 0.3281 - val_acc: 0.9205\n",
      "Epoch 774/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0261 - acc: 0.9943 - val_loss: 0.3294 - val_acc: 0.8977\n",
      "Epoch 775/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0488 - acc: 0.9815 - val_loss: 0.3207 - val_acc: 0.9091\n",
      "Epoch 776/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0293 - acc: 0.9872 - val_loss: 0.3035 - val_acc: 0.9091\n",
      "Epoch 777/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0396 - acc: 0.9886 - val_loss: 0.2918 - val_acc: 0.9034\n",
      "Epoch 778/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0302 - acc: 0.9886 - val_loss: 0.3102 - val_acc: 0.9091\n",
      "Epoch 779/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0252 - acc: 0.9929 - val_loss: 0.3192 - val_acc: 0.9034\n",
      "Epoch 780/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0395 - acc: 0.9886 - val_loss: 0.2953 - val_acc: 0.9148\n",
      "Epoch 781/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0588 - acc: 0.9829 - val_loss: 0.4277 - val_acc: 0.9034\n",
      "Epoch 782/3000\n",
      "702/702 [==============================] - 0s 591us/sample - loss: 0.0466 - acc: 0.9843 - val_loss: 0.3701 - val_acc: 0.9091\n",
      "Epoch 783/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0349 - acc: 0.9886 - val_loss: 0.3039 - val_acc: 0.9148\n",
      "Epoch 784/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0382 - acc: 0.9858 - val_loss: 0.3045 - val_acc: 0.9091\n",
      "Epoch 785/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0383 - acc: 0.9886 - val_loss: 0.3446 - val_acc: 0.9148\n",
      "Epoch 786/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0291 - acc: 0.9900 - val_loss: 0.3610 - val_acc: 0.8977\n",
      "Epoch 787/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0323 - acc: 0.9915 - val_loss: 0.4044 - val_acc: 0.8977\n",
      "Epoch 788/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0566 - acc: 0.9815 - val_loss: 0.2957 - val_acc: 0.9034\n",
      "Epoch 789/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0411 - acc: 0.9858 - val_loss: 0.3173 - val_acc: 0.9091\n",
      "Epoch 790/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0545 - acc: 0.9872 - val_loss: 0.3194 - val_acc: 0.8977\n",
      "Epoch 791/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0593 - acc: 0.9801 - val_loss: 0.2902 - val_acc: 0.8977\n",
      "Epoch 792/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.0334 - acc: 0.9858 - val_loss: 0.3629 - val_acc: 0.9091\n",
      "Epoch 793/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0348 - acc: 0.9872 - val_loss: 0.3497 - val_acc: 0.9091\n",
      "Epoch 794/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0293 - acc: 0.9886 - val_loss: 0.3383 - val_acc: 0.9091\n",
      "Epoch 795/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0326 - acc: 0.9900 - val_loss: 0.3055 - val_acc: 0.9148\n",
      "Epoch 796/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0325 - acc: 0.9929 - val_loss: 0.3119 - val_acc: 0.9148\n",
      "Epoch 797/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0305 - acc: 0.9900 - val_loss: 0.3772 - val_acc: 0.9205\n",
      "Epoch 798/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0345 - acc: 0.9872 - val_loss: 0.3275 - val_acc: 0.9091\n",
      "Epoch 799/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0361 - acc: 0.9872 - val_loss: 0.3227 - val_acc: 0.9091\n",
      "Epoch 800/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.0449 - acc: 0.9829 - val_loss: 0.2656 - val_acc: 0.9148\n",
      "Epoch 801/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0462 - acc: 0.9801 - val_loss: 0.2784 - val_acc: 0.9091\n",
      "Epoch 802/3000\n",
      "702/702 [==============================] - 0s 587us/sample - loss: 0.0346 - acc: 0.9886 - val_loss: 0.4159 - val_acc: 0.9034\n",
      "Epoch 803/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0316 - acc: 0.9929 - val_loss: 0.3530 - val_acc: 0.9034\n",
      "Epoch 804/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0417 - acc: 0.9872 - val_loss: 0.3567 - val_acc: 0.9034\n",
      "Epoch 805/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0471 - acc: 0.9786 - val_loss: 0.3428 - val_acc: 0.8977\n",
      "Epoch 806/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0707 - acc: 0.9786 - val_loss: 0.3676 - val_acc: 0.9034\n",
      "Epoch 807/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0516 - acc: 0.9786 - val_loss: 0.3105 - val_acc: 0.9091\n",
      "Epoch 808/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0421 - acc: 0.9829 - val_loss: 0.2637 - val_acc: 0.9091\n",
      "Epoch 809/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0432 - acc: 0.9843 - val_loss: 0.2961 - val_acc: 0.9148\n",
      "Epoch 810/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0468 - acc: 0.9843 - val_loss: 0.3322 - val_acc: 0.9091\n",
      "Epoch 811/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0310 - acc: 0.9900 - val_loss: 0.3367 - val_acc: 0.9148\n",
      "Epoch 812/3000\n",
      "702/702 [==============================] - 0s 587us/sample - loss: 0.0266 - acc: 0.9915 - val_loss: 0.3660 - val_acc: 0.9034\n",
      "Epoch 813/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0475 - acc: 0.9801 - val_loss: 0.4263 - val_acc: 0.9148\n",
      "Epoch 814/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0340 - acc: 0.9886 - val_loss: 0.3340 - val_acc: 0.9205\n",
      "Epoch 815/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0319 - acc: 0.9900 - val_loss: 0.3290 - val_acc: 0.9091\n",
      "Epoch 816/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.0377 - acc: 0.9858 - val_loss: 0.3666 - val_acc: 0.8977\n",
      "Epoch 817/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0402 - acc: 0.9872 - val_loss: 0.3646 - val_acc: 0.8864\n",
      "Epoch 818/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0502 - acc: 0.9872 - val_loss: 0.3742 - val_acc: 0.8920\n",
      "Epoch 819/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0401 - acc: 0.9886 - val_loss: 0.3119 - val_acc: 0.9091\n",
      "Epoch 820/3000\n",
      "702/702 [==============================] - 0s 539us/sample - loss: 0.0376 - acc: 0.9886 - val_loss: 0.2888 - val_acc: 0.9205\n",
      "Epoch 821/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0343 - acc: 0.9900 - val_loss: 0.3768 - val_acc: 0.9148\n",
      "Epoch 822/3000\n",
      "702/702 [==============================] - 0s 593us/sample - loss: 0.0326 - acc: 0.9900 - val_loss: 0.3614 - val_acc: 0.8920\n",
      "Epoch 823/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.0316 - acc: 0.9915 - val_loss: 0.3640 - val_acc: 0.9034\n",
      "Epoch 824/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.0397 - acc: 0.9886 - val_loss: 0.4283 - val_acc: 0.9148\n",
      "Epoch 825/3000\n",
      "702/702 [==============================] - 0s 538us/sample - loss: 0.0317 - acc: 0.9900 - val_loss: 0.3764 - val_acc: 0.9091\n",
      "Epoch 826/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0315 - acc: 0.9872 - val_loss: 0.3201 - val_acc: 0.9148\n",
      "Epoch 827/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0435 - acc: 0.9843 - val_loss: 0.3143 - val_acc: 0.9091\n",
      "Epoch 828/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.0437 - acc: 0.9858 - val_loss: 0.3811 - val_acc: 0.9034\n",
      "Epoch 829/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0335 - acc: 0.9886 - val_loss: 0.3439 - val_acc: 0.9034\n",
      "Epoch 830/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0581 - acc: 0.9829 - val_loss: 0.3443 - val_acc: 0.9091\n",
      "Epoch 831/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0339 - acc: 0.9915 - val_loss: 0.3839 - val_acc: 0.9034\n",
      "Epoch 832/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.0506 - acc: 0.9872 - val_loss: 0.3364 - val_acc: 0.8920\n",
      "Epoch 833/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0514 - acc: 0.9829 - val_loss: 0.3697 - val_acc: 0.9148\n",
      "Epoch 834/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0705 - acc: 0.9786 - val_loss: 0.3059 - val_acc: 0.8920\n",
      "Epoch 835/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0358 - acc: 0.9900 - val_loss: 0.2242 - val_acc: 0.9261\n",
      "Epoch 836/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0665 - acc: 0.9801 - val_loss: 0.2845 - val_acc: 0.8977\n",
      "Epoch 837/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0400 - acc: 0.9872 - val_loss: 0.3987 - val_acc: 0.9091\n",
      "Epoch 838/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0512 - acc: 0.9801 - val_loss: 0.3350 - val_acc: 0.9205\n",
      "Epoch 839/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0775 - acc: 0.9758 - val_loss: 0.3349 - val_acc: 0.9148\n",
      "Epoch 840/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0326 - acc: 0.9858 - val_loss: 0.4488 - val_acc: 0.9205\n",
      "Epoch 841/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0401 - acc: 0.9815 - val_loss: 0.3432 - val_acc: 0.8977\n",
      "Epoch 842/3000\n",
      "702/702 [==============================] - 0s 591us/sample - loss: 0.0316 - acc: 0.9886 - val_loss: 0.2987 - val_acc: 0.8920\n",
      "Epoch 843/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0380 - acc: 0.9843 - val_loss: 0.3373 - val_acc: 0.8977\n",
      "Epoch 844/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0344 - acc: 0.9900 - val_loss: 0.3443 - val_acc: 0.9148\n",
      "Epoch 845/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0344 - acc: 0.9929 - val_loss: 0.3293 - val_acc: 0.8977\n",
      "Epoch 846/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0311 - acc: 0.9886 - val_loss: 0.2998 - val_acc: 0.9091\n",
      "Epoch 847/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0293 - acc: 0.9900 - val_loss: 0.3142 - val_acc: 0.9148\n",
      "Epoch 848/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0518 - acc: 0.9915 - val_loss: 0.3053 - val_acc: 0.8977\n",
      "Epoch 849/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0455 - acc: 0.9886 - val_loss: 0.3209 - val_acc: 0.9091\n",
      "Epoch 850/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0473 - acc: 0.9858 - val_loss: 0.3325 - val_acc: 0.9034\n",
      "Epoch 851/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0453 - acc: 0.9829 - val_loss: 0.3105 - val_acc: 0.9091\n",
      "Epoch 852/3000\n",
      "702/702 [==============================] - 0s 591us/sample - loss: 0.1027 - acc: 0.9630 - val_loss: 0.3743 - val_acc: 0.8807\n",
      "Epoch 853/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0807 - acc: 0.9758 - val_loss: 0.5952 - val_acc: 0.8977\n",
      "Epoch 854/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0633 - acc: 0.9701 - val_loss: 0.3134 - val_acc: 0.9318\n",
      "Epoch 855/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0431 - acc: 0.9815 - val_loss: 0.3326 - val_acc: 0.9091\n",
      "Epoch 856/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0429 - acc: 0.9872 - val_loss: 0.3807 - val_acc: 0.9148\n",
      "Epoch 857/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.0301 - acc: 0.9915 - val_loss: 0.4472 - val_acc: 0.8977\n",
      "Epoch 858/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0483 - acc: 0.9786 - val_loss: 0.3533 - val_acc: 0.9148\n",
      "Epoch 859/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.0645 - acc: 0.9801 - val_loss: 0.3113 - val_acc: 0.9034\n",
      "Epoch 860/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0277 - acc: 0.9915 - val_loss: 0.3513 - val_acc: 0.9091\n",
      "Epoch 861/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0473 - acc: 0.9858 - val_loss: 0.3271 - val_acc: 0.9205\n",
      "Epoch 862/3000\n",
      "702/702 [==============================] - 0s 581us/sample - loss: 0.0368 - acc: 0.9843 - val_loss: 0.3371 - val_acc: 0.8977\n",
      "Epoch 863/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0344 - acc: 0.9900 - val_loss: 0.3832 - val_acc: 0.8920\n",
      "Epoch 864/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0339 - acc: 0.9872 - val_loss: 0.4016 - val_acc: 0.9091\n",
      "Epoch 865/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0348 - acc: 0.9929 - val_loss: 0.3280 - val_acc: 0.9091\n",
      "Epoch 866/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0257 - acc: 0.9943 - val_loss: 0.3125 - val_acc: 0.8920\n",
      "Epoch 867/3000\n",
      "702/702 [==============================] - 0s 545us/sample - loss: 0.0260 - acc: 0.9943 - val_loss: 0.3307 - val_acc: 0.9034\n",
      "Epoch 868/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0268 - acc: 0.9900 - val_loss: 0.3511 - val_acc: 0.9091\n",
      "Epoch 869/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0246 - acc: 0.9929 - val_loss: 0.3260 - val_acc: 0.8977\n",
      "Epoch 870/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0291 - acc: 0.9915 - val_loss: 0.3691 - val_acc: 0.8977\n",
      "Epoch 871/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0325 - acc: 0.9915 - val_loss: 0.3550 - val_acc: 0.8977\n",
      "Epoch 872/3000\n",
      "702/702 [==============================] - 0s 586us/sample - loss: 0.0291 - acc: 0.9900 - val_loss: 0.3598 - val_acc: 0.9091\n",
      "Epoch 873/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0359 - acc: 0.9858 - val_loss: 0.3495 - val_acc: 0.9148\n",
      "Epoch 874/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0260 - acc: 0.9915 - val_loss: 0.3646 - val_acc: 0.8977\n",
      "Epoch 875/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0299 - acc: 0.9886 - val_loss: 0.3592 - val_acc: 0.8920\n",
      "Epoch 876/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0339 - acc: 0.9900 - val_loss: 0.3568 - val_acc: 0.9091\n",
      "Epoch 877/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0333 - acc: 0.9858 - val_loss: 0.3655 - val_acc: 0.9148\n",
      "Epoch 878/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0339 - acc: 0.9900 - val_loss: 0.3813 - val_acc: 0.8977\n",
      "Epoch 879/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0266 - acc: 0.9929 - val_loss: 0.3432 - val_acc: 0.9034\n",
      "Epoch 880/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0227 - acc: 0.9943 - val_loss: 0.3267 - val_acc: 0.8977\n",
      "Epoch 881/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0397 - acc: 0.9843 - val_loss: 0.3777 - val_acc: 0.9034\n",
      "Epoch 882/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.0355 - acc: 0.9915 - val_loss: 0.4034 - val_acc: 0.9034\n",
      "Epoch 883/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0210 - acc: 0.9957 - val_loss: 0.3724 - val_acc: 0.8977\n",
      "Epoch 884/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0272 - acc: 0.9872 - val_loss: 0.3718 - val_acc: 0.9034\n",
      "Epoch 885/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0307 - acc: 0.9900 - val_loss: 0.4040 - val_acc: 0.9034\n",
      "Epoch 886/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0378 - acc: 0.9886 - val_loss: 0.3268 - val_acc: 0.9091\n",
      "Epoch 887/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0373 - acc: 0.9886 - val_loss: 0.3347 - val_acc: 0.8920\n",
      "Epoch 888/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.0295 - acc: 0.9915 - val_loss: 0.3798 - val_acc: 0.9148\n",
      "Epoch 889/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.0321 - acc: 0.9872 - val_loss: 0.4128 - val_acc: 0.9148\n",
      "Epoch 890/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0407 - acc: 0.9915 - val_loss: 0.3643 - val_acc: 0.8864\n",
      "Epoch 891/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0488 - acc: 0.9815 - val_loss: 0.3484 - val_acc: 0.8977\n",
      "Epoch 892/3000\n",
      "702/702 [==============================] - 0s 618us/sample - loss: 0.0671 - acc: 0.9786 - val_loss: 0.3276 - val_acc: 0.8977\n",
      "Epoch 893/3000\n",
      "702/702 [==============================] - 0s 535us/sample - loss: 0.0375 - acc: 0.9886 - val_loss: 0.3547 - val_acc: 0.8977\n",
      "Epoch 894/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 0.0430 - acc: 0.9915 - val_loss: 0.3355 - val_acc: 0.8864\n",
      "Epoch 895/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.0375 - acc: 0.9872 - val_loss: 0.3557 - val_acc: 0.8864\n",
      "Epoch 896/3000\n",
      "702/702 [==============================] - 0s 566us/sample - loss: 0.0308 - acc: 0.9943 - val_loss: 0.3992 - val_acc: 0.8977\n",
      "Epoch 897/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.0647 - acc: 0.9758 - val_loss: 0.2938 - val_acc: 0.9034\n",
      "Epoch 898/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0408 - acc: 0.9886 - val_loss: 0.3417 - val_acc: 0.8977\n",
      "Epoch 899/3000\n",
      "702/702 [==============================] - 0s 567us/sample - loss: 0.0329 - acc: 0.9900 - val_loss: 0.4384 - val_acc: 0.8807\n",
      "Epoch 900/3000\n",
      "702/702 [==============================] - 0s 573us/sample - loss: 0.0607 - acc: 0.9829 - val_loss: 0.2975 - val_acc: 0.8750\n",
      "Epoch 901/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0468 - acc: 0.9900 - val_loss: 0.2688 - val_acc: 0.9091\n",
      "Epoch 902/3000\n",
      "702/702 [==============================] - 0s 586us/sample - loss: 0.0327 - acc: 0.9886 - val_loss: 0.3725 - val_acc: 0.9034\n",
      "Epoch 903/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.0473 - acc: 0.9858 - val_loss: 0.3680 - val_acc: 0.8977\n",
      "Epoch 904/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.0431 - acc: 0.9843 - val_loss: 0.3238 - val_acc: 0.8807\n",
      "Epoch 905/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0374 - acc: 0.9872 - val_loss: 0.3560 - val_acc: 0.8977\n",
      "Epoch 906/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0350 - acc: 0.9915 - val_loss: 0.3431 - val_acc: 0.9091\n",
      "Epoch 907/3000\n",
      "702/702 [==============================] - 0s 539us/sample - loss: 0.0253 - acc: 0.9900 - val_loss: 0.3584 - val_acc: 0.8977\n",
      "Epoch 908/3000\n",
      "702/702 [==============================] - 0s 545us/sample - loss: 0.0265 - acc: 0.9943 - val_loss: 0.3438 - val_acc: 0.9148\n",
      "Epoch 909/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0364 - acc: 0.9900 - val_loss: 0.3524 - val_acc: 0.8977\n",
      "Epoch 910/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0523 - acc: 0.9872 - val_loss: 0.2760 - val_acc: 0.9148\n",
      "Epoch 911/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0309 - acc: 0.9886 - val_loss: 0.2802 - val_acc: 0.9034\n",
      "Epoch 912/3000\n",
      "702/702 [==============================] - 0s 624us/sample - loss: 0.0332 - acc: 0.9900 - val_loss: 0.3395 - val_acc: 0.8920\n",
      "Epoch 913/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0403 - acc: 0.9900 - val_loss: 0.3712 - val_acc: 0.8977\n",
      "Epoch 914/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0203 - acc: 0.9915 - val_loss: 0.3904 - val_acc: 0.9091\n",
      "Epoch 915/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0211 - acc: 0.9943 - val_loss: 0.3834 - val_acc: 0.8920\n",
      "Epoch 916/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0232 - acc: 0.9872 - val_loss: 0.3307 - val_acc: 0.8977\n",
      "Epoch 917/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0297 - acc: 0.9886 - val_loss: 0.3180 - val_acc: 0.8977\n",
      "Epoch 918/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0263 - acc: 0.9886 - val_loss: 0.4191 - val_acc: 0.8920\n",
      "Epoch 919/3000\n",
      "702/702 [==============================] - 0s 548us/sample - loss: 0.0408 - acc: 0.9943 - val_loss: 0.4239 - val_acc: 0.9148\n",
      "Epoch 920/3000\n",
      "702/702 [==============================] - 0s 553us/sample - loss: 0.0386 - acc: 0.9829 - val_loss: 0.3921 - val_acc: 0.9148\n",
      "Epoch 921/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0240 - acc: 0.9929 - val_loss: 0.3924 - val_acc: 0.8977\n",
      "Epoch 922/3000\n",
      "702/702 [==============================] - 0s 577us/sample - loss: 0.0433 - acc: 0.9815 - val_loss: 0.3976 - val_acc: 0.8977\n",
      "Epoch 923/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0517 - acc: 0.9843 - val_loss: 0.3192 - val_acc: 0.9148\n",
      "Epoch 924/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0295 - acc: 0.9943 - val_loss: 0.2433 - val_acc: 0.9205\n",
      "Epoch 925/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0502 - acc: 0.9815 - val_loss: 0.3114 - val_acc: 0.9091\n",
      "Epoch 926/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0411 - acc: 0.9872 - val_loss: 0.2928 - val_acc: 0.9091\n",
      "Epoch 927/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0369 - acc: 0.9886 - val_loss: 0.2958 - val_acc: 0.8864\n",
      "Epoch 928/3000\n",
      "702/702 [==============================] - 0s 559us/sample - loss: 0.0314 - acc: 0.9900 - val_loss: 0.3069 - val_acc: 0.9091\n",
      "Epoch 929/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.0326 - acc: 0.9900 - val_loss: 0.3596 - val_acc: 0.8920\n",
      "Epoch 930/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0345 - acc: 0.9843 - val_loss: 0.3055 - val_acc: 0.9091\n",
      "Epoch 931/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0313 - acc: 0.9872 - val_loss: 0.3040 - val_acc: 0.9091\n",
      "Epoch 932/3000\n",
      "702/702 [==============================] - 0s 589us/sample - loss: 0.0214 - acc: 0.9943 - val_loss: 0.3172 - val_acc: 0.8977\n",
      "Epoch 933/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0215 - acc: 0.9929 - val_loss: 0.3491 - val_acc: 0.9091\n",
      "Epoch 934/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0331 - acc: 0.9872 - val_loss: 0.3217 - val_acc: 0.9148\n",
      "Epoch 935/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0338 - acc: 0.9900 - val_loss: 0.3186 - val_acc: 0.8920\n",
      "Epoch 936/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0423 - acc: 0.9872 - val_loss: 0.3934 - val_acc: 0.9148\n",
      "Epoch 937/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0201 - acc: 0.9943 - val_loss: 0.4208 - val_acc: 0.8977\n",
      "Epoch 938/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0504 - acc: 0.9843 - val_loss: 0.3874 - val_acc: 0.9091\n",
      "Epoch 939/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0300 - acc: 0.9900 - val_loss: 0.3308 - val_acc: 0.9034\n",
      "Epoch 940/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0451 - acc: 0.9886 - val_loss: 0.3554 - val_acc: 0.8864\n",
      "Epoch 941/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0422 - acc: 0.9872 - val_loss: 0.3511 - val_acc: 0.8977\n",
      "Epoch 942/3000\n",
      "702/702 [==============================] - 0s 601us/sample - loss: 0.0436 - acc: 0.9815 - val_loss: 0.2656 - val_acc: 0.8920\n",
      "Epoch 943/3000\n",
      "702/702 [==============================] - 0s 552us/sample - loss: 0.0499 - acc: 0.9858 - val_loss: 0.2910 - val_acc: 0.9034\n",
      "Epoch 944/3000\n",
      "702/702 [==============================] - 0s 557us/sample - loss: 0.0293 - acc: 0.9900 - val_loss: 0.3864 - val_acc: 0.8920\n",
      "Epoch 945/3000\n",
      "702/702 [==============================] - 0s 554us/sample - loss: 0.0482 - acc: 0.9858 - val_loss: 0.3371 - val_acc: 0.8920\n",
      "Epoch 946/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.0316 - acc: 0.9957 - val_loss: 0.3365 - val_acc: 0.8977\n",
      "Epoch 947/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0333 - acc: 0.9829 - val_loss: 0.3498 - val_acc: 0.9091\n",
      "Epoch 948/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0295 - acc: 0.9900 - val_loss: 0.3266 - val_acc: 0.9091\n",
      "Epoch 949/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0207 - acc: 0.9900 - val_loss: 0.2978 - val_acc: 0.9034\n",
      "Epoch 950/3000\n",
      "702/702 [==============================] - 0s 537us/sample - loss: 0.0289 - acc: 0.9900 - val_loss: 0.3215 - val_acc: 0.9148\n",
      "Epoch 951/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0415 - acc: 0.9900 - val_loss: 0.3484 - val_acc: 0.8920\n",
      "Epoch 952/3000\n",
      "702/702 [==============================] - 0s 593us/sample - loss: 0.0539 - acc: 0.9829 - val_loss: 0.3270 - val_acc: 0.8977\n",
      "Epoch 953/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.0311 - acc: 0.9915 - val_loss: 0.3826 - val_acc: 0.8977\n",
      "Epoch 954/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0328 - acc: 0.9872 - val_loss: 0.3721 - val_acc: 0.9148\n",
      "Epoch 955/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.0310 - acc: 0.9858 - val_loss: 0.3735 - val_acc: 0.9205\n",
      "Epoch 956/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0392 - acc: 0.9843 - val_loss: 0.3098 - val_acc: 0.9205\n",
      "Epoch 957/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0312 - acc: 0.9886 - val_loss: 0.3416 - val_acc: 0.9148\n",
      "Epoch 958/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0450 - acc: 0.9815 - val_loss: 0.3340 - val_acc: 0.8920\n",
      "Epoch 959/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0500 - acc: 0.9872 - val_loss: 0.4110 - val_acc: 0.8977\n",
      "Epoch 960/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0377 - acc: 0.9872 - val_loss: 0.4104 - val_acc: 0.9205\n",
      "Epoch 961/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0547 - acc: 0.9929 - val_loss: 0.3094 - val_acc: 0.9091\n",
      "Epoch 962/3000\n",
      "702/702 [==============================] - 0s 585us/sample - loss: 0.0351 - acc: 0.9886 - val_loss: 0.3582 - val_acc: 0.9091\n",
      "Epoch 963/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0196 - acc: 0.9943 - val_loss: 0.4575 - val_acc: 0.9091\n",
      "Epoch 964/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0371 - acc: 0.9858 - val_loss: 0.3475 - val_acc: 0.9148\n",
      "Epoch 965/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0255 - acc: 0.9915 - val_loss: 0.2741 - val_acc: 0.9034\n",
      "Epoch 966/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0336 - acc: 0.9886 - val_loss: 0.3487 - val_acc: 0.8977\n",
      "Epoch 967/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0250 - acc: 0.9915 - val_loss: 0.3723 - val_acc: 0.9091\n",
      "Epoch 968/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0238 - acc: 0.9929 - val_loss: 0.3617 - val_acc: 0.9034\n",
      "Epoch 969/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0231 - acc: 0.9943 - val_loss: 0.4322 - val_acc: 0.8920\n",
      "Epoch 970/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0219 - acc: 0.9929 - val_loss: 0.4351 - val_acc: 0.8920\n",
      "Epoch 971/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0168 - acc: 0.9972 - val_loss: 0.4167 - val_acc: 0.9034\n",
      "Epoch 972/3000\n",
      "702/702 [==============================] - 0s 584us/sample - loss: 0.0321 - acc: 0.9858 - val_loss: 0.4265 - val_acc: 0.8920\n",
      "Epoch 973/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0312 - acc: 0.9929 - val_loss: 0.3827 - val_acc: 0.8977\n",
      "Epoch 974/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0443 - acc: 0.9872 - val_loss: 0.2946 - val_acc: 0.9261\n",
      "Epoch 975/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0575 - acc: 0.9815 - val_loss: 0.3579 - val_acc: 0.9034\n",
      "Epoch 976/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0357 - acc: 0.9886 - val_loss: 0.3443 - val_acc: 0.8864\n",
      "Epoch 977/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0473 - acc: 0.9858 - val_loss: 0.3182 - val_acc: 0.8807\n",
      "Epoch 978/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0313 - acc: 0.9900 - val_loss: 0.4249 - val_acc: 0.9034\n",
      "Epoch 979/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0524 - acc: 0.9900 - val_loss: 0.3597 - val_acc: 0.9034\n",
      "Epoch 980/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0693 - acc: 0.9772 - val_loss: 0.3175 - val_acc: 0.8977\n",
      "Epoch 981/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0382 - acc: 0.9858 - val_loss: 0.3949 - val_acc: 0.8750\n",
      "Epoch 982/3000\n",
      "702/702 [==============================] - 0s 582us/sample - loss: 0.0432 - acc: 0.9886 - val_loss: 0.4254 - val_acc: 0.9091\n",
      "Epoch 983/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0374 - acc: 0.9929 - val_loss: 0.3016 - val_acc: 0.9148\n",
      "Epoch 984/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0236 - acc: 0.9929 - val_loss: 0.2972 - val_acc: 0.8864\n",
      "Epoch 985/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0278 - acc: 0.9872 - val_loss: 0.3705 - val_acc: 0.9034\n",
      "Epoch 986/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0340 - acc: 0.9915 - val_loss: 0.4266 - val_acc: 0.8864\n",
      "Epoch 987/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0497 - acc: 0.9815 - val_loss: 0.3213 - val_acc: 0.9034\n",
      "Epoch 988/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0264 - acc: 0.9915 - val_loss: 0.3071 - val_acc: 0.9205\n",
      "Epoch 989/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.0333 - acc: 0.9886 - val_loss: 0.3679 - val_acc: 0.9091\n",
      "Epoch 990/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0278 - acc: 0.9900 - val_loss: 0.4135 - val_acc: 0.9034\n",
      "Epoch 991/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0291 - acc: 0.9872 - val_loss: 0.3824 - val_acc: 0.9034\n",
      "Epoch 992/3000\n",
      "702/702 [==============================] - 0s 580us/sample - loss: 0.0206 - acc: 0.9986 - val_loss: 0.3062 - val_acc: 0.8977\n",
      "Epoch 993/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0470 - acc: 0.9815 - val_loss: 0.3365 - val_acc: 0.8864\n",
      "Epoch 994/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0270 - acc: 0.9915 - val_loss: 0.3199 - val_acc: 0.8977\n",
      "Epoch 995/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0205 - acc: 0.9929 - val_loss: 0.3456 - val_acc: 0.9034\n",
      "Epoch 996/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0240 - acc: 0.9915 - val_loss: 0.3175 - val_acc: 0.8977\n",
      "Epoch 997/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 0.0364 - acc: 0.9858 - val_loss: 0.3596 - val_acc: 0.8920\n",
      "Epoch 998/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0197 - acc: 0.9957 - val_loss: 0.3655 - val_acc: 0.9034\n",
      "Epoch 999/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0359 - acc: 0.9900 - val_loss: 0.3517 - val_acc: 0.8977\n",
      "Epoch 1000/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0319 - acc: 0.9929 - val_loss: 0.3304 - val_acc: 0.8920\n",
      "Epoch 1001/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0476 - acc: 0.9858 - val_loss: 0.3102 - val_acc: 0.9034\n",
      "Epoch 1002/3000\n",
      "702/702 [==============================] - 0s 596us/sample - loss: 0.0262 - acc: 0.9915 - val_loss: 0.3762 - val_acc: 0.8920\n",
      "Epoch 1003/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0317 - acc: 0.9900 - val_loss: 0.3967 - val_acc: 0.9034\n",
      "Epoch 1004/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0244 - acc: 0.9886 - val_loss: 0.4171 - val_acc: 0.8920\n",
      "Epoch 1005/3000\n",
      "702/702 [==============================] - 0s 550us/sample - loss: 0.0243 - acc: 0.9915 - val_loss: 0.3528 - val_acc: 0.9091\n",
      "Epoch 1006/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0284 - acc: 0.9886 - val_loss: 0.3246 - val_acc: 0.8920\n",
      "Epoch 1007/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0229 - acc: 0.9915 - val_loss: 0.3606 - val_acc: 0.9034\n",
      "Epoch 1008/3000\n",
      "702/702 [==============================] - 0s 534us/sample - loss: 0.0353 - acc: 0.9900 - val_loss: 0.3511 - val_acc: 0.9034\n",
      "Epoch 1009/3000\n",
      "702/702 [==============================] - 0s 535us/sample - loss: 0.0201 - acc: 0.9943 - val_loss: 0.3751 - val_acc: 0.8977\n",
      "Epoch 1010/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0236 - acc: 0.9943 - val_loss: 0.3791 - val_acc: 0.8920\n",
      "Epoch 1011/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0146 - acc: 0.9972 - val_loss: 0.3834 - val_acc: 0.9034\n",
      "Epoch 1012/3000\n",
      "702/702 [==============================] - 0s 628us/sample - loss: 0.0353 - acc: 0.9858 - val_loss: 0.3505 - val_acc: 0.9091\n",
      "Epoch 1013/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0237 - acc: 0.9915 - val_loss: 0.3672 - val_acc: 0.9091\n",
      "Epoch 1014/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0267 - acc: 0.9943 - val_loss: 0.4060 - val_acc: 0.8977\n",
      "Epoch 1015/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0502 - acc: 0.9843 - val_loss: 0.3098 - val_acc: 0.8920\n",
      "Epoch 1016/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0387 - acc: 0.9872 - val_loss: 0.3077 - val_acc: 0.9034\n",
      "Epoch 1017/3000\n",
      "702/702 [==============================] - 0s 535us/sample - loss: 0.0453 - acc: 0.9872 - val_loss: 0.2775 - val_acc: 0.9205\n",
      "Epoch 1018/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.0707 - acc: 0.9701 - val_loss: 0.2692 - val_acc: 0.9034\n",
      "Epoch 1019/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0691 - acc: 0.9744 - val_loss: 0.4565 - val_acc: 0.8920\n",
      "Epoch 1020/3000\n",
      "702/702 [==============================] - 0s 557us/sample - loss: 0.0583 - acc: 0.9758 - val_loss: 0.3649 - val_acc: 0.9091\n",
      "Epoch 1021/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0613 - acc: 0.9801 - val_loss: 0.3573 - val_acc: 0.9091\n",
      "Epoch 1022/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.0598 - acc: 0.9772 - val_loss: 0.3263 - val_acc: 0.9091\n",
      "Epoch 1023/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0248 - acc: 0.9943 - val_loss: 0.3631 - val_acc: 0.8864\n",
      "Epoch 1024/3000\n",
      "702/702 [==============================] - 0s 545us/sample - loss: 0.0336 - acc: 0.9858 - val_loss: 0.3889 - val_acc: 0.8977\n",
      "Epoch 1025/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0213 - acc: 0.9929 - val_loss: 0.4515 - val_acc: 0.9091\n",
      "Epoch 1026/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0199 - acc: 0.9943 - val_loss: 0.3946 - val_acc: 0.8920\n",
      "Epoch 1027/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 0.0223 - acc: 0.9957 - val_loss: 0.3530 - val_acc: 0.8977\n",
      "Epoch 1028/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0225 - acc: 0.9943 - val_loss: 0.3280 - val_acc: 0.8977\n",
      "Epoch 1029/3000\n",
      "702/702 [==============================] - 0s 550us/sample - loss: 0.0322 - acc: 0.9872 - val_loss: 0.3731 - val_acc: 0.9148\n",
      "Epoch 1030/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0151 - acc: 0.9943 - val_loss: 0.3816 - val_acc: 0.9034\n",
      "Epoch 1031/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0211 - acc: 0.9900 - val_loss: 0.3599 - val_acc: 0.8920\n",
      "Epoch 1032/3000\n",
      "702/702 [==============================] - 0s 604us/sample - loss: 0.0280 - acc: 0.9915 - val_loss: 0.3856 - val_acc: 0.8920\n",
      "Epoch 1033/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 0.0254 - acc: 0.9915 - val_loss: 0.3988 - val_acc: 0.8920\n",
      "Epoch 1034/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.0258 - acc: 0.9886 - val_loss: 0.4897 - val_acc: 0.8977\n",
      "Epoch 1035/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.0125 - acc: 0.9972 - val_loss: 0.5212 - val_acc: 0.8920\n",
      "Epoch 1036/3000\n",
      "702/702 [==============================] - 0s 561us/sample - loss: 0.0524 - acc: 0.9858 - val_loss: 0.3775 - val_acc: 0.8977\n",
      "Epoch 1037/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.0157 - acc: 0.9972 - val_loss: 0.4002 - val_acc: 0.9034\n",
      "Epoch 1038/3000\n",
      "702/702 [==============================] - 0s 542us/sample - loss: 0.0295 - acc: 0.9900 - val_loss: 0.4052 - val_acc: 0.8977\n",
      "Epoch 1039/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0310 - acc: 0.9886 - val_loss: 0.4243 - val_acc: 0.9034\n",
      "Epoch 1040/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0490 - acc: 0.9858 - val_loss: 0.3950 - val_acc: 0.8864\n",
      "Epoch 1041/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0348 - acc: 0.9886 - val_loss: 0.3755 - val_acc: 0.8920\n",
      "Epoch 1042/3000\n",
      "702/702 [==============================] - 0s 586us/sample - loss: 0.0390 - acc: 0.9886 - val_loss: 0.3901 - val_acc: 0.9091\n",
      "Epoch 1043/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0357 - acc: 0.9886 - val_loss: 0.4288 - val_acc: 0.8807\n",
      "Epoch 1044/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0363 - acc: 0.9900 - val_loss: 0.4315 - val_acc: 0.8920\n",
      "Epoch 1045/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0259 - acc: 0.9957 - val_loss: 0.3171 - val_acc: 0.8977\n",
      "Epoch 1046/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0396 - acc: 0.9858 - val_loss: 0.2827 - val_acc: 0.8864\n",
      "Epoch 1047/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0277 - acc: 0.9900 - val_loss: 0.3225 - val_acc: 0.8977\n",
      "Epoch 1048/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0405 - acc: 0.9886 - val_loss: 0.3661 - val_acc: 0.8920\n",
      "Epoch 1049/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0264 - acc: 0.9900 - val_loss: 0.4289 - val_acc: 0.8807\n",
      "Epoch 1050/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0425 - acc: 0.9900 - val_loss: 0.4000 - val_acc: 0.8920\n",
      "Epoch 1051/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0234 - acc: 0.9929 - val_loss: 0.3446 - val_acc: 0.8977\n",
      "Epoch 1052/3000\n",
      "702/702 [==============================] - 0s 587us/sample - loss: 0.0196 - acc: 0.9957 - val_loss: 0.3591 - val_acc: 0.8920\n",
      "Epoch 1053/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0143 - acc: 0.9972 - val_loss: 0.4050 - val_acc: 0.8920\n",
      "Epoch 1054/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0274 - acc: 0.9886 - val_loss: 0.3576 - val_acc: 0.8977\n",
      "Epoch 1055/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0349 - acc: 0.9915 - val_loss: 0.3336 - val_acc: 0.9034\n",
      "Epoch 1056/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0206 - acc: 0.9957 - val_loss: 0.3586 - val_acc: 0.9034\n",
      "Epoch 1057/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0221 - acc: 0.9957 - val_loss: 0.4115 - val_acc: 0.8977\n",
      "Epoch 1058/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0278 - acc: 0.9872 - val_loss: 0.4056 - val_acc: 0.9091\n",
      "Epoch 1059/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0232 - acc: 0.9872 - val_loss: 0.4276 - val_acc: 0.9091\n",
      "Epoch 1060/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0132 - acc: 0.9986 - val_loss: 0.4143 - val_acc: 0.9034\n",
      "Epoch 1061/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0273 - acc: 0.9929 - val_loss: 0.4386 - val_acc: 0.8864\n",
      "Epoch 1062/3000\n",
      "702/702 [==============================] - 0s 586us/sample - loss: 0.0333 - acc: 0.9886 - val_loss: 0.3463 - val_acc: 0.8977\n",
      "Epoch 1063/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0130 - acc: 0.9986 - val_loss: 0.4048 - val_acc: 0.9034\n",
      "Epoch 1064/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0234 - acc: 0.9915 - val_loss: 0.4478 - val_acc: 0.9091\n",
      "Epoch 1065/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0196 - acc: 0.9943 - val_loss: 0.4043 - val_acc: 0.8920\n",
      "Epoch 1066/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0388 - acc: 0.9915 - val_loss: 0.3944 - val_acc: 0.8864\n",
      "Epoch 1067/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0460 - acc: 0.9829 - val_loss: 0.4327 - val_acc: 0.8807\n",
      "Epoch 1068/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0260 - acc: 0.9872 - val_loss: 0.4817 - val_acc: 0.8864\n",
      "Epoch 1069/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0464 - acc: 0.9843 - val_loss: 0.5447 - val_acc: 0.8864\n",
      "Epoch 1070/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0335 - acc: 0.9872 - val_loss: 0.4826 - val_acc: 0.8750\n",
      "Epoch 1071/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0560 - acc: 0.9815 - val_loss: 0.3510 - val_acc: 0.8920\n",
      "Epoch 1072/3000\n",
      "702/702 [==============================] - 0s 582us/sample - loss: 0.0417 - acc: 0.9886 - val_loss: 0.3613 - val_acc: 0.8864\n",
      "Epoch 1073/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0351 - acc: 0.9886 - val_loss: 0.4634 - val_acc: 0.8920\n",
      "Epoch 1074/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0217 - acc: 0.9929 - val_loss: 0.3689 - val_acc: 0.8977\n",
      "Epoch 1075/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0245 - acc: 0.9929 - val_loss: 0.3365 - val_acc: 0.9034\n",
      "Epoch 1076/3000\n",
      "702/702 [==============================] - 0s 537us/sample - loss: 0.0221 - acc: 0.9900 - val_loss: 0.3301 - val_acc: 0.9034\n",
      "Epoch 1077/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0215 - acc: 0.9972 - val_loss: 0.3866 - val_acc: 0.8977\n",
      "Epoch 1078/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0217 - acc: 0.9915 - val_loss: 0.4299 - val_acc: 0.8920\n",
      "Epoch 1079/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0313 - acc: 0.9872 - val_loss: 0.3864 - val_acc: 0.9034\n",
      "Epoch 1080/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0498 - acc: 0.9858 - val_loss: 0.3569 - val_acc: 0.8920\n",
      "Epoch 1081/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0258 - acc: 0.9929 - val_loss: 0.3983 - val_acc: 0.8864\n",
      "Epoch 1082/3000\n",
      "702/702 [==============================] - 0s 584us/sample - loss: 0.0292 - acc: 0.9915 - val_loss: 0.3014 - val_acc: 0.8977\n",
      "Epoch 1083/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0311 - acc: 0.9900 - val_loss: 0.3205 - val_acc: 0.9034\n",
      "Epoch 1084/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0335 - acc: 0.9943 - val_loss: 0.4754 - val_acc: 0.8977\n",
      "Epoch 1085/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0340 - acc: 0.9872 - val_loss: 0.4133 - val_acc: 0.8977\n",
      "Epoch 1086/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0295 - acc: 0.9929 - val_loss: 0.4743 - val_acc: 0.8977\n",
      "Epoch 1087/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0466 - acc: 0.9843 - val_loss: 0.3375 - val_acc: 0.8864\n",
      "Epoch 1088/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0589 - acc: 0.9858 - val_loss: 0.3314 - val_acc: 0.8977\n",
      "Epoch 1089/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0225 - acc: 0.9929 - val_loss: 0.4042 - val_acc: 0.8864\n",
      "Epoch 1090/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0375 - acc: 0.9886 - val_loss: 0.3843 - val_acc: 0.9034\n",
      "Epoch 1091/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0178 - acc: 0.9957 - val_loss: 0.4110 - val_acc: 0.8977\n",
      "Epoch 1092/3000\n",
      "702/702 [==============================] - 0s 583us/sample - loss: 0.0262 - acc: 0.9872 - val_loss: 0.2900 - val_acc: 0.8920\n",
      "Epoch 1093/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0249 - acc: 0.9943 - val_loss: 0.2993 - val_acc: 0.8864\n",
      "Epoch 1094/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0247 - acc: 0.9915 - val_loss: 0.3662 - val_acc: 0.8977\n",
      "Epoch 1095/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0171 - acc: 0.9957 - val_loss: 0.4139 - val_acc: 0.9034\n",
      "Epoch 1096/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0412 - acc: 0.9858 - val_loss: 0.3460 - val_acc: 0.8864\n",
      "Epoch 1097/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0405 - acc: 0.9858 - val_loss: 0.3658 - val_acc: 0.8807\n",
      "Epoch 1098/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0419 - acc: 0.9886 - val_loss: 0.4252 - val_acc: 0.8977\n",
      "Epoch 1099/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0481 - acc: 0.9858 - val_loss: 0.3558 - val_acc: 0.8920\n",
      "Epoch 1100/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0617 - acc: 0.9900 - val_loss: 0.3764 - val_acc: 0.8807\n",
      "Epoch 1101/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0420 - acc: 0.9858 - val_loss: 0.3036 - val_acc: 0.8977\n",
      "Epoch 1102/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.0492 - acc: 0.9929 - val_loss: 0.3450 - val_acc: 0.8920\n",
      "Epoch 1103/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0362 - acc: 0.9915 - val_loss: 0.4477 - val_acc: 0.8807\n",
      "Epoch 1104/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0454 - acc: 0.9829 - val_loss: 0.4208 - val_acc: 0.8693\n",
      "Epoch 1105/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0441 - acc: 0.9829 - val_loss: 0.4989 - val_acc: 0.8750\n",
      "Epoch 1106/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0575 - acc: 0.9843 - val_loss: 0.4986 - val_acc: 0.8864\n",
      "Epoch 1107/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0538 - acc: 0.9786 - val_loss: 0.4639 - val_acc: 0.8580\n",
      "Epoch 1108/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0421 - acc: 0.9858 - val_loss: 0.4092 - val_acc: 0.8977\n",
      "Epoch 1109/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0337 - acc: 0.9872 - val_loss: 0.3562 - val_acc: 0.8977\n",
      "Epoch 1110/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0423 - acc: 0.9858 - val_loss: 0.4028 - val_acc: 0.8920\n",
      "Epoch 1111/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0265 - acc: 0.9929 - val_loss: 0.3410 - val_acc: 0.9034\n",
      "Epoch 1112/3000\n",
      "702/702 [==============================] - 0s 587us/sample - loss: 0.0315 - acc: 0.9843 - val_loss: 0.3091 - val_acc: 0.8920\n",
      "Epoch 1113/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0400 - acc: 0.9900 - val_loss: 0.3645 - val_acc: 0.8864\n",
      "Epoch 1114/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0233 - acc: 0.9886 - val_loss: 0.5346 - val_acc: 0.8807\n",
      "Epoch 1115/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0483 - acc: 0.9815 - val_loss: 0.3557 - val_acc: 0.8920\n",
      "Epoch 1116/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0429 - acc: 0.9858 - val_loss: 0.3166 - val_acc: 0.9091\n",
      "Epoch 1117/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0379 - acc: 0.9886 - val_loss: 0.2775 - val_acc: 0.8977\n",
      "Epoch 1118/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0286 - acc: 0.9915 - val_loss: 0.3216 - val_acc: 0.8977\n",
      "Epoch 1119/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0275 - acc: 0.9900 - val_loss: 0.3007 - val_acc: 0.8977\n",
      "Epoch 1120/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0193 - acc: 0.9943 - val_loss: 0.3241 - val_acc: 0.9148\n",
      "Epoch 1121/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0298 - acc: 0.9915 - val_loss: 0.3637 - val_acc: 0.9091\n",
      "Epoch 1122/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 0.0178 - acc: 0.9929 - val_loss: 0.3933 - val_acc: 0.9091\n",
      "Epoch 1123/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0294 - acc: 0.9900 - val_loss: 0.3681 - val_acc: 0.8864\n",
      "Epoch 1124/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0219 - acc: 0.9886 - val_loss: 0.3479 - val_acc: 0.8977\n",
      "Epoch 1125/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0265 - acc: 0.9915 - val_loss: 0.3302 - val_acc: 0.8920\n",
      "Epoch 1126/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0129 - acc: 0.9972 - val_loss: 0.3360 - val_acc: 0.9034\n",
      "Epoch 1127/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0248 - acc: 0.9929 - val_loss: 0.3707 - val_acc: 0.9034\n",
      "Epoch 1128/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0253 - acc: 0.9915 - val_loss: 0.4060 - val_acc: 0.8977\n",
      "Epoch 1129/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0163 - acc: 0.9943 - val_loss: 0.3707 - val_acc: 0.9091\n",
      "Epoch 1130/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0170 - acc: 0.9957 - val_loss: 0.3834 - val_acc: 0.8977\n",
      "Epoch 1131/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0255 - acc: 0.9929 - val_loss: 0.3993 - val_acc: 0.8864\n",
      "Epoch 1132/3000\n",
      "702/702 [==============================] - 0s 569us/sample - loss: 0.0366 - acc: 0.9915 - val_loss: 0.4446 - val_acc: 0.9148\n",
      "Epoch 1133/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0372 - acc: 0.9886 - val_loss: 0.5182 - val_acc: 0.9034\n",
      "Epoch 1134/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0274 - acc: 0.9972 - val_loss: 0.4514 - val_acc: 0.9091\n",
      "Epoch 1135/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0208 - acc: 0.9943 - val_loss: 0.3759 - val_acc: 0.8920\n",
      "Epoch 1136/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0221 - acc: 0.9900 - val_loss: 0.3998 - val_acc: 0.8864\n",
      "Epoch 1137/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0301 - acc: 0.9915 - val_loss: 0.3841 - val_acc: 0.8864\n",
      "Epoch 1138/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0315 - acc: 0.9943 - val_loss: 0.3002 - val_acc: 0.8977\n",
      "Epoch 1139/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0409 - acc: 0.9843 - val_loss: 0.2879 - val_acc: 0.9091\n",
      "Epoch 1140/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0302 - acc: 0.9915 - val_loss: 0.4111 - val_acc: 0.8864\n",
      "Epoch 1141/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0262 - acc: 0.9915 - val_loss: 0.4291 - val_acc: 0.8920\n",
      "Epoch 1142/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 0.0184 - acc: 0.9957 - val_loss: 0.3893 - val_acc: 0.8920\n",
      "Epoch 1143/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0160 - acc: 0.9943 - val_loss: 0.3749 - val_acc: 0.8807\n",
      "Epoch 1144/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0238 - acc: 0.9929 - val_loss: 0.3698 - val_acc: 0.8864\n",
      "Epoch 1145/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0134 - acc: 0.9972 - val_loss: 0.3487 - val_acc: 0.9034\n",
      "Epoch 1146/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0237 - acc: 0.9915 - val_loss: 0.3515 - val_acc: 0.9034\n",
      "Epoch 1147/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0193 - acc: 0.9943 - val_loss: 0.3266 - val_acc: 0.9034\n",
      "Epoch 1148/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0275 - acc: 0.9915 - val_loss: 0.3741 - val_acc: 0.8977\n",
      "Epoch 1149/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0336 - acc: 0.9886 - val_loss: 0.3531 - val_acc: 0.8864\n",
      "Epoch 1150/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0270 - acc: 0.9915 - val_loss: 0.3404 - val_acc: 0.8920\n",
      "Epoch 1151/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0384 - acc: 0.9929 - val_loss: 0.3770 - val_acc: 0.8977\n",
      "Epoch 1152/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.0178 - acc: 0.9943 - val_loss: 0.2953 - val_acc: 0.9034\n",
      "Epoch 1153/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0329 - acc: 0.9900 - val_loss: 0.4422 - val_acc: 0.8920\n",
      "Epoch 1154/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0300 - acc: 0.9872 - val_loss: 0.3351 - val_acc: 0.9034\n",
      "Epoch 1155/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0315 - acc: 0.9915 - val_loss: 0.2928 - val_acc: 0.9091\n",
      "Epoch 1156/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0367 - acc: 0.9929 - val_loss: 0.2617 - val_acc: 0.9091\n",
      "Epoch 1157/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0360 - acc: 0.9829 - val_loss: 0.2644 - val_acc: 0.9034\n",
      "Epoch 1158/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0214 - acc: 0.9957 - val_loss: 0.2978 - val_acc: 0.8920\n",
      "Epoch 1159/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0291 - acc: 0.9872 - val_loss: 0.2912 - val_acc: 0.8864\n",
      "Epoch 1160/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0258 - acc: 0.9943 - val_loss: 0.3074 - val_acc: 0.9091\n",
      "Epoch 1161/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0327 - acc: 0.9915 - val_loss: 0.4047 - val_acc: 0.8977\n",
      "Epoch 1162/3000\n",
      "702/702 [==============================] - 0s 577us/sample - loss: 0.0358 - acc: 0.9886 - val_loss: 0.5091 - val_acc: 0.8977\n",
      "Epoch 1163/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0423 - acc: 0.9829 - val_loss: 0.4341 - val_acc: 0.8864\n",
      "Epoch 1164/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0224 - acc: 0.9929 - val_loss: 0.4463 - val_acc: 0.8864\n",
      "Epoch 1165/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0233 - acc: 0.9943 - val_loss: 0.3386 - val_acc: 0.8977\n",
      "Epoch 1166/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0296 - acc: 0.9929 - val_loss: 0.3818 - val_acc: 0.8920\n",
      "Epoch 1167/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0254 - acc: 0.9915 - val_loss: 0.3530 - val_acc: 0.8920\n",
      "Epoch 1168/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0401 - acc: 0.9858 - val_loss: 0.3951 - val_acc: 0.9034\n",
      "Epoch 1169/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0291 - acc: 0.9943 - val_loss: 0.3354 - val_acc: 0.8920\n",
      "Epoch 1170/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0277 - acc: 0.9886 - val_loss: 0.3318 - val_acc: 0.8920\n",
      "Epoch 1171/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0313 - acc: 0.9872 - val_loss: 0.3587 - val_acc: 0.8977\n",
      "Epoch 1172/3000\n",
      "702/702 [==============================] - 0s 586us/sample - loss: 0.0357 - acc: 0.9900 - val_loss: 0.3592 - val_acc: 0.8920\n",
      "Epoch 1173/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0142 - acc: 0.9957 - val_loss: 0.2975 - val_acc: 0.8920\n",
      "Epoch 1174/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0148 - acc: 0.9957 - val_loss: 0.3114 - val_acc: 0.8920\n",
      "Epoch 1175/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0272 - acc: 0.9915 - val_loss: 0.3649 - val_acc: 0.8864\n",
      "Epoch 1176/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0292 - acc: 0.9900 - val_loss: 0.2889 - val_acc: 0.8977\n",
      "Epoch 1177/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0250 - acc: 0.9929 - val_loss: 0.3002 - val_acc: 0.9034\n",
      "Epoch 1178/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0216 - acc: 0.9943 - val_loss: 0.3670 - val_acc: 0.8864\n",
      "Epoch 1179/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0112 - acc: 0.9972 - val_loss: 0.5021 - val_acc: 0.8977\n",
      "Epoch 1180/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0276 - acc: 0.9929 - val_loss: 0.3853 - val_acc: 0.8920\n",
      "Epoch 1181/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0155 - acc: 0.9943 - val_loss: 0.3644 - val_acc: 0.8920\n",
      "Epoch 1182/3000\n",
      "702/702 [==============================] - 0s 578us/sample - loss: 0.0245 - acc: 0.9929 - val_loss: 0.3698 - val_acc: 0.8807\n",
      "Epoch 1183/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0165 - acc: 0.9972 - val_loss: 0.4118 - val_acc: 0.8920\n",
      "Epoch 1184/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0175 - acc: 0.9957 - val_loss: 0.3880 - val_acc: 0.8977\n",
      "Epoch 1185/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0194 - acc: 0.9929 - val_loss: 0.3632 - val_acc: 0.8977\n",
      "Epoch 1186/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0326 - acc: 0.9915 - val_loss: 0.3852 - val_acc: 0.8920\n",
      "Epoch 1187/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0210 - acc: 0.9943 - val_loss: 0.4266 - val_acc: 0.8920\n",
      "Epoch 1188/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0259 - acc: 0.9929 - val_loss: 0.3515 - val_acc: 0.9091\n",
      "Epoch 1189/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0226 - acc: 0.9915 - val_loss: 0.3393 - val_acc: 0.8977\n",
      "Epoch 1190/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0149 - acc: 0.9943 - val_loss: 0.3094 - val_acc: 0.8977\n",
      "Epoch 1191/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0119 - acc: 0.9986 - val_loss: 0.3219 - val_acc: 0.9091\n",
      "Epoch 1192/3000\n",
      "702/702 [==============================] - 0s 607us/sample - loss: 0.0269 - acc: 0.9929 - val_loss: 0.3742 - val_acc: 0.9034\n",
      "Epoch 1193/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.0200 - acc: 0.9943 - val_loss: 0.3942 - val_acc: 0.8977\n",
      "Epoch 1194/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0271 - acc: 0.9929 - val_loss: 0.3817 - val_acc: 0.8977\n",
      "Epoch 1195/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0197 - acc: 0.9915 - val_loss: 0.3643 - val_acc: 0.9034\n",
      "Epoch 1196/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0122 - acc: 0.9943 - val_loss: 0.4010 - val_acc: 0.9034\n",
      "Epoch 1197/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.0088 - acc: 0.9972 - val_loss: 0.4369 - val_acc: 0.8977\n",
      "Epoch 1198/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0144 - acc: 0.9957 - val_loss: 0.4248 - val_acc: 0.8977\n",
      "Epoch 1199/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0155 - acc: 0.9943 - val_loss: 0.3953 - val_acc: 0.8977\n",
      "Epoch 1200/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0382 - acc: 0.9858 - val_loss: 0.3632 - val_acc: 0.8977\n",
      "Epoch 1201/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0350 - acc: 0.9886 - val_loss: 0.3504 - val_acc: 0.8864\n",
      "Epoch 1202/3000\n",
      "702/702 [==============================] - 0s 578us/sample - loss: 0.0363 - acc: 0.9872 - val_loss: 0.4100 - val_acc: 0.8920\n",
      "Epoch 1203/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0644 - acc: 0.9829 - val_loss: 0.4525 - val_acc: 0.8750\n",
      "Epoch 1204/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0415 - acc: 0.9843 - val_loss: 0.4396 - val_acc: 0.8864\n",
      "Epoch 1205/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0294 - acc: 0.9858 - val_loss: 0.3388 - val_acc: 0.9034\n",
      "Epoch 1206/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0595 - acc: 0.9843 - val_loss: 0.4301 - val_acc: 0.9034\n",
      "Epoch 1207/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0720 - acc: 0.9758 - val_loss: 0.3303 - val_acc: 0.8977\n",
      "Epoch 1208/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0809 - acc: 0.9687 - val_loss: 0.2754 - val_acc: 0.9034\n",
      "Epoch 1209/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0432 - acc: 0.9843 - val_loss: 0.4871 - val_acc: 0.8864\n",
      "Epoch 1210/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0685 - acc: 0.9801 - val_loss: 0.3389 - val_acc: 0.9034\n",
      "Epoch 1211/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0919 - acc: 0.9758 - val_loss: 0.3922 - val_acc: 0.8523\n",
      "Epoch 1212/3000\n",
      "702/702 [==============================] - 0s 588us/sample - loss: 0.0802 - acc: 0.9729 - val_loss: 0.3206 - val_acc: 0.8920\n",
      "Epoch 1213/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0716 - acc: 0.9815 - val_loss: 0.3067 - val_acc: 0.8977\n",
      "Epoch 1214/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0283 - acc: 0.9915 - val_loss: 0.2694 - val_acc: 0.9091\n",
      "Epoch 1215/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0224 - acc: 0.9943 - val_loss: 0.3462 - val_acc: 0.8864\n",
      "Epoch 1216/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0400 - acc: 0.9815 - val_loss: 0.3453 - val_acc: 0.8977\n",
      "Epoch 1217/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0468 - acc: 0.9801 - val_loss: 0.2880 - val_acc: 0.8920\n",
      "Epoch 1218/3000\n",
      "702/702 [==============================] - 0s 538us/sample - loss: 0.0555 - acc: 0.9915 - val_loss: 0.2855 - val_acc: 0.8920\n",
      "Epoch 1219/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.0340 - acc: 0.9858 - val_loss: 0.3262 - val_acc: 0.8920\n",
      "Epoch 1220/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0219 - acc: 0.9957 - val_loss: 0.3176 - val_acc: 0.8977\n",
      "Epoch 1221/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0150 - acc: 0.9972 - val_loss: 0.3812 - val_acc: 0.8864\n",
      "Epoch 1222/3000\n",
      "702/702 [==============================] - 0s 593us/sample - loss: 0.0199 - acc: 0.9929 - val_loss: 0.3752 - val_acc: 0.8864\n",
      "Epoch 1223/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0168 - acc: 0.9957 - val_loss: 0.3388 - val_acc: 0.8977\n",
      "Epoch 1224/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0237 - acc: 0.9943 - val_loss: 0.3591 - val_acc: 0.9034\n",
      "Epoch 1225/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.0309 - acc: 0.9900 - val_loss: 0.3313 - val_acc: 0.8920\n",
      "Epoch 1226/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0536 - acc: 0.9858 - val_loss: 0.3949 - val_acc: 0.8977\n",
      "Epoch 1227/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0210 - acc: 0.9957 - val_loss: 0.4220 - val_acc: 0.8807\n",
      "Epoch 1228/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0512 - acc: 0.9843 - val_loss: 0.3157 - val_acc: 0.8920\n",
      "Epoch 1229/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0299 - acc: 0.9929 - val_loss: 0.3184 - val_acc: 0.9091\n",
      "Epoch 1230/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0316 - acc: 0.9943 - val_loss: 0.2798 - val_acc: 0.9148\n",
      "Epoch 1231/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0207 - acc: 0.9972 - val_loss: 0.2645 - val_acc: 0.9091\n",
      "Epoch 1232/3000\n",
      "702/702 [==============================] - 0s 611us/sample - loss: 0.0165 - acc: 0.9972 - val_loss: 0.3626 - val_acc: 0.8977\n",
      "Epoch 1233/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0300 - acc: 0.9929 - val_loss: 0.3267 - val_acc: 0.9034\n",
      "Epoch 1234/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0217 - acc: 0.9929 - val_loss: 0.2977 - val_acc: 0.9034\n",
      "Epoch 1235/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0259 - acc: 0.9915 - val_loss: 0.3586 - val_acc: 0.9034\n",
      "Epoch 1236/3000\n",
      "702/702 [==============================] - 0s 544us/sample - loss: 0.0149 - acc: 0.9986 - val_loss: 0.3921 - val_acc: 0.8920\n",
      "Epoch 1237/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0100 - acc: 0.9957 - val_loss: 0.3725 - val_acc: 0.8920\n",
      "Epoch 1238/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0317 - acc: 0.9900 - val_loss: 0.3481 - val_acc: 0.9034\n",
      "Epoch 1239/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0291 - acc: 0.9929 - val_loss: 0.3519 - val_acc: 0.9034\n",
      "Epoch 1240/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0132 - acc: 0.9972 - val_loss: 0.3846 - val_acc: 0.8920\n",
      "Epoch 1241/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0212 - acc: 0.9943 - val_loss: 0.3483 - val_acc: 0.8977\n",
      "Epoch 1242/3000\n",
      "702/702 [==============================] - 0s 609us/sample - loss: 0.0114 - acc: 0.9986 - val_loss: 0.3661 - val_acc: 0.8864\n",
      "Epoch 1243/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0157 - acc: 0.9943 - val_loss: 0.3929 - val_acc: 0.8750\n",
      "Epoch 1244/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0136 - acc: 0.9972 - val_loss: 0.3688 - val_acc: 0.8864\n",
      "Epoch 1245/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0174 - acc: 0.9957 - val_loss: 0.3405 - val_acc: 0.9034\n",
      "Epoch 1246/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0183 - acc: 0.9943 - val_loss: 0.3593 - val_acc: 0.8920\n",
      "Epoch 1247/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0156 - acc: 0.9943 - val_loss: 0.3725 - val_acc: 0.8864\n",
      "Epoch 1248/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0131 - acc: 0.9986 - val_loss: 0.3295 - val_acc: 0.8977\n",
      "Epoch 1249/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0200 - acc: 0.9943 - val_loss: 0.3290 - val_acc: 0.8977\n",
      "Epoch 1250/3000\n",
      "702/702 [==============================] - 0s 550us/sample - loss: 0.0113 - acc: 0.9957 - val_loss: 0.3791 - val_acc: 0.8920\n",
      "Epoch 1251/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0230 - acc: 0.9943 - val_loss: 0.3334 - val_acc: 0.8977\n",
      "Epoch 1252/3000\n",
      "702/702 [==============================] - 0s 591us/sample - loss: 0.0091 - acc: 0.9986 - val_loss: 0.3175 - val_acc: 0.9091\n",
      "Epoch 1253/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.0137 - acc: 0.9957 - val_loss: 0.3547 - val_acc: 0.9091\n",
      "Epoch 1254/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0291 - acc: 0.9900 - val_loss: 0.4670 - val_acc: 0.9034\n",
      "Epoch 1255/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0218 - acc: 0.9929 - val_loss: 0.3857 - val_acc: 0.9148\n",
      "Epoch 1256/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0082 - acc: 0.9986 - val_loss: 0.3125 - val_acc: 0.8977\n",
      "Epoch 1257/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0205 - acc: 0.9943 - val_loss: 0.2966 - val_acc: 0.8977\n",
      "Epoch 1258/3000\n",
      "702/702 [==============================] - 0s 544us/sample - loss: 0.0122 - acc: 0.9972 - val_loss: 0.3796 - val_acc: 0.9034\n",
      "Epoch 1259/3000\n",
      "702/702 [==============================] - 0s 546us/sample - loss: 0.0141 - acc: 0.9957 - val_loss: 0.4163 - val_acc: 0.8920\n",
      "Epoch 1260/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0183 - acc: 0.9943 - val_loss: 0.3919 - val_acc: 0.9034\n",
      "Epoch 1261/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0275 - acc: 0.9943 - val_loss: 0.3313 - val_acc: 0.9034\n",
      "Epoch 1262/3000\n",
      "702/702 [==============================] - 0s 625us/sample - loss: 0.0175 - acc: 0.9943 - val_loss: 0.3428 - val_acc: 0.8920\n",
      "Epoch 1263/3000\n",
      "702/702 [==============================] - 0s 543us/sample - loss: 0.0225 - acc: 0.9929 - val_loss: 0.3952 - val_acc: 0.9091\n",
      "Epoch 1264/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0119 - acc: 0.9957 - val_loss: 0.4472 - val_acc: 0.8977\n",
      "Epoch 1265/3000\n",
      "702/702 [==============================] - 0s 551us/sample - loss: 0.0108 - acc: 0.9972 - val_loss: 0.3702 - val_acc: 0.8977\n",
      "Epoch 1266/3000\n",
      "702/702 [==============================] - 0s 568us/sample - loss: 0.0203 - acc: 0.9915 - val_loss: 0.3955 - val_acc: 0.8920\n",
      "Epoch 1267/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0389 - acc: 0.9915 - val_loss: 0.4363 - val_acc: 0.8864\n",
      "Epoch 1268/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.0208 - acc: 0.9972 - val_loss: 0.4584 - val_acc: 0.8977\n",
      "Epoch 1269/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0254 - acc: 0.9915 - val_loss: 0.4369 - val_acc: 0.9034\n",
      "Epoch 1270/3000\n",
      "702/702 [==============================] - 0s 546us/sample - loss: 0.0238 - acc: 0.9929 - val_loss: 0.3959 - val_acc: 0.8977\n",
      "Epoch 1271/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0355 - acc: 0.9929 - val_loss: 0.3427 - val_acc: 0.8920\n",
      "Epoch 1272/3000\n",
      "702/702 [==============================] - 0s 607us/sample - loss: 0.0262 - acc: 0.9943 - val_loss: 0.3105 - val_acc: 0.9034\n",
      "Epoch 1273/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0208 - acc: 0.9886 - val_loss: 0.3453 - val_acc: 0.8920\n",
      "Epoch 1274/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0093 - acc: 0.9972 - val_loss: 0.3944 - val_acc: 0.8977\n",
      "Epoch 1275/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0192 - acc: 0.9943 - val_loss: 0.4113 - val_acc: 0.8977\n",
      "Epoch 1276/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0247 - acc: 0.9943 - val_loss: 0.5017 - val_acc: 0.8920\n",
      "Epoch 1277/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0076 - acc: 0.9986 - val_loss: 0.5363 - val_acc: 0.9034\n",
      "Epoch 1278/3000\n",
      "702/702 [==============================] - 0s 553us/sample - loss: 0.0313 - acc: 0.9900 - val_loss: 0.4008 - val_acc: 0.8977\n",
      "Epoch 1279/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 0.0259 - acc: 0.9943 - val_loss: 0.3525 - val_acc: 0.8864\n",
      "Epoch 1280/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0380 - acc: 0.9872 - val_loss: 0.3996 - val_acc: 0.8920\n",
      "Epoch 1281/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0452 - acc: 0.9915 - val_loss: 0.4050 - val_acc: 0.8977\n",
      "Epoch 1282/3000\n",
      "702/702 [==============================] - 0s 588us/sample - loss: 0.0298 - acc: 0.9929 - val_loss: 0.4145 - val_acc: 0.9091\n",
      "Epoch 1283/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0192 - acc: 0.9943 - val_loss: 0.4880 - val_acc: 0.8977\n",
      "Epoch 1284/3000\n",
      "702/702 [==============================] - 0s 546us/sample - loss: 0.0208 - acc: 0.9957 - val_loss: 0.5103 - val_acc: 0.8920\n",
      "Epoch 1285/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0453 - acc: 0.9915 - val_loss: 0.4345 - val_acc: 0.8750\n",
      "Epoch 1286/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0191 - acc: 0.9943 - val_loss: 0.3790 - val_acc: 0.9091\n",
      "Epoch 1287/3000\n",
      "702/702 [==============================] - 0s 549us/sample - loss: 0.0161 - acc: 0.9929 - val_loss: 0.3948 - val_acc: 0.9091\n",
      "Epoch 1288/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0131 - acc: 0.9972 - val_loss: 0.4767 - val_acc: 0.8977\n",
      "Epoch 1289/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0230 - acc: 0.9915 - val_loss: 0.3967 - val_acc: 0.8920\n",
      "Epoch 1290/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0227 - acc: 0.9943 - val_loss: 0.4130 - val_acc: 0.9034\n",
      "Epoch 1291/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0262 - acc: 0.9915 - val_loss: 0.4703 - val_acc: 0.8977\n",
      "Epoch 1292/3000\n",
      "702/702 [==============================] - 0s 616us/sample - loss: 0.0284 - acc: 0.9915 - val_loss: 0.5547 - val_acc: 0.8920\n",
      "Epoch 1293/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0338 - acc: 0.9872 - val_loss: 0.4604 - val_acc: 0.8977\n",
      "Epoch 1294/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0137 - acc: 0.9972 - val_loss: 0.4432 - val_acc: 0.8920\n",
      "Epoch 1295/3000\n",
      "702/702 [==============================] - 0s 542us/sample - loss: 0.0148 - acc: 0.9957 - val_loss: 0.5215 - val_acc: 0.8920\n",
      "Epoch 1296/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0126 - acc: 0.9957 - val_loss: 0.6434 - val_acc: 0.8807\n",
      "Epoch 1297/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0121 - acc: 0.9957 - val_loss: 0.6036 - val_acc: 0.8920\n",
      "Epoch 1298/3000\n",
      "702/702 [==============================] - 0s 549us/sample - loss: 0.0201 - acc: 0.9957 - val_loss: 0.4816 - val_acc: 0.8920\n",
      "Epoch 1299/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0161 - acc: 0.9943 - val_loss: 0.4176 - val_acc: 0.8977\n",
      "Epoch 1300/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0195 - acc: 0.9972 - val_loss: 0.4686 - val_acc: 0.8636\n",
      "Epoch 1301/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0385 - acc: 0.9858 - val_loss: 0.5160 - val_acc: 0.8920\n",
      "Epoch 1302/3000\n",
      "702/702 [==============================] - 0s 582us/sample - loss: 0.0307 - acc: 0.9929 - val_loss: 0.3729 - val_acc: 0.9034\n",
      "Epoch 1303/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0360 - acc: 0.9886 - val_loss: 0.4082 - val_acc: 0.8977\n",
      "Epoch 1304/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0176 - acc: 0.9957 - val_loss: 0.4681 - val_acc: 0.8977\n",
      "Epoch 1305/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0429 - acc: 0.9858 - val_loss: 0.3739 - val_acc: 0.8977\n",
      "Epoch 1306/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0259 - acc: 0.9900 - val_loss: 0.4194 - val_acc: 0.9034\n",
      "Epoch 1307/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0290 - acc: 0.9886 - val_loss: 0.3796 - val_acc: 0.9034\n",
      "Epoch 1308/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.0246 - acc: 0.9915 - val_loss: 0.3511 - val_acc: 0.8977\n",
      "Epoch 1309/3000\n",
      "702/702 [==============================] - 0s 548us/sample - loss: 0.0362 - acc: 0.9957 - val_loss: 0.3806 - val_acc: 0.8977\n",
      "Epoch 1310/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0258 - acc: 0.9915 - val_loss: 0.3876 - val_acc: 0.8807\n",
      "Epoch 1311/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0147 - acc: 0.9957 - val_loss: 0.4382 - val_acc: 0.8693\n",
      "Epoch 1312/3000\n",
      "702/702 [==============================] - 0s 584us/sample - loss: 0.0149 - acc: 0.9972 - val_loss: 0.4011 - val_acc: 0.8920\n",
      "Epoch 1313/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0195 - acc: 0.9957 - val_loss: 0.3338 - val_acc: 0.9034\n",
      "Epoch 1314/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0299 - acc: 0.9915 - val_loss: 0.2968 - val_acc: 0.9091\n",
      "Epoch 1315/3000\n",
      "702/702 [==============================] - 0s 538us/sample - loss: 0.0199 - acc: 0.9943 - val_loss: 0.3465 - val_acc: 0.8977\n",
      "Epoch 1316/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0131 - acc: 0.9972 - val_loss: 0.4629 - val_acc: 0.8920\n",
      "Epoch 1317/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0195 - acc: 0.9943 - val_loss: 0.4243 - val_acc: 0.8977\n",
      "Epoch 1318/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.0189 - acc: 0.9929 - val_loss: 0.4187 - val_acc: 0.8920\n",
      "Epoch 1319/3000\n",
      "702/702 [==============================] - 0s 543us/sample - loss: 0.0155 - acc: 0.9943 - val_loss: 0.5392 - val_acc: 0.8977\n",
      "Epoch 1320/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0195 - acc: 0.9900 - val_loss: 0.4733 - val_acc: 0.8920\n",
      "Epoch 1321/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0146 - acc: 0.9929 - val_loss: 0.5028 - val_acc: 0.8864\n",
      "Epoch 1322/3000\n",
      "702/702 [==============================] - 0s 584us/sample - loss: 0.0137 - acc: 0.9957 - val_loss: 0.5692 - val_acc: 0.9091\n",
      "Epoch 1323/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0382 - acc: 0.9929 - val_loss: 0.3833 - val_acc: 0.8977\n",
      "Epoch 1324/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0230 - acc: 0.9915 - val_loss: 0.3730 - val_acc: 0.8977\n",
      "Epoch 1325/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0213 - acc: 0.9957 - val_loss: 0.4169 - val_acc: 0.8864\n",
      "Epoch 1326/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0180 - acc: 0.9957 - val_loss: 0.4264 - val_acc: 0.8807\n",
      "Epoch 1327/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0171 - acc: 0.9943 - val_loss: 0.4144 - val_acc: 0.8920\n",
      "Epoch 1328/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0112 - acc: 0.9957 - val_loss: 0.4102 - val_acc: 0.8977\n",
      "Epoch 1329/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0103 - acc: 0.9972 - val_loss: 0.3858 - val_acc: 0.9034\n",
      "Epoch 1330/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0153 - acc: 0.9957 - val_loss: 0.4591 - val_acc: 0.9091\n",
      "Epoch 1331/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0176 - acc: 0.9957 - val_loss: 0.4905 - val_acc: 0.9091\n",
      "Epoch 1332/3000\n",
      "702/702 [==============================] - 0s 591us/sample - loss: 0.0168 - acc: 0.9943 - val_loss: 0.3786 - val_acc: 0.8920\n",
      "Epoch 1333/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.0191 - acc: 0.9957 - val_loss: 0.3561 - val_acc: 0.8864\n",
      "Epoch 1334/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.0153 - acc: 0.9943 - val_loss: 0.4032 - val_acc: 0.8977\n",
      "Epoch 1335/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0083 - acc: 0.9986 - val_loss: 0.4656 - val_acc: 0.9091\n",
      "Epoch 1336/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0122 - acc: 0.9943 - val_loss: 0.4503 - val_acc: 0.9034\n",
      "Epoch 1337/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0194 - acc: 0.9929 - val_loss: 0.4224 - val_acc: 0.9034\n",
      "Epoch 1338/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.0100 - acc: 0.9972 - val_loss: 0.3715 - val_acc: 0.8920\n",
      "Epoch 1339/3000\n",
      "702/702 [==============================] - 0s 538us/sample - loss: 0.0239 - acc: 0.9943 - val_loss: 0.5325 - val_acc: 0.8864\n",
      "Epoch 1340/3000\n",
      "702/702 [==============================] - 0s 540us/sample - loss: 0.0166 - acc: 0.9957 - val_loss: 0.4461 - val_acc: 0.8920\n",
      "Epoch 1341/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0122 - acc: 0.9957 - val_loss: 0.3439 - val_acc: 0.8977\n",
      "Epoch 1342/3000\n",
      "702/702 [==============================] - 0s 583us/sample - loss: 0.0160 - acc: 0.9929 - val_loss: 0.4190 - val_acc: 0.8750\n",
      "Epoch 1343/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0222 - acc: 0.9872 - val_loss: 0.5194 - val_acc: 0.9091\n",
      "Epoch 1344/3000\n",
      "702/702 [==============================] - 0s 561us/sample - loss: 0.0158 - acc: 0.9915 - val_loss: 0.4668 - val_acc: 0.8920\n",
      "Epoch 1345/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0287 - acc: 0.9915 - val_loss: 0.3997 - val_acc: 0.8864\n",
      "Epoch 1346/3000\n",
      "702/702 [==============================] - 0s 552us/sample - loss: 0.0088 - acc: 0.9972 - val_loss: 0.4293 - val_acc: 0.8864\n",
      "Epoch 1347/3000\n",
      "702/702 [==============================] - 0s 555us/sample - loss: 0.0177 - acc: 0.9929 - val_loss: 0.4999 - val_acc: 0.8920\n",
      "Epoch 1348/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0214 - acc: 0.9929 - val_loss: 0.4857 - val_acc: 0.8864\n",
      "Epoch 1349/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0280 - acc: 0.9915 - val_loss: 0.3926 - val_acc: 0.8864\n",
      "Epoch 1350/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0839 - acc: 0.9744 - val_loss: 0.3589 - val_acc: 0.8864\n",
      "Epoch 1351/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0458 - acc: 0.9858 - val_loss: 0.4572 - val_acc: 0.9034\n",
      "Epoch 1352/3000\n",
      "702/702 [==============================] - 0s 568us/sample - loss: 0.0662 - acc: 0.9843 - val_loss: 0.4031 - val_acc: 0.9034\n",
      "Epoch 1353/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0226 - acc: 0.9929 - val_loss: 0.3270 - val_acc: 0.8920\n",
      "Epoch 1354/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0386 - acc: 0.9872 - val_loss: 0.3902 - val_acc: 0.8977\n",
      "Epoch 1355/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0177 - acc: 0.9929 - val_loss: 0.4885 - val_acc: 0.9034\n",
      "Epoch 1356/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0141 - acc: 0.9929 - val_loss: 0.5476 - val_acc: 0.8977\n",
      "Epoch 1357/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0203 - acc: 0.9915 - val_loss: 0.4444 - val_acc: 0.8977\n",
      "Epoch 1358/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0173 - acc: 0.9929 - val_loss: 0.3888 - val_acc: 0.9091\n",
      "Epoch 1359/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0154 - acc: 0.9957 - val_loss: 0.4531 - val_acc: 0.8864\n",
      "Epoch 1360/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0235 - acc: 0.9900 - val_loss: 0.5451 - val_acc: 0.9034\n",
      "Epoch 1361/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0148 - acc: 0.9957 - val_loss: 0.4884 - val_acc: 0.8864\n",
      "Epoch 1362/3000\n",
      "702/702 [==============================] - 0s 602us/sample - loss: 0.0184 - acc: 0.9900 - val_loss: 0.4084 - val_acc: 0.8920\n",
      "Epoch 1363/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0218 - acc: 0.9957 - val_loss: 0.5167 - val_acc: 0.8977\n",
      "Epoch 1364/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0402 - acc: 0.9872 - val_loss: 0.4271 - val_acc: 0.8977\n",
      "Epoch 1365/3000\n",
      "702/702 [==============================] - 0s 538us/sample - loss: 0.0346 - acc: 0.9886 - val_loss: 0.4832 - val_acc: 0.8977\n",
      "Epoch 1366/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0266 - acc: 0.9915 - val_loss: 0.3564 - val_acc: 0.9148\n",
      "Epoch 1367/3000\n",
      "702/702 [==============================] - 0s 537us/sample - loss: 0.0337 - acc: 0.9929 - val_loss: 0.3448 - val_acc: 0.9091\n",
      "Epoch 1368/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0542 - acc: 0.9872 - val_loss: 0.3742 - val_acc: 0.8750\n",
      "Epoch 1369/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0238 - acc: 0.9929 - val_loss: 0.4703 - val_acc: 0.8977\n",
      "Epoch 1370/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0341 - acc: 0.9900 - val_loss: 0.3959 - val_acc: 0.8977\n",
      "Epoch 1371/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0317 - acc: 0.9900 - val_loss: 0.3761 - val_acc: 0.9091\n",
      "Epoch 1372/3000\n",
      "702/702 [==============================] - 0s 588us/sample - loss: 0.0159 - acc: 0.9929 - val_loss: 0.4133 - val_acc: 0.8750\n",
      "Epoch 1373/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0283 - acc: 0.9929 - val_loss: 0.5026 - val_acc: 0.8920\n",
      "Epoch 1374/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0249 - acc: 0.9929 - val_loss: 0.3880 - val_acc: 0.9034\n",
      "Epoch 1375/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0213 - acc: 0.9957 - val_loss: 0.3631 - val_acc: 0.9034\n",
      "Epoch 1376/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0221 - acc: 0.9943 - val_loss: 0.3522 - val_acc: 0.8864\n",
      "Epoch 1377/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.0175 - acc: 0.9943 - val_loss: 0.3466 - val_acc: 0.8864\n",
      "Epoch 1378/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.0116 - acc: 0.9957 - val_loss: 0.3695 - val_acc: 0.8864\n",
      "Epoch 1379/3000\n",
      "702/702 [==============================] - 0s 553us/sample - loss: 0.0151 - acc: 0.9943 - val_loss: 0.4314 - val_acc: 0.8920\n",
      "Epoch 1380/3000\n",
      "702/702 [==============================] - 0s 540us/sample - loss: 0.0216 - acc: 0.9929 - val_loss: 0.4027 - val_acc: 0.8977\n",
      "Epoch 1381/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0310 - acc: 0.9915 - val_loss: 0.3438 - val_acc: 0.9148\n",
      "Epoch 1382/3000\n",
      "702/702 [==============================] - 0s 561us/sample - loss: 0.0226 - acc: 0.9915 - val_loss: 0.4292 - val_acc: 0.8977\n",
      "Epoch 1383/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0213 - acc: 0.9886 - val_loss: 0.5636 - val_acc: 0.8750\n",
      "Epoch 1384/3000\n",
      "702/702 [==============================] - 0s 543us/sample - loss: 0.0260 - acc: 0.9943 - val_loss: 0.3790 - val_acc: 0.8920\n",
      "Epoch 1385/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0265 - acc: 0.9858 - val_loss: 0.3681 - val_acc: 0.8920\n",
      "Epoch 1386/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0308 - acc: 0.9929 - val_loss: 0.4173 - val_acc: 0.8920\n",
      "Epoch 1387/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0361 - acc: 0.9915 - val_loss: 0.4229 - val_acc: 0.9034\n",
      "Epoch 1388/3000\n",
      "702/702 [==============================] - 0s 534us/sample - loss: 0.0229 - acc: 0.9929 - val_loss: 0.4106 - val_acc: 0.8864\n",
      "Epoch 1389/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0164 - acc: 0.9957 - val_loss: 0.4261 - val_acc: 0.8750\n",
      "Epoch 1390/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0250 - acc: 0.9915 - val_loss: 0.3696 - val_acc: 0.8920\n",
      "Epoch 1391/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0240 - acc: 0.9900 - val_loss: 0.3638 - val_acc: 0.8977\n",
      "Epoch 1392/3000\n",
      "702/702 [==============================] - 0s 580us/sample - loss: 0.0217 - acc: 0.9929 - val_loss: 0.3935 - val_acc: 0.8977\n",
      "Epoch 1393/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0191 - acc: 0.9929 - val_loss: 0.4056 - val_acc: 0.8920\n",
      "Epoch 1394/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0139 - acc: 0.9943 - val_loss: 0.4085 - val_acc: 0.8864\n",
      "Epoch 1395/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0183 - acc: 0.9943 - val_loss: 0.4211 - val_acc: 0.8920\n",
      "Epoch 1396/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0039 - acc: 1.0000 - val_loss: 0.4632 - val_acc: 0.8864\n",
      "Epoch 1397/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0167 - acc: 0.9972 - val_loss: 0.4906 - val_acc: 0.8977\n",
      "Epoch 1398/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0231 - acc: 0.9929 - val_loss: 0.4386 - val_acc: 0.9034\n",
      "Epoch 1399/3000\n",
      "702/702 [==============================] - 0s 544us/sample - loss: 0.0206 - acc: 0.9915 - val_loss: 0.4224 - val_acc: 0.9034\n",
      "Epoch 1400/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0249 - acc: 0.9900 - val_loss: 0.4175 - val_acc: 0.8920\n",
      "Epoch 1401/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0102 - acc: 0.9986 - val_loss: 0.4859 - val_acc: 0.9034\n",
      "Epoch 1402/3000\n",
      "702/702 [==============================] - 0s 605us/sample - loss: 0.0152 - acc: 0.9957 - val_loss: 0.4170 - val_acc: 0.8977\n",
      "Epoch 1403/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.0176 - acc: 0.9943 - val_loss: 0.3877 - val_acc: 0.8920\n",
      "Epoch 1404/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0118 - acc: 0.9986 - val_loss: 0.4462 - val_acc: 0.9091\n",
      "Epoch 1405/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.0266 - acc: 0.9886 - val_loss: 0.4113 - val_acc: 0.9034\n",
      "Epoch 1406/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 0.0156 - acc: 0.9957 - val_loss: 0.3960 - val_acc: 0.9091\n",
      "Epoch 1407/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0216 - acc: 0.9957 - val_loss: 0.3692 - val_acc: 0.8977\n",
      "Epoch 1408/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0097 - acc: 0.9972 - val_loss: 0.4360 - val_acc: 0.8920\n",
      "Epoch 1409/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0090 - acc: 0.9986 - val_loss: 0.4782 - val_acc: 0.8977\n",
      "Epoch 1410/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0075 - acc: 1.0000 - val_loss: 0.4389 - val_acc: 0.8864\n",
      "Epoch 1411/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0100 - acc: 0.9957 - val_loss: 0.4549 - val_acc: 0.9034\n",
      "Epoch 1412/3000\n",
      "702/702 [==============================] - 0s 597us/sample - loss: 0.0248 - acc: 0.9943 - val_loss: 0.4599 - val_acc: 0.8977\n",
      "Epoch 1413/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0237 - acc: 0.9886 - val_loss: 0.4067 - val_acc: 0.8977\n",
      "Epoch 1414/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0178 - acc: 0.9929 - val_loss: 0.4097 - val_acc: 0.8750\n",
      "Epoch 1415/3000\n",
      "702/702 [==============================] - 0s 544us/sample - loss: 0.0314 - acc: 0.9929 - val_loss: 0.5187 - val_acc: 0.8977\n",
      "Epoch 1416/3000\n",
      "702/702 [==============================] - 0s 551us/sample - loss: 0.0275 - acc: 0.9915 - val_loss: 0.5479 - val_acc: 0.8693\n",
      "Epoch 1417/3000\n",
      "702/702 [==============================] - 0s 550us/sample - loss: 0.0307 - acc: 0.9900 - val_loss: 0.4903 - val_acc: 0.8977\n",
      "Epoch 1418/3000\n",
      "702/702 [==============================] - 0s 548us/sample - loss: 0.0101 - acc: 0.9972 - val_loss: 0.4622 - val_acc: 0.8977\n",
      "Epoch 1419/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0261 - acc: 0.9957 - val_loss: 0.4165 - val_acc: 0.9034\n",
      "Epoch 1420/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.0097 - acc: 0.9986 - val_loss: 0.4530 - val_acc: 0.9034\n",
      "Epoch 1421/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0124 - acc: 0.9972 - val_loss: 0.5144 - val_acc: 0.9034\n",
      "Epoch 1422/3000\n",
      "702/702 [==============================] - 0s 573us/sample - loss: 0.0129 - acc: 0.9957 - val_loss: 0.4991 - val_acc: 0.9034\n",
      "Epoch 1423/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0214 - acc: 0.9915 - val_loss: 0.4734 - val_acc: 0.8920\n",
      "Epoch 1424/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0107 - acc: 0.9957 - val_loss: 0.5411 - val_acc: 0.8864\n",
      "Epoch 1425/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0204 - acc: 0.9929 - val_loss: 0.3983 - val_acc: 0.9034\n",
      "Epoch 1426/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0149 - acc: 0.9972 - val_loss: 0.3322 - val_acc: 0.8920\n",
      "Epoch 1427/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0115 - acc: 0.9957 - val_loss: 0.4196 - val_acc: 0.8977\n",
      "Epoch 1428/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0192 - acc: 0.9929 - val_loss: 0.4348 - val_acc: 0.8977\n",
      "Epoch 1429/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0349 - acc: 0.9900 - val_loss: 0.3778 - val_acc: 0.8977\n",
      "Epoch 1430/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0140 - acc: 0.9943 - val_loss: 0.3806 - val_acc: 0.9148\n",
      "Epoch 1431/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0285 - acc: 0.9900 - val_loss: 0.3959 - val_acc: 0.8920\n",
      "Epoch 1432/3000\n",
      "702/702 [==============================] - 0s 590us/sample - loss: 0.0141 - acc: 0.9957 - val_loss: 0.4288 - val_acc: 0.8864\n",
      "Epoch 1433/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0251 - acc: 0.9915 - val_loss: 0.4307 - val_acc: 0.8920\n",
      "Epoch 1434/3000\n",
      "702/702 [==============================] - 0s 539us/sample - loss: 0.0239 - acc: 0.9915 - val_loss: 0.3640 - val_acc: 0.8977\n",
      "Epoch 1435/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0152 - acc: 0.9957 - val_loss: 0.4636 - val_acc: 0.8920\n",
      "Epoch 1436/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0129 - acc: 0.9972 - val_loss: 0.5049 - val_acc: 0.8807\n",
      "Epoch 1437/3000\n",
      "702/702 [==============================] - 0s 534us/sample - loss: 0.0177 - acc: 0.9943 - val_loss: 0.6113 - val_acc: 0.8920\n",
      "Epoch 1438/3000\n",
      "702/702 [==============================] - 0s 553us/sample - loss: 0.0128 - acc: 0.9943 - val_loss: 0.5276 - val_acc: 0.8920\n",
      "Epoch 1439/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0295 - acc: 0.9929 - val_loss: 0.6115 - val_acc: 0.8864\n",
      "Epoch 1440/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0216 - acc: 0.9972 - val_loss: 0.5720 - val_acc: 0.8920\n",
      "Epoch 1441/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0152 - acc: 0.9957 - val_loss: 0.3371 - val_acc: 0.9034\n",
      "Epoch 1442/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 0.0354 - acc: 0.9886 - val_loss: 0.4068 - val_acc: 0.8977\n",
      "Epoch 1443/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0085 - acc: 1.0000 - val_loss: 0.5532 - val_acc: 0.9034\n",
      "Epoch 1444/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0203 - acc: 0.9915 - val_loss: 0.4766 - val_acc: 0.8977\n",
      "Epoch 1445/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0498 - acc: 0.9829 - val_loss: 0.4125 - val_acc: 0.9034\n",
      "Epoch 1446/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0276 - acc: 0.9915 - val_loss: 0.4267 - val_acc: 0.8977\n",
      "Epoch 1447/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.0293 - acc: 0.9929 - val_loss: 0.3534 - val_acc: 0.8807\n",
      "Epoch 1448/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0207 - acc: 0.9929 - val_loss: 0.3447 - val_acc: 0.8920\n",
      "Epoch 1449/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0331 - acc: 0.9886 - val_loss: 0.4073 - val_acc: 0.8864\n",
      "Epoch 1450/3000\n",
      "702/702 [==============================] - 0s 535us/sample - loss: 0.0275 - acc: 0.9915 - val_loss: 0.4921 - val_acc: 0.8693\n",
      "Epoch 1451/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0272 - acc: 0.9929 - val_loss: 0.4515 - val_acc: 0.8920\n",
      "Epoch 1452/3000\n",
      "702/702 [==============================] - 0s 583us/sample - loss: 0.0285 - acc: 0.9886 - val_loss: 0.4350 - val_acc: 0.8864\n",
      "Epoch 1453/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0197 - acc: 0.9957 - val_loss: 0.3502 - val_acc: 0.9034\n",
      "Epoch 1454/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0453 - acc: 0.9858 - val_loss: 0.3980 - val_acc: 0.8977\n",
      "Epoch 1455/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0453 - acc: 0.9915 - val_loss: 0.4932 - val_acc: 0.8864\n",
      "Epoch 1456/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0363 - acc: 0.9886 - val_loss: 0.3528 - val_acc: 0.8920\n",
      "Epoch 1457/3000\n",
      "702/702 [==============================] - 0s 540us/sample - loss: 0.0354 - acc: 0.9900 - val_loss: 0.4304 - val_acc: 0.8977\n",
      "Epoch 1458/3000\n",
      "702/702 [==============================] - 0s 542us/sample - loss: 0.0179 - acc: 0.9943 - val_loss: 0.5943 - val_acc: 0.8920\n",
      "Epoch 1459/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0308 - acc: 0.9900 - val_loss: 0.4525 - val_acc: 0.9091\n",
      "Epoch 1460/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0115 - acc: 0.9972 - val_loss: 0.3733 - val_acc: 0.9091\n",
      "Epoch 1461/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0292 - acc: 0.9929 - val_loss: 0.4098 - val_acc: 0.9034\n",
      "Epoch 1462/3000\n",
      "702/702 [==============================] - 0s 581us/sample - loss: 0.0127 - acc: 0.9972 - val_loss: 0.5997 - val_acc: 0.8977\n",
      "Epoch 1463/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0410 - acc: 0.9872 - val_loss: 0.4488 - val_acc: 0.9034\n",
      "Epoch 1464/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0134 - acc: 0.9957 - val_loss: 0.3993 - val_acc: 0.8977\n",
      "Epoch 1465/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0230 - acc: 0.9929 - val_loss: 0.4510 - val_acc: 0.8920\n",
      "Epoch 1466/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0184 - acc: 0.9929 - val_loss: 0.4707 - val_acc: 0.8864\n",
      "Epoch 1467/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0259 - acc: 0.9915 - val_loss: 0.4255 - val_acc: 0.9034\n",
      "Epoch 1468/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0132 - acc: 0.9972 - val_loss: 0.4897 - val_acc: 0.9034\n",
      "Epoch 1469/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0229 - acc: 0.9929 - val_loss: 0.5542 - val_acc: 0.8864\n",
      "Epoch 1470/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0091 - acc: 0.9972 - val_loss: 0.4825 - val_acc: 0.8864\n",
      "Epoch 1471/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0183 - acc: 0.9929 - val_loss: 0.3954 - val_acc: 0.8920\n",
      "Epoch 1472/3000\n",
      "702/702 [==============================] - 0s 577us/sample - loss: 0.0327 - acc: 0.9886 - val_loss: 0.3873 - val_acc: 0.9148\n",
      "Epoch 1473/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0116 - acc: 0.9957 - val_loss: 0.4257 - val_acc: 0.8920\n",
      "Epoch 1474/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0200 - acc: 0.9957 - val_loss: 0.3702 - val_acc: 0.8977\n",
      "Epoch 1475/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0253 - acc: 0.9915 - val_loss: 0.3407 - val_acc: 0.8920\n",
      "Epoch 1476/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 0.0151 - acc: 0.9972 - val_loss: 0.4006 - val_acc: 0.8920\n",
      "Epoch 1477/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0101 - acc: 0.9986 - val_loss: 0.4773 - val_acc: 0.8920\n",
      "Epoch 1478/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0179 - acc: 0.9929 - val_loss: 0.5816 - val_acc: 0.8807\n",
      "Epoch 1479/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0233 - acc: 0.9915 - val_loss: 0.5280 - val_acc: 0.8920\n",
      "Epoch 1480/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0208 - acc: 0.9915 - val_loss: 0.3779 - val_acc: 0.9091\n",
      "Epoch 1481/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0191 - acc: 0.9929 - val_loss: 0.3734 - val_acc: 0.9091\n",
      "Epoch 1482/3000\n",
      "702/702 [==============================] - 0s 590us/sample - loss: 0.0169 - acc: 0.9957 - val_loss: 0.4186 - val_acc: 0.8920\n",
      "Epoch 1483/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0335 - acc: 0.9886 - val_loss: 0.3641 - val_acc: 0.9034\n",
      "Epoch 1484/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0248 - acc: 0.9929 - val_loss: 0.3724 - val_acc: 0.8864\n",
      "Epoch 1485/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0185 - acc: 0.9943 - val_loss: 0.4128 - val_acc: 0.8977\n",
      "Epoch 1486/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0235 - acc: 0.9915 - val_loss: 0.4333 - val_acc: 0.8977\n",
      "Epoch 1487/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0211 - acc: 0.9957 - val_loss: 0.3817 - val_acc: 0.8920\n",
      "Epoch 1488/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.0098 - acc: 0.9972 - val_loss: 0.3792 - val_acc: 0.8920\n",
      "Epoch 1489/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.0088 - acc: 0.9986 - val_loss: 0.4865 - val_acc: 0.8977\n",
      "Epoch 1490/3000\n",
      "702/702 [==============================] - 0s 553us/sample - loss: 0.0164 - acc: 0.9957 - val_loss: 0.4728 - val_acc: 0.8864\n",
      "Epoch 1491/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0231 - acc: 0.9943 - val_loss: 0.4549 - val_acc: 0.9091\n",
      "Epoch 1492/3000\n",
      "702/702 [==============================] - 0s 588us/sample - loss: 0.0131 - acc: 0.9972 - val_loss: 0.4901 - val_acc: 0.8977\n",
      "Epoch 1493/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0138 - acc: 0.9943 - val_loss: 0.5056 - val_acc: 0.8977\n",
      "Epoch 1494/3000\n",
      "702/702 [==============================] - 0s 554us/sample - loss: 0.0124 - acc: 0.9943 - val_loss: 0.4472 - val_acc: 0.9034\n",
      "Epoch 1495/3000\n",
      "702/702 [==============================] - 0s 535us/sample - loss: 0.0319 - acc: 0.9943 - val_loss: 0.3815 - val_acc: 0.9091\n",
      "Epoch 1496/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0181 - acc: 0.9929 - val_loss: 0.3412 - val_acc: 0.9091\n",
      "Epoch 1497/3000\n",
      "702/702 [==============================] - 0s 538us/sample - loss: 0.0195 - acc: 0.9915 - val_loss: 0.4341 - val_acc: 0.9091\n",
      "Epoch 1498/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0139 - acc: 0.9972 - val_loss: 0.4503 - val_acc: 0.9091\n",
      "Epoch 1499/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0127 - acc: 0.9986 - val_loss: 0.4097 - val_acc: 0.9034\n",
      "Epoch 1500/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0125 - acc: 0.9943 - val_loss: 0.3973 - val_acc: 0.9261\n",
      "Epoch 1501/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0193 - acc: 0.9957 - val_loss: 0.4135 - val_acc: 0.8977\n",
      "Epoch 1502/3000\n",
      "702/702 [==============================] - 0s 580us/sample - loss: 0.0152 - acc: 0.9943 - val_loss: 0.4105 - val_acc: 0.9034\n",
      "Epoch 1503/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0288 - acc: 0.9943 - val_loss: 0.3684 - val_acc: 0.9091\n",
      "Epoch 1504/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0259 - acc: 0.9943 - val_loss: 0.3997 - val_acc: 0.8920\n",
      "Epoch 1505/3000\n",
      "702/702 [==============================] - 0s 554us/sample - loss: 0.0143 - acc: 0.9929 - val_loss: 0.3801 - val_acc: 0.9091\n",
      "Epoch 1506/3000\n",
      "702/702 [==============================] - 0s 545us/sample - loss: 0.0126 - acc: 0.9957 - val_loss: 0.4729 - val_acc: 0.8977\n",
      "Epoch 1507/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0251 - acc: 0.9943 - val_loss: 0.4486 - val_acc: 0.8920\n",
      "Epoch 1508/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.0174 - acc: 0.9957 - val_loss: 0.3625 - val_acc: 0.9034\n",
      "Epoch 1509/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0293 - acc: 0.9929 - val_loss: 0.4124 - val_acc: 0.8920\n",
      "Epoch 1510/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0120 - acc: 0.9943 - val_loss: 0.5063 - val_acc: 0.8977\n",
      "Epoch 1511/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0106 - acc: 0.9972 - val_loss: 0.4962 - val_acc: 0.8864\n",
      "Epoch 1512/3000\n",
      "702/702 [==============================] - 0s 587us/sample - loss: 0.0331 - acc: 0.9915 - val_loss: 0.4061 - val_acc: 0.8977\n",
      "Epoch 1513/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0123 - acc: 0.9957 - val_loss: 0.3948 - val_acc: 0.8920\n",
      "Epoch 1514/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0287 - acc: 0.9929 - val_loss: 0.5113 - val_acc: 0.8977\n",
      "Epoch 1515/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0151 - acc: 0.9915 - val_loss: 0.4454 - val_acc: 0.8977\n",
      "Epoch 1516/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0209 - acc: 0.9943 - val_loss: 0.4180 - val_acc: 0.9034\n",
      "Epoch 1517/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0342 - acc: 0.9886 - val_loss: 0.3602 - val_acc: 0.9091\n",
      "Epoch 1518/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0196 - acc: 0.9943 - val_loss: 0.4200 - val_acc: 0.9091\n",
      "Epoch 1519/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0226 - acc: 0.9929 - val_loss: 0.5429 - val_acc: 0.8977\n",
      "Epoch 1520/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0226 - acc: 0.9943 - val_loss: 0.4099 - val_acc: 0.8977\n",
      "Epoch 1521/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0295 - acc: 0.9872 - val_loss: 0.3282 - val_acc: 0.9148\n",
      "Epoch 1522/3000\n",
      "702/702 [==============================] - 0s 580us/sample - loss: 0.0252 - acc: 0.9929 - val_loss: 0.3362 - val_acc: 0.8977\n",
      "Epoch 1523/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0334 - acc: 0.9915 - val_loss: 0.3646 - val_acc: 0.8920\n",
      "Epoch 1524/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0277 - acc: 0.9915 - val_loss: 0.3522 - val_acc: 0.9091\n",
      "Epoch 1525/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0316 - acc: 0.9886 - val_loss: 0.4746 - val_acc: 0.8864\n",
      "Epoch 1526/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0366 - acc: 0.9886 - val_loss: 0.4245 - val_acc: 0.9034\n",
      "Epoch 1527/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0168 - acc: 0.9943 - val_loss: 0.4215 - val_acc: 0.9034\n",
      "Epoch 1528/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0169 - acc: 0.9915 - val_loss: 0.4586 - val_acc: 0.8977\n",
      "Epoch 1529/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0093 - acc: 1.0000 - val_loss: 0.5203 - val_acc: 0.8750\n",
      "Epoch 1530/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0292 - acc: 0.9886 - val_loss: 0.5528 - val_acc: 0.8920\n",
      "Epoch 1531/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0217 - acc: 0.9886 - val_loss: 0.4178 - val_acc: 0.9091\n",
      "Epoch 1532/3000\n",
      "702/702 [==============================] - 0s 587us/sample - loss: 0.0258 - acc: 0.9886 - val_loss: 0.3625 - val_acc: 0.9034\n",
      "Epoch 1533/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0370 - acc: 0.9886 - val_loss: 0.5763 - val_acc: 0.9034\n",
      "Epoch 1534/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0123 - acc: 0.9957 - val_loss: 0.5114 - val_acc: 0.8977\n",
      "Epoch 1535/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0068 - acc: 1.0000 - val_loss: 0.4234 - val_acc: 0.8977\n",
      "Epoch 1536/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0123 - acc: 0.9986 - val_loss: 0.3938 - val_acc: 0.8977\n",
      "Epoch 1537/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0135 - acc: 0.9957 - val_loss: 0.4029 - val_acc: 0.8977\n",
      "Epoch 1538/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0191 - acc: 0.9915 - val_loss: 0.4615 - val_acc: 0.8977\n",
      "Epoch 1539/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0129 - acc: 0.9943 - val_loss: 0.5266 - val_acc: 0.9091\n",
      "Epoch 1540/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0085 - acc: 0.9986 - val_loss: 0.6216 - val_acc: 0.9034\n",
      "Epoch 1541/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0092 - acc: 0.9957 - val_loss: 0.4128 - val_acc: 0.8920\n",
      "Epoch 1542/3000\n",
      "702/702 [==============================] - 0s 582us/sample - loss: 0.0217 - acc: 0.9915 - val_loss: 0.3583 - val_acc: 0.9091\n",
      "Epoch 1543/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0390 - acc: 0.9900 - val_loss: 0.3753 - val_acc: 0.9034\n",
      "Epoch 1544/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0181 - acc: 0.9915 - val_loss: 0.4157 - val_acc: 0.8977\n",
      "Epoch 1545/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0210 - acc: 0.9915 - val_loss: 0.3955 - val_acc: 0.8977\n",
      "Epoch 1546/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0207 - acc: 0.9929 - val_loss: 0.4229 - val_acc: 0.8864\n",
      "Epoch 1547/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0283 - acc: 0.9929 - val_loss: 0.3714 - val_acc: 0.9091\n",
      "Epoch 1548/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0240 - acc: 0.9915 - val_loss: 0.3730 - val_acc: 0.8920\n",
      "Epoch 1549/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0243 - acc: 0.9900 - val_loss: 0.4024 - val_acc: 0.8864\n",
      "Epoch 1550/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0132 - acc: 0.9943 - val_loss: 0.4886 - val_acc: 0.8864\n",
      "Epoch 1551/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0072 - acc: 0.9986 - val_loss: 0.5790 - val_acc: 0.8920\n",
      "Epoch 1552/3000\n",
      "702/702 [==============================] - 0s 577us/sample - loss: 0.0085 - acc: 0.9972 - val_loss: 0.5727 - val_acc: 0.8920\n",
      "Epoch 1553/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0160 - acc: 0.9972 - val_loss: 0.5780 - val_acc: 0.9034\n",
      "Epoch 1554/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0183 - acc: 0.9957 - val_loss: 0.5729 - val_acc: 0.8920\n",
      "Epoch 1555/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0182 - acc: 0.9929 - val_loss: 0.5383 - val_acc: 0.8864\n",
      "Epoch 1556/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0320 - acc: 0.9900 - val_loss: 0.4292 - val_acc: 0.8977\n",
      "Epoch 1557/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0423 - acc: 0.9915 - val_loss: 0.4065 - val_acc: 0.8977\n",
      "Epoch 1558/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0135 - acc: 0.9957 - val_loss: 0.3593 - val_acc: 0.8977\n",
      "Epoch 1559/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0148 - acc: 0.9957 - val_loss: 0.4271 - val_acc: 0.9091\n",
      "Epoch 1560/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0198 - acc: 0.9943 - val_loss: 0.4514 - val_acc: 0.8977\n",
      "Epoch 1561/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0248 - acc: 0.9900 - val_loss: 0.5427 - val_acc: 0.9034\n",
      "Epoch 1562/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.0186 - acc: 0.9957 - val_loss: 0.4019 - val_acc: 0.9034\n",
      "Epoch 1563/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0126 - acc: 0.9986 - val_loss: 0.3469 - val_acc: 0.9034\n",
      "Epoch 1564/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0185 - acc: 0.9915 - val_loss: 0.4371 - val_acc: 0.8977\n",
      "Epoch 1565/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0246 - acc: 0.9915 - val_loss: 0.5476 - val_acc: 0.8864\n",
      "Epoch 1566/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0286 - acc: 0.9886 - val_loss: 0.6412 - val_acc: 0.8693\n",
      "Epoch 1567/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0643 - acc: 0.9843 - val_loss: 0.3960 - val_acc: 0.8977\n",
      "Epoch 1568/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0820 - acc: 0.9872 - val_loss: 0.2609 - val_acc: 0.9091\n",
      "Epoch 1569/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0376 - acc: 0.9872 - val_loss: 0.3812 - val_acc: 0.8807\n",
      "Epoch 1570/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0391 - acc: 0.9886 - val_loss: 0.3446 - val_acc: 0.9034\n",
      "Epoch 1571/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0229 - acc: 0.9929 - val_loss: 0.3739 - val_acc: 0.8977\n",
      "Epoch 1572/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.0201 - acc: 0.9915 - val_loss: 0.3627 - val_acc: 0.8977\n",
      "Epoch 1573/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0158 - acc: 0.9957 - val_loss: 0.3368 - val_acc: 0.9148\n",
      "Epoch 1574/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0209 - acc: 0.9943 - val_loss: 0.3236 - val_acc: 0.9091\n",
      "Epoch 1575/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0135 - acc: 0.9943 - val_loss: 0.3660 - val_acc: 0.8864\n",
      "Epoch 1576/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0149 - acc: 0.9957 - val_loss: 0.3719 - val_acc: 0.9034\n",
      "Epoch 1577/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0127 - acc: 0.9929 - val_loss: 0.3843 - val_acc: 0.8920\n",
      "Epoch 1578/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0318 - acc: 0.9886 - val_loss: 0.3359 - val_acc: 0.8977\n",
      "Epoch 1579/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0137 - acc: 0.9972 - val_loss: 0.3427 - val_acc: 0.9091\n",
      "Epoch 1580/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0159 - acc: 0.9943 - val_loss: 0.4356 - val_acc: 0.8864\n",
      "Epoch 1581/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0104 - acc: 0.9972 - val_loss: 0.5396 - val_acc: 0.8920\n",
      "Epoch 1582/3000\n",
      "702/702 [==============================] - 0s 581us/sample - loss: 0.0111 - acc: 0.9957 - val_loss: 0.4897 - val_acc: 0.8864\n",
      "Epoch 1583/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0134 - acc: 0.9986 - val_loss: 0.3615 - val_acc: 0.8977\n",
      "Epoch 1584/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0114 - acc: 0.9957 - val_loss: 0.3312 - val_acc: 0.9034\n",
      "Epoch 1585/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0103 - acc: 0.9957 - val_loss: 0.3296 - val_acc: 0.9034\n",
      "Epoch 1586/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0136 - acc: 0.9972 - val_loss: 0.3532 - val_acc: 0.9034\n",
      "Epoch 1587/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0134 - acc: 0.9957 - val_loss: 0.3693 - val_acc: 0.9148\n",
      "Epoch 1588/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0126 - acc: 0.9986 - val_loss: 0.4213 - val_acc: 0.9091\n",
      "Epoch 1589/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0177 - acc: 0.9915 - val_loss: 0.4258 - val_acc: 0.8920\n",
      "Epoch 1590/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0113 - acc: 0.9972 - val_loss: 0.3747 - val_acc: 0.9034\n",
      "Epoch 1591/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0102 - acc: 0.9972 - val_loss: 0.3939 - val_acc: 0.9034\n",
      "Epoch 1592/3000\n",
      "702/702 [==============================] - 0s 585us/sample - loss: 0.0131 - acc: 0.9943 - val_loss: 0.3765 - val_acc: 0.9091\n",
      "Epoch 1593/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0077 - acc: 0.9986 - val_loss: 0.3959 - val_acc: 0.9091\n",
      "Epoch 1594/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0187 - acc: 0.9929 - val_loss: 0.4136 - val_acc: 0.9091\n",
      "Epoch 1595/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0060 - acc: 0.9986 - val_loss: 0.4491 - val_acc: 0.9091\n",
      "Epoch 1596/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0148 - acc: 0.9943 - val_loss: 0.3918 - val_acc: 0.9148\n",
      "Epoch 1597/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0067 - acc: 0.9986 - val_loss: 0.3989 - val_acc: 0.9034\n",
      "Epoch 1598/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0122 - acc: 0.9957 - val_loss: 0.4325 - val_acc: 0.9034\n",
      "Epoch 1599/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0203 - acc: 0.9915 - val_loss: 0.4709 - val_acc: 0.9148\n",
      "Epoch 1600/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0199 - acc: 0.9929 - val_loss: 0.4513 - val_acc: 0.9034\n",
      "Epoch 1601/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0332 - acc: 0.9900 - val_loss: 0.3377 - val_acc: 0.9091\n",
      "Epoch 1602/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.0217 - acc: 0.9943 - val_loss: 0.3388 - val_acc: 0.9148\n",
      "Epoch 1603/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0333 - acc: 0.9929 - val_loss: 0.3295 - val_acc: 0.8977\n",
      "Epoch 1604/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0210 - acc: 0.9900 - val_loss: 0.3546 - val_acc: 0.8977\n",
      "Epoch 1605/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0246 - acc: 0.9929 - val_loss: 0.3630 - val_acc: 0.8920\n",
      "Epoch 1606/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0137 - acc: 0.9957 - val_loss: 0.3426 - val_acc: 0.8920\n",
      "Epoch 1607/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0371 - acc: 0.9872 - val_loss: 0.4550 - val_acc: 0.8920\n",
      "Epoch 1608/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0267 - acc: 0.9915 - val_loss: 0.5846 - val_acc: 0.8920\n",
      "Epoch 1609/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0287 - acc: 0.9943 - val_loss: 0.4675 - val_acc: 0.9091\n",
      "Epoch 1610/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0198 - acc: 0.9943 - val_loss: 0.5440 - val_acc: 0.9091\n",
      "Epoch 1611/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0179 - acc: 0.9972 - val_loss: 0.3958 - val_acc: 0.9034\n",
      "Epoch 1612/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.0229 - acc: 0.9943 - val_loss: 0.3740 - val_acc: 0.8977\n",
      "Epoch 1613/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0095 - acc: 1.0000 - val_loss: 0.4808 - val_acc: 0.8977\n",
      "Epoch 1614/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0272 - acc: 0.9929 - val_loss: 0.4071 - val_acc: 0.9034\n",
      "Epoch 1615/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0150 - acc: 0.9929 - val_loss: 0.4845 - val_acc: 0.8693\n",
      "Epoch 1616/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0301 - acc: 0.9929 - val_loss: 0.4240 - val_acc: 0.9034\n",
      "Epoch 1617/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0359 - acc: 0.9929 - val_loss: 0.4504 - val_acc: 0.9205\n",
      "Epoch 1618/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0224 - acc: 0.9929 - val_loss: 0.3568 - val_acc: 0.9091\n",
      "Epoch 1619/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0072 - acc: 1.0000 - val_loss: 0.3307 - val_acc: 0.9034\n",
      "Epoch 1620/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0167 - acc: 0.9972 - val_loss: 0.3381 - val_acc: 0.8977\n",
      "Epoch 1621/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0286 - acc: 0.9915 - val_loss: 0.3406 - val_acc: 0.8977\n",
      "Epoch 1622/3000\n",
      "702/702 [==============================] - 0s 569us/sample - loss: 0.0194 - acc: 0.9929 - val_loss: 0.3395 - val_acc: 0.9091\n",
      "Epoch 1623/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0086 - acc: 0.9986 - val_loss: 0.3147 - val_acc: 0.9034\n",
      "Epoch 1624/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0174 - acc: 0.9957 - val_loss: 0.4193 - val_acc: 0.9091\n",
      "Epoch 1625/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0328 - acc: 0.9915 - val_loss: 0.5312 - val_acc: 0.8977\n",
      "Epoch 1626/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0122 - acc: 0.9972 - val_loss: 0.4653 - val_acc: 0.8920\n",
      "Epoch 1627/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0081 - acc: 0.9972 - val_loss: 0.4402 - val_acc: 0.8920\n",
      "Epoch 1628/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0084 - acc: 0.9972 - val_loss: 0.4648 - val_acc: 0.9034\n",
      "Epoch 1629/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0136 - acc: 0.9957 - val_loss: 0.4748 - val_acc: 0.8920\n",
      "Epoch 1630/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0100 - acc: 0.9957 - val_loss: 0.4985 - val_acc: 0.8977\n",
      "Epoch 1631/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0124 - acc: 0.9943 - val_loss: 0.4978 - val_acc: 0.8977\n",
      "Epoch 1632/3000\n",
      "702/702 [==============================] - 0s 581us/sample - loss: 0.0146 - acc: 0.9943 - val_loss: 0.4427 - val_acc: 0.9034\n",
      "Epoch 1633/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0165 - acc: 0.9929 - val_loss: 0.4586 - val_acc: 0.8920\n",
      "Epoch 1634/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0074 - acc: 0.9986 - val_loss: 0.4946 - val_acc: 0.8977\n",
      "Epoch 1635/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0108 - acc: 0.9972 - val_loss: 0.4515 - val_acc: 0.9091\n",
      "Epoch 1636/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0051 - acc: 1.0000 - val_loss: 0.4291 - val_acc: 0.9091\n",
      "Epoch 1637/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0067 - acc: 0.9972 - val_loss: 0.3874 - val_acc: 0.9205\n",
      "Epoch 1638/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0073 - acc: 0.9986 - val_loss: 0.3755 - val_acc: 0.9034\n",
      "Epoch 1639/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0205 - acc: 0.9929 - val_loss: 0.3511 - val_acc: 0.9148\n",
      "Epoch 1640/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0104 - acc: 0.9957 - val_loss: 0.3753 - val_acc: 0.9034\n",
      "Epoch 1641/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0162 - acc: 0.9972 - val_loss: 0.3851 - val_acc: 0.8977\n",
      "Epoch 1642/3000\n",
      "702/702 [==============================] - 0s 583us/sample - loss: 0.0236 - acc: 0.9929 - val_loss: 0.4179 - val_acc: 0.8977\n",
      "Epoch 1643/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0217 - acc: 0.9900 - val_loss: 0.4693 - val_acc: 0.8977\n",
      "Epoch 1644/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0078 - acc: 0.9986 - val_loss: 0.4560 - val_acc: 0.8977\n",
      "Epoch 1645/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0182 - acc: 0.9929 - val_loss: 0.4241 - val_acc: 0.9091\n",
      "Epoch 1646/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0171 - acc: 0.9943 - val_loss: 0.4311 - val_acc: 0.9148\n",
      "Epoch 1647/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0249 - acc: 0.9943 - val_loss: 0.5159 - val_acc: 0.8750\n",
      "Epoch 1648/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0347 - acc: 0.9915 - val_loss: 0.4960 - val_acc: 0.8864\n",
      "Epoch 1649/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0132 - acc: 0.9957 - val_loss: 0.4485 - val_acc: 0.8920\n",
      "Epoch 1650/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0097 - acc: 0.9972 - val_loss: 0.4689 - val_acc: 0.8977\n",
      "Epoch 1651/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0120 - acc: 0.9957 - val_loss: 0.5024 - val_acc: 0.9034\n",
      "Epoch 1652/3000\n",
      "702/702 [==============================] - 0s 581us/sample - loss: 0.0074 - acc: 0.9986 - val_loss: 0.4738 - val_acc: 0.9034\n",
      "Epoch 1653/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0098 - acc: 0.9972 - val_loss: 0.4477 - val_acc: 0.9034\n",
      "Epoch 1654/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0172 - acc: 0.9972 - val_loss: 0.4712 - val_acc: 0.9034\n",
      "Epoch 1655/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0051 - acc: 0.9986 - val_loss: 0.4978 - val_acc: 0.8977\n",
      "Epoch 1656/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0142 - acc: 0.9915 - val_loss: 0.4933 - val_acc: 0.9034\n",
      "Epoch 1657/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0071 - acc: 0.9986 - val_loss: 0.4647 - val_acc: 0.9091\n",
      "Epoch 1658/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0149 - acc: 0.9957 - val_loss: 0.4849 - val_acc: 0.8977\n",
      "Epoch 1659/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0140 - acc: 0.9957 - val_loss: 0.5000 - val_acc: 0.8977\n",
      "Epoch 1660/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0031 - acc: 1.0000 - val_loss: 0.5214 - val_acc: 0.9034\n",
      "Epoch 1661/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0214 - acc: 0.9915 - val_loss: 0.3719 - val_acc: 0.9148\n",
      "Epoch 1662/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.0097 - acc: 0.9957 - val_loss: 0.3381 - val_acc: 0.9034\n",
      "Epoch 1663/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0128 - acc: 0.9957 - val_loss: 0.3796 - val_acc: 0.8920\n",
      "Epoch 1664/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0141 - acc: 0.9957 - val_loss: 0.4165 - val_acc: 0.8864\n",
      "Epoch 1665/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0234 - acc: 0.9929 - val_loss: 0.3201 - val_acc: 0.8920\n",
      "Epoch 1666/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0141 - acc: 0.9943 - val_loss: 0.3197 - val_acc: 0.9148\n",
      "Epoch 1667/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0114 - acc: 0.9972 - val_loss: 0.3742 - val_acc: 0.9034\n",
      "Epoch 1668/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0098 - acc: 0.9957 - val_loss: 0.4511 - val_acc: 0.9091\n",
      "Epoch 1669/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0107 - acc: 0.9972 - val_loss: 0.4591 - val_acc: 0.9091\n",
      "Epoch 1670/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0211 - acc: 0.9929 - val_loss: 0.3929 - val_acc: 0.8977\n",
      "Epoch 1671/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0110 - acc: 0.9943 - val_loss: 0.3480 - val_acc: 0.9205\n",
      "Epoch 1672/3000\n",
      "702/702 [==============================] - 0s 569us/sample - loss: 0.0151 - acc: 0.9957 - val_loss: 0.3773 - val_acc: 0.9034\n",
      "Epoch 1673/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0080 - acc: 0.9986 - val_loss: 0.4269 - val_acc: 0.8977\n",
      "Epoch 1674/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0127 - acc: 0.9943 - val_loss: 0.4046 - val_acc: 0.8977\n",
      "Epoch 1675/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0202 - acc: 0.9915 - val_loss: 0.4749 - val_acc: 0.8864\n",
      "Epoch 1676/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0308 - acc: 0.9915 - val_loss: 0.4691 - val_acc: 0.8920\n",
      "Epoch 1677/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0304 - acc: 0.9886 - val_loss: 0.4527 - val_acc: 0.8920\n",
      "Epoch 1678/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0293 - acc: 0.9943 - val_loss: 0.3581 - val_acc: 0.8920\n",
      "Epoch 1679/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0293 - acc: 0.9900 - val_loss: 0.3690 - val_acc: 0.8977\n",
      "Epoch 1680/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0206 - acc: 0.9943 - val_loss: 0.4717 - val_acc: 0.8864\n",
      "Epoch 1681/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0202 - acc: 0.9929 - val_loss: 0.4387 - val_acc: 0.9034\n",
      "Epoch 1682/3000\n",
      "702/702 [==============================] - 0s 581us/sample - loss: 0.0229 - acc: 0.9886 - val_loss: 0.5490 - val_acc: 0.8920\n",
      "Epoch 1683/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0205 - acc: 0.9943 - val_loss: 0.3963 - val_acc: 0.8977\n",
      "Epoch 1684/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0233 - acc: 0.9915 - val_loss: 0.3599 - val_acc: 0.8807\n",
      "Epoch 1685/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.0243 - acc: 0.9943 - val_loss: 0.3998 - val_acc: 0.9091\n",
      "Epoch 1686/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0152 - acc: 0.9957 - val_loss: 0.4409 - val_acc: 0.8977\n",
      "Epoch 1687/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0094 - acc: 0.9986 - val_loss: 0.4635 - val_acc: 0.8977\n",
      "Epoch 1688/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0159 - acc: 0.9957 - val_loss: 0.4851 - val_acc: 0.8977\n",
      "Epoch 1689/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0149 - acc: 0.9943 - val_loss: 0.4871 - val_acc: 0.8977\n",
      "Epoch 1690/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0158 - acc: 0.9943 - val_loss: 0.4110 - val_acc: 0.9034\n",
      "Epoch 1691/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0163 - acc: 0.9957 - val_loss: 0.4171 - val_acc: 0.9034\n",
      "Epoch 1692/3000\n",
      "702/702 [==============================] - 0s 585us/sample - loss: 0.0124 - acc: 0.9972 - val_loss: 0.5064 - val_acc: 0.8977\n",
      "Epoch 1693/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0195 - acc: 0.9957 - val_loss: 0.6002 - val_acc: 0.8977\n",
      "Epoch 1694/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0141 - acc: 0.9957 - val_loss: 0.5705 - val_acc: 0.8977\n",
      "Epoch 1695/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0422 - acc: 0.9843 - val_loss: 0.5162 - val_acc: 0.8977\n",
      "Epoch 1696/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0162 - acc: 0.9972 - val_loss: 0.4142 - val_acc: 0.9034\n",
      "Epoch 1697/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0356 - acc: 0.9915 - val_loss: 0.3502 - val_acc: 0.9034\n",
      "Epoch 1698/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0175 - acc: 0.9943 - val_loss: 0.4076 - val_acc: 0.8977\n",
      "Epoch 1699/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0205 - acc: 0.9943 - val_loss: 0.3665 - val_acc: 0.8977\n",
      "Epoch 1700/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0133 - acc: 0.9972 - val_loss: 0.4253 - val_acc: 0.8920\n",
      "Epoch 1701/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0188 - acc: 0.9943 - val_loss: 0.4374 - val_acc: 0.8864\n",
      "Epoch 1702/3000\n",
      "702/702 [==============================] - 0s 587us/sample - loss: 0.0239 - acc: 0.9915 - val_loss: 0.3883 - val_acc: 0.9091\n",
      "Epoch 1703/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0244 - acc: 0.9929 - val_loss: 0.4261 - val_acc: 0.8920\n",
      "Epoch 1704/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0293 - acc: 0.9915 - val_loss: 0.4153 - val_acc: 0.8864\n",
      "Epoch 1705/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0302 - acc: 0.9886 - val_loss: 0.4497 - val_acc: 0.8920\n",
      "Epoch 1706/3000\n",
      "702/702 [==============================] - 0s 534us/sample - loss: 0.0158 - acc: 0.9943 - val_loss: 0.4551 - val_acc: 0.9091\n",
      "Epoch 1707/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0105 - acc: 0.9986 - val_loss: 0.4884 - val_acc: 0.8977\n",
      "Epoch 1708/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0135 - acc: 0.9972 - val_loss: 0.4933 - val_acc: 0.8977\n",
      "Epoch 1709/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0153 - acc: 0.9957 - val_loss: 0.5478 - val_acc: 0.8920\n",
      "Epoch 1710/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0054 - acc: 0.9986 - val_loss: 0.5318 - val_acc: 0.8864\n",
      "Epoch 1711/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0120 - acc: 0.9957 - val_loss: 0.4890 - val_acc: 0.8920\n",
      "Epoch 1712/3000\n",
      "702/702 [==============================] - 0s 589us/sample - loss: 0.0140 - acc: 0.9957 - val_loss: 0.5163 - val_acc: 0.8977\n",
      "Epoch 1713/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0138 - acc: 0.9943 - val_loss: 0.5203 - val_acc: 0.8920\n",
      "Epoch 1714/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0189 - acc: 0.9886 - val_loss: 0.4591 - val_acc: 0.9091\n",
      "Epoch 1715/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0306 - acc: 0.9915 - val_loss: 0.4027 - val_acc: 0.9034\n",
      "Epoch 1716/3000\n",
      "702/702 [==============================] - 0s 558us/sample - loss: 0.0136 - acc: 0.9957 - val_loss: 0.5192 - val_acc: 0.8920\n",
      "Epoch 1717/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0196 - acc: 0.9943 - val_loss: 0.6411 - val_acc: 0.8864\n",
      "Epoch 1718/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.0150 - acc: 0.9943 - val_loss: 0.5994 - val_acc: 0.9034\n",
      "Epoch 1719/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0196 - acc: 0.9943 - val_loss: 0.4838 - val_acc: 0.8920\n",
      "Epoch 1720/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0136 - acc: 0.9972 - val_loss: 0.4464 - val_acc: 0.9148\n",
      "Epoch 1721/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0061 - acc: 1.0000 - val_loss: 0.5070 - val_acc: 0.9034\n",
      "Epoch 1722/3000\n",
      "702/702 [==============================] - 0s 580us/sample - loss: 0.0064 - acc: 1.0000 - val_loss: 0.5926 - val_acc: 0.8977\n",
      "Epoch 1723/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0154 - acc: 0.9957 - val_loss: 0.6050 - val_acc: 0.9148\n",
      "Epoch 1724/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0109 - acc: 0.9957 - val_loss: 0.5708 - val_acc: 0.9148\n",
      "Epoch 1725/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0156 - acc: 0.9929 - val_loss: 0.5847 - val_acc: 0.8864\n",
      "Epoch 1726/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0166 - acc: 0.9957 - val_loss: 0.5455 - val_acc: 0.8864\n",
      "Epoch 1727/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0150 - acc: 0.9972 - val_loss: 0.4883 - val_acc: 0.8864\n",
      "Epoch 1728/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0292 - acc: 0.9886 - val_loss: 0.6397 - val_acc: 0.8750\n",
      "Epoch 1729/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0147 - acc: 0.9943 - val_loss: 0.4446 - val_acc: 0.8864\n",
      "Epoch 1730/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0373 - acc: 0.9915 - val_loss: 0.4357 - val_acc: 0.9091\n",
      "Epoch 1731/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0345 - acc: 0.9886 - val_loss: 0.4956 - val_acc: 0.8864\n",
      "Epoch 1732/3000\n",
      "702/702 [==============================] - 0s 582us/sample - loss: 0.0357 - acc: 0.9886 - val_loss: 0.4862 - val_acc: 0.8920\n",
      "Epoch 1733/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0093 - acc: 0.9986 - val_loss: 0.5468 - val_acc: 0.8920\n",
      "Epoch 1734/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.0540 - acc: 0.9886 - val_loss: 0.6106 - val_acc: 0.8864\n",
      "Epoch 1735/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0183 - acc: 0.9886 - val_loss: 0.4746 - val_acc: 0.9034\n",
      "Epoch 1736/3000\n",
      "702/702 [==============================] - 0s 555us/sample - loss: 0.0126 - acc: 0.9972 - val_loss: 0.5027 - val_acc: 0.9148\n",
      "Epoch 1737/3000\n",
      "702/702 [==============================] - 0s 543us/sample - loss: 0.0183 - acc: 0.9929 - val_loss: 0.4943 - val_acc: 0.8807\n",
      "Epoch 1738/3000\n",
      "702/702 [==============================] - 0s 542us/sample - loss: 0.0263 - acc: 0.9957 - val_loss: 0.4424 - val_acc: 0.8920\n",
      "Epoch 1739/3000\n",
      "702/702 [==============================] - 0s 535us/sample - loss: 0.0121 - acc: 0.9972 - val_loss: 0.4829 - val_acc: 0.8977\n",
      "Epoch 1740/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.0060 - acc: 0.9986 - val_loss: 0.4870 - val_acc: 0.9148\n",
      "Epoch 1741/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0102 - acc: 0.9972 - val_loss: 0.5060 - val_acc: 0.9148\n",
      "Epoch 1742/3000\n",
      "702/702 [==============================] - 0s 647us/sample - loss: 0.0202 - acc: 0.9943 - val_loss: 0.4984 - val_acc: 0.8920\n",
      "Epoch 1743/3000\n",
      "702/702 [==============================] - 0s 567us/sample - loss: 0.0177 - acc: 0.9929 - val_loss: 0.4434 - val_acc: 0.9034\n",
      "Epoch 1744/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0180 - acc: 0.9943 - val_loss: 0.4364 - val_acc: 0.9034\n",
      "Epoch 1745/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0102 - acc: 0.9972 - val_loss: 0.4183 - val_acc: 0.9091\n",
      "Epoch 1746/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0079 - acc: 1.0000 - val_loss: 0.5152 - val_acc: 0.9034\n",
      "Epoch 1747/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.0127 - acc: 0.9972 - val_loss: 0.4397 - val_acc: 0.9091\n",
      "Epoch 1748/3000\n",
      "702/702 [==============================] - 0s 538us/sample - loss: 0.0090 - acc: 0.9957 - val_loss: 0.4139 - val_acc: 0.9034\n",
      "Epoch 1749/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 0.0186 - acc: 0.9915 - val_loss: 0.4559 - val_acc: 0.9148\n",
      "Epoch 1750/3000\n",
      "702/702 [==============================] - 0s 547us/sample - loss: 0.0168 - acc: 0.9900 - val_loss: 0.5127 - val_acc: 0.8864\n",
      "Epoch 1751/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0172 - acc: 0.9943 - val_loss: 0.4477 - val_acc: 0.8977\n",
      "Epoch 1752/3000\n",
      "702/702 [==============================] - 0s 615us/sample - loss: 0.0283 - acc: 0.9886 - val_loss: 0.5753 - val_acc: 0.8977\n",
      "Epoch 1753/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0501 - acc: 0.9858 - val_loss: 0.3754 - val_acc: 0.9091\n",
      "Epoch 1754/3000\n",
      "702/702 [==============================] - 0s 534us/sample - loss: 0.0290 - acc: 0.9915 - val_loss: 0.3473 - val_acc: 0.9091\n",
      "Epoch 1755/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0214 - acc: 0.9972 - val_loss: 0.3917 - val_acc: 0.9318\n",
      "Epoch 1756/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.0412 - acc: 0.9872 - val_loss: 0.4501 - val_acc: 0.8920\n",
      "Epoch 1757/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0274 - acc: 0.9843 - val_loss: 0.4572 - val_acc: 0.8864\n",
      "Epoch 1758/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0144 - acc: 0.9943 - val_loss: 0.4418 - val_acc: 0.8977\n",
      "Epoch 1759/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0160 - acc: 0.9943 - val_loss: 0.4559 - val_acc: 0.8977\n",
      "Epoch 1760/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0119 - acc: 0.9972 - val_loss: 0.4556 - val_acc: 0.9091\n",
      "Epoch 1761/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0062 - acc: 1.0000 - val_loss: 0.4744 - val_acc: 0.9091\n",
      "Epoch 1762/3000\n",
      "702/702 [==============================] - 0s 601us/sample - loss: 0.0147 - acc: 0.9943 - val_loss: 0.5385 - val_acc: 0.9148\n",
      "Epoch 1763/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0145 - acc: 0.9972 - val_loss: 0.6442 - val_acc: 0.8864\n",
      "Epoch 1764/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0191 - acc: 0.9957 - val_loss: 0.5166 - val_acc: 0.9091\n",
      "Epoch 1765/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0347 - acc: 0.9872 - val_loss: 0.4195 - val_acc: 0.9091\n",
      "Epoch 1766/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0085 - acc: 0.9986 - val_loss: 0.4305 - val_acc: 0.8977\n",
      "Epoch 1767/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.0252 - acc: 0.9943 - val_loss: 0.5092 - val_acc: 0.9034\n",
      "Epoch 1768/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0228 - acc: 0.9957 - val_loss: 0.5366 - val_acc: 0.9091\n",
      "Epoch 1769/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.0147 - acc: 0.9943 - val_loss: 0.5559 - val_acc: 0.9091\n",
      "Epoch 1770/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0298 - acc: 0.9886 - val_loss: 0.4546 - val_acc: 0.8864\n",
      "Epoch 1771/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0143 - acc: 0.9972 - val_loss: 0.4290 - val_acc: 0.8864\n",
      "Epoch 1772/3000\n",
      "702/702 [==============================] - 0s 596us/sample - loss: 0.0049 - acc: 0.9986 - val_loss: 0.5372 - val_acc: 0.9034\n",
      "Epoch 1773/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0210 - acc: 0.9929 - val_loss: 0.5441 - val_acc: 0.9034\n",
      "Epoch 1774/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.0177 - acc: 0.9943 - val_loss: 0.4947 - val_acc: 0.9261\n",
      "Epoch 1775/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0123 - acc: 0.9943 - val_loss: 0.5206 - val_acc: 0.8977\n",
      "Epoch 1776/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0483 - acc: 0.9843 - val_loss: 0.5204 - val_acc: 0.9148\n",
      "Epoch 1777/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0272 - acc: 0.9886 - val_loss: 0.4066 - val_acc: 0.9261\n",
      "Epoch 1778/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0075 - acc: 0.9986 - val_loss: 0.4068 - val_acc: 0.8920\n",
      "Epoch 1779/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0075 - acc: 0.9986 - val_loss: 0.4404 - val_acc: 0.8864\n",
      "Epoch 1780/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0136 - acc: 0.9986 - val_loss: 0.4516 - val_acc: 0.8807\n",
      "Epoch 1781/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0212 - acc: 0.9943 - val_loss: 0.4336 - val_acc: 0.8977\n",
      "Epoch 1782/3000\n",
      "702/702 [==============================] - 0s 594us/sample - loss: 0.0059 - acc: 1.0000 - val_loss: 0.4516 - val_acc: 0.8977\n",
      "Epoch 1783/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0125 - acc: 0.9972 - val_loss: 0.5133 - val_acc: 0.8864\n",
      "Epoch 1784/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0144 - acc: 0.9957 - val_loss: 0.5285 - val_acc: 0.8920\n",
      "Epoch 1785/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.0164 - acc: 0.9929 - val_loss: 0.5296 - val_acc: 0.8977\n",
      "Epoch 1786/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0112 - acc: 0.9957 - val_loss: 0.5386 - val_acc: 0.8977\n",
      "Epoch 1787/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0144 - acc: 0.9972 - val_loss: 0.4362 - val_acc: 0.9034\n",
      "Epoch 1788/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0143 - acc: 0.9972 - val_loss: 0.4484 - val_acc: 0.8977\n",
      "Epoch 1789/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0123 - acc: 0.9957 - val_loss: 0.4862 - val_acc: 0.8920\n",
      "Epoch 1790/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0141 - acc: 0.9972 - val_loss: 0.5247 - val_acc: 0.8977\n",
      "Epoch 1791/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0138 - acc: 0.9957 - val_loss: 0.4822 - val_acc: 0.8864\n",
      "Epoch 1792/3000\n",
      "702/702 [==============================] - 0s 597us/sample - loss: 0.0094 - acc: 0.9972 - val_loss: 0.4581 - val_acc: 0.8920\n",
      "Epoch 1793/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0106 - acc: 0.9986 - val_loss: 0.4599 - val_acc: 0.8864\n",
      "Epoch 1794/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.0074 - acc: 0.9972 - val_loss: 0.4889 - val_acc: 0.8920\n",
      "Epoch 1795/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0096 - acc: 0.9943 - val_loss: 0.4659 - val_acc: 0.8920\n",
      "Epoch 1796/3000\n",
      "702/702 [==============================] - 0s 535us/sample - loss: 0.0162 - acc: 0.9957 - val_loss: 0.4614 - val_acc: 0.8864\n",
      "Epoch 1797/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0107 - acc: 0.9957 - val_loss: 0.5042 - val_acc: 0.8864\n",
      "Epoch 1798/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0177 - acc: 0.9943 - val_loss: 0.5892 - val_acc: 0.8864\n",
      "Epoch 1799/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0083 - acc: 0.9972 - val_loss: 0.6374 - val_acc: 0.8807\n",
      "Epoch 1800/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0220 - acc: 0.9943 - val_loss: 0.4659 - val_acc: 0.9091\n",
      "Epoch 1801/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0101 - acc: 0.9986 - val_loss: 0.4998 - val_acc: 0.9091\n",
      "Epoch 1802/3000\n",
      "702/702 [==============================] - 0s 598us/sample - loss: 0.0311 - acc: 0.9915 - val_loss: 0.5255 - val_acc: 0.8920\n",
      "Epoch 1803/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0154 - acc: 0.9957 - val_loss: 0.4415 - val_acc: 0.8920\n",
      "Epoch 1804/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.0078 - acc: 0.9972 - val_loss: 0.3946 - val_acc: 0.8977\n",
      "Epoch 1805/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0245 - acc: 0.9957 - val_loss: 0.3941 - val_acc: 0.9034\n",
      "Epoch 1806/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0078 - acc: 0.9972 - val_loss: 0.4176 - val_acc: 0.9034\n",
      "Epoch 1807/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0044 - acc: 1.0000 - val_loss: 0.4627 - val_acc: 0.9034\n",
      "Epoch 1808/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0073 - acc: 0.9972 - val_loss: 0.5015 - val_acc: 0.8864\n",
      "Epoch 1809/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0098 - acc: 0.9972 - val_loss: 0.4590 - val_acc: 0.8977\n",
      "Epoch 1810/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0128 - acc: 0.9943 - val_loss: 0.4116 - val_acc: 0.8977\n",
      "Epoch 1811/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0182 - acc: 0.9929 - val_loss: 0.3961 - val_acc: 0.8864\n",
      "Epoch 1812/3000\n",
      "702/702 [==============================] - 0s 597us/sample - loss: 0.0391 - acc: 0.9915 - val_loss: 0.5673 - val_acc: 0.9034\n",
      "Epoch 1813/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0125 - acc: 0.9972 - val_loss: 0.5097 - val_acc: 0.8864\n",
      "Epoch 1814/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0136 - acc: 0.9972 - val_loss: 0.5360 - val_acc: 0.8864\n",
      "Epoch 1815/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0125 - acc: 0.9943 - val_loss: 0.7882 - val_acc: 0.8636\n",
      "Epoch 1816/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0218 - acc: 0.9915 - val_loss: 0.4789 - val_acc: 0.8920\n",
      "Epoch 1817/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0292 - acc: 0.9943 - val_loss: 0.4165 - val_acc: 0.9091\n",
      "Epoch 1818/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0129 - acc: 0.9943 - val_loss: 0.5098 - val_acc: 0.8920\n",
      "Epoch 1819/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.0294 - acc: 0.9900 - val_loss: 0.5099 - val_acc: 0.8920\n",
      "Epoch 1820/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0338 - acc: 0.9915 - val_loss: 0.4588 - val_acc: 0.9034\n",
      "Epoch 1821/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0139 - acc: 0.9957 - val_loss: 0.5304 - val_acc: 0.9034\n",
      "Epoch 1822/3000\n",
      "702/702 [==============================] - 0s 594us/sample - loss: 0.0112 - acc: 0.9957 - val_loss: 0.4878 - val_acc: 0.8864\n",
      "Epoch 1823/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0192 - acc: 0.9972 - val_loss: 0.4875 - val_acc: 0.8807\n",
      "Epoch 1824/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0224 - acc: 0.9972 - val_loss: 0.4734 - val_acc: 0.8864\n",
      "Epoch 1825/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0202 - acc: 0.9929 - val_loss: 0.4644 - val_acc: 0.8977\n",
      "Epoch 1826/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0143 - acc: 0.9957 - val_loss: 0.6003 - val_acc: 0.9034\n",
      "Epoch 1827/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0190 - acc: 0.9929 - val_loss: 0.4932 - val_acc: 0.8920\n",
      "Epoch 1828/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.0118 - acc: 0.9986 - val_loss: 0.4292 - val_acc: 0.8920\n",
      "Epoch 1829/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0262 - acc: 0.9915 - val_loss: 0.4868 - val_acc: 0.8977\n",
      "Epoch 1830/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0115 - acc: 0.9957 - val_loss: 0.5943 - val_acc: 0.9091\n",
      "Epoch 1831/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0163 - acc: 0.9957 - val_loss: 0.5935 - val_acc: 0.9091\n",
      "Epoch 1832/3000\n",
      "702/702 [==============================] - 0s 595us/sample - loss: 0.0191 - acc: 0.9929 - val_loss: 0.3794 - val_acc: 0.8977\n",
      "Epoch 1833/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0205 - acc: 0.9929 - val_loss: 0.3609 - val_acc: 0.8977\n",
      "Epoch 1834/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0115 - acc: 0.9957 - val_loss: 0.3868 - val_acc: 0.9034\n",
      "Epoch 1835/3000\n",
      "702/702 [==============================] - 0s 534us/sample - loss: 0.0206 - acc: 0.9929 - val_loss: 0.4249 - val_acc: 0.8977\n",
      "Epoch 1836/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0038 - acc: 0.9986 - val_loss: 0.4866 - val_acc: 0.9091\n",
      "Epoch 1837/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0095 - acc: 0.9972 - val_loss: 0.5155 - val_acc: 0.8977\n",
      "Epoch 1838/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0038 - acc: 0.9986 - val_loss: 0.5694 - val_acc: 0.9034\n",
      "Epoch 1839/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0096 - acc: 0.9986 - val_loss: 0.5002 - val_acc: 0.9091\n",
      "Epoch 1840/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0124 - acc: 0.9943 - val_loss: 0.4665 - val_acc: 0.9091\n",
      "Epoch 1841/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0063 - acc: 0.9972 - val_loss: 0.4794 - val_acc: 0.8977\n",
      "Epoch 1842/3000\n",
      "702/702 [==============================] - 0s 601us/sample - loss: 0.0131 - acc: 0.9957 - val_loss: 0.4305 - val_acc: 0.8920\n",
      "Epoch 1843/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0379 - acc: 0.9900 - val_loss: 0.4163 - val_acc: 0.9034\n",
      "Epoch 1844/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0061 - acc: 0.9986 - val_loss: 0.4282 - val_acc: 0.8977\n",
      "Epoch 1845/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.0293 - acc: 0.9900 - val_loss: 0.3513 - val_acc: 0.9148\n",
      "Epoch 1846/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0087 - acc: 0.9972 - val_loss: 0.4013 - val_acc: 0.8977\n",
      "Epoch 1847/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0188 - acc: 0.9900 - val_loss: 0.4823 - val_acc: 0.8977\n",
      "Epoch 1848/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0252 - acc: 0.9915 - val_loss: 0.5958 - val_acc: 0.8807\n",
      "Epoch 1849/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0198 - acc: 0.9929 - val_loss: 0.4795 - val_acc: 0.8920\n",
      "Epoch 1850/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0264 - acc: 0.9929 - val_loss: 0.4666 - val_acc: 0.9148\n",
      "Epoch 1851/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0192 - acc: 0.9915 - val_loss: 0.5062 - val_acc: 0.8977\n",
      "Epoch 1852/3000\n",
      "702/702 [==============================] - 0s 591us/sample - loss: 0.0224 - acc: 0.9929 - val_loss: 0.4827 - val_acc: 0.9091\n",
      "Epoch 1853/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0209 - acc: 0.9943 - val_loss: 0.4557 - val_acc: 0.9034\n",
      "Epoch 1854/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0091 - acc: 0.9986 - val_loss: 0.3894 - val_acc: 0.9148\n",
      "Epoch 1855/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0043 - acc: 1.0000 - val_loss: 0.3693 - val_acc: 0.9205\n",
      "Epoch 1856/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0061 - acc: 0.9986 - val_loss: 0.3624 - val_acc: 0.9091\n",
      "Epoch 1857/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0097 - acc: 0.9943 - val_loss: 0.3485 - val_acc: 0.9091\n",
      "Epoch 1858/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0180 - acc: 0.9972 - val_loss: 0.4218 - val_acc: 0.9034\n",
      "Epoch 1859/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0049 - acc: 1.0000 - val_loss: 0.4640 - val_acc: 0.9034\n",
      "Epoch 1860/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0073 - acc: 0.9986 - val_loss: 0.3748 - val_acc: 0.9034\n",
      "Epoch 1861/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0071 - acc: 0.9972 - val_loss: 0.4460 - val_acc: 0.9091\n",
      "Epoch 1862/3000\n",
      "702/702 [==============================] - 0s 581us/sample - loss: 0.0071 - acc: 0.9972 - val_loss: 0.4967 - val_acc: 0.9034\n",
      "Epoch 1863/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0105 - acc: 0.9972 - val_loss: 0.5592 - val_acc: 0.9034\n",
      "Epoch 1864/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0146 - acc: 0.9957 - val_loss: 0.5334 - val_acc: 0.9034\n",
      "Epoch 1865/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0074 - acc: 0.9972 - val_loss: 0.5696 - val_acc: 0.8920\n",
      "Epoch 1866/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0092 - acc: 0.9972 - val_loss: 0.4777 - val_acc: 0.8977\n",
      "Epoch 1867/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0053 - acc: 1.0000 - val_loss: 0.4878 - val_acc: 0.9148\n",
      "Epoch 1868/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0052 - acc: 0.9986 - val_loss: 0.5004 - val_acc: 0.9091\n",
      "Epoch 1869/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0093 - acc: 0.9986 - val_loss: 0.4508 - val_acc: 0.9034\n",
      "Epoch 1870/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0099 - acc: 0.9957 - val_loss: 0.5136 - val_acc: 0.8977\n",
      "Epoch 1871/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0256 - acc: 0.9943 - val_loss: 0.3857 - val_acc: 0.9091\n",
      "Epoch 1872/3000\n",
      "702/702 [==============================] - 0s 603us/sample - loss: 0.0362 - acc: 0.9900 - val_loss: 0.4940 - val_acc: 0.8977\n",
      "Epoch 1873/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0261 - acc: 0.9915 - val_loss: 0.6962 - val_acc: 0.8864\n",
      "Epoch 1874/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0295 - acc: 0.9900 - val_loss: 0.5343 - val_acc: 0.8920\n",
      "Epoch 1875/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0194 - acc: 0.9915 - val_loss: 0.4159 - val_acc: 0.8750\n",
      "Epoch 1876/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0400 - acc: 0.9858 - val_loss: 0.4577 - val_acc: 0.8864\n",
      "Epoch 1877/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0478 - acc: 0.9843 - val_loss: 0.6049 - val_acc: 0.8807\n",
      "Epoch 1878/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0283 - acc: 0.9886 - val_loss: 0.2986 - val_acc: 0.9034\n",
      "Epoch 1879/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0374 - acc: 0.9915 - val_loss: 0.3258 - val_acc: 0.8864\n",
      "Epoch 1880/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0232 - acc: 0.9943 - val_loss: 0.5226 - val_acc: 0.8750\n",
      "Epoch 1881/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0328 - acc: 0.9915 - val_loss: 0.4526 - val_acc: 0.8920\n",
      "Epoch 1882/3000\n",
      "702/702 [==============================] - 0s 597us/sample - loss: 0.0303 - acc: 0.9886 - val_loss: 0.4362 - val_acc: 0.8807\n",
      "Epoch 1883/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0186 - acc: 0.9972 - val_loss: 0.5948 - val_acc: 0.8750\n",
      "Epoch 1884/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0280 - acc: 0.9929 - val_loss: 0.5827 - val_acc: 0.8864\n",
      "Epoch 1885/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0213 - acc: 0.9929 - val_loss: 0.3794 - val_acc: 0.8864\n",
      "Epoch 1886/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.0394 - acc: 0.9886 - val_loss: 0.3092 - val_acc: 0.9034\n",
      "Epoch 1887/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0553 - acc: 0.9843 - val_loss: 0.4225 - val_acc: 0.8807\n",
      "Epoch 1888/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0264 - acc: 0.9915 - val_loss: 0.4047 - val_acc: 0.8750\n",
      "Epoch 1889/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0291 - acc: 0.9900 - val_loss: 0.3996 - val_acc: 0.9034\n",
      "Epoch 1890/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0154 - acc: 0.9957 - val_loss: 0.4352 - val_acc: 0.8977\n",
      "Epoch 1891/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0080 - acc: 0.9972 - val_loss: 0.4995 - val_acc: 0.8920\n",
      "Epoch 1892/3000\n",
      "702/702 [==============================] - 0s 596us/sample - loss: 0.0292 - acc: 0.9972 - val_loss: 0.4262 - val_acc: 0.9034\n",
      "Epoch 1893/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0128 - acc: 0.9957 - val_loss: 0.4084 - val_acc: 0.9034\n",
      "Epoch 1894/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0113 - acc: 0.9986 - val_loss: 0.3918 - val_acc: 0.8920\n",
      "Epoch 1895/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0105 - acc: 0.9972 - val_loss: 0.4202 - val_acc: 0.8977\n",
      "Epoch 1896/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0138 - acc: 0.9915 - val_loss: 0.5080 - val_acc: 0.9034\n",
      "Epoch 1897/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0136 - acc: 0.9972 - val_loss: 0.4294 - val_acc: 0.9091\n",
      "Epoch 1898/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.0209 - acc: 0.9943 - val_loss: 0.4492 - val_acc: 0.9148\n",
      "Epoch 1899/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0204 - acc: 0.9943 - val_loss: 0.4438 - val_acc: 0.9148\n",
      "Epoch 1900/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0161 - acc: 0.9957 - val_loss: 0.5848 - val_acc: 0.9034\n",
      "Epoch 1901/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0115 - acc: 0.9972 - val_loss: 0.6613 - val_acc: 0.9091\n",
      "Epoch 1902/3000\n",
      "702/702 [==============================] - 0s 586us/sample - loss: 0.0332 - acc: 0.9886 - val_loss: 0.4477 - val_acc: 0.8977\n",
      "Epoch 1903/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0278 - acc: 0.9929 - val_loss: 0.4204 - val_acc: 0.9148\n",
      "Epoch 1904/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0074 - acc: 0.9986 - val_loss: 0.6007 - val_acc: 0.9205\n",
      "Epoch 1905/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0256 - acc: 0.9929 - val_loss: 0.5633 - val_acc: 0.8977\n",
      "Epoch 1906/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0150 - acc: 0.9943 - val_loss: 0.6351 - val_acc: 0.8977\n",
      "Epoch 1907/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0134 - acc: 0.9957 - val_loss: 0.7012 - val_acc: 0.9034\n",
      "Epoch 1908/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0033 - acc: 1.0000 - val_loss: 0.6964 - val_acc: 0.8920\n",
      "Epoch 1909/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0073 - acc: 0.9972 - val_loss: 0.5502 - val_acc: 0.9034\n",
      "Epoch 1910/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0054 - acc: 0.9986 - val_loss: 0.5212 - val_acc: 0.9091\n",
      "Epoch 1911/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0075 - acc: 0.9986 - val_loss: 0.4783 - val_acc: 0.9091\n",
      "Epoch 1912/3000\n",
      "702/702 [==============================] - 0s 594us/sample - loss: 0.0077 - acc: 0.9957 - val_loss: 0.4264 - val_acc: 0.9091\n",
      "Epoch 1913/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0092 - acc: 0.9972 - val_loss: 0.4710 - val_acc: 0.9034\n",
      "Epoch 1914/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0141 - acc: 0.9957 - val_loss: 0.5736 - val_acc: 0.9034\n",
      "Epoch 1915/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0144 - acc: 0.9943 - val_loss: 0.4434 - val_acc: 0.9261\n",
      "Epoch 1916/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0076 - acc: 0.9972 - val_loss: 0.4395 - val_acc: 0.9034\n",
      "Epoch 1917/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0093 - acc: 0.9972 - val_loss: 0.5137 - val_acc: 0.8920\n",
      "Epoch 1918/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0060 - acc: 0.9986 - val_loss: 0.5147 - val_acc: 0.8977\n",
      "Epoch 1919/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.0076 - acc: 1.0000 - val_loss: 0.5006 - val_acc: 0.9034\n",
      "Epoch 1920/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0076 - acc: 0.9972 - val_loss: 0.5015 - val_acc: 0.9034\n",
      "Epoch 1921/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0107 - acc: 0.9943 - val_loss: 0.5124 - val_acc: 0.9034\n",
      "Epoch 1922/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.0087 - acc: 0.9972 - val_loss: 0.4512 - val_acc: 0.9034\n",
      "Epoch 1923/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0291 - acc: 0.9915 - val_loss: 0.4982 - val_acc: 0.8920\n",
      "Epoch 1924/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0159 - acc: 0.9943 - val_loss: 0.5401 - val_acc: 0.8977\n",
      "Epoch 1925/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.0126 - acc: 0.9972 - val_loss: 0.5601 - val_acc: 0.8977\n",
      "Epoch 1926/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0161 - acc: 0.9943 - val_loss: 0.6284 - val_acc: 0.9148\n",
      "Epoch 1927/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0089 - acc: 0.9972 - val_loss: 0.6000 - val_acc: 0.9034\n",
      "Epoch 1928/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0179 - acc: 0.9915 - val_loss: 0.4747 - val_acc: 0.9148\n",
      "Epoch 1929/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0252 - acc: 0.9929 - val_loss: 0.5676 - val_acc: 0.8920\n",
      "Epoch 1930/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0119 - acc: 0.9957 - val_loss: 0.7678 - val_acc: 0.9091\n",
      "Epoch 1931/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0094 - acc: 0.9943 - val_loss: 0.8256 - val_acc: 0.8977\n",
      "Epoch 1932/3000\n",
      "702/702 [==============================] - 0s 594us/sample - loss: 0.0223 - acc: 0.9943 - val_loss: 0.5155 - val_acc: 0.8807\n",
      "Epoch 1933/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0131 - acc: 0.9943 - val_loss: 0.5005 - val_acc: 0.8977\n",
      "Epoch 1934/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0130 - acc: 0.9957 - val_loss: 0.6526 - val_acc: 0.8977\n",
      "Epoch 1935/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0079 - acc: 0.9972 - val_loss: 0.7034 - val_acc: 0.8920\n",
      "Epoch 1936/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0100 - acc: 0.9943 - val_loss: 0.8012 - val_acc: 0.8977\n",
      "Epoch 1937/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0130 - acc: 0.9943 - val_loss: 0.8945 - val_acc: 0.8920\n",
      "Epoch 1938/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.0241 - acc: 0.9929 - val_loss: 0.7871 - val_acc: 0.9034\n",
      "Epoch 1939/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0114 - acc: 0.9943 - val_loss: 0.6764 - val_acc: 0.8864\n",
      "Epoch 1940/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0060 - acc: 1.0000 - val_loss: 0.5778 - val_acc: 0.8977\n",
      "Epoch 1941/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0096 - acc: 0.9957 - val_loss: 0.6717 - val_acc: 0.8977\n",
      "Epoch 1942/3000\n",
      "702/702 [==============================] - 0s 588us/sample - loss: 0.0067 - acc: 0.9986 - val_loss: 0.6793 - val_acc: 0.9034\n",
      "Epoch 1943/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0108 - acc: 0.9943 - val_loss: 0.5975 - val_acc: 0.8977\n",
      "Epoch 1944/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0257 - acc: 0.9943 - val_loss: 0.5686 - val_acc: 0.9034\n",
      "Epoch 1945/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0207 - acc: 0.9929 - val_loss: 0.5317 - val_acc: 0.9034\n",
      "Epoch 1946/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0235 - acc: 0.9929 - val_loss: 0.7166 - val_acc: 0.8864\n",
      "Epoch 1947/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0252 - acc: 0.9929 - val_loss: 0.6835 - val_acc: 0.8750\n",
      "Epoch 1948/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0452 - acc: 0.9843 - val_loss: 0.3828 - val_acc: 0.8977\n",
      "Epoch 1949/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0280 - acc: 0.9943 - val_loss: 0.3271 - val_acc: 0.9091\n",
      "Epoch 1950/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0246 - acc: 0.9929 - val_loss: 0.5837 - val_acc: 0.9091\n",
      "Epoch 1951/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0170 - acc: 0.9929 - val_loss: 0.5700 - val_acc: 0.8977\n",
      "Epoch 1952/3000\n",
      "702/702 [==============================] - 0s 585us/sample - loss: 0.0143 - acc: 0.9915 - val_loss: 0.5225 - val_acc: 0.9034\n",
      "Epoch 1953/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0249 - acc: 0.9929 - val_loss: 0.4905 - val_acc: 0.9148\n",
      "Epoch 1954/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0154 - acc: 0.9915 - val_loss: 0.3812 - val_acc: 0.9091\n",
      "Epoch 1955/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0267 - acc: 0.9915 - val_loss: 0.6138 - val_acc: 0.8807\n",
      "Epoch 1956/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.0181 - acc: 0.9943 - val_loss: 0.5635 - val_acc: 0.8864\n",
      "Epoch 1957/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0157 - acc: 0.9957 - val_loss: 0.4012 - val_acc: 0.8977\n",
      "Epoch 1958/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0145 - acc: 0.9957 - val_loss: 0.4525 - val_acc: 0.8977\n",
      "Epoch 1959/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.0121 - acc: 0.9957 - val_loss: 0.5544 - val_acc: 0.9148\n",
      "Epoch 1960/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0130 - acc: 0.9986 - val_loss: 0.5544 - val_acc: 0.9091\n",
      "Epoch 1961/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0207 - acc: 0.9957 - val_loss: 0.5124 - val_acc: 0.8977\n",
      "Epoch 1962/3000\n",
      "702/702 [==============================] - 0s 594us/sample - loss: 0.0097 - acc: 0.9972 - val_loss: 0.4700 - val_acc: 0.9034\n",
      "Epoch 1963/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0112 - acc: 0.9929 - val_loss: 0.4806 - val_acc: 0.9148\n",
      "Epoch 1964/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0170 - acc: 0.9943 - val_loss: 0.4868 - val_acc: 0.9261\n",
      "Epoch 1965/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0215 - acc: 0.9929 - val_loss: 0.5816 - val_acc: 0.9148\n",
      "Epoch 1966/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0162 - acc: 0.9943 - val_loss: 0.5720 - val_acc: 0.8750\n",
      "Epoch 1967/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0382 - acc: 0.9900 - val_loss: 0.6637 - val_acc: 0.8864\n",
      "Epoch 1968/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0247 - acc: 0.9915 - val_loss: 0.5468 - val_acc: 0.8920\n",
      "Epoch 1969/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0121 - acc: 0.9943 - val_loss: 0.4586 - val_acc: 0.8920\n",
      "Epoch 1970/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0092 - acc: 0.9972 - val_loss: 0.4141 - val_acc: 0.9091\n",
      "Epoch 1971/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0235 - acc: 0.9900 - val_loss: 0.5556 - val_acc: 0.8977\n",
      "Epoch 1972/3000\n",
      "702/702 [==============================] - 0s 613us/sample - loss: 0.0180 - acc: 0.9972 - val_loss: 0.5443 - val_acc: 0.9034\n",
      "Epoch 1973/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.0101 - acc: 0.9972 - val_loss: 0.4383 - val_acc: 0.9034\n",
      "Epoch 1974/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0124 - acc: 0.9972 - val_loss: 0.4545 - val_acc: 0.9091\n",
      "Epoch 1975/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0189 - acc: 0.9957 - val_loss: 0.5092 - val_acc: 0.9091\n",
      "Epoch 1976/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0049 - acc: 0.9986 - val_loss: 0.4994 - val_acc: 0.9148\n",
      "Epoch 1977/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0238 - acc: 0.9900 - val_loss: 0.4786 - val_acc: 0.8977\n",
      "Epoch 1978/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0089 - acc: 1.0000 - val_loss: 0.6233 - val_acc: 0.8864\n",
      "Epoch 1979/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0103 - acc: 0.9972 - val_loss: 0.7286 - val_acc: 0.8807\n",
      "Epoch 1980/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0217 - acc: 0.9929 - val_loss: 0.4272 - val_acc: 0.8977\n",
      "Epoch 1981/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0172 - acc: 0.9943 - val_loss: 0.4078 - val_acc: 0.9034\n",
      "Epoch 1982/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.0136 - acc: 0.9943 - val_loss: 0.5533 - val_acc: 0.8920\n",
      "Epoch 1983/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0068 - acc: 0.9986 - val_loss: 0.5344 - val_acc: 0.8977\n",
      "Epoch 1984/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0068 - acc: 0.9986 - val_loss: 0.4777 - val_acc: 0.8977\n",
      "Epoch 1985/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.0099 - acc: 0.9986 - val_loss: 0.4326 - val_acc: 0.8977\n",
      "Epoch 1986/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0141 - acc: 0.9957 - val_loss: 0.4432 - val_acc: 0.8977\n",
      "Epoch 1987/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.0145 - acc: 0.9943 - val_loss: 0.5000 - val_acc: 0.8977\n",
      "Epoch 1988/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0302 - acc: 0.9886 - val_loss: 0.6165 - val_acc: 0.9034\n",
      "Epoch 1989/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0125 - acc: 0.9957 - val_loss: 0.5486 - val_acc: 0.8920\n",
      "Epoch 1990/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0308 - acc: 0.9915 - val_loss: 0.4686 - val_acc: 0.9034\n",
      "Epoch 1991/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0022 - acc: 1.0000 - val_loss: 0.4632 - val_acc: 0.9034\n",
      "Epoch 1992/3000\n",
      "702/702 [==============================] - 0s 597us/sample - loss: 0.0136 - acc: 0.9957 - val_loss: 0.4568 - val_acc: 0.9034\n",
      "Epoch 1993/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0115 - acc: 0.9957 - val_loss: 0.4421 - val_acc: 0.9034\n",
      "Epoch 1994/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0192 - acc: 0.9929 - val_loss: 0.5751 - val_acc: 0.9034\n",
      "Epoch 1995/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0063 - acc: 0.9986 - val_loss: 0.5543 - val_acc: 0.9034\n",
      "Epoch 1996/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.0112 - acc: 0.9972 - val_loss: 0.4444 - val_acc: 0.8977\n",
      "Epoch 1997/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0063 - acc: 0.9986 - val_loss: 0.4088 - val_acc: 0.9034\n",
      "Epoch 1998/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0108 - acc: 0.9972 - val_loss: 0.5766 - val_acc: 0.9034\n",
      "Epoch 1999/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0222 - acc: 0.9929 - val_loss: 0.5243 - val_acc: 0.9034\n",
      "Epoch 2000/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0163 - acc: 0.9929 - val_loss: 0.4494 - val_acc: 0.9091\n",
      "Epoch 2001/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0119 - acc: 0.9957 - val_loss: 0.4202 - val_acc: 0.9034\n",
      "Epoch 2002/3000\n",
      "702/702 [==============================] - 0s 593us/sample - loss: 0.0105 - acc: 0.9972 - val_loss: 0.4898 - val_acc: 0.9034\n",
      "Epoch 2003/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0086 - acc: 0.9972 - val_loss: 0.5204 - val_acc: 0.8977\n",
      "Epoch 2004/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0159 - acc: 0.9957 - val_loss: 0.4335 - val_acc: 0.9091\n",
      "Epoch 2005/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0070 - acc: 0.9986 - val_loss: 0.4201 - val_acc: 0.9205\n",
      "Epoch 2006/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.0131 - acc: 0.9943 - val_loss: 0.6369 - val_acc: 0.9034\n",
      "Epoch 2007/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0115 - acc: 0.9972 - val_loss: 0.6499 - val_acc: 0.9148\n",
      "Epoch 2008/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0144 - acc: 0.9972 - val_loss: 0.5172 - val_acc: 0.8977\n",
      "Epoch 2009/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0105 - acc: 0.9972 - val_loss: 0.5080 - val_acc: 0.9091\n",
      "Epoch 2010/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0100 - acc: 0.9972 - val_loss: 0.4970 - val_acc: 0.8977\n",
      "Epoch 2011/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0213 - acc: 0.9929 - val_loss: 0.4280 - val_acc: 0.9034\n",
      "Epoch 2012/3000\n",
      "702/702 [==============================] - 0s 598us/sample - loss: 0.0174 - acc: 0.9929 - val_loss: 0.4433 - val_acc: 0.9148\n",
      "Epoch 2013/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0113 - acc: 0.9957 - val_loss: 0.5868 - val_acc: 0.9148\n",
      "Epoch 2014/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0191 - acc: 0.9957 - val_loss: 0.4579 - val_acc: 0.8977\n",
      "Epoch 2015/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0158 - acc: 0.9929 - val_loss: 0.3937 - val_acc: 0.8977\n",
      "Epoch 2016/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0256 - acc: 0.9886 - val_loss: 0.4706 - val_acc: 0.9205\n",
      "Epoch 2017/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0364 - acc: 0.9943 - val_loss: 0.5042 - val_acc: 0.9148\n",
      "Epoch 2018/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0155 - acc: 0.9929 - val_loss: 0.4065 - val_acc: 0.9034\n",
      "Epoch 2019/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0157 - acc: 0.9943 - val_loss: 0.3785 - val_acc: 0.8977\n",
      "Epoch 2020/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0102 - acc: 0.9957 - val_loss: 0.4765 - val_acc: 0.8977\n",
      "Epoch 2021/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0186 - acc: 0.9943 - val_loss: 0.5315 - val_acc: 0.9091\n",
      "Epoch 2022/3000\n",
      "702/702 [==============================] - 0s 593us/sample - loss: 0.0161 - acc: 0.9943 - val_loss: 0.5020 - val_acc: 0.9091\n",
      "Epoch 2023/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0386 - acc: 0.9886 - val_loss: 0.4459 - val_acc: 0.8920\n",
      "Epoch 2024/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0117 - acc: 0.9943 - val_loss: 0.4257 - val_acc: 0.9091\n",
      "Epoch 2025/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0151 - acc: 0.9929 - val_loss: 0.4621 - val_acc: 0.9034\n",
      "Epoch 2026/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0199 - acc: 0.9915 - val_loss: 0.5090 - val_acc: 0.9091\n",
      "Epoch 2027/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0136 - acc: 0.9972 - val_loss: 0.5188 - val_acc: 0.9034\n",
      "Epoch 2028/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0067 - acc: 0.9986 - val_loss: 0.5588 - val_acc: 0.8977\n",
      "Epoch 2029/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0142 - acc: 0.9957 - val_loss: 0.6029 - val_acc: 0.8977\n",
      "Epoch 2030/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0080 - acc: 0.9957 - val_loss: 0.5432 - val_acc: 0.9034\n",
      "Epoch 2031/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0081 - acc: 0.9957 - val_loss: 0.4585 - val_acc: 0.9091\n",
      "Epoch 2032/3000\n",
      "702/702 [==============================] - 0s 589us/sample - loss: 0.0085 - acc: 0.9972 - val_loss: 0.4686 - val_acc: 0.9034\n",
      "Epoch 2033/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0137 - acc: 0.9957 - val_loss: 0.5663 - val_acc: 0.8977\n",
      "Epoch 2034/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0161 - acc: 0.9957 - val_loss: 0.4838 - val_acc: 0.8977\n",
      "Epoch 2035/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0115 - acc: 0.9943 - val_loss: 0.4225 - val_acc: 0.8977\n",
      "Epoch 2036/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0095 - acc: 0.9986 - val_loss: 0.3815 - val_acc: 0.9034\n",
      "Epoch 2037/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0090 - acc: 0.9986 - val_loss: 0.3757 - val_acc: 0.9034\n",
      "Epoch 2038/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0167 - acc: 0.9957 - val_loss: 0.4638 - val_acc: 0.9034\n",
      "Epoch 2039/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0236 - acc: 0.9943 - val_loss: 0.6247 - val_acc: 0.9034\n",
      "Epoch 2040/3000\n",
      "702/702 [==============================] - 0s 535us/sample - loss: 0.0109 - acc: 0.9957 - val_loss: 0.5490 - val_acc: 0.8977\n",
      "Epoch 2041/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0058 - acc: 0.9972 - val_loss: 0.5014 - val_acc: 0.8864\n",
      "Epoch 2042/3000\n",
      "702/702 [==============================] - 0s 588us/sample - loss: 0.0085 - acc: 0.9986 - val_loss: 0.5122 - val_acc: 0.8864\n",
      "Epoch 2043/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0217 - acc: 0.9957 - val_loss: 0.4410 - val_acc: 0.8977\n",
      "Epoch 2044/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0165 - acc: 0.9957 - val_loss: 0.4680 - val_acc: 0.8977\n",
      "Epoch 2045/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0060 - acc: 0.9972 - val_loss: 0.5634 - val_acc: 0.8977\n",
      "Epoch 2046/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0373 - acc: 0.9957 - val_loss: 0.6505 - val_acc: 0.8920\n",
      "Epoch 2047/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0304 - acc: 0.9915 - val_loss: 0.6192 - val_acc: 0.8920\n",
      "Epoch 2048/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0196 - acc: 0.9929 - val_loss: 0.5172 - val_acc: 0.8977\n",
      "Epoch 2049/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0092 - acc: 0.9972 - val_loss: 0.4340 - val_acc: 0.9034\n",
      "Epoch 2050/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0202 - acc: 0.9957 - val_loss: 0.4486 - val_acc: 0.9091\n",
      "Epoch 2051/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0124 - acc: 0.9972 - val_loss: 0.5288 - val_acc: 0.9091\n",
      "Epoch 2052/3000\n",
      "702/702 [==============================] - 0s 588us/sample - loss: 0.0129 - acc: 0.9957 - val_loss: 0.5652 - val_acc: 0.9034\n",
      "Epoch 2053/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0157 - acc: 0.9957 - val_loss: 0.5483 - val_acc: 0.9034\n",
      "Epoch 2054/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0098 - acc: 0.9972 - val_loss: 0.6075 - val_acc: 0.8977\n",
      "Epoch 2055/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0097 - acc: 0.9957 - val_loss: 0.6900 - val_acc: 0.8920\n",
      "Epoch 2056/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0153 - acc: 0.9957 - val_loss: 0.7105 - val_acc: 0.8977\n",
      "Epoch 2057/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0095 - acc: 0.9986 - val_loss: 0.6727 - val_acc: 0.9091\n",
      "Epoch 2058/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0087 - acc: 0.9957 - val_loss: 0.6253 - val_acc: 0.9034\n",
      "Epoch 2059/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0068 - acc: 0.9986 - val_loss: 0.6158 - val_acc: 0.9034\n",
      "Epoch 2060/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0083 - acc: 0.9972 - val_loss: 0.5928 - val_acc: 0.8977\n",
      "Epoch 2061/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0089 - acc: 0.9957 - val_loss: 0.5597 - val_acc: 0.8977\n",
      "Epoch 2062/3000\n",
      "702/702 [==============================] - 0s 578us/sample - loss: 0.0056 - acc: 0.9986 - val_loss: 0.6791 - val_acc: 0.8977\n",
      "Epoch 2063/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0147 - acc: 0.9943 - val_loss: 0.4012 - val_acc: 0.9034\n",
      "Epoch 2064/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0173 - acc: 0.9957 - val_loss: 0.3418 - val_acc: 0.9091\n",
      "Epoch 2065/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0135 - acc: 0.9972 - val_loss: 0.3860 - val_acc: 0.9034\n",
      "Epoch 2066/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0076 - acc: 0.9972 - val_loss: 0.4840 - val_acc: 0.8977\n",
      "Epoch 2067/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0032 - acc: 1.0000 - val_loss: 0.5416 - val_acc: 0.9034\n",
      "Epoch 2068/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0022 - acc: 1.0000 - val_loss: 0.5978 - val_acc: 0.9034\n",
      "Epoch 2069/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0046 - acc: 1.0000 - val_loss: 0.5523 - val_acc: 0.9034\n",
      "Epoch 2070/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0037 - acc: 1.0000 - val_loss: 0.5430 - val_acc: 0.9034\n",
      "Epoch 2071/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0111 - acc: 0.9972 - val_loss: 0.5165 - val_acc: 0.9148\n",
      "Epoch 2072/3000\n",
      "702/702 [==============================] - 0s 580us/sample - loss: 0.0026 - acc: 1.0000 - val_loss: 0.5188 - val_acc: 0.9034\n",
      "Epoch 2073/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0087 - acc: 0.9972 - val_loss: 0.5557 - val_acc: 0.9034\n",
      "Epoch 2074/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0081 - acc: 0.9957 - val_loss: 0.5409 - val_acc: 0.9205\n",
      "Epoch 2075/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0126 - acc: 0.9972 - val_loss: 0.5644 - val_acc: 0.9034\n",
      "Epoch 2076/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0046 - acc: 1.0000 - val_loss: 0.6220 - val_acc: 0.8977\n",
      "Epoch 2077/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0106 - acc: 0.9972 - val_loss: 0.5320 - val_acc: 0.8920\n",
      "Epoch 2078/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0111 - acc: 0.9957 - val_loss: 0.5076 - val_acc: 0.8920\n",
      "Epoch 2079/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0053 - acc: 0.9986 - val_loss: 0.5274 - val_acc: 0.8750\n",
      "Epoch 2080/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0116 - acc: 0.9972 - val_loss: 0.5632 - val_acc: 0.8807\n",
      "Epoch 2081/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0038 - acc: 1.0000 - val_loss: 0.5947 - val_acc: 0.8920\n",
      "Epoch 2082/3000\n",
      "702/702 [==============================] - 0s 588us/sample - loss: 0.0057 - acc: 0.9972 - val_loss: 0.6317 - val_acc: 0.8977\n",
      "Epoch 2083/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0276 - acc: 0.9957 - val_loss: 0.5997 - val_acc: 0.9205\n",
      "Epoch 2084/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0151 - acc: 0.9929 - val_loss: 0.5284 - val_acc: 0.8920\n",
      "Epoch 2085/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0226 - acc: 0.9929 - val_loss: 0.4995 - val_acc: 0.8807\n",
      "Epoch 2086/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0192 - acc: 0.9943 - val_loss: 0.5055 - val_acc: 0.8750\n",
      "Epoch 2087/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0097 - acc: 0.9986 - val_loss: 0.6808 - val_acc: 0.8920\n",
      "Epoch 2088/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0183 - acc: 0.9915 - val_loss: 0.7645 - val_acc: 0.8864\n",
      "Epoch 2089/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.0405 - acc: 0.9929 - val_loss: 0.3928 - val_acc: 0.8920\n",
      "Epoch 2090/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0355 - acc: 0.9886 - val_loss: 0.4231 - val_acc: 0.8807\n",
      "Epoch 2091/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0364 - acc: 0.9900 - val_loss: 0.5467 - val_acc: 0.9148\n",
      "Epoch 2092/3000\n",
      "702/702 [==============================] - 0s 585us/sample - loss: 0.0108 - acc: 0.9957 - val_loss: 0.8699 - val_acc: 0.9091\n",
      "Epoch 2093/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0318 - acc: 0.9915 - val_loss: 0.5716 - val_acc: 0.9034\n",
      "Epoch 2094/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0082 - acc: 0.9986 - val_loss: 0.4470 - val_acc: 0.8864\n",
      "Epoch 2095/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0228 - acc: 0.9957 - val_loss: 0.5109 - val_acc: 0.8864\n",
      "Epoch 2096/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0168 - acc: 0.9943 - val_loss: 0.4656 - val_acc: 0.8920\n",
      "Epoch 2097/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0095 - acc: 0.9986 - val_loss: 0.4593 - val_acc: 0.8920\n",
      "Epoch 2098/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0087 - acc: 0.9972 - val_loss: 0.4589 - val_acc: 0.9091\n",
      "Epoch 2099/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0040 - acc: 0.9986 - val_loss: 0.5146 - val_acc: 0.8920\n",
      "Epoch 2100/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0167 - acc: 0.9957 - val_loss: 0.4940 - val_acc: 0.8977\n",
      "Epoch 2101/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0092 - acc: 0.9957 - val_loss: 0.4272 - val_acc: 0.9034\n",
      "Epoch 2102/3000\n",
      "702/702 [==============================] - 0s 584us/sample - loss: 0.0056 - acc: 0.9986 - val_loss: 0.4227 - val_acc: 0.9205\n",
      "Epoch 2103/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0131 - acc: 0.9943 - val_loss: 0.4773 - val_acc: 0.9091\n",
      "Epoch 2104/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0067 - acc: 0.9986 - val_loss: 0.5088 - val_acc: 0.9034\n",
      "Epoch 2105/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.0240 - acc: 0.9957 - val_loss: 0.5496 - val_acc: 0.9034\n",
      "Epoch 2106/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0062 - acc: 0.9986 - val_loss: 0.5206 - val_acc: 0.8920\n",
      "Epoch 2107/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0072 - acc: 0.9986 - val_loss: 0.5139 - val_acc: 0.8920\n",
      "Epoch 2108/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0108 - acc: 0.9986 - val_loss: 0.5554 - val_acc: 0.8977\n",
      "Epoch 2109/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0044 - acc: 1.0000 - val_loss: 0.6385 - val_acc: 0.9034\n",
      "Epoch 2110/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0091 - acc: 0.9972 - val_loss: 0.6182 - val_acc: 0.8977\n",
      "Epoch 2111/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0200 - acc: 0.9972 - val_loss: 0.5581 - val_acc: 0.8977\n",
      "Epoch 2112/3000\n",
      "702/702 [==============================] - 0s 582us/sample - loss: 0.0099 - acc: 0.9972 - val_loss: 0.6697 - val_acc: 0.8807\n",
      "Epoch 2113/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0248 - acc: 0.9943 - val_loss: 0.7175 - val_acc: 0.8807\n",
      "Epoch 2114/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0201 - acc: 0.9929 - val_loss: 0.5162 - val_acc: 0.8977\n",
      "Epoch 2115/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0114 - acc: 0.9986 - val_loss: 0.4197 - val_acc: 0.9148\n",
      "Epoch 2116/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0215 - acc: 0.9900 - val_loss: 0.3646 - val_acc: 0.8977\n",
      "Epoch 2117/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0219 - acc: 0.9957 - val_loss: 0.4960 - val_acc: 0.9034\n",
      "Epoch 2118/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0128 - acc: 0.9957 - val_loss: 0.4179 - val_acc: 0.8864\n",
      "Epoch 2119/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0071 - acc: 1.0000 - val_loss: 0.4369 - val_acc: 0.8807\n",
      "Epoch 2120/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0121 - acc: 0.9986 - val_loss: 0.4860 - val_acc: 0.8750\n",
      "Epoch 2121/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0184 - acc: 0.9943 - val_loss: 0.5348 - val_acc: 0.8977\n",
      "Epoch 2122/3000\n",
      "702/702 [==============================] - 0s 582us/sample - loss: 0.0082 - acc: 0.9972 - val_loss: 0.5204 - val_acc: 0.8920\n",
      "Epoch 2123/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0114 - acc: 0.9972 - val_loss: 0.5256 - val_acc: 0.8807\n",
      "Epoch 2124/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0178 - acc: 0.9957 - val_loss: 0.5723 - val_acc: 0.8750\n",
      "Epoch 2125/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0088 - acc: 0.9957 - val_loss: 0.6015 - val_acc: 0.8920\n",
      "Epoch 2126/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0129 - acc: 0.9972 - val_loss: 0.5855 - val_acc: 0.8920\n",
      "Epoch 2127/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0128 - acc: 0.9957 - val_loss: 0.5777 - val_acc: 0.8920\n",
      "Epoch 2128/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0088 - acc: 0.9972 - val_loss: 0.5592 - val_acc: 0.8977\n",
      "Epoch 2129/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0085 - acc: 0.9957 - val_loss: 0.5884 - val_acc: 0.8977\n",
      "Epoch 2130/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0119 - acc: 0.9972 - val_loss: 0.7820 - val_acc: 0.9034\n",
      "Epoch 2131/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0281 - acc: 0.9943 - val_loss: 0.6469 - val_acc: 0.9148\n",
      "Epoch 2132/3000\n",
      "702/702 [==============================] - 0s 594us/sample - loss: 0.0142 - acc: 0.9972 - val_loss: 0.4916 - val_acc: 0.8807\n",
      "Epoch 2133/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0215 - acc: 0.9915 - val_loss: 0.5482 - val_acc: 0.9034\n",
      "Epoch 2134/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0124 - acc: 0.9943 - val_loss: 0.8098 - val_acc: 0.8750\n",
      "Epoch 2135/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0260 - acc: 0.9929 - val_loss: 0.4819 - val_acc: 0.9034\n",
      "Epoch 2136/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0247 - acc: 0.9915 - val_loss: 0.4373 - val_acc: 0.8807\n",
      "Epoch 2137/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0133 - acc: 0.9943 - val_loss: 0.5446 - val_acc: 0.8920\n",
      "Epoch 2138/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0276 - acc: 0.9929 - val_loss: 0.5159 - val_acc: 0.8920\n",
      "Epoch 2139/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0101 - acc: 0.9957 - val_loss: 0.5077 - val_acc: 0.9091\n",
      "Epoch 2140/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0208 - acc: 0.9943 - val_loss: 0.5102 - val_acc: 0.8864\n",
      "Epoch 2141/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0128 - acc: 0.9957 - val_loss: 0.4788 - val_acc: 0.9034\n",
      "Epoch 2142/3000\n",
      "702/702 [==============================] - 0s 589us/sample - loss: 0.0156 - acc: 0.9972 - val_loss: 0.5186 - val_acc: 0.8977\n",
      "Epoch 2143/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0067 - acc: 0.9986 - val_loss: 0.5743 - val_acc: 0.8920\n",
      "Epoch 2144/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0183 - acc: 0.9943 - val_loss: 0.5558 - val_acc: 0.8920\n",
      "Epoch 2145/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0178 - acc: 0.9972 - val_loss: 0.4609 - val_acc: 0.8977\n",
      "Epoch 2146/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0093 - acc: 0.9972 - val_loss: 0.4408 - val_acc: 0.8864\n",
      "Epoch 2147/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0080 - acc: 0.9986 - val_loss: 0.4902 - val_acc: 0.8920\n",
      "Epoch 2148/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0172 - acc: 0.9957 - val_loss: 0.6098 - val_acc: 0.8977\n",
      "Epoch 2149/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0085 - acc: 0.9972 - val_loss: 0.5821 - val_acc: 0.9034\n",
      "Epoch 2150/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0037 - acc: 1.0000 - val_loss: 0.5269 - val_acc: 0.8977\n",
      "Epoch 2151/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0115 - acc: 0.9972 - val_loss: 0.4832 - val_acc: 0.8920\n",
      "Epoch 2152/3000\n",
      "702/702 [==============================] - 0s 585us/sample - loss: 0.0107 - acc: 0.9957 - val_loss: 0.4847 - val_acc: 0.9034\n",
      "Epoch 2153/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0126 - acc: 0.9943 - val_loss: 0.4411 - val_acc: 0.8864\n",
      "Epoch 2154/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0117 - acc: 0.9957 - val_loss: 0.4115 - val_acc: 0.9034\n",
      "Epoch 2155/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0070 - acc: 0.9986 - val_loss: 0.4558 - val_acc: 0.9034\n",
      "Epoch 2156/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0029 - acc: 1.0000 - val_loss: 0.5610 - val_acc: 0.8920\n",
      "Epoch 2157/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0029 - acc: 1.0000 - val_loss: 0.6432 - val_acc: 0.8920\n",
      "Epoch 2158/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0066 - acc: 0.9986 - val_loss: 0.5911 - val_acc: 0.8864\n",
      "Epoch 2159/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0073 - acc: 0.9972 - val_loss: 0.5961 - val_acc: 0.8864\n",
      "Epoch 2160/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.0055 - acc: 0.9972 - val_loss: 0.5409 - val_acc: 0.8977\n",
      "Epoch 2161/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0072 - acc: 0.9986 - val_loss: 0.5287 - val_acc: 0.8920\n",
      "Epoch 2162/3000\n",
      "702/702 [==============================] - 0s 581us/sample - loss: 0.0083 - acc: 0.9986 - val_loss: 0.6073 - val_acc: 0.8977\n",
      "Epoch 2163/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0053 - acc: 0.9972 - val_loss: 0.6375 - val_acc: 0.9034\n",
      "Epoch 2164/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0063 - acc: 0.9986 - val_loss: 0.4643 - val_acc: 0.9091\n",
      "Epoch 2165/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0054 - acc: 1.0000 - val_loss: 0.4580 - val_acc: 0.9091\n",
      "Epoch 2166/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0035 - acc: 1.0000 - val_loss: 0.5466 - val_acc: 0.8920\n",
      "Epoch 2167/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0169 - acc: 0.9957 - val_loss: 0.5714 - val_acc: 0.9034\n",
      "Epoch 2168/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0094 - acc: 0.9957 - val_loss: 0.5744 - val_acc: 0.8864\n",
      "Epoch 2169/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0088 - acc: 0.9972 - val_loss: 0.4487 - val_acc: 0.8920\n",
      "Epoch 2170/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0072 - acc: 0.9986 - val_loss: 0.4178 - val_acc: 0.8864\n",
      "Epoch 2171/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0199 - acc: 0.9986 - val_loss: 0.4957 - val_acc: 0.9034\n",
      "Epoch 2172/3000\n",
      "702/702 [==============================] - 0s 586us/sample - loss: 0.0292 - acc: 0.9915 - val_loss: 0.4449 - val_acc: 0.8864\n",
      "Epoch 2173/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0322 - acc: 0.9886 - val_loss: 0.4779 - val_acc: 0.9034\n",
      "Epoch 2174/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0166 - acc: 0.9972 - val_loss: 0.6182 - val_acc: 0.8864\n",
      "Epoch 2175/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0196 - acc: 0.9943 - val_loss: 0.5231 - val_acc: 0.8977\n",
      "Epoch 2176/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0136 - acc: 0.9957 - val_loss: 0.4127 - val_acc: 0.9091\n",
      "Epoch 2177/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0118 - acc: 0.9957 - val_loss: 0.3694 - val_acc: 0.9148\n",
      "Epoch 2178/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0146 - acc: 0.9972 - val_loss: 0.3795 - val_acc: 0.9148\n",
      "Epoch 2179/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0250 - acc: 0.9972 - val_loss: 0.4039 - val_acc: 0.9091\n",
      "Epoch 2180/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0274 - acc: 0.9943 - val_loss: 0.4824 - val_acc: 0.9091\n",
      "Epoch 2181/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0220 - acc: 0.9929 - val_loss: 0.4459 - val_acc: 0.8920\n",
      "Epoch 2182/3000\n",
      "702/702 [==============================] - 0s 588us/sample - loss: 0.0103 - acc: 0.9986 - val_loss: 0.3434 - val_acc: 0.9034\n",
      "Epoch 2183/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0213 - acc: 0.9929 - val_loss: 0.4383 - val_acc: 0.9034\n",
      "Epoch 2184/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0164 - acc: 0.9929 - val_loss: 0.5873 - val_acc: 0.8977\n",
      "Epoch 2185/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0234 - acc: 0.9929 - val_loss: 0.5611 - val_acc: 0.8977\n",
      "Epoch 2186/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0165 - acc: 0.9943 - val_loss: 0.5441 - val_acc: 0.9034\n",
      "Epoch 2187/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0213 - acc: 0.9943 - val_loss: 0.5637 - val_acc: 0.8920\n",
      "Epoch 2188/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0130 - acc: 0.9972 - val_loss: 0.5306 - val_acc: 0.9034\n",
      "Epoch 2189/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0023 - acc: 1.0000 - val_loss: 0.4824 - val_acc: 0.8920\n",
      "Epoch 2190/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0129 - acc: 0.9986 - val_loss: 0.4852 - val_acc: 0.9034\n",
      "Epoch 2191/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0092 - acc: 0.9986 - val_loss: 0.4752 - val_acc: 0.9034\n",
      "Epoch 2192/3000\n",
      "702/702 [==============================] - 0s 595us/sample - loss: 0.0032 - acc: 1.0000 - val_loss: 0.4902 - val_acc: 0.9091\n",
      "Epoch 2193/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0269 - acc: 0.9900 - val_loss: 0.4158 - val_acc: 0.8977\n",
      "Epoch 2194/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0105 - acc: 0.9972 - val_loss: 0.3405 - val_acc: 0.9091\n",
      "Epoch 2195/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0258 - acc: 0.9915 - val_loss: 0.4250 - val_acc: 0.8864\n",
      "Epoch 2196/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0310 - acc: 0.9929 - val_loss: 0.6330 - val_acc: 0.8807\n",
      "Epoch 2197/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0488 - acc: 0.9843 - val_loss: 0.4775 - val_acc: 0.8920\n",
      "Epoch 2198/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0518 - acc: 0.9872 - val_loss: 0.5617 - val_acc: 0.9034\n",
      "Epoch 2199/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0538 - acc: 0.9843 - val_loss: 0.4880 - val_acc: 0.8920\n",
      "Epoch 2200/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.0207 - acc: 0.9943 - val_loss: 0.3935 - val_acc: 0.8977\n",
      "Epoch 2201/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0263 - acc: 0.9972 - val_loss: 0.4295 - val_acc: 0.8977\n",
      "Epoch 2202/3000\n",
      "702/702 [==============================] - 0s 590us/sample - loss: 0.0228 - acc: 0.9915 - val_loss: 0.5333 - val_acc: 0.9034\n",
      "Epoch 2203/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0085 - acc: 0.9986 - val_loss: 0.5533 - val_acc: 0.9091\n",
      "Epoch 2204/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0083 - acc: 0.9972 - val_loss: 0.5043 - val_acc: 0.9034\n",
      "Epoch 2205/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0058 - acc: 0.9972 - val_loss: 0.4343 - val_acc: 0.8977\n",
      "Epoch 2206/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0078 - acc: 0.9986 - val_loss: 0.5146 - val_acc: 0.8977\n",
      "Epoch 2207/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0317 - acc: 0.9886 - val_loss: 0.4764 - val_acc: 0.8864\n",
      "Epoch 2208/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0238 - acc: 0.9972 - val_loss: 0.3384 - val_acc: 0.9148\n",
      "Epoch 2209/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0210 - acc: 0.9929 - val_loss: 0.3236 - val_acc: 0.9034\n",
      "Epoch 2210/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0186 - acc: 0.9929 - val_loss: 0.5594 - val_acc: 0.8864\n",
      "Epoch 2211/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0366 - acc: 0.9915 - val_loss: 0.6562 - val_acc: 0.8750\n",
      "Epoch 2212/3000\n",
      "702/702 [==============================] - 0s 602us/sample - loss: 0.0340 - acc: 0.9915 - val_loss: 0.3636 - val_acc: 0.9091\n",
      "Epoch 2213/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0167 - acc: 0.9943 - val_loss: 0.3115 - val_acc: 0.9205\n",
      "Epoch 2214/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0195 - acc: 0.9972 - val_loss: 0.4200 - val_acc: 0.9034\n",
      "Epoch 2215/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0103 - acc: 0.9957 - val_loss: 0.4692 - val_acc: 0.8807\n",
      "Epoch 2216/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.0177 - acc: 0.9943 - val_loss: 0.4779 - val_acc: 0.8977\n",
      "Epoch 2217/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0095 - acc: 0.9972 - val_loss: 0.4674 - val_acc: 0.9091\n",
      "Epoch 2218/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.0163 - acc: 0.9957 - val_loss: 0.3846 - val_acc: 0.9034\n",
      "Epoch 2219/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0086 - acc: 0.9972 - val_loss: 0.4221 - val_acc: 0.8864\n",
      "Epoch 2220/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.0112 - acc: 0.9972 - val_loss: 0.4748 - val_acc: 0.9034\n",
      "Epoch 2221/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0296 - acc: 0.9886 - val_loss: 0.4937 - val_acc: 0.9034\n",
      "Epoch 2222/3000\n",
      "702/702 [==============================] - 0s 601us/sample - loss: 0.0192 - acc: 0.9943 - val_loss: 0.4812 - val_acc: 0.9091\n",
      "Epoch 2223/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0194 - acc: 0.9929 - val_loss: 0.4470 - val_acc: 0.8977\n",
      "Epoch 2224/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0051 - acc: 0.9972 - val_loss: 0.4304 - val_acc: 0.9091\n",
      "Epoch 2225/3000\n",
      "702/702 [==============================] - 0s 535us/sample - loss: 0.0336 - acc: 0.9929 - val_loss: 0.4298 - val_acc: 0.9148\n",
      "Epoch 2226/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0089 - acc: 0.9943 - val_loss: 0.3787 - val_acc: 0.9205\n",
      "Epoch 2227/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0243 - acc: 0.9915 - val_loss: 0.3658 - val_acc: 0.9034\n",
      "Epoch 2228/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0029 - acc: 1.0000 - val_loss: 0.4320 - val_acc: 0.8807\n",
      "Epoch 2229/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0198 - acc: 0.9957 - val_loss: 0.4168 - val_acc: 0.9034\n",
      "Epoch 2230/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0217 - acc: 0.9943 - val_loss: 0.3769 - val_acc: 0.9148\n",
      "Epoch 2231/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0108 - acc: 0.9957 - val_loss: 0.4048 - val_acc: 0.9148\n",
      "Epoch 2232/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.0079 - acc: 0.9972 - val_loss: 0.4861 - val_acc: 0.9148\n",
      "Epoch 2233/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0048 - acc: 0.9986 - val_loss: 0.4618 - val_acc: 0.9034\n",
      "Epoch 2234/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0327 - acc: 0.9957 - val_loss: 0.4336 - val_acc: 0.9034\n",
      "Epoch 2235/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0125 - acc: 0.9972 - val_loss: 0.3474 - val_acc: 0.9148\n",
      "Epoch 2236/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0255 - acc: 0.9972 - val_loss: 0.5363 - val_acc: 0.8977\n",
      "Epoch 2237/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0356 - acc: 0.9915 - val_loss: 0.5612 - val_acc: 0.9034\n",
      "Epoch 2238/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0299 - acc: 0.9929 - val_loss: 0.3906 - val_acc: 0.8920\n",
      "Epoch 2239/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0133 - acc: 0.9943 - val_loss: 0.4053 - val_acc: 0.8977\n",
      "Epoch 2240/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0229 - acc: 0.9943 - val_loss: 0.4760 - val_acc: 0.9034\n",
      "Epoch 2241/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0164 - acc: 0.9943 - val_loss: 0.4581 - val_acc: 0.8864\n",
      "Epoch 2242/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.0261 - acc: 0.9915 - val_loss: 0.4391 - val_acc: 0.8864\n",
      "Epoch 2243/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0161 - acc: 0.9915 - val_loss: 0.4471 - val_acc: 0.8977\n",
      "Epoch 2244/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0311 - acc: 0.9915 - val_loss: 0.4233 - val_acc: 0.9091\n",
      "Epoch 2245/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0150 - acc: 0.9943 - val_loss: 0.4857 - val_acc: 0.8864\n",
      "Epoch 2246/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0111 - acc: 0.9972 - val_loss: 0.6166 - val_acc: 0.8977\n",
      "Epoch 2247/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0241 - acc: 0.9943 - val_loss: 0.5211 - val_acc: 0.8977\n",
      "Epoch 2248/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0082 - acc: 0.9957 - val_loss: 0.5152 - val_acc: 0.8977\n",
      "Epoch 2249/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0118 - acc: 0.9986 - val_loss: 0.5099 - val_acc: 0.8977\n",
      "Epoch 2250/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0079 - acc: 0.9986 - val_loss: 0.5245 - val_acc: 0.8977\n",
      "Epoch 2251/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0081 - acc: 0.9972 - val_loss: 0.4809 - val_acc: 0.8864\n",
      "Epoch 2252/3000\n",
      "702/702 [==============================] - 0s 593us/sample - loss: 0.0044 - acc: 0.9986 - val_loss: 0.4668 - val_acc: 0.8864\n",
      "Epoch 2253/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0089 - acc: 0.9986 - val_loss: 0.4594 - val_acc: 0.8864\n",
      "Epoch 2254/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0038 - acc: 1.0000 - val_loss: 0.5204 - val_acc: 0.8977\n",
      "Epoch 2255/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0079 - acc: 0.9957 - val_loss: 0.5059 - val_acc: 0.9034\n",
      "Epoch 2256/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0068 - acc: 0.9986 - val_loss: 0.4559 - val_acc: 0.8920\n",
      "Epoch 2257/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0210 - acc: 0.9900 - val_loss: 0.5043 - val_acc: 0.9034\n",
      "Epoch 2258/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0174 - acc: 0.9929 - val_loss: 0.5431 - val_acc: 0.8977\n",
      "Epoch 2259/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0244 - acc: 0.9929 - val_loss: 0.5678 - val_acc: 0.8807\n",
      "Epoch 2260/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0130 - acc: 0.9943 - val_loss: 0.5108 - val_acc: 0.8920\n",
      "Epoch 2261/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0105 - acc: 0.9957 - val_loss: 0.4302 - val_acc: 0.8977\n",
      "Epoch 2262/3000\n",
      "702/702 [==============================] - 0s 595us/sample - loss: 0.0081 - acc: 0.9986 - val_loss: 0.4845 - val_acc: 0.8920\n",
      "Epoch 2263/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0131 - acc: 0.9943 - val_loss: 0.5569 - val_acc: 0.9034\n",
      "Epoch 2264/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0209 - acc: 0.9929 - val_loss: 0.4470 - val_acc: 0.8864\n",
      "Epoch 2265/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0114 - acc: 0.9943 - val_loss: 0.3725 - val_acc: 0.8920\n",
      "Epoch 2266/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0169 - acc: 0.9929 - val_loss: 0.4027 - val_acc: 0.8977\n",
      "Epoch 2267/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0134 - acc: 0.9972 - val_loss: 0.6351 - val_acc: 0.8977\n",
      "Epoch 2268/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0143 - acc: 0.9943 - val_loss: 0.6783 - val_acc: 0.8977\n",
      "Epoch 2269/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0139 - acc: 0.9957 - val_loss: 0.6971 - val_acc: 0.8920\n",
      "Epoch 2270/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0055 - acc: 0.9972 - val_loss: 0.6421 - val_acc: 0.8977\n",
      "Epoch 2271/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0073 - acc: 0.9986 - val_loss: 0.5364 - val_acc: 0.8920\n",
      "Epoch 2272/3000\n",
      "702/702 [==============================] - 0s 599us/sample - loss: 0.0070 - acc: 0.9957 - val_loss: 0.4529 - val_acc: 0.8977\n",
      "Epoch 2273/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0148 - acc: 0.9957 - val_loss: 0.3879 - val_acc: 0.8977\n",
      "Epoch 2274/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0157 - acc: 0.9915 - val_loss: 0.4231 - val_acc: 0.9034\n",
      "Epoch 2275/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0074 - acc: 0.9986 - val_loss: 0.5796 - val_acc: 0.8864\n",
      "Epoch 2276/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0211 - acc: 0.9943 - val_loss: 0.4689 - val_acc: 0.8977\n",
      "Epoch 2277/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0099 - acc: 0.9972 - val_loss: 0.4032 - val_acc: 0.9091\n",
      "Epoch 2278/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0099 - acc: 0.9972 - val_loss: 0.4340 - val_acc: 0.8977\n",
      "Epoch 2279/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0091 - acc: 0.9972 - val_loss: 0.4707 - val_acc: 0.8864\n",
      "Epoch 2280/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0106 - acc: 0.9972 - val_loss: 0.4334 - val_acc: 0.8523\n",
      "Epoch 2281/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0229 - acc: 0.9900 - val_loss: 0.3884 - val_acc: 0.8864\n",
      "Epoch 2282/3000\n",
      "702/702 [==============================] - 0s 596us/sample - loss: 0.0101 - acc: 0.9943 - val_loss: 0.4587 - val_acc: 0.9034\n",
      "Epoch 2283/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0064 - acc: 0.9986 - val_loss: 0.4848 - val_acc: 0.8807\n",
      "Epoch 2284/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0121 - acc: 0.9943 - val_loss: 0.5286 - val_acc: 0.8920\n",
      "Epoch 2285/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0117 - acc: 0.9972 - val_loss: 0.5088 - val_acc: 0.9091\n",
      "Epoch 2286/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0036 - acc: 1.0000 - val_loss: 0.4902 - val_acc: 0.9034\n",
      "Epoch 2287/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0041 - acc: 0.9986 - val_loss: 0.5168 - val_acc: 0.8920\n",
      "Epoch 2288/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0077 - acc: 0.9972 - val_loss: 0.5507 - val_acc: 0.8920\n",
      "Epoch 2289/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0108 - acc: 0.9943 - val_loss: 0.4815 - val_acc: 0.8977\n",
      "Epoch 2290/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0025 - acc: 1.0000 - val_loss: 0.4962 - val_acc: 0.8977\n",
      "Epoch 2291/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0143 - acc: 0.9943 - val_loss: 0.5218 - val_acc: 0.9034\n",
      "Epoch 2292/3000\n",
      "702/702 [==============================] - 0s 585us/sample - loss: 0.0079 - acc: 0.9957 - val_loss: 0.5583 - val_acc: 0.9034\n",
      "Epoch 2293/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0180 - acc: 0.9929 - val_loss: 0.6693 - val_acc: 0.8977\n",
      "Epoch 2294/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.0053 - acc: 0.9986 - val_loss: 0.7135 - val_acc: 0.8977\n",
      "Epoch 2295/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0340 - acc: 0.9858 - val_loss: 0.5260 - val_acc: 0.8864\n",
      "Epoch 2296/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0265 - acc: 0.9886 - val_loss: 0.3622 - val_acc: 0.8920\n",
      "Epoch 2297/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0200 - acc: 0.9929 - val_loss: 0.4578 - val_acc: 0.8864\n",
      "Epoch 2298/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0048 - acc: 1.0000 - val_loss: 0.5900 - val_acc: 0.8920\n",
      "Epoch 2299/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0246 - acc: 0.9929 - val_loss: 0.6224 - val_acc: 0.8864\n",
      "Epoch 2300/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0024 - acc: 1.0000 - val_loss: 0.5874 - val_acc: 0.8807\n",
      "Epoch 2301/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0085 - acc: 0.9957 - val_loss: 0.5084 - val_acc: 0.8977\n",
      "Epoch 2302/3000\n",
      "702/702 [==============================] - 0s 621us/sample - loss: 0.0049 - acc: 0.9972 - val_loss: 0.4864 - val_acc: 0.8977\n",
      "Epoch 2303/3000\n",
      "702/702 [==============================] - 0s 544us/sample - loss: 0.0135 - acc: 0.9957 - val_loss: 0.5696 - val_acc: 0.8920\n",
      "Epoch 2304/3000\n",
      "702/702 [==============================] - 0s 538us/sample - loss: 0.0078 - acc: 0.9972 - val_loss: 0.6761 - val_acc: 0.9034\n",
      "Epoch 2305/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0159 - acc: 0.9957 - val_loss: 0.7293 - val_acc: 0.8977\n",
      "Epoch 2306/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0170 - acc: 0.9957 - val_loss: 0.6554 - val_acc: 0.8920\n",
      "Epoch 2307/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0059 - acc: 0.9972 - val_loss: 0.5574 - val_acc: 0.8977\n",
      "Epoch 2308/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0237 - acc: 0.9929 - val_loss: 0.4563 - val_acc: 0.8920\n",
      "Epoch 2309/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0085 - acc: 1.0000 - val_loss: 0.4269 - val_acc: 0.8977\n",
      "Epoch 2310/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.0289 - acc: 0.9957 - val_loss: 0.5342 - val_acc: 0.8864\n",
      "Epoch 2311/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0125 - acc: 0.9943 - val_loss: 0.6694 - val_acc: 0.8977\n",
      "Epoch 2312/3000\n",
      "702/702 [==============================] - 0s 591us/sample - loss: 0.0126 - acc: 0.9972 - val_loss: 0.5755 - val_acc: 0.8977\n",
      "Epoch 2313/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0049 - acc: 0.9986 - val_loss: 0.4637 - val_acc: 0.9034\n",
      "Epoch 2314/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0141 - acc: 0.9957 - val_loss: 0.4424 - val_acc: 0.8977\n",
      "Epoch 2315/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0200 - acc: 0.9986 - val_loss: 0.4922 - val_acc: 0.9034\n",
      "Epoch 2316/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0054 - acc: 0.9986 - val_loss: 0.5890 - val_acc: 0.9148\n",
      "Epoch 2317/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.0104 - acc: 0.9943 - val_loss: 0.3899 - val_acc: 0.9034\n",
      "Epoch 2318/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0198 - acc: 0.9929 - val_loss: 0.3518 - val_acc: 0.9034\n",
      "Epoch 2319/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0158 - acc: 0.9986 - val_loss: 0.4048 - val_acc: 0.9034\n",
      "Epoch 2320/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0066 - acc: 0.9972 - val_loss: 0.4025 - val_acc: 0.8920\n",
      "Epoch 2321/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0091 - acc: 0.9986 - val_loss: 0.3803 - val_acc: 0.9205\n",
      "Epoch 2322/3000\n",
      "702/702 [==============================] - 0s 601us/sample - loss: 0.0073 - acc: 0.9972 - val_loss: 0.4313 - val_acc: 0.8977\n",
      "Epoch 2323/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0137 - acc: 0.9957 - val_loss: 0.5243 - val_acc: 0.9034\n",
      "Epoch 2324/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0119 - acc: 0.9957 - val_loss: 0.5486 - val_acc: 0.8977\n",
      "Epoch 2325/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0150 - acc: 0.9943 - val_loss: 0.5105 - val_acc: 0.8864\n",
      "Epoch 2326/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0149 - acc: 0.9943 - val_loss: 0.4507 - val_acc: 0.8864\n",
      "Epoch 2327/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0175 - acc: 0.9957 - val_loss: 0.4248 - val_acc: 0.8864\n",
      "Epoch 2328/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0059 - acc: 0.9986 - val_loss: 0.4477 - val_acc: 0.8807\n",
      "Epoch 2329/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0098 - acc: 0.9972 - val_loss: 0.4225 - val_acc: 0.9034\n",
      "Epoch 2330/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0049 - acc: 1.0000 - val_loss: 0.4350 - val_acc: 0.8977\n",
      "Epoch 2331/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0073 - acc: 0.9986 - val_loss: 0.5085 - val_acc: 0.8920\n",
      "Epoch 2332/3000\n",
      "702/702 [==============================] - 0s 594us/sample - loss: 0.0103 - acc: 0.9957 - val_loss: 0.4755 - val_acc: 0.8864\n",
      "Epoch 2333/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0101 - acc: 0.9943 - val_loss: 0.4492 - val_acc: 0.8920\n",
      "Epoch 2334/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0174 - acc: 0.9957 - val_loss: 0.4357 - val_acc: 0.9034\n",
      "Epoch 2335/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.0196 - acc: 0.9972 - val_loss: 0.4454 - val_acc: 0.9034\n",
      "Epoch 2336/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0039 - acc: 0.9986 - val_loss: 0.4938 - val_acc: 0.9034\n",
      "Epoch 2337/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0071 - acc: 0.9986 - val_loss: 0.5264 - val_acc: 0.9034\n",
      "Epoch 2338/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0100 - acc: 0.9957 - val_loss: 0.4073 - val_acc: 0.8920\n",
      "Epoch 2339/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0186 - acc: 0.9929 - val_loss: 0.4919 - val_acc: 0.8807\n",
      "Epoch 2340/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0087 - acc: 0.9986 - val_loss: 0.5052 - val_acc: 0.8977\n",
      "Epoch 2341/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0061 - acc: 0.9972 - val_loss: 0.6322 - val_acc: 0.9034\n",
      "Epoch 2342/3000\n",
      "702/702 [==============================] - 0s 594us/sample - loss: 0.0046 - acc: 0.9986 - val_loss: 0.6774 - val_acc: 0.8977\n",
      "Epoch 2343/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0099 - acc: 0.9957 - val_loss: 0.6775 - val_acc: 0.8864\n",
      "Epoch 2344/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0120 - acc: 0.9957 - val_loss: 0.6065 - val_acc: 0.8750\n",
      "Epoch 2345/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0133 - acc: 0.9972 - val_loss: 0.5665 - val_acc: 0.8920\n",
      "Epoch 2346/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0036 - acc: 1.0000 - val_loss: 0.5233 - val_acc: 0.8920\n",
      "Epoch 2347/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0043 - acc: 0.9986 - val_loss: 0.4998 - val_acc: 0.8920\n",
      "Epoch 2348/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0099 - acc: 0.9972 - val_loss: 0.4711 - val_acc: 0.8920\n",
      "Epoch 2349/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0081 - acc: 0.9986 - val_loss: 0.5282 - val_acc: 0.8977\n",
      "Epoch 2350/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0037 - acc: 0.9986 - val_loss: 0.5169 - val_acc: 0.8920\n",
      "Epoch 2351/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0066 - acc: 0.9972 - val_loss: 0.5170 - val_acc: 0.8977\n",
      "Epoch 2352/3000\n",
      "702/702 [==============================] - 0s 586us/sample - loss: 0.0189 - acc: 0.9943 - val_loss: 0.6119 - val_acc: 0.8920\n",
      "Epoch 2353/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0083 - acc: 0.9972 - val_loss: 0.7009 - val_acc: 0.8977\n",
      "Epoch 2354/3000\n",
      "702/702 [==============================] - 0s 546us/sample - loss: 0.0220 - acc: 0.9943 - val_loss: 0.5305 - val_acc: 0.8920\n",
      "Epoch 2355/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.0148 - acc: 0.9972 - val_loss: 0.4771 - val_acc: 0.8920\n",
      "Epoch 2356/3000\n",
      "702/702 [==============================] - 0s 546us/sample - loss: 0.0094 - acc: 0.9986 - val_loss: 0.5148 - val_acc: 0.9034\n",
      "Epoch 2357/3000\n",
      "702/702 [==============================] - 0s 538us/sample - loss: 0.0069 - acc: 0.9972 - val_loss: 0.5668 - val_acc: 0.9034\n",
      "Epoch 2358/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 0.0095 - acc: 0.9986 - val_loss: 0.5824 - val_acc: 0.9034\n",
      "Epoch 2359/3000\n",
      "702/702 [==============================] - 0s 543us/sample - loss: 0.0043 - acc: 0.9986 - val_loss: 0.6006 - val_acc: 0.8977\n",
      "Epoch 2360/3000\n",
      "702/702 [==============================] - 0s 535us/sample - loss: 0.0137 - acc: 0.9957 - val_loss: 0.5964 - val_acc: 0.9034\n",
      "Epoch 2361/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0107 - acc: 0.9972 - val_loss: 0.5175 - val_acc: 0.8977\n",
      "Epoch 2362/3000\n",
      "702/702 [==============================] - 0s 607us/sample - loss: 0.0107 - acc: 0.9957 - val_loss: 0.4034 - val_acc: 0.8977\n",
      "Epoch 2363/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.0265 - acc: 0.9943 - val_loss: 0.4150 - val_acc: 0.9148\n",
      "Epoch 2364/3000\n",
      "702/702 [==============================] - 0s 535us/sample - loss: 0.0105 - acc: 0.9972 - val_loss: 0.5591 - val_acc: 0.8977\n",
      "Epoch 2365/3000\n",
      "702/702 [==============================] - 0s 541us/sample - loss: 0.0048 - acc: 0.9986 - val_loss: 0.6831 - val_acc: 0.9091\n",
      "Epoch 2366/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.0123 - acc: 0.9986 - val_loss: 0.5970 - val_acc: 0.9034\n",
      "Epoch 2367/3000\n",
      "702/702 [==============================] - 0s 538us/sample - loss: 0.0099 - acc: 0.9972 - val_loss: 0.3984 - val_acc: 0.8977\n",
      "Epoch 2368/3000\n",
      "702/702 [==============================] - 0s 535us/sample - loss: 0.0091 - acc: 0.9957 - val_loss: 0.4276 - val_acc: 0.9034\n",
      "Epoch 2369/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.0059 - acc: 1.0000 - val_loss: 0.4628 - val_acc: 0.8864\n",
      "Epoch 2370/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0058 - acc: 0.9986 - val_loss: 0.5947 - val_acc: 0.8920\n",
      "Epoch 2371/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 0.7266 - val_acc: 0.9034\n",
      "Epoch 2372/3000\n",
      "702/702 [==============================] - 0s 588us/sample - loss: 0.0362 - acc: 0.9929 - val_loss: 0.4275 - val_acc: 0.8977\n",
      "Epoch 2373/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0166 - acc: 0.9915 - val_loss: 0.4166 - val_acc: 0.8920\n",
      "Epoch 2374/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0186 - acc: 0.9929 - val_loss: 0.4394 - val_acc: 0.8864\n",
      "Epoch 2375/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0109 - acc: 0.9957 - val_loss: 0.3886 - val_acc: 0.8977\n",
      "Epoch 2376/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0097 - acc: 0.9972 - val_loss: 0.4654 - val_acc: 0.9034\n",
      "Epoch 2377/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0166 - acc: 0.9972 - val_loss: 0.3782 - val_acc: 0.8920\n",
      "Epoch 2378/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0300 - acc: 0.9929 - val_loss: 0.4205 - val_acc: 0.8920\n",
      "Epoch 2379/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0056 - acc: 0.9972 - val_loss: 0.4908 - val_acc: 0.8864\n",
      "Epoch 2380/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0171 - acc: 0.9929 - val_loss: 0.3943 - val_acc: 0.8977\n",
      "Epoch 2381/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0369 - acc: 0.9929 - val_loss: 0.3373 - val_acc: 0.8977\n",
      "Epoch 2382/3000\n",
      "702/702 [==============================] - 0s 601us/sample - loss: 0.0176 - acc: 0.9929 - val_loss: 0.4268 - val_acc: 0.8920\n",
      "Epoch 2383/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0108 - acc: 0.9972 - val_loss: 0.7025 - val_acc: 0.8977\n",
      "Epoch 2384/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0457 - acc: 0.9886 - val_loss: 0.5290 - val_acc: 0.8920\n",
      "Epoch 2385/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0035 - acc: 1.0000 - val_loss: 0.5014 - val_acc: 0.8920\n",
      "Epoch 2386/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0084 - acc: 0.9957 - val_loss: 0.4841 - val_acc: 0.8920\n",
      "Epoch 2387/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0122 - acc: 0.9943 - val_loss: 0.4929 - val_acc: 0.8977\n",
      "Epoch 2388/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0179 - acc: 0.9900 - val_loss: 0.3375 - val_acc: 0.8977\n",
      "Epoch 2389/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0118 - acc: 0.9957 - val_loss: 0.3988 - val_acc: 0.8807\n",
      "Epoch 2390/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0212 - acc: 0.9915 - val_loss: 0.5214 - val_acc: 0.8750\n",
      "Epoch 2391/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0143 - acc: 0.9972 - val_loss: 0.5865 - val_acc: 0.8864\n",
      "Epoch 2392/3000\n",
      "702/702 [==============================] - 0s 596us/sample - loss: 0.0094 - acc: 0.9972 - val_loss: 0.5611 - val_acc: 0.8807\n",
      "Epoch 2393/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0130 - acc: 0.9943 - val_loss: 0.5503 - val_acc: 0.8977\n",
      "Epoch 2394/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0162 - acc: 0.9943 - val_loss: 0.5718 - val_acc: 0.8864\n",
      "Epoch 2395/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0073 - acc: 0.9986 - val_loss: 0.5355 - val_acc: 0.8977\n",
      "Epoch 2396/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0062 - acc: 0.9972 - val_loss: 0.4718 - val_acc: 0.8920\n",
      "Epoch 2397/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0080 - acc: 0.9972 - val_loss: 0.4117 - val_acc: 0.8920\n",
      "Epoch 2398/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0125 - acc: 0.9957 - val_loss: 0.4289 - val_acc: 0.8920\n",
      "Epoch 2399/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0055 - acc: 0.9972 - val_loss: 0.4963 - val_acc: 0.8920\n",
      "Epoch 2400/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0090 - acc: 0.9972 - val_loss: 0.5601 - val_acc: 0.8864\n",
      "Epoch 2401/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0129 - acc: 0.9986 - val_loss: 0.5335 - val_acc: 0.8864\n",
      "Epoch 2402/3000\n",
      "702/702 [==============================] - 0s 593us/sample - loss: 0.0087 - acc: 0.9972 - val_loss: 0.4998 - val_acc: 0.8864\n",
      "Epoch 2403/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0196 - acc: 0.9943 - val_loss: 0.6240 - val_acc: 0.8807\n",
      "Epoch 2404/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0254 - acc: 0.9943 - val_loss: 0.6826 - val_acc: 0.8977\n",
      "Epoch 2405/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0281 - acc: 0.9915 - val_loss: 0.6176 - val_acc: 0.8864\n",
      "Epoch 2406/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0042 - acc: 1.0000 - val_loss: 0.6292 - val_acc: 0.8807\n",
      "Epoch 2407/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0206 - acc: 0.9943 - val_loss: 0.4798 - val_acc: 0.8977\n",
      "Epoch 2408/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0134 - acc: 0.9957 - val_loss: 0.4704 - val_acc: 0.9034\n",
      "Epoch 2409/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0165 - acc: 0.9929 - val_loss: 0.4659 - val_acc: 0.9034\n",
      "Epoch 2410/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0200 - acc: 0.9972 - val_loss: 0.5319 - val_acc: 0.8750\n",
      "Epoch 2411/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0120 - acc: 0.9957 - val_loss: 0.5171 - val_acc: 0.8977\n",
      "Epoch 2412/3000\n",
      "702/702 [==============================] - 0s 594us/sample - loss: 0.0085 - acc: 0.9972 - val_loss: 0.5069 - val_acc: 0.9034\n",
      "Epoch 2413/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0261 - acc: 0.9943 - val_loss: 0.5539 - val_acc: 0.8920\n",
      "Epoch 2414/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0186 - acc: 0.9943 - val_loss: 0.5758 - val_acc: 0.8920\n",
      "Epoch 2415/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0080 - acc: 0.9986 - val_loss: 0.5706 - val_acc: 0.8920\n",
      "Epoch 2416/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0106 - acc: 0.9972 - val_loss: 0.4318 - val_acc: 0.8864\n",
      "Epoch 2417/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0120 - acc: 0.9929 - val_loss: 0.3759 - val_acc: 0.8977\n",
      "Epoch 2418/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0069 - acc: 0.9986 - val_loss: 0.4156 - val_acc: 0.9034\n",
      "Epoch 2419/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0163 - acc: 0.9929 - val_loss: 0.5165 - val_acc: 0.8977\n",
      "Epoch 2420/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0083 - acc: 0.9986 - val_loss: 0.4507 - val_acc: 0.8977\n",
      "Epoch 2421/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0097 - acc: 0.9972 - val_loss: 0.4656 - val_acc: 0.8864\n",
      "Epoch 2422/3000\n",
      "702/702 [==============================] - 0s 604us/sample - loss: 0.0140 - acc: 0.9943 - val_loss: 0.5077 - val_acc: 0.9034\n",
      "Epoch 2423/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0205 - acc: 0.9915 - val_loss: 0.4588 - val_acc: 0.8920\n",
      "Epoch 2424/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0143 - acc: 0.9972 - val_loss: 0.4201 - val_acc: 0.8920\n",
      "Epoch 2425/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0173 - acc: 0.9929 - val_loss: 0.4815 - val_acc: 0.8920\n",
      "Epoch 2426/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0084 - acc: 0.9972 - val_loss: 0.5776 - val_acc: 0.9091\n",
      "Epoch 2427/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0082 - acc: 0.9957 - val_loss: 0.5625 - val_acc: 0.8977\n",
      "Epoch 2428/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0128 - acc: 0.9972 - val_loss: 0.5265 - val_acc: 0.8920\n",
      "Epoch 2429/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0114 - acc: 0.9972 - val_loss: 0.4773 - val_acc: 0.8807\n",
      "Epoch 2430/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0077 - acc: 0.9986 - val_loss: 0.5423 - val_acc: 0.8920\n",
      "Epoch 2431/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0114 - acc: 0.9972 - val_loss: 0.5127 - val_acc: 0.8750\n",
      "Epoch 2432/3000\n",
      "702/702 [==============================] - 0s 599us/sample - loss: 0.0077 - acc: 0.9972 - val_loss: 0.4915 - val_acc: 0.8920\n",
      "Epoch 2433/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0141 - acc: 0.9972 - val_loss: 0.4959 - val_acc: 0.8977\n",
      "Epoch 2434/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0068 - acc: 0.9972 - val_loss: 0.4413 - val_acc: 0.8864\n",
      "Epoch 2435/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0072 - acc: 0.9986 - val_loss: 0.4656 - val_acc: 0.8864\n",
      "Epoch 2436/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0100 - acc: 0.9972 - val_loss: 0.6580 - val_acc: 0.8977\n",
      "Epoch 2437/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0101 - acc: 0.9972 - val_loss: 0.7625 - val_acc: 0.8977\n",
      "Epoch 2438/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0098 - acc: 0.9957 - val_loss: 0.6544 - val_acc: 0.9034\n",
      "Epoch 2439/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0217 - acc: 0.9915 - val_loss: 0.5283 - val_acc: 0.8864\n",
      "Epoch 2440/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0050 - acc: 0.9986 - val_loss: 0.4499 - val_acc: 0.8977\n",
      "Epoch 2441/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0208 - acc: 0.9929 - val_loss: 0.4385 - val_acc: 0.8807\n",
      "Epoch 2442/3000\n",
      "702/702 [==============================] - 0s 605us/sample - loss: 0.0103 - acc: 0.9943 - val_loss: 0.5710 - val_acc: 0.8864\n",
      "Epoch 2443/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0264 - acc: 0.9943 - val_loss: 0.4401 - val_acc: 0.8920\n",
      "Epoch 2444/3000\n",
      "702/702 [==============================] - 0s 534us/sample - loss: 0.0139 - acc: 0.9957 - val_loss: 0.4910 - val_acc: 0.8920\n",
      "Epoch 2445/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0050 - acc: 0.9986 - val_loss: 0.6539 - val_acc: 0.8864\n",
      "Epoch 2446/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0223 - acc: 0.9900 - val_loss: 0.5829 - val_acc: 0.8977\n",
      "Epoch 2447/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0081 - acc: 0.9972 - val_loss: 0.4782 - val_acc: 0.8977\n",
      "Epoch 2448/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0390 - acc: 0.9872 - val_loss: 0.3270 - val_acc: 0.8920\n",
      "Epoch 2449/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0298 - acc: 0.9872 - val_loss: 0.5073 - val_acc: 0.9034\n",
      "Epoch 2450/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.0167 - acc: 0.9957 - val_loss: 0.5449 - val_acc: 0.8920\n",
      "Epoch 2451/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0173 - acc: 0.9943 - val_loss: 0.3808 - val_acc: 0.8920\n",
      "Epoch 2452/3000\n",
      "702/702 [==============================] - 0s 583us/sample - loss: 0.0157 - acc: 0.9943 - val_loss: 0.3692 - val_acc: 0.8920\n",
      "Epoch 2453/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0183 - acc: 0.9957 - val_loss: 0.6425 - val_acc: 0.8750\n",
      "Epoch 2454/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0211 - acc: 0.9943 - val_loss: 0.6906 - val_acc: 0.8807\n",
      "Epoch 2455/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0134 - acc: 0.9957 - val_loss: 0.6576 - val_acc: 0.8864\n",
      "Epoch 2456/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0148 - acc: 0.9929 - val_loss: 0.6388 - val_acc: 0.8920\n",
      "Epoch 2457/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0072 - acc: 0.9957 - val_loss: 0.6502 - val_acc: 0.8920\n",
      "Epoch 2458/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0030 - acc: 1.0000 - val_loss: 0.6650 - val_acc: 0.8977\n",
      "Epoch 2459/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0088 - acc: 0.9972 - val_loss: 0.7145 - val_acc: 0.8977\n",
      "Epoch 2460/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0052 - acc: 0.9986 - val_loss: 0.7407 - val_acc: 0.8977\n",
      "Epoch 2461/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0024 - acc: 1.0000 - val_loss: 0.7537 - val_acc: 0.9091\n",
      "Epoch 2462/3000\n",
      "702/702 [==============================] - 0s 595us/sample - loss: 0.0030 - acc: 0.9986 - val_loss: 0.7948 - val_acc: 0.9091\n",
      "Epoch 2463/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0177 - acc: 0.9972 - val_loss: 0.6852 - val_acc: 0.8864\n",
      "Epoch 2464/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0116 - acc: 0.9972 - val_loss: 0.5395 - val_acc: 0.8864\n",
      "Epoch 2465/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0301 - acc: 0.9886 - val_loss: 0.4677 - val_acc: 0.8920\n",
      "Epoch 2466/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0088 - acc: 0.9972 - val_loss: 0.5060 - val_acc: 0.9034\n",
      "Epoch 2467/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0080 - acc: 0.9972 - val_loss: 0.5823 - val_acc: 0.8977\n",
      "Epoch 2468/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0066 - acc: 0.9986 - val_loss: 0.5963 - val_acc: 0.8920\n",
      "Epoch 2469/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0073 - acc: 0.9972 - val_loss: 0.5810 - val_acc: 0.8864\n",
      "Epoch 2470/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 0.5869 - val_acc: 0.8807\n",
      "Epoch 2471/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0058 - acc: 0.9986 - val_loss: 0.6159 - val_acc: 0.8864\n",
      "Epoch 2472/3000\n",
      "702/702 [==============================] - 0s 595us/sample - loss: 0.0180 - acc: 0.9972 - val_loss: 0.5449 - val_acc: 0.8864\n",
      "Epoch 2473/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0152 - acc: 0.9957 - val_loss: 0.6027 - val_acc: 0.8864\n",
      "Epoch 2474/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0170 - acc: 0.9957 - val_loss: 0.7253 - val_acc: 0.8977\n",
      "Epoch 2475/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0082 - acc: 0.9986 - val_loss: 0.6875 - val_acc: 0.9034\n",
      "Epoch 2476/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0046 - acc: 0.9986 - val_loss: 0.5948 - val_acc: 0.8807\n",
      "Epoch 2477/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0063 - acc: 0.9972 - val_loss: 0.5385 - val_acc: 0.8807\n",
      "Epoch 2478/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0053 - acc: 0.9972 - val_loss: 0.5577 - val_acc: 0.8807\n",
      "Epoch 2479/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0037 - acc: 0.9986 - val_loss: 0.6804 - val_acc: 0.8807\n",
      "Epoch 2480/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0127 - acc: 0.9943 - val_loss: 0.6511 - val_acc: 0.8920\n",
      "Epoch 2481/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0094 - acc: 0.9972 - val_loss: 0.6124 - val_acc: 0.8977\n",
      "Epoch 2482/3000\n",
      "702/702 [==============================] - 0s 617us/sample - loss: 0.0079 - acc: 0.9986 - val_loss: 0.5834 - val_acc: 0.9034\n",
      "Epoch 2483/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0045 - acc: 0.9986 - val_loss: 0.5275 - val_acc: 0.8920\n",
      "Epoch 2484/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0103 - acc: 0.9929 - val_loss: 0.4592 - val_acc: 0.8864\n",
      "Epoch 2485/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0080 - acc: 0.9972 - val_loss: 0.4589 - val_acc: 0.8864\n",
      "Epoch 2486/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0038 - acc: 1.0000 - val_loss: 0.4853 - val_acc: 0.8864\n",
      "Epoch 2487/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0176 - acc: 0.9943 - val_loss: 0.5082 - val_acc: 0.8864\n",
      "Epoch 2488/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0071 - acc: 0.9972 - val_loss: 0.5835 - val_acc: 0.8807\n",
      "Epoch 2489/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0053 - acc: 1.0000 - val_loss: 0.6616 - val_acc: 0.8807\n",
      "Epoch 2490/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0192 - acc: 0.9957 - val_loss: 0.6535 - val_acc: 0.8750\n",
      "Epoch 2491/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0126 - acc: 0.9972 - val_loss: 0.4990 - val_acc: 0.8807\n",
      "Epoch 2492/3000\n",
      "702/702 [==============================] - 0s 592us/sample - loss: 0.0062 - acc: 0.9986 - val_loss: 0.4877 - val_acc: 0.8807\n",
      "Epoch 2493/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0029 - acc: 1.0000 - val_loss: 0.5824 - val_acc: 0.8864\n",
      "Epoch 2494/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0099 - acc: 0.9957 - val_loss: 0.5963 - val_acc: 0.8920\n",
      "Epoch 2495/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0077 - acc: 0.9986 - val_loss: 0.4767 - val_acc: 0.8977\n",
      "Epoch 2496/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0103 - acc: 0.9957 - val_loss: 0.4523 - val_acc: 0.9091\n",
      "Epoch 2497/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0050 - acc: 0.9986 - val_loss: 0.5306 - val_acc: 0.9034\n",
      "Epoch 2498/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0103 - acc: 0.9943 - val_loss: 0.6421 - val_acc: 0.9091\n",
      "Epoch 2499/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0184 - acc: 0.9943 - val_loss: 0.6703 - val_acc: 0.9205\n",
      "Epoch 2500/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0139 - acc: 0.9972 - val_loss: 0.4668 - val_acc: 0.9148\n",
      "Epoch 2501/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0173 - acc: 0.9972 - val_loss: 0.3702 - val_acc: 0.9034\n",
      "Epoch 2502/3000\n",
      "702/702 [==============================] - 0s 592us/sample - loss: 0.0146 - acc: 0.9957 - val_loss: 0.4326 - val_acc: 0.8920\n",
      "Epoch 2503/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0175 - acc: 0.9957 - val_loss: 0.5549 - val_acc: 0.8864\n",
      "Epoch 2504/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0112 - acc: 0.9986 - val_loss: 0.5949 - val_acc: 0.9091\n",
      "Epoch 2505/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0021 - acc: 0.9986 - val_loss: 0.5254 - val_acc: 0.9091\n",
      "Epoch 2506/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0082 - acc: 0.9972 - val_loss: 0.5128 - val_acc: 0.9034\n",
      "Epoch 2507/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0051 - acc: 0.9972 - val_loss: 0.5320 - val_acc: 0.8977\n",
      "Epoch 2508/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0107 - acc: 0.9972 - val_loss: 0.5746 - val_acc: 0.8920\n",
      "Epoch 2509/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0031 - acc: 1.0000 - val_loss: 0.5744 - val_acc: 0.8977\n",
      "Epoch 2510/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0110 - acc: 0.9957 - val_loss: 0.5517 - val_acc: 0.8920\n",
      "Epoch 2511/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0041 - acc: 0.9986 - val_loss: 0.5390 - val_acc: 0.8864\n",
      "Epoch 2512/3000\n",
      "702/702 [==============================] - 0s 594us/sample - loss: 0.0040 - acc: 1.0000 - val_loss: 0.5729 - val_acc: 0.8920\n",
      "Epoch 2513/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 0.5771 - val_acc: 0.9034\n",
      "Epoch 2514/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0030 - acc: 0.9986 - val_loss: 0.5502 - val_acc: 0.8977\n",
      "Epoch 2515/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0071 - acc: 0.9972 - val_loss: 0.5370 - val_acc: 0.8977\n",
      "Epoch 2516/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0171 - acc: 0.9957 - val_loss: 0.5701 - val_acc: 0.8977\n",
      "Epoch 2517/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0102 - acc: 0.9972 - val_loss: 0.7242 - val_acc: 0.8920\n",
      "Epoch 2518/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0091 - acc: 0.9986 - val_loss: 0.8417 - val_acc: 0.8977\n",
      "Epoch 2519/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0278 - acc: 0.9943 - val_loss: 0.6102 - val_acc: 0.8920\n",
      "Epoch 2520/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0235 - acc: 0.9915 - val_loss: 0.4500 - val_acc: 0.8864\n",
      "Epoch 2521/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0149 - acc: 0.9943 - val_loss: 0.4299 - val_acc: 0.9091\n",
      "Epoch 2522/3000\n",
      "702/702 [==============================] - 0s 590us/sample - loss: 0.0236 - acc: 0.9943 - val_loss: 0.4042 - val_acc: 0.8977\n",
      "Epoch 2523/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0049 - acc: 0.9986 - val_loss: 0.3975 - val_acc: 0.9091\n",
      "Epoch 2524/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0246 - acc: 0.9943 - val_loss: 0.4128 - val_acc: 0.9091\n",
      "Epoch 2525/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0028 - acc: 1.0000 - val_loss: 0.4203 - val_acc: 0.9034\n",
      "Epoch 2526/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0080 - acc: 0.9986 - val_loss: 0.5833 - val_acc: 0.9148\n",
      "Epoch 2527/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0227 - acc: 0.9915 - val_loss: 0.5999 - val_acc: 0.8977\n",
      "Epoch 2528/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0309 - acc: 0.9957 - val_loss: 0.4946 - val_acc: 0.8750\n",
      "Epoch 2529/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0270 - acc: 0.9900 - val_loss: 0.4096 - val_acc: 0.8864\n",
      "Epoch 2530/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0161 - acc: 0.9943 - val_loss: 0.4221 - val_acc: 0.9034\n",
      "Epoch 2531/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0067 - acc: 0.9986 - val_loss: 0.5852 - val_acc: 0.8977\n",
      "Epoch 2532/3000\n",
      "702/702 [==============================] - 0s 600us/sample - loss: 0.0243 - acc: 0.9900 - val_loss: 0.6032 - val_acc: 0.9091\n",
      "Epoch 2533/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0205 - acc: 0.9929 - val_loss: 0.3440 - val_acc: 0.8807\n",
      "Epoch 2534/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0308 - acc: 0.9872 - val_loss: 0.3282 - val_acc: 0.8977\n",
      "Epoch 2535/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0246 - acc: 0.9900 - val_loss: 0.3922 - val_acc: 0.9034\n",
      "Epoch 2536/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0190 - acc: 0.9943 - val_loss: 0.4634 - val_acc: 0.8920\n",
      "Epoch 2537/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0193 - acc: 0.9943 - val_loss: 0.5689 - val_acc: 0.8864\n",
      "Epoch 2538/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0229 - acc: 0.9957 - val_loss: 0.4064 - val_acc: 0.8977\n",
      "Epoch 2539/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0193 - acc: 0.9957 - val_loss: 0.4571 - val_acc: 0.9091\n",
      "Epoch 2540/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0101 - acc: 0.9957 - val_loss: 0.4139 - val_acc: 0.9148\n",
      "Epoch 2541/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0219 - acc: 0.9915 - val_loss: 0.3705 - val_acc: 0.8977\n",
      "Epoch 2542/3000\n",
      "702/702 [==============================] - 0s 585us/sample - loss: 0.0129 - acc: 0.9929 - val_loss: 0.4317 - val_acc: 0.9034\n",
      "Epoch 2543/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0070 - acc: 0.9957 - val_loss: 0.5396 - val_acc: 0.8977\n",
      "Epoch 2544/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0068 - acc: 0.9986 - val_loss: 0.6461 - val_acc: 0.9034\n",
      "Epoch 2545/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0049 - acc: 0.9986 - val_loss: 0.7242 - val_acc: 0.9034\n",
      "Epoch 2546/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0114 - acc: 0.9943 - val_loss: 0.5990 - val_acc: 0.9205\n",
      "Epoch 2547/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0166 - acc: 0.9943 - val_loss: 0.4312 - val_acc: 0.8977\n",
      "Epoch 2548/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0042 - acc: 1.0000 - val_loss: 0.4244 - val_acc: 0.8864\n",
      "Epoch 2549/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0096 - acc: 0.9943 - val_loss: 0.4443 - val_acc: 0.8807\n",
      "Epoch 2550/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0041 - acc: 1.0000 - val_loss: 0.4768 - val_acc: 0.8920\n",
      "Epoch 2551/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0051 - acc: 0.9986 - val_loss: 0.4907 - val_acc: 0.8977\n",
      "Epoch 2552/3000\n",
      "702/702 [==============================] - 0s 599us/sample - loss: 0.0056 - acc: 0.9986 - val_loss: 0.5648 - val_acc: 0.8977\n",
      "Epoch 2553/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0085 - acc: 0.9986 - val_loss: 0.6323 - val_acc: 0.8977\n",
      "Epoch 2554/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0030 - acc: 0.9986 - val_loss: 0.6524 - val_acc: 0.8977\n",
      "Epoch 2555/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0317 - acc: 0.9872 - val_loss: 0.5291 - val_acc: 0.8977\n",
      "Epoch 2556/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 0.4453 - val_acc: 0.8977\n",
      "Epoch 2557/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0115 - acc: 0.9943 - val_loss: 0.4323 - val_acc: 0.8977\n",
      "Epoch 2558/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0075 - acc: 0.9972 - val_loss: 0.4520 - val_acc: 0.8864\n",
      "Epoch 2559/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0029 - acc: 1.0000 - val_loss: 0.4656 - val_acc: 0.8977\n",
      "Epoch 2560/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0086 - acc: 0.9957 - val_loss: 0.5127 - val_acc: 0.8977\n",
      "Epoch 2561/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0045 - acc: 0.9986 - val_loss: 0.5941 - val_acc: 0.9091\n",
      "Epoch 2562/3000\n",
      "702/702 [==============================] - 0s 582us/sample - loss: 0.0040 - acc: 0.9972 - val_loss: 0.6330 - val_acc: 0.9148\n",
      "Epoch 2563/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0237 - acc: 0.9972 - val_loss: 0.6008 - val_acc: 0.9148\n",
      "Epoch 2564/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0086 - acc: 0.9972 - val_loss: 0.5636 - val_acc: 0.9034\n",
      "Epoch 2565/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0037 - acc: 0.9986 - val_loss: 0.5584 - val_acc: 0.9034\n",
      "Epoch 2566/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0049 - acc: 0.9986 - val_loss: 0.5280 - val_acc: 0.9091\n",
      "Epoch 2567/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0029 - acc: 1.0000 - val_loss: 0.4875 - val_acc: 0.9091\n",
      "Epoch 2568/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0080 - acc: 0.9957 - val_loss: 0.4693 - val_acc: 0.8920\n",
      "Epoch 2569/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0042 - acc: 0.9986 - val_loss: 0.4907 - val_acc: 0.8977\n",
      "Epoch 2570/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0169 - acc: 0.9943 - val_loss: 0.6039 - val_acc: 0.9034\n",
      "Epoch 2571/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0466 - acc: 0.9872 - val_loss: 0.5372 - val_acc: 0.8750\n",
      "Epoch 2572/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.0509 - acc: 0.9886 - val_loss: 0.5909 - val_acc: 0.8864\n",
      "Epoch 2573/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0192 - acc: 0.9957 - val_loss: 0.8212 - val_acc: 0.8977\n",
      "Epoch 2574/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0299 - acc: 0.9957 - val_loss: 0.6660 - val_acc: 0.8920\n",
      "Epoch 2575/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0109 - acc: 0.9972 - val_loss: 0.6226 - val_acc: 0.8864\n",
      "Epoch 2576/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0239 - acc: 0.9900 - val_loss: 0.8849 - val_acc: 0.8920\n",
      "Epoch 2577/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0280 - acc: 0.9929 - val_loss: 0.6781 - val_acc: 0.8864\n",
      "Epoch 2578/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0210 - acc: 0.9957 - val_loss: 0.4817 - val_acc: 0.8920\n",
      "Epoch 2579/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0352 - acc: 0.9900 - val_loss: 0.3916 - val_acc: 0.8920\n",
      "Epoch 2580/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0120 - acc: 0.9972 - val_loss: 0.4211 - val_acc: 0.8920\n",
      "Epoch 2581/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0111 - acc: 0.9972 - val_loss: 0.4309 - val_acc: 0.8864\n",
      "Epoch 2582/3000\n",
      "702/702 [==============================] - 0s 594us/sample - loss: 0.0559 - acc: 0.9915 - val_loss: 0.6877 - val_acc: 0.9034\n",
      "Epoch 2583/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0328 - acc: 0.9943 - val_loss: 0.4389 - val_acc: 0.8636\n",
      "Epoch 2584/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0498 - acc: 0.9772 - val_loss: 0.3423 - val_acc: 0.8977\n",
      "Epoch 2585/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0140 - acc: 0.9943 - val_loss: 0.4023 - val_acc: 0.8977\n",
      "Epoch 2586/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0324 - acc: 0.9915 - val_loss: 0.4480 - val_acc: 0.8750\n",
      "Epoch 2587/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0292 - acc: 0.9929 - val_loss: 0.3953 - val_acc: 0.8750\n",
      "Epoch 2588/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0309 - acc: 0.9915 - val_loss: 0.4239 - val_acc: 0.9034\n",
      "Epoch 2589/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0161 - acc: 0.9915 - val_loss: 0.5092 - val_acc: 0.8977\n",
      "Epoch 2590/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0149 - acc: 0.9943 - val_loss: 0.5027 - val_acc: 0.9034\n",
      "Epoch 2591/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0082 - acc: 0.9972 - val_loss: 0.5079 - val_acc: 0.8977\n",
      "Epoch 2592/3000\n",
      "702/702 [==============================] - 0s 593us/sample - loss: 0.0111 - acc: 0.9986 - val_loss: 0.5323 - val_acc: 0.8977\n",
      "Epoch 2593/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0067 - acc: 0.9986 - val_loss: 0.5718 - val_acc: 0.9034\n",
      "Epoch 2594/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0070 - acc: 0.9957 - val_loss: 0.5528 - val_acc: 0.9034\n",
      "Epoch 2595/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0032 - acc: 1.0000 - val_loss: 0.5404 - val_acc: 0.9034\n",
      "Epoch 2596/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0101 - acc: 0.9957 - val_loss: 0.5891 - val_acc: 0.9148\n",
      "Epoch 2597/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0018 - acc: 1.0000 - val_loss: 0.6049 - val_acc: 0.9091\n",
      "Epoch 2598/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0117 - acc: 0.9957 - val_loss: 0.6190 - val_acc: 0.9091\n",
      "Epoch 2599/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0050 - acc: 0.9986 - val_loss: 0.6147 - val_acc: 0.9148\n",
      "Epoch 2600/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0083 - acc: 0.9986 - val_loss: 0.5768 - val_acc: 0.9034\n",
      "Epoch 2601/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0031 - acc: 1.0000 - val_loss: 0.5621 - val_acc: 0.9034\n",
      "Epoch 2602/3000\n",
      "702/702 [==============================] - 0s 584us/sample - loss: 0.0104 - acc: 0.9943 - val_loss: 0.5364 - val_acc: 0.9091\n",
      "Epoch 2603/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0034 - acc: 0.9986 - val_loss: 0.5158 - val_acc: 0.9034\n",
      "Epoch 2604/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0069 - acc: 0.9972 - val_loss: 0.4960 - val_acc: 0.8977\n",
      "Epoch 2605/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0010 - acc: 1.0000 - val_loss: 0.4925 - val_acc: 0.8977\n",
      "Epoch 2606/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.0252 - acc: 0.9929 - val_loss: 0.4948 - val_acc: 0.8977\n",
      "Epoch 2607/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0112 - acc: 0.9957 - val_loss: 0.5677 - val_acc: 0.8977\n",
      "Epoch 2608/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0066 - acc: 0.9972 - val_loss: 0.5885 - val_acc: 0.8977\n",
      "Epoch 2609/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0158 - acc: 0.9915 - val_loss: 0.4960 - val_acc: 0.8807\n",
      "Epoch 2610/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0052 - acc: 0.9972 - val_loss: 0.4015 - val_acc: 0.8920\n",
      "Epoch 2611/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0198 - acc: 0.9957 - val_loss: 0.3757 - val_acc: 0.8864\n",
      "Epoch 2612/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.0062 - acc: 0.9986 - val_loss: 0.3778 - val_acc: 0.8977\n",
      "Epoch 2613/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0049 - acc: 1.0000 - val_loss: 0.4060 - val_acc: 0.9091\n",
      "Epoch 2614/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0176 - acc: 0.9972 - val_loss: 0.4701 - val_acc: 0.8977\n",
      "Epoch 2615/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0031 - acc: 0.9986 - val_loss: 0.5478 - val_acc: 0.9148\n",
      "Epoch 2616/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0058 - acc: 0.9986 - val_loss: 0.5487 - val_acc: 0.9205\n",
      "Epoch 2617/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0218 - acc: 0.9929 - val_loss: 0.5543 - val_acc: 0.9034\n",
      "Epoch 2618/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0167 - acc: 0.9957 - val_loss: 0.4715 - val_acc: 0.8864\n",
      "Epoch 2619/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0114 - acc: 0.9943 - val_loss: 0.4338 - val_acc: 0.8920\n",
      "Epoch 2620/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0061 - acc: 0.9986 - val_loss: 0.4438 - val_acc: 0.8977\n",
      "Epoch 2621/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0262 - acc: 0.9972 - val_loss: 0.4197 - val_acc: 0.9091\n",
      "Epoch 2622/3000\n",
      "702/702 [==============================] - 0s 587us/sample - loss: 0.0280 - acc: 0.9900 - val_loss: 0.4429 - val_acc: 0.8977\n",
      "Epoch 2623/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0164 - acc: 0.9943 - val_loss: 0.4584 - val_acc: 0.8920\n",
      "Epoch 2624/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0194 - acc: 0.9957 - val_loss: 0.4680 - val_acc: 0.8864\n",
      "Epoch 2625/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0075 - acc: 0.9986 - val_loss: 0.5623 - val_acc: 0.8807\n",
      "Epoch 2626/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0302 - acc: 0.9915 - val_loss: 0.5872 - val_acc: 0.8920\n",
      "Epoch 2627/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0104 - acc: 0.9943 - val_loss: 0.6628 - val_acc: 0.8977\n",
      "Epoch 2628/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0061 - acc: 0.9986 - val_loss: 0.6732 - val_acc: 0.9034\n",
      "Epoch 2629/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0064 - acc: 0.9986 - val_loss: 0.6055 - val_acc: 0.9034\n",
      "Epoch 2630/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0094 - acc: 0.9957 - val_loss: 0.4919 - val_acc: 0.8977\n",
      "Epoch 2631/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0086 - acc: 0.9986 - val_loss: 0.4760 - val_acc: 0.8977\n",
      "Epoch 2632/3000\n",
      "702/702 [==============================] - 0s 590us/sample - loss: 0.0065 - acc: 0.9986 - val_loss: 0.5494 - val_acc: 0.8977\n",
      "Epoch 2633/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0140 - acc: 0.9972 - val_loss: 0.5522 - val_acc: 0.8977\n",
      "Epoch 2634/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0035 - acc: 0.9986 - val_loss: 0.5389 - val_acc: 0.9034\n",
      "Epoch 2635/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0173 - acc: 0.9972 - val_loss: 0.4831 - val_acc: 0.9034\n",
      "Epoch 2636/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 0.4519 - val_acc: 0.9034\n",
      "Epoch 2637/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.0069 - acc: 0.9972 - val_loss: 0.4748 - val_acc: 0.8920\n",
      "Epoch 2638/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0141 - acc: 0.9986 - val_loss: 0.5111 - val_acc: 0.8750\n",
      "Epoch 2639/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0050 - acc: 0.9986 - val_loss: 0.5597 - val_acc: 0.8864\n",
      "Epoch 2640/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0221 - acc: 0.9957 - val_loss: 0.5836 - val_acc: 0.8920\n",
      "Epoch 2641/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0141 - acc: 0.9957 - val_loss: 0.4811 - val_acc: 0.8920\n",
      "Epoch 2642/3000\n",
      "702/702 [==============================] - 0s 581us/sample - loss: 0.0091 - acc: 0.9972 - val_loss: 0.5635 - val_acc: 0.8977\n",
      "Epoch 2643/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0035 - acc: 1.0000 - val_loss: 0.5553 - val_acc: 0.8977\n",
      "Epoch 2644/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0096 - acc: 0.9957 - val_loss: 0.4492 - val_acc: 0.8920\n",
      "Epoch 2645/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0100 - acc: 0.9957 - val_loss: 0.4626 - val_acc: 0.8977\n",
      "Epoch 2646/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0145 - acc: 0.9943 - val_loss: 0.4932 - val_acc: 0.9148\n",
      "Epoch 2647/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0071 - acc: 0.9986 - val_loss: 0.4241 - val_acc: 0.8920\n",
      "Epoch 2648/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0169 - acc: 0.9957 - val_loss: 0.5189 - val_acc: 0.8920\n",
      "Epoch 2649/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0192 - acc: 0.9957 - val_loss: 0.6668 - val_acc: 0.8977\n",
      "Epoch 2650/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0082 - acc: 0.9972 - val_loss: 0.4336 - val_acc: 0.9034\n",
      "Epoch 2651/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0097 - acc: 0.9972 - val_loss: 0.4220 - val_acc: 0.8977\n",
      "Epoch 2652/3000\n",
      "702/702 [==============================] - 0s 587us/sample - loss: 0.0113 - acc: 0.9957 - val_loss: 0.4273 - val_acc: 0.8977\n",
      "Epoch 2653/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0258 - acc: 0.9929 - val_loss: 0.4791 - val_acc: 0.8864\n",
      "Epoch 2654/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0125 - acc: 0.9943 - val_loss: 0.5245 - val_acc: 0.8977\n",
      "Epoch 2655/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0036 - acc: 0.9986 - val_loss: 0.5595 - val_acc: 0.9034\n",
      "Epoch 2656/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0042 - acc: 0.9986 - val_loss: 0.5803 - val_acc: 0.9034\n",
      "Epoch 2657/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 0.6037 - val_acc: 0.8977\n",
      "Epoch 2658/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0034 - acc: 1.0000 - val_loss: 0.6104 - val_acc: 0.9034\n",
      "Epoch 2659/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0021 - acc: 0.9986 - val_loss: 0.6075 - val_acc: 0.9034\n",
      "Epoch 2660/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0038 - acc: 0.9986 - val_loss: 0.6178 - val_acc: 0.8977\n",
      "Epoch 2661/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0088 - acc: 0.9957 - val_loss: 0.5839 - val_acc: 0.8977\n",
      "Epoch 2662/3000\n",
      "702/702 [==============================] - 0s 598us/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 0.5779 - val_acc: 0.8920\n",
      "Epoch 2663/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0080 - acc: 0.9972 - val_loss: 0.5546 - val_acc: 0.8977\n",
      "Epoch 2664/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0091 - acc: 0.9957 - val_loss: 0.4889 - val_acc: 0.9034\n",
      "Epoch 2665/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0094 - acc: 0.9957 - val_loss: 0.5005 - val_acc: 0.9034\n",
      "Epoch 2666/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0032 - acc: 1.0000 - val_loss: 0.7039 - val_acc: 0.9091\n",
      "Epoch 2667/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0198 - acc: 0.9957 - val_loss: 0.6660 - val_acc: 0.8977\n",
      "Epoch 2668/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0094 - acc: 0.9972 - val_loss: 0.5903 - val_acc: 0.8977\n",
      "Epoch 2669/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0281 - acc: 0.9929 - val_loss: 0.5216 - val_acc: 0.8977\n",
      "Epoch 2670/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0259 - acc: 0.9943 - val_loss: 0.6248 - val_acc: 0.9205\n",
      "Epoch 2671/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0098 - acc: 0.9986 - val_loss: 0.5455 - val_acc: 0.9091\n",
      "Epoch 2672/3000\n",
      "702/702 [==============================] - 0s 594us/sample - loss: 0.0128 - acc: 0.9943 - val_loss: 0.5495 - val_acc: 0.8977\n",
      "Epoch 2673/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0101 - acc: 0.9972 - val_loss: 0.6495 - val_acc: 0.8864\n",
      "Epoch 2674/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0090 - acc: 0.9957 - val_loss: 0.5856 - val_acc: 0.8920\n",
      "Epoch 2675/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0030 - acc: 0.9986 - val_loss: 0.5644 - val_acc: 0.9091\n",
      "Epoch 2676/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0103 - acc: 0.9943 - val_loss: 0.5300 - val_acc: 0.9091\n",
      "Epoch 2677/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0123 - acc: 0.9972 - val_loss: 0.4134 - val_acc: 0.9091\n",
      "Epoch 2678/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0186 - acc: 0.9986 - val_loss: 0.3828 - val_acc: 0.9091\n",
      "Epoch 2679/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0053 - acc: 0.9972 - val_loss: 0.3925 - val_acc: 0.9034\n",
      "Epoch 2680/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0055 - acc: 0.9986 - val_loss: 0.4493 - val_acc: 0.9034\n",
      "Epoch 2681/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0021 - acc: 1.0000 - val_loss: 0.4997 - val_acc: 0.9091\n",
      "Epoch 2682/3000\n",
      "702/702 [==============================] - 0s 595us/sample - loss: 0.0153 - acc: 0.9972 - val_loss: 0.5180 - val_acc: 0.9034\n",
      "Epoch 2683/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0046 - acc: 0.9972 - val_loss: 0.4386 - val_acc: 0.9148\n",
      "Epoch 2684/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0307 - acc: 0.9915 - val_loss: 0.3495 - val_acc: 0.9034\n",
      "Epoch 2685/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0151 - acc: 0.9957 - val_loss: 0.6466 - val_acc: 0.8693\n",
      "Epoch 2686/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0121 - acc: 0.9929 - val_loss: 0.7059 - val_acc: 0.8807\n",
      "Epoch 2687/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0043 - acc: 0.9986 - val_loss: 0.5867 - val_acc: 0.8807\n",
      "Epoch 2688/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0044 - acc: 0.9986 - val_loss: 0.5312 - val_acc: 0.8864\n",
      "Epoch 2689/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0197 - acc: 0.9929 - val_loss: 0.4720 - val_acc: 0.9034\n",
      "Epoch 2690/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0096 - acc: 0.9972 - val_loss: 0.4520 - val_acc: 0.8977\n",
      "Epoch 2691/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0037 - acc: 0.9986 - val_loss: 0.4597 - val_acc: 0.9034\n",
      "Epoch 2692/3000\n",
      "702/702 [==============================] - 0s 590us/sample - loss: 0.0033 - acc: 0.9986 - val_loss: 0.4746 - val_acc: 0.8920\n",
      "Epoch 2693/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0184 - acc: 0.9943 - val_loss: 0.4540 - val_acc: 0.8920\n",
      "Epoch 2694/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0067 - acc: 0.9972 - val_loss: 0.4055 - val_acc: 0.8864\n",
      "Epoch 2695/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0159 - acc: 0.9972 - val_loss: 0.4582 - val_acc: 0.8864\n",
      "Epoch 2696/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0148 - acc: 0.9943 - val_loss: 0.6885 - val_acc: 0.9034\n",
      "Epoch 2697/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0100 - acc: 0.9929 - val_loss: 0.7173 - val_acc: 0.9034\n",
      "Epoch 2698/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0142 - acc: 0.9972 - val_loss: 0.4882 - val_acc: 0.9148\n",
      "Epoch 2699/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0119 - acc: 0.9972 - val_loss: 0.4359 - val_acc: 0.9034\n",
      "Epoch 2700/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0073 - acc: 0.9943 - val_loss: 0.4424 - val_acc: 0.8977\n",
      "Epoch 2701/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0062 - acc: 0.9986 - val_loss: 0.4464 - val_acc: 0.8977\n",
      "Epoch 2702/3000\n",
      "702/702 [==============================] - 0s 584us/sample - loss: 0.0107 - acc: 0.9986 - val_loss: 0.4720 - val_acc: 0.8750\n",
      "Epoch 2703/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0147 - acc: 0.9943 - val_loss: 0.5318 - val_acc: 0.8864\n",
      "Epoch 2704/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0075 - acc: 0.9986 - val_loss: 0.6205 - val_acc: 0.8807\n",
      "Epoch 2705/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0160 - acc: 0.9957 - val_loss: 0.6899 - val_acc: 0.8920\n",
      "Epoch 2706/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0102 - acc: 0.9957 - val_loss: 0.5576 - val_acc: 0.8977\n",
      "Epoch 2707/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0198 - acc: 0.9929 - val_loss: 0.5550 - val_acc: 0.8977\n",
      "Epoch 2708/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0242 - acc: 0.9943 - val_loss: 0.5899 - val_acc: 0.9091\n",
      "Epoch 2709/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0045 - acc: 1.0000 - val_loss: 0.6209 - val_acc: 0.9034\n",
      "Epoch 2710/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0127 - acc: 0.9972 - val_loss: 0.5911 - val_acc: 0.8920\n",
      "Epoch 2711/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0079 - acc: 0.9986 - val_loss: 0.4805 - val_acc: 0.8864\n",
      "Epoch 2712/3000\n",
      "702/702 [==============================] - 0s 586us/sample - loss: 0.0089 - acc: 0.9972 - val_loss: 0.4487 - val_acc: 0.8864\n",
      "Epoch 2713/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0064 - acc: 0.9986 - val_loss: 0.4962 - val_acc: 0.9034\n",
      "Epoch 2714/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0082 - acc: 0.9972 - val_loss: 0.5703 - val_acc: 0.9091\n",
      "Epoch 2715/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0344 - acc: 0.9915 - val_loss: 0.5150 - val_acc: 0.9205\n",
      "Epoch 2716/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0079 - acc: 0.9972 - val_loss: 0.6231 - val_acc: 0.8920\n",
      "Epoch 2717/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0570 - acc: 0.9815 - val_loss: 0.4318 - val_acc: 0.8920\n",
      "Epoch 2718/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0363 - acc: 0.9858 - val_loss: 0.4456 - val_acc: 0.8920\n",
      "Epoch 2719/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0236 - acc: 0.9900 - val_loss: 0.5624 - val_acc: 0.8920\n",
      "Epoch 2720/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0089 - acc: 0.9929 - val_loss: 0.4608 - val_acc: 0.8807\n",
      "Epoch 2721/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0084 - acc: 0.9972 - val_loss: 0.5230 - val_acc: 0.8977\n",
      "Epoch 2722/3000\n",
      "702/702 [==============================] - 0s 589us/sample - loss: 0.0135 - acc: 0.9943 - val_loss: 0.5902 - val_acc: 0.9034\n",
      "Epoch 2723/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0065 - acc: 0.9957 - val_loss: 0.6726 - val_acc: 0.8864\n",
      "Epoch 2724/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0095 - acc: 0.9986 - val_loss: 0.7285 - val_acc: 0.8977\n",
      "Epoch 2725/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0163 - acc: 0.9957 - val_loss: 0.5716 - val_acc: 0.8977\n",
      "Epoch 2726/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0164 - acc: 0.9943 - val_loss: 0.5106 - val_acc: 0.8920\n",
      "Epoch 2727/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0206 - acc: 0.9929 - val_loss: 0.6017 - val_acc: 0.9034\n",
      "Epoch 2728/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0073 - acc: 0.9986 - val_loss: 0.7171 - val_acc: 0.8977\n",
      "Epoch 2729/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0042 - acc: 1.0000 - val_loss: 0.6962 - val_acc: 0.9034\n",
      "Epoch 2730/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0098 - acc: 0.9957 - val_loss: 0.7037 - val_acc: 0.9091\n",
      "Epoch 2731/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0025 - acc: 1.0000 - val_loss: 0.7353 - val_acc: 0.8977\n",
      "Epoch 2732/3000\n",
      "702/702 [==============================] - 0s 602us/sample - loss: 0.0019 - acc: 1.0000 - val_loss: 0.7410 - val_acc: 0.9034\n",
      "Epoch 2733/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0029 - acc: 0.9986 - val_loss: 0.7353 - val_acc: 0.9034\n",
      "Epoch 2734/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0081 - acc: 0.9972 - val_loss: 0.6101 - val_acc: 0.9034\n",
      "Epoch 2735/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0029 - acc: 0.9986 - val_loss: 0.5671 - val_acc: 0.9091\n",
      "Epoch 2736/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0219 - acc: 0.9957 - val_loss: 0.6234 - val_acc: 0.9034\n",
      "Epoch 2737/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0301 - acc: 0.9957 - val_loss: 0.7653 - val_acc: 0.8864\n",
      "Epoch 2738/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0105 - acc: 0.9986 - val_loss: 0.4598 - val_acc: 0.8977\n",
      "Epoch 2739/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0368 - acc: 0.9886 - val_loss: 0.4287 - val_acc: 0.8977\n",
      "Epoch 2740/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0136 - acc: 0.9957 - val_loss: 0.4936 - val_acc: 0.8977\n",
      "Epoch 2741/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0070 - acc: 0.9986 - val_loss: 0.4718 - val_acc: 0.8864\n",
      "Epoch 2742/3000\n",
      "702/702 [==============================] - 0s 594us/sample - loss: 0.0174 - acc: 0.9957 - val_loss: 0.4536 - val_acc: 0.8807\n",
      "Epoch 2743/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0086 - acc: 0.9986 - val_loss: 0.4617 - val_acc: 0.8864\n",
      "Epoch 2744/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0048 - acc: 0.9986 - val_loss: 0.5039 - val_acc: 0.8977\n",
      "Epoch 2745/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0143 - acc: 0.9957 - val_loss: 0.5917 - val_acc: 0.8920\n",
      "Epoch 2746/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0153 - acc: 0.9957 - val_loss: 0.4522 - val_acc: 0.8977\n",
      "Epoch 2747/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0076 - acc: 0.9986 - val_loss: 0.4062 - val_acc: 0.9034\n",
      "Epoch 2748/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0128 - acc: 0.9957 - val_loss: 0.5346 - val_acc: 0.8864\n",
      "Epoch 2749/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0084 - acc: 0.9972 - val_loss: 0.6392 - val_acc: 0.8977\n",
      "Epoch 2750/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0154 - acc: 0.9972 - val_loss: 0.6876 - val_acc: 0.8977\n",
      "Epoch 2751/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0132 - acc: 0.9972 - val_loss: 0.7372 - val_acc: 0.8977\n",
      "Epoch 2752/3000\n",
      "702/702 [==============================] - 0s 589us/sample - loss: 0.0040 - acc: 0.9986 - val_loss: 0.7746 - val_acc: 0.8977\n",
      "Epoch 2753/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0055 - acc: 0.9986 - val_loss: 0.7228 - val_acc: 0.8920\n",
      "Epoch 2754/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0168 - acc: 0.9972 - val_loss: 0.7247 - val_acc: 0.8864\n",
      "Epoch 2755/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0241 - acc: 0.9943 - val_loss: 0.6893 - val_acc: 0.8977\n",
      "Epoch 2756/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0073 - acc: 0.9972 - val_loss: 0.4604 - val_acc: 0.8920\n",
      "Epoch 2757/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0064 - acc: 0.9986 - val_loss: 0.4030 - val_acc: 0.8920\n",
      "Epoch 2758/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0094 - acc: 0.9986 - val_loss: 0.4863 - val_acc: 0.9091\n",
      "Epoch 2759/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0117 - acc: 0.9929 - val_loss: 0.6061 - val_acc: 0.8920\n",
      "Epoch 2760/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0204 - acc: 0.9915 - val_loss: 0.6117 - val_acc: 0.8977\n",
      "Epoch 2761/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0045 - acc: 0.9986 - val_loss: 0.6368 - val_acc: 0.8920\n",
      "Epoch 2762/3000\n",
      "702/702 [==============================] - 0s 587us/sample - loss: 0.0049 - acc: 0.9972 - val_loss: 0.6354 - val_acc: 0.9148\n",
      "Epoch 2763/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0194 - acc: 0.9929 - val_loss: 0.5952 - val_acc: 0.9034\n",
      "Epoch 2764/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0055 - acc: 0.9986 - val_loss: 0.5177 - val_acc: 0.8920\n",
      "Epoch 2765/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0046 - acc: 0.9986 - val_loss: 0.5182 - val_acc: 0.8920\n",
      "Epoch 2766/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0066 - acc: 0.9986 - val_loss: 0.4553 - val_acc: 0.8977\n",
      "Epoch 2767/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0049 - acc: 1.0000 - val_loss: 0.4903 - val_acc: 0.8920\n",
      "Epoch 2768/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0030 - acc: 1.0000 - val_loss: 0.5350 - val_acc: 0.8864\n",
      "Epoch 2769/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0084 - acc: 0.9972 - val_loss: 0.6065 - val_acc: 0.8920\n",
      "Epoch 2770/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0101 - acc: 0.9972 - val_loss: 0.5325 - val_acc: 0.9034\n",
      "Epoch 2771/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0034 - acc: 0.9986 - val_loss: 0.5187 - val_acc: 0.9091\n",
      "Epoch 2772/3000\n",
      "702/702 [==============================] - 0s 578us/sample - loss: 0.0205 - acc: 0.9929 - val_loss: 0.6253 - val_acc: 0.8920\n",
      "Epoch 2773/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.0054 - acc: 0.9986 - val_loss: 0.7846 - val_acc: 0.8977\n",
      "Epoch 2774/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0150 - acc: 0.9943 - val_loss: 0.6927 - val_acc: 0.9034\n",
      "Epoch 2775/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0154 - acc: 0.9943 - val_loss: 0.5999 - val_acc: 0.8920\n",
      "Epoch 2776/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0060 - acc: 0.9972 - val_loss: 0.5981 - val_acc: 0.8977\n",
      "Epoch 2777/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0090 - acc: 0.9972 - val_loss: 0.6239 - val_acc: 0.8864\n",
      "Epoch 2778/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0136 - acc: 0.9972 - val_loss: 0.4520 - val_acc: 0.8977\n",
      "Epoch 2779/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0145 - acc: 0.9972 - val_loss: 0.4605 - val_acc: 0.8977\n",
      "Epoch 2780/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 0.4810 - val_acc: 0.9034\n",
      "Epoch 2781/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0197 - acc: 0.9957 - val_loss: 0.5360 - val_acc: 0.8977\n",
      "Epoch 2782/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 0.7656 - val_acc: 0.8920\n",
      "Epoch 2783/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0126 - acc: 0.9972 - val_loss: 0.7112 - val_acc: 0.9034\n",
      "Epoch 2784/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0053 - acc: 0.9986 - val_loss: 0.5934 - val_acc: 0.9034\n",
      "Epoch 2785/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0077 - acc: 0.9972 - val_loss: 0.5604 - val_acc: 0.8977\n",
      "Epoch 2786/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0148 - acc: 0.9929 - val_loss: 0.5966 - val_acc: 0.9034\n",
      "Epoch 2787/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0058 - acc: 0.9972 - val_loss: 0.6349 - val_acc: 0.9091\n",
      "Epoch 2788/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0082 - acc: 0.9986 - val_loss: 0.6662 - val_acc: 0.9091\n",
      "Epoch 2789/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0177 - acc: 0.9957 - val_loss: 0.5122 - val_acc: 0.9091\n",
      "Epoch 2790/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0044 - acc: 0.9986 - val_loss: 0.4174 - val_acc: 0.8920\n",
      "Epoch 2791/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0035 - acc: 1.0000 - val_loss: 0.4258 - val_acc: 0.8750\n",
      "Epoch 2792/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.0059 - acc: 1.0000 - val_loss: 0.4579 - val_acc: 0.8864\n",
      "Epoch 2793/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0052 - acc: 0.9986 - val_loss: 0.5125 - val_acc: 0.8920\n",
      "Epoch 2794/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0054 - acc: 0.9972 - val_loss: 0.5427 - val_acc: 0.9091\n",
      "Epoch 2795/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0024 - acc: 1.0000 - val_loss: 0.5548 - val_acc: 0.9034\n",
      "Epoch 2796/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0104 - acc: 0.9972 - val_loss: 0.4680 - val_acc: 0.9034\n",
      "Epoch 2797/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0054 - acc: 1.0000 - val_loss: 0.4000 - val_acc: 0.8920\n",
      "Epoch 2798/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0025 - acc: 1.0000 - val_loss: 0.3967 - val_acc: 0.9034\n",
      "Epoch 2799/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0023 - acc: 1.0000 - val_loss: 0.4953 - val_acc: 0.8977\n",
      "Epoch 2800/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0043 - acc: 0.9986 - val_loss: 0.5759 - val_acc: 0.9034\n",
      "Epoch 2801/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0045 - acc: 0.9986 - val_loss: 0.6602 - val_acc: 0.9034\n",
      "Epoch 2802/3000\n",
      "702/702 [==============================] - 0s 591us/sample - loss: 0.0092 - acc: 0.9972 - val_loss: 0.6414 - val_acc: 0.9091\n",
      "Epoch 2803/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0062 - acc: 0.9972 - val_loss: 0.6379 - val_acc: 0.8977\n",
      "Epoch 2804/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0053 - acc: 0.9986 - val_loss: 0.6240 - val_acc: 0.8977\n",
      "Epoch 2805/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 0.6302 - val_acc: 0.8977\n",
      "Epoch 2806/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0127 - acc: 0.9943 - val_loss: 0.6117 - val_acc: 0.9034\n",
      "Epoch 2807/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0050 - acc: 0.9972 - val_loss: 0.5961 - val_acc: 0.9034\n",
      "Epoch 2808/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0039 - acc: 0.9986 - val_loss: 0.5648 - val_acc: 0.9091\n",
      "Epoch 2809/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0021 - acc: 1.0000 - val_loss: 0.4903 - val_acc: 0.9034\n",
      "Epoch 2810/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0042 - acc: 0.9986 - val_loss: 0.4322 - val_acc: 0.8977\n",
      "Epoch 2811/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0030 - acc: 0.9986 - val_loss: 0.4221 - val_acc: 0.9091\n",
      "Epoch 2812/3000\n",
      "702/702 [==============================] - 0s 593us/sample - loss: 0.0051 - acc: 0.9986 - val_loss: 0.5397 - val_acc: 0.9034\n",
      "Epoch 2813/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 0.6350 - val_acc: 0.9034\n",
      "Epoch 2814/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0031 - acc: 0.9986 - val_loss: 0.6498 - val_acc: 0.9034\n",
      "Epoch 2815/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0127 - acc: 0.9972 - val_loss: 0.5103 - val_acc: 0.8864\n",
      "Epoch 2816/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0072 - acc: 0.9972 - val_loss: 0.7278 - val_acc: 0.8977\n",
      "Epoch 2817/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0265 - acc: 0.9972 - val_loss: 0.7362 - val_acc: 0.8977\n",
      "Epoch 2818/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0250 - acc: 0.9943 - val_loss: 0.4346 - val_acc: 0.8977\n",
      "Epoch 2819/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0047 - acc: 0.9986 - val_loss: 0.3484 - val_acc: 0.9091\n",
      "Epoch 2820/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.0072 - acc: 0.9972 - val_loss: 0.3879 - val_acc: 0.8920\n",
      "Epoch 2821/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0141 - acc: 0.9957 - val_loss: 0.4855 - val_acc: 0.8920\n",
      "Epoch 2822/3000\n",
      "702/702 [==============================] - 0s 589us/sample - loss: 0.0063 - acc: 0.9986 - val_loss: 0.5543 - val_acc: 0.8977\n",
      "Epoch 2823/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0128 - acc: 0.9957 - val_loss: 0.4448 - val_acc: 0.8977\n",
      "Epoch 2824/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0234 - acc: 0.9915 - val_loss: 0.4285 - val_acc: 0.9091\n",
      "Epoch 2825/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0153 - acc: 0.9943 - val_loss: 0.5072 - val_acc: 0.8807\n",
      "Epoch 2826/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0277 - acc: 0.9915 - val_loss: 0.5809 - val_acc: 0.8920\n",
      "Epoch 2827/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0061 - acc: 0.9986 - val_loss: 0.4197 - val_acc: 0.8977\n",
      "Epoch 2828/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0108 - acc: 0.9972 - val_loss: 0.6414 - val_acc: 0.8864\n",
      "Epoch 2829/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0150 - acc: 0.9957 - val_loss: 0.5643 - val_acc: 0.8864\n",
      "Epoch 2830/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0043 - acc: 1.0000 - val_loss: 0.5237 - val_acc: 0.8864\n",
      "Epoch 2831/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0173 - acc: 0.9957 - val_loss: 0.5622 - val_acc: 0.8807\n",
      "Epoch 2832/3000\n",
      "702/702 [==============================] - 0s 583us/sample - loss: 0.0148 - acc: 0.9957 - val_loss: 0.5964 - val_acc: 0.8864\n",
      "Epoch 2833/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0060 - acc: 0.9972 - val_loss: 0.5882 - val_acc: 0.8977\n",
      "Epoch 2834/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0084 - acc: 0.9986 - val_loss: 0.5444 - val_acc: 0.8977\n",
      "Epoch 2835/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0035 - acc: 1.0000 - val_loss: 0.5113 - val_acc: 0.8864\n",
      "Epoch 2836/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0156 - acc: 0.9943 - val_loss: 0.4667 - val_acc: 0.8864\n",
      "Epoch 2837/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0036 - acc: 0.9986 - val_loss: 0.4312 - val_acc: 0.8807\n",
      "Epoch 2838/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0083 - acc: 0.9957 - val_loss: 0.4003 - val_acc: 0.8920\n",
      "Epoch 2839/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0090 - acc: 0.9943 - val_loss: 0.4180 - val_acc: 0.8977\n",
      "Epoch 2840/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0059 - acc: 0.9972 - val_loss: 0.4971 - val_acc: 0.8864\n",
      "Epoch 2841/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0165 - acc: 0.9957 - val_loss: 0.5217 - val_acc: 0.8920\n",
      "Epoch 2842/3000\n",
      "702/702 [==============================] - 0s 584us/sample - loss: 0.0025 - acc: 1.0000 - val_loss: 0.5010 - val_acc: 0.8920\n",
      "Epoch 2843/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0019 - acc: 1.0000 - val_loss: 0.4976 - val_acc: 0.8920\n",
      "Epoch 2844/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0248 - acc: 0.9957 - val_loss: 0.4740 - val_acc: 0.8920\n",
      "Epoch 2845/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 0.7086 - val_acc: 0.8807\n",
      "Epoch 2846/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0048 - acc: 0.9986 - val_loss: 0.7094 - val_acc: 0.8864\n",
      "Epoch 2847/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0055 - acc: 0.9986 - val_loss: 0.6540 - val_acc: 0.8807\n",
      "Epoch 2848/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0023 - acc: 1.0000 - val_loss: 0.5659 - val_acc: 0.8920\n",
      "Epoch 2849/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0035 - acc: 0.9986 - val_loss: 0.5182 - val_acc: 0.8977\n",
      "Epoch 2850/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0063 - acc: 0.9986 - val_loss: 0.5203 - val_acc: 0.8977\n",
      "Epoch 2851/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0061 - acc: 0.9986 - val_loss: 0.5237 - val_acc: 0.8977\n",
      "Epoch 2852/3000\n",
      "702/702 [==============================] - 0s 588us/sample - loss: 0.0180 - acc: 0.9943 - val_loss: 0.6639 - val_acc: 0.8920\n",
      "Epoch 2853/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 0.6952 - val_acc: 0.8920\n",
      "Epoch 2854/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0044 - acc: 0.9986 - val_loss: 0.6969 - val_acc: 0.9091\n",
      "Epoch 2855/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0059 - acc: 0.9986 - val_loss: 0.6771 - val_acc: 0.8920\n",
      "Epoch 2856/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0121 - acc: 0.9957 - val_loss: 0.5872 - val_acc: 0.9034\n",
      "Epoch 2857/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0081 - acc: 0.9957 - val_loss: 0.5704 - val_acc: 0.8977\n",
      "Epoch 2858/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0077 - acc: 0.9986 - val_loss: 0.5968 - val_acc: 0.8977\n",
      "Epoch 2859/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0083 - acc: 0.9957 - val_loss: 0.6000 - val_acc: 0.8977\n",
      "Epoch 2860/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0073 - acc: 0.9986 - val_loss: 0.6878 - val_acc: 0.8750\n",
      "Epoch 2861/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0052 - acc: 0.9986 - val_loss: 0.7376 - val_acc: 0.8864\n",
      "Epoch 2862/3000\n",
      "702/702 [==============================] - 0s 577us/sample - loss: 0.0240 - acc: 0.9943 - val_loss: 0.7274 - val_acc: 0.8864\n",
      "Epoch 2863/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0026 - acc: 0.9986 - val_loss: 0.5816 - val_acc: 0.8864\n",
      "Epoch 2864/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0095 - acc: 0.9957 - val_loss: 0.5785 - val_acc: 0.8864\n",
      "Epoch 2865/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0086 - acc: 0.9972 - val_loss: 0.5942 - val_acc: 0.8920\n",
      "Epoch 2866/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0034 - acc: 1.0000 - val_loss: 0.6464 - val_acc: 0.9034\n",
      "Epoch 2867/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0276 - acc: 0.9943 - val_loss: 0.4642 - val_acc: 0.9034\n",
      "Epoch 2868/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.0027 - acc: 1.0000 - val_loss: 0.4655 - val_acc: 0.9034\n",
      "Epoch 2869/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0103 - acc: 0.9972 - val_loss: 0.4313 - val_acc: 0.8977\n",
      "Epoch 2870/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0098 - acc: 0.9972 - val_loss: 0.5178 - val_acc: 0.9034\n",
      "Epoch 2871/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0085 - acc: 0.9972 - val_loss: 0.5513 - val_acc: 0.9034\n",
      "Epoch 2872/3000\n",
      "702/702 [==============================] - 0s 591us/sample - loss: 0.0186 - acc: 0.9943 - val_loss: 0.4332 - val_acc: 0.9034\n",
      "Epoch 2873/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0121 - acc: 0.9957 - val_loss: 0.3789 - val_acc: 0.9091\n",
      "Epoch 2874/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0058 - acc: 1.0000 - val_loss: 0.3879 - val_acc: 0.9091\n",
      "Epoch 2875/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0144 - acc: 0.9986 - val_loss: 0.4367 - val_acc: 0.8864\n",
      "Epoch 2876/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0052 - acc: 0.9986 - val_loss: 0.5562 - val_acc: 0.8977\n",
      "Epoch 2877/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0119 - acc: 0.9957 - val_loss: 0.5881 - val_acc: 0.9034\n",
      "Epoch 2878/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0142 - acc: 0.9957 - val_loss: 0.5896 - val_acc: 0.9034\n",
      "Epoch 2879/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0134 - acc: 0.9972 - val_loss: 0.5520 - val_acc: 0.8977\n",
      "Epoch 2880/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0074 - acc: 0.9972 - val_loss: 0.5980 - val_acc: 0.8920\n",
      "Epoch 2881/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0086 - acc: 0.9972 - val_loss: 0.6639 - val_acc: 0.8920\n",
      "Epoch 2882/3000\n",
      "702/702 [==============================] - 0s 585us/sample - loss: 0.0051 - acc: 0.9986 - val_loss: 0.6195 - val_acc: 0.8920\n",
      "Epoch 2883/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0023 - acc: 1.0000 - val_loss: 0.5437 - val_acc: 0.8864\n",
      "Epoch 2884/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0022 - acc: 0.9986 - val_loss: 0.5368 - val_acc: 0.8864\n",
      "Epoch 2885/3000\n",
      "702/702 [==============================] - 0s 534us/sample - loss: 0.0074 - acc: 0.9972 - val_loss: 0.6049 - val_acc: 0.8920\n",
      "Epoch 2886/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0070 - acc: 0.9986 - val_loss: 0.6416 - val_acc: 0.8920\n",
      "Epoch 2887/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0068 - acc: 0.9972 - val_loss: 0.6142 - val_acc: 0.8864\n",
      "Epoch 2888/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0103 - acc: 0.9929 - val_loss: 0.5793 - val_acc: 0.8920\n",
      "Epoch 2889/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0027 - acc: 1.0000 - val_loss: 0.5065 - val_acc: 0.8864\n",
      "Epoch 2890/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0069 - acc: 0.9943 - val_loss: 0.5424 - val_acc: 0.8864\n",
      "Epoch 2891/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0067 - acc: 0.9986 - val_loss: 0.6438 - val_acc: 0.8920\n",
      "Epoch 2892/3000\n",
      "702/702 [==============================] - 0s 589us/sample - loss: 0.0139 - acc: 0.9972 - val_loss: 0.7468 - val_acc: 0.9091\n",
      "Epoch 2893/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0129 - acc: 0.9957 - val_loss: 0.5700 - val_acc: 0.8920\n",
      "Epoch 2894/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0128 - acc: 0.9929 - val_loss: 0.4583 - val_acc: 0.8864\n",
      "Epoch 2895/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0070 - acc: 0.9986 - val_loss: 0.5252 - val_acc: 0.8807\n",
      "Epoch 2896/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0081 - acc: 0.9972 - val_loss: 0.5340 - val_acc: 0.8864\n",
      "Epoch 2897/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0100 - acc: 0.9986 - val_loss: 0.4758 - val_acc: 0.9034\n",
      "Epoch 2898/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0054 - acc: 0.9972 - val_loss: 0.5024 - val_acc: 0.8920\n",
      "Epoch 2899/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0060 - acc: 0.9986 - val_loss: 0.5357 - val_acc: 0.8864\n",
      "Epoch 2900/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0073 - acc: 0.9957 - val_loss: 0.5105 - val_acc: 0.8864\n",
      "Epoch 2901/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0031 - acc: 0.9986 - val_loss: 0.5220 - val_acc: 0.8920\n",
      "Epoch 2902/3000\n",
      "702/702 [==============================] - 0s 569us/sample - loss: 0.0068 - acc: 0.9972 - val_loss: 0.4933 - val_acc: 0.8864\n",
      "Epoch 2903/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0060 - acc: 0.9972 - val_loss: 0.5799 - val_acc: 0.8807\n",
      "Epoch 2904/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0053 - acc: 1.0000 - val_loss: 0.5996 - val_acc: 0.8807\n",
      "Epoch 2905/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0132 - acc: 0.9943 - val_loss: 0.5618 - val_acc: 0.8864\n",
      "Epoch 2906/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0036 - acc: 1.0000 - val_loss: 0.5228 - val_acc: 0.8864\n",
      "Epoch 2907/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0129 - acc: 0.9957 - val_loss: 0.5464 - val_acc: 0.8864\n",
      "Epoch 2908/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0034 - acc: 0.9986 - val_loss: 0.6252 - val_acc: 0.8977\n",
      "Epoch 2909/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0156 - acc: 0.9957 - val_loss: 0.6279 - val_acc: 0.8977\n",
      "Epoch 2910/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 0.5856 - val_acc: 0.8977\n",
      "Epoch 2911/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0110 - acc: 0.9957 - val_loss: 0.5293 - val_acc: 0.8920\n",
      "Epoch 2912/3000\n",
      "702/702 [==============================] - 0s 591us/sample - loss: 0.0257 - acc: 0.9900 - val_loss: 0.5775 - val_acc: 0.8750\n",
      "Epoch 2913/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0208 - acc: 0.9915 - val_loss: 0.6739 - val_acc: 0.8864\n",
      "Epoch 2914/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.0247 - acc: 0.9929 - val_loss: 0.5511 - val_acc: 0.8864\n",
      "Epoch 2915/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0585 - acc: 0.9801 - val_loss: 0.5593 - val_acc: 0.8920\n",
      "Epoch 2916/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0504 - acc: 0.9815 - val_loss: 0.6179 - val_acc: 0.8977\n",
      "Epoch 2917/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0146 - acc: 0.9957 - val_loss: 0.5432 - val_acc: 0.8750\n",
      "Epoch 2918/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0180 - acc: 0.9986 - val_loss: 0.4786 - val_acc: 0.8864\n",
      "Epoch 2919/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0165 - acc: 0.9957 - val_loss: 0.5508 - val_acc: 0.8920\n",
      "Epoch 2920/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0143 - acc: 0.9972 - val_loss: 0.6331 - val_acc: 0.9034\n",
      "Epoch 2921/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0253 - acc: 0.9929 - val_loss: 0.5098 - val_acc: 0.9034\n",
      "Epoch 2922/3000\n",
      "702/702 [==============================] - 0s 585us/sample - loss: 0.0278 - acc: 0.9886 - val_loss: 0.4922 - val_acc: 0.8864\n",
      "Epoch 2923/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0246 - acc: 0.9915 - val_loss: 0.5087 - val_acc: 0.8864\n",
      "Epoch 2924/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0218 - acc: 0.9915 - val_loss: 0.3624 - val_acc: 0.9091\n",
      "Epoch 2925/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0333 - acc: 0.9900 - val_loss: 0.4419 - val_acc: 0.9091\n",
      "Epoch 2926/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.0124 - acc: 0.9972 - val_loss: 0.4680 - val_acc: 0.8920\n",
      "Epoch 2927/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0077 - acc: 0.9986 - val_loss: 0.4416 - val_acc: 0.8977\n",
      "Epoch 2928/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0252 - acc: 0.9943 - val_loss: 0.4303 - val_acc: 0.8977\n",
      "Epoch 2929/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.0159 - acc: 0.9915 - val_loss: 0.4624 - val_acc: 0.8977\n",
      "Epoch 2930/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0087 - acc: 0.9972 - val_loss: 0.5260 - val_acc: 0.8977\n",
      "Epoch 2931/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0061 - acc: 0.9972 - val_loss: 0.5844 - val_acc: 0.8977\n",
      "Epoch 2932/3000\n",
      "702/702 [==============================] - 0s 586us/sample - loss: 0.0027 - acc: 1.0000 - val_loss: 0.5844 - val_acc: 0.8977\n",
      "Epoch 2933/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0037 - acc: 0.9986 - val_loss: 0.6075 - val_acc: 0.8977\n",
      "Epoch 2934/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0111 - acc: 0.9986 - val_loss: 0.5596 - val_acc: 0.8807\n",
      "Epoch 2935/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.0106 - acc: 0.9972 - val_loss: 0.5480 - val_acc: 0.8977\n",
      "Epoch 2936/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0183 - acc: 0.9900 - val_loss: 0.4629 - val_acc: 0.8920\n",
      "Epoch 2937/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0108 - acc: 0.9972 - val_loss: 0.4133 - val_acc: 0.8920\n",
      "Epoch 2938/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0064 - acc: 1.0000 - val_loss: 0.4080 - val_acc: 0.8977\n",
      "Epoch 2939/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0122 - acc: 0.9972 - val_loss: 0.4377 - val_acc: 0.9091\n",
      "Epoch 2940/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.0073 - acc: 0.9972 - val_loss: 0.4830 - val_acc: 0.9148\n",
      "Epoch 2941/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0196 - acc: 0.9972 - val_loss: 0.5436 - val_acc: 0.8977\n",
      "Epoch 2942/3000\n",
      "702/702 [==============================] - 0s 589us/sample - loss: 0.0056 - acc: 0.9986 - val_loss: 0.5005 - val_acc: 0.8977\n",
      "Epoch 2943/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0196 - acc: 0.9943 - val_loss: 0.4279 - val_acc: 0.8977\n",
      "Epoch 2944/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0032 - acc: 1.0000 - val_loss: 0.4480 - val_acc: 0.8920\n",
      "Epoch 2945/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0059 - acc: 0.9986 - val_loss: 0.4659 - val_acc: 0.8977\n",
      "Epoch 2946/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0088 - acc: 0.9957 - val_loss: 0.5417 - val_acc: 0.8977\n",
      "Epoch 2947/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.0055 - acc: 0.9986 - val_loss: 0.5541 - val_acc: 0.8920\n",
      "Epoch 2948/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0171 - acc: 0.9972 - val_loss: 0.6017 - val_acc: 0.8920\n",
      "Epoch 2949/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0079 - acc: 0.9972 - val_loss: 0.6649 - val_acc: 0.8864\n",
      "Epoch 2950/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0054 - acc: 0.9986 - val_loss: 0.5447 - val_acc: 0.8920\n",
      "Epoch 2951/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0061 - acc: 0.9986 - val_loss: 0.4800 - val_acc: 0.9148\n",
      "Epoch 2952/3000\n",
      "702/702 [==============================] - 0s 595us/sample - loss: 0.0076 - acc: 0.9957 - val_loss: 0.4660 - val_acc: 0.9034\n",
      "Epoch 2953/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0072 - acc: 0.9972 - val_loss: 0.4657 - val_acc: 0.9091\n",
      "Epoch 2954/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 0.0038 - acc: 0.9986 - val_loss: 0.4903 - val_acc: 0.9091\n",
      "Epoch 2955/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0023 - acc: 1.0000 - val_loss: 0.5240 - val_acc: 0.9091\n",
      "Epoch 2956/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0061 - acc: 0.9972 - val_loss: 0.5753 - val_acc: 0.8977\n",
      "Epoch 2957/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0056 - acc: 0.9972 - val_loss: 0.6281 - val_acc: 0.8920\n",
      "Epoch 2958/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0188 - acc: 0.9943 - val_loss: 0.5515 - val_acc: 0.8977\n",
      "Epoch 2959/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0228 - acc: 0.9915 - val_loss: 0.3331 - val_acc: 0.9091\n",
      "Epoch 2960/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0138 - acc: 0.9957 - val_loss: 0.3072 - val_acc: 0.8977\n",
      "Epoch 2961/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0186 - acc: 0.9972 - val_loss: 0.3835 - val_acc: 0.8920\n",
      "Epoch 2962/3000\n",
      "702/702 [==============================] - 0s 586us/sample - loss: 0.0216 - acc: 0.9943 - val_loss: 0.6499 - val_acc: 0.8750\n",
      "Epoch 2963/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0057 - acc: 0.9986 - val_loss: 0.7457 - val_acc: 0.8864\n",
      "Epoch 2964/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0076 - acc: 0.9972 - val_loss: 0.6480 - val_acc: 0.8693\n",
      "Epoch 2965/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0231 - acc: 0.9943 - val_loss: 0.4854 - val_acc: 0.8864\n",
      "Epoch 2966/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0121 - acc: 0.9957 - val_loss: 0.5125 - val_acc: 0.8920\n",
      "Epoch 2967/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.0132 - acc: 0.9972 - val_loss: 0.5411 - val_acc: 0.9091\n",
      "Epoch 2968/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0102 - acc: 0.9957 - val_loss: 0.5773 - val_acc: 0.9034\n",
      "Epoch 2969/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0040 - acc: 0.9986 - val_loss: 0.5839 - val_acc: 0.8920\n",
      "Epoch 2970/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.0079 - acc: 0.9972 - val_loss: 0.5704 - val_acc: 0.8807\n",
      "Epoch 2971/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 0.5551 - val_acc: 0.8864\n",
      "Epoch 2972/3000\n",
      "702/702 [==============================] - 0s 596us/sample - loss: 0.0070 - acc: 0.9986 - val_loss: 0.5721 - val_acc: 0.8750\n",
      "Epoch 2973/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.0074 - acc: 0.9986 - val_loss: 0.5559 - val_acc: 0.8807\n",
      "Epoch 2974/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0197 - acc: 0.9957 - val_loss: 0.5355 - val_acc: 0.8864\n",
      "Epoch 2975/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 9.8786e-04 - acc: 1.0000 - val_loss: 0.5477 - val_acc: 0.8977\n",
      "Epoch 2976/3000\n",
      "702/702 [==============================] - 0s 534us/sample - loss: 0.0060 - acc: 0.9972 - val_loss: 0.5622 - val_acc: 0.8920\n",
      "Epoch 2977/3000\n",
      "702/702 [==============================] - 0s 538us/sample - loss: 0.0135 - acc: 0.9972 - val_loss: 0.5531 - val_acc: 0.8977\n",
      "Epoch 2978/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.0098 - acc: 0.9943 - val_loss: 0.5007 - val_acc: 0.8977\n",
      "Epoch 2979/3000\n",
      "702/702 [==============================] - 0s 535us/sample - loss: 0.0141 - acc: 0.9972 - val_loss: 0.5010 - val_acc: 0.8977\n",
      "Epoch 2980/3000\n",
      "702/702 [==============================] - 0s 540us/sample - loss: 0.0046 - acc: 0.9986 - val_loss: 0.5476 - val_acc: 0.9034\n",
      "Epoch 2981/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0062 - acc: 0.9972 - val_loss: 0.4654 - val_acc: 0.9034\n",
      "Epoch 2982/3000\n",
      "702/702 [==============================] - 0s 608us/sample - loss: 0.0089 - acc: 0.9957 - val_loss: 0.4569 - val_acc: 0.8920\n",
      "Epoch 2983/3000\n",
      "702/702 [==============================] - 0s 542us/sample - loss: 0.0057 - acc: 0.9972 - val_loss: 0.4771 - val_acc: 0.8920\n",
      "Epoch 2984/3000\n",
      "702/702 [==============================] - 0s 532us/sample - loss: 0.0084 - acc: 0.9986 - val_loss: 0.5600 - val_acc: 0.8977\n",
      "Epoch 2985/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 0.0084 - acc: 0.9957 - val_loss: 0.6183 - val_acc: 0.9034\n",
      "Epoch 2986/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.0133 - acc: 0.9957 - val_loss: 0.5953 - val_acc: 0.9034\n",
      "Epoch 2987/3000\n",
      "702/702 [==============================] - 0s 544us/sample - loss: 0.0088 - acc: 0.9972 - val_loss: 0.4835 - val_acc: 0.8920\n",
      "Epoch 2988/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0041 - acc: 0.9986 - val_loss: 0.5408 - val_acc: 0.8920\n",
      "Epoch 2989/3000\n",
      "702/702 [==============================] - 0s 535us/sample - loss: 0.0090 - acc: 0.9972 - val_loss: 0.5854 - val_acc: 0.8920\n",
      "Epoch 2990/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.0058 - acc: 0.9986 - val_loss: 0.6557 - val_acc: 0.8920\n",
      "Epoch 2991/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0203 - acc: 0.9943 - val_loss: 0.6931 - val_acc: 0.9034\n",
      "Epoch 2992/3000\n",
      "702/702 [==============================] - 0s 605us/sample - loss: 0.0090 - acc: 0.9972 - val_loss: 0.6479 - val_acc: 0.8920\n",
      "Epoch 2993/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.0050 - acc: 0.9986 - val_loss: 0.6044 - val_acc: 0.8977\n",
      "Epoch 2994/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0018 - acc: 1.0000 - val_loss: 0.5851 - val_acc: 0.8977\n",
      "Epoch 2995/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.0018 - acc: 1.0000 - val_loss: 0.5776 - val_acc: 0.8977\n",
      "Epoch 2996/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0026 - acc: 1.0000 - val_loss: 0.5971 - val_acc: 0.8977\n",
      "Epoch 2997/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0040 - acc: 1.0000 - val_loss: 0.6902 - val_acc: 0.8920\n",
      "Epoch 2998/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.0046 - acc: 0.9986 - val_loss: 0.7804 - val_acc: 0.8977\n",
      "Epoch 2999/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.0027 - acc: 0.9986 - val_loss: 0.8517 - val_acc: 0.8920\n",
      "Epoch 3000/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0204 - acc: 0.9957 - val_loss: 0.7146 - val_acc: 0.9034\n",
      "training_time :  1529.7028176784515\n"
     ]
    }
   ],
   "source": [
    "model = Mynet_squeeze()\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "start_time = time.time()\n",
    "history = model.fit(trainX,trainY, \n",
    "                    epochs=3000, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    verbose=1, \n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[tf.keras.callbacks.TensorBoard(log_dir='../logs/'+IN_DIR_PATH+'/events', histogram_freq=10, write_graph=True,),\n",
    "#                                tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=700,verbose=0,mode='auto')\n",
    "                              ]\n",
    "                   )\n",
    "end_time = time.time()-start_time\n",
    "print('training_time : ',end_time)\n",
    "score = model.evaluate(valX, valY, batch_size=BATCH_SIZE, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "# loss\n",
    "def plot_history_loss(fit):\n",
    "    # Plot the loss in the history\n",
    "    axL.plot(fit.history['loss'],label=\"loss for training\")\n",
    "    axL.plot(fit.history['val_loss'],label=\"loss for validation\")\n",
    "    axL.set_title('model loss')\n",
    "    axL.set_xlabel('epoch')\n",
    "    axL.set_ylabel('loss')\n",
    "    axL.set_ylim([0,3])\n",
    "    axL.legend(loc='upper right')\n",
    "\n",
    "# acc\n",
    "def plot_history_acc(fit):\n",
    "    # Plot the loss in the history\n",
    "    axR.plot(fit.history['acc'],label=\"loss for training\")\n",
    "    axR.plot(fit.history['val_acc'],label=\"loss for validation\")\n",
    "    axR.set_title('model accuracy')\n",
    "    axR.set_xlabel('epoch')\n",
    "    axR.set_ylabel('accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "plot_history_loss(history)\n",
    "plot_history_acc(history)\n",
    "plt.show()\n",
    "fig.savefig('../logs/'+IN_DIR_PATH+'/loss_acc.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8589"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y = model.predict(valX)\n",
    "pred_y_c = np.argmax(pred_y,axis=1)\n",
    "# pred_y_one_hot = np.identity(len(class_list))[pred_y_c]\n",
    "true_y = np.argmax(valY,axis=1)\n",
    "confusion_mtx = confusion_matrix(true_y, pred_y_c)\n",
    "np.savetxt('../logs/'+IN_DIR_PATH+'/cm.csv', confusion_mtx)\n",
    "plot_confusion_matrix(confusion_mtx, target_names=class_list)\n",
    "plt.show()\n",
    "cr = classification_report(true_y,pred_y_c,target_names=class_list)\n",
    "f = open('../logs/'+IN_DIR_PATH+'/cr.txt','w')\n",
    "f.write(cr)\n",
    "f.close()\n",
    "f = open('../logs/'+IN_DIR_PATH+'/trainingtime.txt','w')\n",
    "f.write(str(end_time))\n",
    "f.close()\n",
    "\n",
    "keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.3.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
