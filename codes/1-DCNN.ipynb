{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory growth: True\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import Input,Model,optimizers\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Conv1D, UpSampling1D,Activation\n",
    "from tensorflow.keras.layers import MaxPooling1D, BatchNormalization, Flatten,GlobalMaxPooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, classification_report,plot_confusion_matrix\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print('memory growth:', tf.config.experimental.get_memory_growth(physical_devices[0]))\n",
    "else:\n",
    "    print(\"Not enough GPU hardware devices available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_RATE = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define\n",
    "EPOCH = 3000\n",
    "BATCH_SIZE = 256\n",
    "MINIBATCH = 64\n",
    "DROP_RATE = 0.5\n",
    "\n",
    "FC_SIZE = 50\n",
    "FILTER_SIZE = 64\n",
    "\n",
    "DATA_START = 250\n",
    "DATA_LEN = 4096\n",
    "\n",
    "IN_DIR_PATH = \"sa321_y\"\n",
    "\n",
    "os.makedirs('../logs/'+IN_DIR_PATH+'/events',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/976 [00:00<?, ?it/s]/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/ipykernel_launcher.py:22: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "100%|██████████| 976/976 [00:41<00:00, 23.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# input_dir and out_dir\n",
    "input_dir_path = \"../preprocessed/train_\"+IN_DIR_PATH+\"/*/*.csv\"\n",
    "input_dir = glob.glob(input_dir_path)\n",
    "input_num = len(input_dir)\n",
    "\n",
    "class_list = np.array([])\n",
    "all_data = np.array([])\n",
    "all_label = np.array([])\n",
    "\n",
    "init_flg = True\n",
    "\n",
    "for d in tqdm(input_dir):\n",
    "    # print(d)\n",
    "    if \".DS_Store\" in d:\n",
    "        os.remove(d)\n",
    "        input_dir.remove(d)\n",
    "        continue\n",
    "    \n",
    "    end_index = d.rfind('/')\n",
    "    start_index = d[:end_index].rfind('/')\n",
    "    class_name = d[start_index+1:end_index]\n",
    "    if class_name not in class_list:\n",
    "        class_list = np.append(class_list,class_name)\n",
    "        # print(class_name)\n",
    "    \n",
    "    label = np.where(class_list == class_name)\n",
    "    all_label = np.append(all_label,label)\n",
    "    \n",
    "    data = np.loadtxt(d, delimiter=\",\")\n",
    "    \n",
    "    # axis_time = np.vstack(data[DATA_START:DATA_START+DATA_LEN,0])\n",
    "    axis_value = np.hstack([data[DATA_START:DATA_START+DATA_LEN, 1]])\n",
    "    \n",
    "    # preprocessed = np.hstack([axis_time,axis_value])\n",
    "    if init_flg:\n",
    "        all_data = axis_value\n",
    "        init_flg = False\n",
    "    else:\n",
    "        all_data = np.vstack([all_data,axis_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_label = to_categorical(all_label,class_list.shape[0])\n",
    "\n",
    "p = np.random.permutation(input_num)\n",
    "shuffled_data = all_data[p]\n",
    "shuffled_label = one_hot_label[p]\n",
    "\n",
    "trainX = shuffled_data[:int(input_num*VAL_RATE)]\n",
    "valX = shuffled_data[int(input_num*VAL_RATE):]\n",
    "trainY = shuffled_label[:int(input_num*VAL_RATE)]\n",
    "valY = shuffled_label[int(input_num*VAL_RATE):]\n",
    "\n",
    "trainX = np.reshape(trainX,(int(input_num*VAL_RATE),DATA_LEN,1)).astype(np.float32)\n",
    "valX = np.reshape(valX,(input_num-int(input_num*VAL_RATE),DATA_LEN,1)).astype(np.float32)\n",
    "# trainY = np.reshape(trainY,(int(input_num*VAL_RATE),class_list.shape[0],1))\n",
    "# valY = np.reshape(valY,(input_num-int(input_num*VAL_RATE),class_list.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878, 4096, 1)\n",
      "(878, 9)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mynet():\n",
    "    inputs = Input(shape=(DATA_LEN,1))\n",
    "    # Due to memory limitation, images will resized on-the-fly.\n",
    "    x = Conv1D(FILTER_SIZE, 5, padding='same', input_shape=(DATA_LEN,1), activation=None)(inputs)\n",
    "    x = Conv1D(FILTER_SIZE, 5, padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Dropout(DROP_RATE)(x)\n",
    "\n",
    "    x = Conv1D(FILTER_SIZE*2, 5, padding='same')(x)\n",
    "    x = Conv1D(FILTER_SIZE*2, 5, padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Dropout(DROP_RATE)(x)\n",
    "    \n",
    "    x = Conv1D(FILTER_SIZE*4, 5, padding='same')(x)\n",
    "    x = Conv1D(FILTER_SIZE*4, 5, padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Dropout(DROP_RATE)(x)\n",
    "    \n",
    "    fc = Flatten()(x)\n",
    "    fc = Dense(FC_SIZE*2, activation='relu')(fc)\n",
    "    fc = BatchNormalization()(fc)\n",
    "    fc = Dropout(DROP_RATE)(fc)\n",
    "    \n",
    "    fc = Dense(FC_SIZE, activation='relu')(fc)\n",
    "    fc = BatchNormalization()(fc)\n",
    "    fc = Dropout(DROP_RATE)(fc)\n",
    "    \n",
    "    fc = Dense(class_list.shape[0])(fc)\n",
    "    softmax = Activation('softmax')(fc)\n",
    "    model = Model(inputs=inputs, outputs=softmax)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mynet_squeeze():\n",
    "    inputs = Input(shape=(DATA_LEN,1))\n",
    "    # Due to memory limitation, images will resized on-the-fly.\n",
    "    x = Conv1D(FILTER_SIZE, 5, padding='same', input_shape=(DATA_LEN,1), activation='relu')(inputs)\n",
    "    x = Conv1D(FILTER_SIZE, 5, padding='same',activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Conv1D(int(FILTER_SIZE*2), 5, padding='same',activation='relu')(x)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Dropout(DROP_RATE)(x)\n",
    "    x = Conv1D(int(FILTER_SIZE), 5, padding='same', activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "    \n",
    "    fc = GlobalMaxPooling1D()(x)\n",
    "    fc = Dropout(DROP_RATE)(fc)\n",
    "    fc = Flatten()(fc)\n",
    "    fc = Dense(class_list.shape[0])(fc)\n",
    "    softmax = Activation('softmax')(fc)\n",
    "    model = Model(inputs=inputs, outputs=softmax)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig('../logs/'+IN_DIR_PATH+'/cm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4096, 1)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 4096, 64)          384       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 4096, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 2048, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 2048, 128)         41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1024, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1024, 64)          41024     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 9)                 585       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 103,625\n",
      "Trainable params: 103,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 702 samples, validate on 176 samples\n",
      "Epoch 1/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 134.8902 - acc: 0.1197 - val_loss: 4.0683 - val_acc: 0.1761\n",
      "Epoch 2/3000\n",
      "702/702 [==============================] - 0s 594us/sample - loss: 9.2504 - acc: 0.1040 - val_loss: 2.1495 - val_acc: 0.1989\n",
      "Epoch 3/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 2.6335 - acc: 0.1567 - val_loss: 1.9641 - val_acc: 0.2443\n",
      "Epoch 4/3000\n",
      "702/702 [==============================] - 0s 530us/sample - loss: 2.1366 - acc: 0.1966 - val_loss: 1.8458 - val_acc: 0.4148\n",
      "Epoch 5/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 1.9583 - acc: 0.2863 - val_loss: 1.7890 - val_acc: 0.4375\n",
      "Epoch 6/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 1.8740 - acc: 0.3048 - val_loss: 1.7452 - val_acc: 0.4489\n",
      "Epoch 7/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 1.7855 - acc: 0.3960 - val_loss: 1.5853 - val_acc: 0.5625\n",
      "Epoch 8/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 1.7774 - acc: 0.3775 - val_loss: 1.6332 - val_acc: 0.5625\n",
      "Epoch 9/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 1.6910 - acc: 0.4544 - val_loss: 1.5255 - val_acc: 0.6080\n",
      "Epoch 10/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 1.6543 - acc: 0.4373 - val_loss: 1.4802 - val_acc: 0.5625\n",
      "Epoch 11/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.6159 - acc: 0.4630 - val_loss: 1.4339 - val_acc: 0.6420\n",
      "Epoch 12/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 1.5555 - acc: 0.4943 - val_loss: 1.4221 - val_acc: 0.6534\n",
      "Epoch 13/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 1.5312 - acc: 0.5128 - val_loss: 1.3374 - val_acc: 0.6761\n",
      "Epoch 14/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 1.5265 - acc: 0.4886 - val_loss: 1.3743 - val_acc: 0.6420\n",
      "Epoch 15/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 1.4815 - acc: 0.5285 - val_loss: 1.3209 - val_acc: 0.6875\n",
      "Epoch 16/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 1.4508 - acc: 0.5527 - val_loss: 1.3308 - val_acc: 0.6307\n",
      "Epoch 17/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.4097 - acc: 0.5228 - val_loss: 1.2503 - val_acc: 0.6989\n",
      "Epoch 18/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.3291 - acc: 0.5840 - val_loss: 1.2643 - val_acc: 0.6534\n",
      "Epoch 19/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 1.3144 - acc: 0.5840 - val_loss: 1.1909 - val_acc: 0.7159\n",
      "Epoch 20/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 1.2504 - acc: 0.5954 - val_loss: 1.1956 - val_acc: 0.6193\n",
      "Epoch 21/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.2422 - acc: 0.6040 - val_loss: 1.1436 - val_acc: 0.7216\n",
      "Epoch 22/3000\n",
      "702/702 [==============================] - 0s 560us/sample - loss: 1.2071 - acc: 0.6282 - val_loss: 1.1413 - val_acc: 0.6932\n",
      "Epoch 23/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 1.1843 - acc: 0.6353 - val_loss: 1.0862 - val_acc: 0.7159\n",
      "Epoch 24/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 1.1474 - acc: 0.6368 - val_loss: 1.0755 - val_acc: 0.7102\n",
      "Epoch 25/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 1.1132 - acc: 0.6325 - val_loss: 1.0298 - val_acc: 0.6989\n",
      "Epoch 26/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 1.1198 - acc: 0.6254 - val_loss: 1.0358 - val_acc: 0.7102\n",
      "Epoch 27/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.0554 - acc: 0.6510 - val_loss: 1.0357 - val_acc: 0.6875\n",
      "Epoch 28/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 1.0530 - acc: 0.6453 - val_loss: 1.0132 - val_acc: 0.7102\n",
      "Epoch 29/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.0686 - acc: 0.6496 - val_loss: 0.9914 - val_acc: 0.6932\n",
      "Epoch 30/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 1.0635 - acc: 0.6538 - val_loss: 1.0052 - val_acc: 0.6989\n",
      "Epoch 31/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.0349 - acc: 0.6282 - val_loss: 0.9961 - val_acc: 0.7159\n",
      "Epoch 32/3000\n",
      "702/702 [==============================] - 0s 553us/sample - loss: 1.0299 - acc: 0.6524 - val_loss: 0.9667 - val_acc: 0.6989\n",
      "Epoch 33/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 1.0509 - acc: 0.6311 - val_loss: 0.9672 - val_acc: 0.7159\n",
      "Epoch 34/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.0830 - acc: 0.6254 - val_loss: 0.9850 - val_acc: 0.7159\n",
      "Epoch 35/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.0283 - acc: 0.6553 - val_loss: 0.9965 - val_acc: 0.7102\n",
      "Epoch 36/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 1.0356 - acc: 0.6339 - val_loss: 0.9709 - val_acc: 0.7216\n",
      "Epoch 37/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 1.0059 - acc: 0.6652 - val_loss: 0.9648 - val_acc: 0.7045\n",
      "Epoch 38/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.9564 - acc: 0.6681 - val_loss: 0.9400 - val_acc: 0.7330\n",
      "Epoch 39/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.9729 - acc: 0.6695 - val_loss: 0.9708 - val_acc: 0.7102\n",
      "Epoch 40/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.9622 - acc: 0.6610 - val_loss: 0.8903 - val_acc: 0.7330\n",
      "Epoch 41/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.9989 - acc: 0.6567 - val_loss: 0.9246 - val_acc: 0.7216\n",
      "Epoch 42/3000\n",
      "702/702 [==============================] - 0s 561us/sample - loss: 0.9000 - acc: 0.6724 - val_loss: 0.8908 - val_acc: 0.7273\n",
      "Epoch 43/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.9631 - acc: 0.6624 - val_loss: 0.9155 - val_acc: 0.7557\n",
      "Epoch 44/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.8970 - acc: 0.6809 - val_loss: 0.8803 - val_acc: 0.7330\n",
      "Epoch 45/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.9315 - acc: 0.6652 - val_loss: 0.8971 - val_acc: 0.7273\n",
      "Epoch 46/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.9279 - acc: 0.6624 - val_loss: 0.8995 - val_acc: 0.7500\n",
      "Epoch 47/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.9521 - acc: 0.6538 - val_loss: 0.8755 - val_acc: 0.7386\n",
      "Epoch 48/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.8821 - acc: 0.6838 - val_loss: 0.8680 - val_acc: 0.7557\n",
      "Epoch 49/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.8631 - acc: 0.6937 - val_loss: 0.8497 - val_acc: 0.7614\n",
      "Epoch 50/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.8582 - acc: 0.7151 - val_loss: 0.8205 - val_acc: 0.7614\n",
      "Epoch 51/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.8781 - acc: 0.6766 - val_loss: 0.8112 - val_acc: 0.7557\n",
      "Epoch 52/3000\n",
      "702/702 [==============================] - 0s 560us/sample - loss: 0.9065 - acc: 0.6909 - val_loss: 0.8363 - val_acc: 0.7670\n",
      "Epoch 53/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.8568 - acc: 0.7037 - val_loss: 0.7994 - val_acc: 0.7670\n",
      "Epoch 54/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.8723 - acc: 0.6952 - val_loss: 0.8360 - val_acc: 0.7330\n",
      "Epoch 55/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.9164 - acc: 0.6524 - val_loss: 0.8334 - val_acc: 0.7614\n",
      "Epoch 56/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.8357 - acc: 0.6781 - val_loss: 0.8354 - val_acc: 0.7614\n",
      "Epoch 57/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.8773 - acc: 0.6838 - val_loss: 0.7701 - val_acc: 0.7784\n",
      "Epoch 58/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.8363 - acc: 0.6895 - val_loss: 0.7945 - val_acc: 0.7727\n",
      "Epoch 59/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.8447 - acc: 0.7080 - val_loss: 0.8080 - val_acc: 0.7670\n",
      "Epoch 60/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.8214 - acc: 0.6952 - val_loss: 0.7586 - val_acc: 0.7784\n",
      "Epoch 61/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.8109 - acc: 0.6966 - val_loss: 0.7750 - val_acc: 0.7614\n",
      "Epoch 62/3000\n",
      "702/702 [==============================] - 0s 550us/sample - loss: 0.7777 - acc: 0.7236 - val_loss: 0.7836 - val_acc: 0.7727\n",
      "Epoch 63/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.7931 - acc: 0.7308 - val_loss: 0.7548 - val_acc: 0.7727\n",
      "Epoch 64/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.7955 - acc: 0.7009 - val_loss: 0.7598 - val_acc: 0.7670\n",
      "Epoch 65/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.7916 - acc: 0.7293 - val_loss: 0.7789 - val_acc: 0.7784\n",
      "Epoch 66/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.7965 - acc: 0.6952 - val_loss: 0.7675 - val_acc: 0.7784\n",
      "Epoch 67/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.7379 - acc: 0.7393 - val_loss: 0.7917 - val_acc: 0.7614\n",
      "Epoch 68/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.7868 - acc: 0.7094 - val_loss: 0.7639 - val_acc: 0.7727\n",
      "Epoch 69/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.7895 - acc: 0.7151 - val_loss: 0.7763 - val_acc: 0.7500\n",
      "Epoch 70/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.7793 - acc: 0.7009 - val_loss: 0.7372 - val_acc: 0.7841\n",
      "Epoch 71/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.7421 - acc: 0.7379 - val_loss: 0.7476 - val_acc: 0.7614\n",
      "Epoch 72/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.7290 - acc: 0.7365 - val_loss: 0.7332 - val_acc: 0.7670\n",
      "Epoch 73/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.7777 - acc: 0.7023 - val_loss: 0.7311 - val_acc: 0.7727\n",
      "Epoch 74/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.7600 - acc: 0.7094 - val_loss: 0.7571 - val_acc: 0.7670\n",
      "Epoch 75/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.7540 - acc: 0.7436 - val_loss: 0.7330 - val_acc: 0.7670\n",
      "Epoch 76/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.7613 - acc: 0.7265 - val_loss: 0.7521 - val_acc: 0.7670\n",
      "Epoch 77/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.8009 - acc: 0.6994 - val_loss: 0.7743 - val_acc: 0.7443\n",
      "Epoch 78/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.7515 - acc: 0.7293 - val_loss: 0.7429 - val_acc: 0.7670\n",
      "Epoch 79/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.7417 - acc: 0.7293 - val_loss: 0.7216 - val_acc: 0.7670\n",
      "Epoch 80/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.7417 - acc: 0.6880 - val_loss: 0.7257 - val_acc: 0.7784\n",
      "Epoch 81/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.7647 - acc: 0.7137 - val_loss: 0.7177 - val_acc: 0.7727\n",
      "Epoch 82/3000\n",
      "702/702 [==============================] - 0s 561us/sample - loss: 0.7658 - acc: 0.7251 - val_loss: 0.7240 - val_acc: 0.7841\n",
      "Epoch 83/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.7215 - acc: 0.7450 - val_loss: 0.7075 - val_acc: 0.7784\n",
      "Epoch 84/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.7508 - acc: 0.7222 - val_loss: 0.7181 - val_acc: 0.7784\n",
      "Epoch 85/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.7676 - acc: 0.7165 - val_loss: 0.7409 - val_acc: 0.7784\n",
      "Epoch 86/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.7339 - acc: 0.7436 - val_loss: 0.7114 - val_acc: 0.7670\n",
      "Epoch 87/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.6748 - acc: 0.7650 - val_loss: 0.6934 - val_acc: 0.7898\n",
      "Epoch 88/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.7340 - acc: 0.7365 - val_loss: 0.7003 - val_acc: 0.7727\n",
      "Epoch 89/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.6948 - acc: 0.7521 - val_loss: 0.7172 - val_acc: 0.7784\n",
      "Epoch 90/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.6971 - acc: 0.7336 - val_loss: 0.7007 - val_acc: 0.7727\n",
      "Epoch 91/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.7483 - acc: 0.7365 - val_loss: 0.6952 - val_acc: 0.7898\n",
      "Epoch 92/3000\n",
      "702/702 [==============================] - 0s 559us/sample - loss: 0.7326 - acc: 0.7336 - val_loss: 0.7485 - val_acc: 0.7386\n",
      "Epoch 93/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.7613 - acc: 0.7293 - val_loss: 0.7377 - val_acc: 0.7614\n",
      "Epoch 94/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.7615 - acc: 0.7379 - val_loss: 0.7299 - val_acc: 0.7727\n",
      "Epoch 95/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.7270 - acc: 0.7251 - val_loss: 0.6910 - val_acc: 0.7955\n",
      "Epoch 96/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.7063 - acc: 0.7479 - val_loss: 0.7194 - val_acc: 0.7614\n",
      "Epoch 97/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.7384 - acc: 0.7322 - val_loss: 0.6856 - val_acc: 0.7784\n",
      "Epoch 98/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.6990 - acc: 0.7393 - val_loss: 0.6663 - val_acc: 0.7898\n",
      "Epoch 99/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.6519 - acc: 0.7450 - val_loss: 0.6742 - val_acc: 0.7898\n",
      "Epoch 100/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.6865 - acc: 0.7422 - val_loss: 0.6994 - val_acc: 0.7557\n",
      "Epoch 101/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.6927 - acc: 0.7336 - val_loss: 0.6949 - val_acc: 0.7784\n",
      "Epoch 102/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.6863 - acc: 0.7493 - val_loss: 0.7073 - val_acc: 0.7898\n",
      "Epoch 103/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.6667 - acc: 0.7721 - val_loss: 0.6558 - val_acc: 0.7841\n",
      "Epoch 104/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.6349 - acc: 0.7692 - val_loss: 0.6392 - val_acc: 0.7841\n",
      "Epoch 105/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.6782 - acc: 0.7536 - val_loss: 0.6476 - val_acc: 0.7727\n",
      "Epoch 106/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.6899 - acc: 0.7422 - val_loss: 0.6743 - val_acc: 0.7727\n",
      "Epoch 107/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.6829 - acc: 0.7279 - val_loss: 0.6331 - val_acc: 0.8068\n",
      "Epoch 108/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.6447 - acc: 0.7464 - val_loss: 0.6363 - val_acc: 0.7727\n",
      "Epoch 109/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.6405 - acc: 0.7550 - val_loss: 0.6608 - val_acc: 0.7500\n",
      "Epoch 110/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.6637 - acc: 0.7450 - val_loss: 0.6489 - val_acc: 0.7614\n",
      "Epoch 111/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.6215 - acc: 0.7678 - val_loss: 0.6218 - val_acc: 0.7898\n",
      "Epoch 112/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 0.6427 - acc: 0.7593 - val_loss: 0.6280 - val_acc: 0.7727\n",
      "Epoch 113/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.6574 - acc: 0.7493 - val_loss: 0.6401 - val_acc: 0.7670\n",
      "Epoch 114/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.6319 - acc: 0.7764 - val_loss: 0.6520 - val_acc: 0.7727\n",
      "Epoch 115/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.6149 - acc: 0.7493 - val_loss: 0.6556 - val_acc: 0.7727\n",
      "Epoch 116/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.6406 - acc: 0.7678 - val_loss: 0.6400 - val_acc: 0.7784\n",
      "Epoch 117/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.6300 - acc: 0.7593 - val_loss: 0.6458 - val_acc: 0.7670\n",
      "Epoch 118/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.6644 - acc: 0.7393 - val_loss: 0.6630 - val_acc: 0.7670\n",
      "Epoch 119/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.6022 - acc: 0.7707 - val_loss: 0.6434 - val_acc: 0.7955\n",
      "Epoch 120/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.6188 - acc: 0.7692 - val_loss: 0.6335 - val_acc: 0.7670\n",
      "Epoch 121/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5927 - acc: 0.7721 - val_loss: 0.6402 - val_acc: 0.7727\n",
      "Epoch 122/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 0.6225 - acc: 0.7792 - val_loss: 0.6354 - val_acc: 0.7670\n",
      "Epoch 123/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.6139 - acc: 0.7735 - val_loss: 0.6132 - val_acc: 0.8011\n",
      "Epoch 124/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.6313 - acc: 0.7806 - val_loss: 0.6448 - val_acc: 0.7557\n",
      "Epoch 125/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.6634 - acc: 0.7564 - val_loss: 0.6733 - val_acc: 0.7614\n",
      "Epoch 126/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.6642 - acc: 0.7450 - val_loss: 0.6826 - val_acc: 0.7614\n",
      "Epoch 127/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.6448 - acc: 0.7479 - val_loss: 0.6539 - val_acc: 0.7784\n",
      "Epoch 128/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.6162 - acc: 0.7593 - val_loss: 0.6786 - val_acc: 0.7500\n",
      "Epoch 129/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.6843 - acc: 0.7479 - val_loss: 0.6304 - val_acc: 0.7727\n",
      "Epoch 130/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.6209 - acc: 0.7650 - val_loss: 0.6177 - val_acc: 0.7784\n",
      "Epoch 131/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.6141 - acc: 0.7607 - val_loss: 0.6674 - val_acc: 0.7670\n",
      "Epoch 132/3000\n",
      "702/702 [==============================] - 0s 555us/sample - loss: 0.6207 - acc: 0.7550 - val_loss: 0.6668 - val_acc: 0.7898\n",
      "Epoch 133/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.6450 - acc: 0.7536 - val_loss: 0.6487 - val_acc: 0.7955\n",
      "Epoch 134/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.6169 - acc: 0.7607 - val_loss: 0.6621 - val_acc: 0.7841\n",
      "Epoch 135/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.5899 - acc: 0.7692 - val_loss: 0.6389 - val_acc: 0.8011\n",
      "Epoch 136/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.6066 - acc: 0.7692 - val_loss: 0.6497 - val_acc: 0.7670\n",
      "Epoch 137/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.6273 - acc: 0.7521 - val_loss: 0.6384 - val_acc: 0.8011\n",
      "Epoch 138/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.6764 - acc: 0.7578 - val_loss: 0.6912 - val_acc: 0.7557\n",
      "Epoch 139/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.6155 - acc: 0.7550 - val_loss: 0.6609 - val_acc: 0.7784\n",
      "Epoch 140/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.5979 - acc: 0.7621 - val_loss: 0.6373 - val_acc: 0.7784\n",
      "Epoch 141/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.6562 - acc: 0.7650 - val_loss: 0.6359 - val_acc: 0.7898\n",
      "Epoch 142/3000\n",
      "702/702 [==============================] - 0s 554us/sample - loss: 0.6745 - acc: 0.7507 - val_loss: 0.6593 - val_acc: 0.7841\n",
      "Epoch 143/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.6165 - acc: 0.7507 - val_loss: 0.6421 - val_acc: 0.7727\n",
      "Epoch 144/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.5506 - acc: 0.7806 - val_loss: 0.6282 - val_acc: 0.7784\n",
      "Epoch 145/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.6046 - acc: 0.7521 - val_loss: 0.6491 - val_acc: 0.7727\n",
      "Epoch 146/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.6203 - acc: 0.7621 - val_loss: 0.6303 - val_acc: 0.8068\n",
      "Epoch 147/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.5895 - acc: 0.7664 - val_loss: 0.6404 - val_acc: 0.7670\n",
      "Epoch 148/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.5850 - acc: 0.7678 - val_loss: 0.6369 - val_acc: 0.7784\n",
      "Epoch 149/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.6452 - acc: 0.7493 - val_loss: 0.6406 - val_acc: 0.7727\n",
      "Epoch 150/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.5532 - acc: 0.7920 - val_loss: 0.6183 - val_acc: 0.8011\n",
      "Epoch 151/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.6419 - acc: 0.7593 - val_loss: 0.6688 - val_acc: 0.7500\n",
      "Epoch 152/3000\n",
      "702/702 [==============================] - 0s 560us/sample - loss: 0.6123 - acc: 0.7550 - val_loss: 0.6590 - val_acc: 0.7955\n",
      "Epoch 153/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.6442 - acc: 0.7621 - val_loss: 0.6733 - val_acc: 0.7557\n",
      "Epoch 154/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.5854 - acc: 0.7607 - val_loss: 0.6198 - val_acc: 0.8011\n",
      "Epoch 155/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.5736 - acc: 0.7678 - val_loss: 0.6723 - val_acc: 0.7727\n",
      "Epoch 156/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.6089 - acc: 0.7764 - val_loss: 0.6483 - val_acc: 0.7841\n",
      "Epoch 157/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.5564 - acc: 0.7821 - val_loss: 0.6202 - val_acc: 0.7955\n",
      "Epoch 158/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.5954 - acc: 0.7692 - val_loss: 0.6463 - val_acc: 0.7784\n",
      "Epoch 159/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.5616 - acc: 0.7749 - val_loss: 0.6423 - val_acc: 0.7670\n",
      "Epoch 160/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.5601 - acc: 0.7707 - val_loss: 0.6063 - val_acc: 0.7955\n",
      "Epoch 161/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5836 - acc: 0.7721 - val_loss: 0.6366 - val_acc: 0.7727\n",
      "Epoch 162/3000\n",
      "702/702 [==============================] - 0s 564us/sample - loss: 0.5616 - acc: 0.7821 - val_loss: 0.6368 - val_acc: 0.7727\n",
      "Epoch 163/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.5424 - acc: 0.7635 - val_loss: 0.6506 - val_acc: 0.7500\n",
      "Epoch 164/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.5693 - acc: 0.7792 - val_loss: 0.6396 - val_acc: 0.8011\n",
      "Epoch 165/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.6026 - acc: 0.7607 - val_loss: 0.6700 - val_acc: 0.7500\n",
      "Epoch 166/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.6078 - acc: 0.7564 - val_loss: 0.6038 - val_acc: 0.8068\n",
      "Epoch 167/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.6015 - acc: 0.7764 - val_loss: 0.6474 - val_acc: 0.7557\n",
      "Epoch 168/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.6293 - acc: 0.7479 - val_loss: 0.5926 - val_acc: 0.8011\n",
      "Epoch 169/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.5427 - acc: 0.7920 - val_loss: 0.6090 - val_acc: 0.7898\n",
      "Epoch 170/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.5853 - acc: 0.7835 - val_loss: 0.6142 - val_acc: 0.7841\n",
      "Epoch 171/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5493 - acc: 0.7835 - val_loss: 0.6048 - val_acc: 0.7841\n",
      "Epoch 172/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.5613 - acc: 0.7692 - val_loss: 0.5873 - val_acc: 0.7955\n",
      "Epoch 173/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.5262 - acc: 0.7963 - val_loss: 0.6119 - val_acc: 0.7670\n",
      "Epoch 174/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.5886 - acc: 0.7749 - val_loss: 0.6040 - val_acc: 0.7614\n",
      "Epoch 175/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.5637 - acc: 0.7835 - val_loss: 0.6057 - val_acc: 0.7898\n",
      "Epoch 176/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.6159 - acc: 0.7664 - val_loss: 0.6557 - val_acc: 0.7614\n",
      "Epoch 177/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.5708 - acc: 0.7778 - val_loss: 0.6042 - val_acc: 0.7841\n",
      "Epoch 178/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.5774 - acc: 0.7621 - val_loss: 0.5896 - val_acc: 0.7841\n",
      "Epoch 179/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.5803 - acc: 0.7735 - val_loss: 0.6080 - val_acc: 0.7898\n",
      "Epoch 180/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.5498 - acc: 0.7835 - val_loss: 0.6008 - val_acc: 0.8182\n",
      "Epoch 181/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5592 - acc: 0.7692 - val_loss: 0.6454 - val_acc: 0.7670\n",
      "Epoch 182/3000\n",
      "702/702 [==============================] - 0s 559us/sample - loss: 0.5993 - acc: 0.7707 - val_loss: 0.6015 - val_acc: 0.8068\n",
      "Epoch 183/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.5592 - acc: 0.7778 - val_loss: 0.6026 - val_acc: 0.7955\n",
      "Epoch 184/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.5494 - acc: 0.7863 - val_loss: 0.5854 - val_acc: 0.8011\n",
      "Epoch 185/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.5652 - acc: 0.7821 - val_loss: 0.5934 - val_acc: 0.8125\n",
      "Epoch 186/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.5619 - acc: 0.7934 - val_loss: 0.6129 - val_acc: 0.7898\n",
      "Epoch 187/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.5486 - acc: 0.7849 - val_loss: 0.5954 - val_acc: 0.8011\n",
      "Epoch 188/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.5553 - acc: 0.7764 - val_loss: 0.5992 - val_acc: 0.7784\n",
      "Epoch 189/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.5782 - acc: 0.7764 - val_loss: 0.6056 - val_acc: 0.8011\n",
      "Epoch 190/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.5549 - acc: 0.7792 - val_loss: 0.5852 - val_acc: 0.8182\n",
      "Epoch 191/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5420 - acc: 0.7835 - val_loss: 0.6551 - val_acc: 0.7898\n",
      "Epoch 192/3000\n",
      "702/702 [==============================] - 0s 561us/sample - loss: 0.5597 - acc: 0.7792 - val_loss: 0.6627 - val_acc: 0.7841\n",
      "Epoch 193/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.6301 - acc: 0.7550 - val_loss: 0.6829 - val_acc: 0.7557\n",
      "Epoch 194/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.5859 - acc: 0.7635 - val_loss: 0.5971 - val_acc: 0.8182\n",
      "Epoch 195/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.5366 - acc: 0.7892 - val_loss: 0.5878 - val_acc: 0.7955\n",
      "Epoch 196/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.5486 - acc: 0.7906 - val_loss: 0.5953 - val_acc: 0.7898\n",
      "Epoch 197/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.5340 - acc: 0.7963 - val_loss: 0.6017 - val_acc: 0.7898\n",
      "Epoch 198/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.5468 - acc: 0.7863 - val_loss: 0.5956 - val_acc: 0.7898\n",
      "Epoch 199/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.5354 - acc: 0.7920 - val_loss: 0.5928 - val_acc: 0.7784\n",
      "Epoch 200/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.5628 - acc: 0.7892 - val_loss: 0.6313 - val_acc: 0.7614\n",
      "Epoch 201/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5653 - acc: 0.7650 - val_loss: 0.6180 - val_acc: 0.7898\n",
      "Epoch 202/3000\n",
      "702/702 [==============================] - 0s 561us/sample - loss: 0.5175 - acc: 0.7877 - val_loss: 0.6187 - val_acc: 0.7670\n",
      "Epoch 203/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.5341 - acc: 0.7877 - val_loss: 0.6069 - val_acc: 0.7784\n",
      "Epoch 204/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.5633 - acc: 0.7764 - val_loss: 0.5783 - val_acc: 0.7841\n",
      "Epoch 205/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.5285 - acc: 0.7849 - val_loss: 0.5756 - val_acc: 0.7841\n",
      "Epoch 206/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.5444 - acc: 0.7821 - val_loss: 0.6095 - val_acc: 0.7670\n",
      "Epoch 207/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.5432 - acc: 0.7906 - val_loss: 0.6243 - val_acc: 0.7898\n",
      "Epoch 208/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.5599 - acc: 0.7906 - val_loss: 0.5842 - val_acc: 0.7841\n",
      "Epoch 209/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.5421 - acc: 0.7892 - val_loss: 0.5821 - val_acc: 0.7784\n",
      "Epoch 210/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.5793 - acc: 0.7692 - val_loss: 0.6153 - val_acc: 0.7784\n",
      "Epoch 211/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5356 - acc: 0.7778 - val_loss: 0.6195 - val_acc: 0.8068\n",
      "Epoch 212/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.5416 - acc: 0.7778 - val_loss: 0.6126 - val_acc: 0.7784\n",
      "Epoch 213/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.5386 - acc: 0.7977 - val_loss: 0.5839 - val_acc: 0.8011\n",
      "Epoch 214/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.5111 - acc: 0.7977 - val_loss: 0.5907 - val_acc: 0.7898\n",
      "Epoch 215/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.4951 - acc: 0.7991 - val_loss: 0.6036 - val_acc: 0.8068\n",
      "Epoch 216/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.4927 - acc: 0.7963 - val_loss: 0.5776 - val_acc: 0.8239\n",
      "Epoch 217/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.5319 - acc: 0.7821 - val_loss: 0.6230 - val_acc: 0.7784\n",
      "Epoch 218/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.5242 - acc: 0.7892 - val_loss: 0.6057 - val_acc: 0.7841\n",
      "Epoch 219/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.5379 - acc: 0.8063 - val_loss: 0.5998 - val_acc: 0.7727\n",
      "Epoch 220/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.5463 - acc: 0.7849 - val_loss: 0.5744 - val_acc: 0.7841\n",
      "Epoch 221/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5063 - acc: 0.7949 - val_loss: 0.6176 - val_acc: 0.7500\n",
      "Epoch 222/3000\n",
      "702/702 [==============================] - 0s 558us/sample - loss: 0.5002 - acc: 0.7934 - val_loss: 0.6041 - val_acc: 0.7841\n",
      "Epoch 223/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.5176 - acc: 0.7934 - val_loss: 0.6179 - val_acc: 0.7557\n",
      "Epoch 224/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.5334 - acc: 0.7863 - val_loss: 0.5706 - val_acc: 0.8011\n",
      "Epoch 225/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.5377 - acc: 0.7749 - val_loss: 0.6516 - val_acc: 0.7443\n",
      "Epoch 226/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.5441 - acc: 0.7692 - val_loss: 0.5790 - val_acc: 0.8011\n",
      "Epoch 227/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.5718 - acc: 0.7664 - val_loss: 0.6806 - val_acc: 0.7273\n",
      "Epoch 228/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.6216 - acc: 0.7407 - val_loss: 0.5815 - val_acc: 0.8068\n",
      "Epoch 229/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.5113 - acc: 0.8006 - val_loss: 0.6109 - val_acc: 0.8068\n",
      "Epoch 230/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.5284 - acc: 0.7949 - val_loss: 0.5771 - val_acc: 0.7955\n",
      "Epoch 231/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5389 - acc: 0.7849 - val_loss: 0.5905 - val_acc: 0.7670\n",
      "Epoch 232/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 0.5513 - acc: 0.7849 - val_loss: 0.5638 - val_acc: 0.8125\n",
      "Epoch 233/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.5340 - acc: 0.7977 - val_loss: 0.6009 - val_acc: 0.7955\n",
      "Epoch 234/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.4974 - acc: 0.7892 - val_loss: 0.6067 - val_acc: 0.8011\n",
      "Epoch 235/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.5297 - acc: 0.7934 - val_loss: 0.6052 - val_acc: 0.7670\n",
      "Epoch 236/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.5205 - acc: 0.7892 - val_loss: 0.5931 - val_acc: 0.7670\n",
      "Epoch 237/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.5576 - acc: 0.7863 - val_loss: 0.6243 - val_acc: 0.7500\n",
      "Epoch 238/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.5078 - acc: 0.8034 - val_loss: 0.5617 - val_acc: 0.7955\n",
      "Epoch 239/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.4946 - acc: 0.7877 - val_loss: 0.5666 - val_acc: 0.7841\n",
      "Epoch 240/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.4768 - acc: 0.8162 - val_loss: 0.5770 - val_acc: 0.7727\n",
      "Epoch 241/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5144 - acc: 0.7877 - val_loss: 0.5495 - val_acc: 0.8068\n",
      "Epoch 242/3000\n",
      "702/702 [==============================] - 0s 567us/sample - loss: 0.5610 - acc: 0.7735 - val_loss: 0.6485 - val_acc: 0.7500\n",
      "Epoch 243/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.5586 - acc: 0.7849 - val_loss: 0.5749 - val_acc: 0.8011\n",
      "Epoch 244/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.5193 - acc: 0.7949 - val_loss: 0.5986 - val_acc: 0.7670\n",
      "Epoch 245/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.5046 - acc: 0.7835 - val_loss: 0.5664 - val_acc: 0.7898\n",
      "Epoch 246/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.5298 - acc: 0.7849 - val_loss: 0.5954 - val_acc: 0.7841\n",
      "Epoch 247/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.5423 - acc: 0.7749 - val_loss: 0.5680 - val_acc: 0.7955\n",
      "Epoch 248/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.5094 - acc: 0.7892 - val_loss: 0.5541 - val_acc: 0.8011\n",
      "Epoch 249/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.5074 - acc: 0.8034 - val_loss: 0.5661 - val_acc: 0.8068\n",
      "Epoch 250/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.5408 - acc: 0.7821 - val_loss: 0.6368 - val_acc: 0.7386\n",
      "Epoch 251/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5320 - acc: 0.7949 - val_loss: 0.5453 - val_acc: 0.8011\n",
      "Epoch 252/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 0.5765 - acc: 0.7707 - val_loss: 0.6716 - val_acc: 0.7557\n",
      "Epoch 253/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.5826 - acc: 0.7792 - val_loss: 0.5647 - val_acc: 0.7955\n",
      "Epoch 254/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.6310 - acc: 0.7536 - val_loss: 0.6548 - val_acc: 0.7386\n",
      "Epoch 255/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.5579 - acc: 0.7692 - val_loss: 0.6005 - val_acc: 0.7614\n",
      "Epoch 256/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.5157 - acc: 0.7977 - val_loss: 0.5894 - val_acc: 0.7784\n",
      "Epoch 257/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.5086 - acc: 0.8020 - val_loss: 0.6269 - val_acc: 0.7500\n",
      "Epoch 258/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.5208 - acc: 0.7863 - val_loss: 0.5652 - val_acc: 0.8011\n",
      "Epoch 259/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.5098 - acc: 0.7806 - val_loss: 0.6535 - val_acc: 0.7330\n",
      "Epoch 260/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.6921 - acc: 0.7393 - val_loss: 0.6908 - val_acc: 0.7557\n",
      "Epoch 261/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.6630 - acc: 0.7521 - val_loss: 0.6102 - val_acc: 0.7898\n",
      "Epoch 262/3000\n",
      "702/702 [==============================] - 0s 564us/sample - loss: 0.5219 - acc: 0.7821 - val_loss: 0.6029 - val_acc: 0.7898\n",
      "Epoch 263/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4878 - acc: 0.8048 - val_loss: 0.6139 - val_acc: 0.8011\n",
      "Epoch 264/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.6469 - acc: 0.7593 - val_loss: 0.6916 - val_acc: 0.7330\n",
      "Epoch 265/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.5465 - acc: 0.7593 - val_loss: 0.6918 - val_acc: 0.7670\n",
      "Epoch 266/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.5396 - acc: 0.7707 - val_loss: 0.7544 - val_acc: 0.7500\n",
      "Epoch 267/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.5505 - acc: 0.7778 - val_loss: 0.6756 - val_acc: 0.7727\n",
      "Epoch 268/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.5422 - acc: 0.7749 - val_loss: 0.6373 - val_acc: 0.7670\n",
      "Epoch 269/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.5648 - acc: 0.7635 - val_loss: 0.6922 - val_acc: 0.7500\n",
      "Epoch 270/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.6632 - acc: 0.7493 - val_loss: 0.6515 - val_acc: 0.7784\n",
      "Epoch 271/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5654 - acc: 0.7906 - val_loss: 0.5971 - val_acc: 0.7955\n",
      "Epoch 272/3000\n",
      "702/702 [==============================] - 0s 567us/sample - loss: 0.5222 - acc: 0.7920 - val_loss: 0.6062 - val_acc: 0.7898\n",
      "Epoch 273/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.5530 - acc: 0.7835 - val_loss: 0.5795 - val_acc: 0.7841\n",
      "Epoch 274/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.4681 - acc: 0.8034 - val_loss: 0.5873 - val_acc: 0.7784\n",
      "Epoch 275/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.4865 - acc: 0.8134 - val_loss: 0.5792 - val_acc: 0.7784\n",
      "Epoch 276/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.5352 - acc: 0.7920 - val_loss: 0.5758 - val_acc: 0.7841\n",
      "Epoch 277/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.4578 - acc: 0.8091 - val_loss: 0.5601 - val_acc: 0.7898\n",
      "Epoch 278/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.4788 - acc: 0.7991 - val_loss: 0.5562 - val_acc: 0.8068\n",
      "Epoch 279/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.4747 - acc: 0.7991 - val_loss: 0.5705 - val_acc: 0.7898\n",
      "Epoch 280/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.4848 - acc: 0.7963 - val_loss: 0.5429 - val_acc: 0.8068\n",
      "Epoch 281/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5154 - acc: 0.7650 - val_loss: 0.6305 - val_acc: 0.7330\n",
      "Epoch 282/3000\n",
      "702/702 [==============================] - 0s 560us/sample - loss: 0.5530 - acc: 0.7821 - val_loss: 0.5613 - val_acc: 0.7955\n",
      "Epoch 283/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.5353 - acc: 0.7949 - val_loss: 0.5793 - val_acc: 0.7557\n",
      "Epoch 284/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.5037 - acc: 0.7863 - val_loss: 0.5757 - val_acc: 0.7670\n",
      "Epoch 285/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4883 - acc: 0.8006 - val_loss: 0.5622 - val_acc: 0.7955\n",
      "Epoch 286/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.4889 - acc: 0.7991 - val_loss: 0.5675 - val_acc: 0.7955\n",
      "Epoch 287/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.5002 - acc: 0.7863 - val_loss: 0.5912 - val_acc: 0.7784\n",
      "Epoch 288/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.4787 - acc: 0.8120 - val_loss: 0.5868 - val_acc: 0.7727\n",
      "Epoch 289/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.4829 - acc: 0.8105 - val_loss: 0.5870 - val_acc: 0.7727\n",
      "Epoch 290/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.4795 - acc: 0.7963 - val_loss: 0.5695 - val_acc: 0.7670\n",
      "Epoch 291/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4992 - acc: 0.7977 - val_loss: 0.5668 - val_acc: 0.7955\n",
      "Epoch 292/3000\n",
      "702/702 [==============================] - 0s 569us/sample - loss: 0.5004 - acc: 0.7949 - val_loss: 0.5639 - val_acc: 0.7898\n",
      "Epoch 293/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.4992 - acc: 0.8020 - val_loss: 0.5626 - val_acc: 0.7898\n",
      "Epoch 294/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.4732 - acc: 0.7991 - val_loss: 0.5789 - val_acc: 0.7670\n",
      "Epoch 295/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4954 - acc: 0.7949 - val_loss: 0.5574 - val_acc: 0.8068\n",
      "Epoch 296/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4945 - acc: 0.7906 - val_loss: 0.6422 - val_acc: 0.7557\n",
      "Epoch 297/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.5173 - acc: 0.7877 - val_loss: 0.5668 - val_acc: 0.7898\n",
      "Epoch 298/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.4775 - acc: 0.8091 - val_loss: 0.6132 - val_acc: 0.7784\n",
      "Epoch 299/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.4574 - acc: 0.8077 - val_loss: 0.5671 - val_acc: 0.7955\n",
      "Epoch 300/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.4655 - acc: 0.8105 - val_loss: 0.6365 - val_acc: 0.7614\n",
      "Epoch 301/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4588 - acc: 0.8048 - val_loss: 0.5852 - val_acc: 0.7955\n",
      "Epoch 302/3000\n",
      "702/702 [==============================] - 0s 564us/sample - loss: 0.4589 - acc: 0.8077 - val_loss: 0.6365 - val_acc: 0.7670\n",
      "Epoch 303/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4611 - acc: 0.8134 - val_loss: 0.5820 - val_acc: 0.7784\n",
      "Epoch 304/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.4298 - acc: 0.8148 - val_loss: 0.6006 - val_acc: 0.7898\n",
      "Epoch 305/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4365 - acc: 0.8191 - val_loss: 0.6312 - val_acc: 0.7727\n",
      "Epoch 306/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.4668 - acc: 0.8177 - val_loss: 0.6308 - val_acc: 0.7557\n",
      "Epoch 307/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4721 - acc: 0.7877 - val_loss: 0.5738 - val_acc: 0.8011\n",
      "Epoch 308/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.4806 - acc: 0.8034 - val_loss: 0.6120 - val_acc: 0.7784\n",
      "Epoch 309/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.4650 - acc: 0.8020 - val_loss: 0.5648 - val_acc: 0.7784\n",
      "Epoch 310/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.4634 - acc: 0.8091 - val_loss: 0.5594 - val_acc: 0.7898\n",
      "Epoch 311/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4474 - acc: 0.8205 - val_loss: 0.5528 - val_acc: 0.7955\n",
      "Epoch 312/3000\n",
      "702/702 [==============================] - 0s 559us/sample - loss: 0.4837 - acc: 0.8091 - val_loss: 0.5488 - val_acc: 0.7841\n",
      "Epoch 313/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4606 - acc: 0.8091 - val_loss: 0.5630 - val_acc: 0.7727\n",
      "Epoch 314/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4670 - acc: 0.8048 - val_loss: 0.5791 - val_acc: 0.7557\n",
      "Epoch 315/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.4450 - acc: 0.8148 - val_loss: 0.5337 - val_acc: 0.8125\n",
      "Epoch 316/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.4591 - acc: 0.7963 - val_loss: 0.5713 - val_acc: 0.7841\n",
      "Epoch 317/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.4230 - acc: 0.8148 - val_loss: 0.5287 - val_acc: 0.7898\n",
      "Epoch 318/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.4714 - acc: 0.8034 - val_loss: 0.5396 - val_acc: 0.7841\n",
      "Epoch 319/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.4476 - acc: 0.8006 - val_loss: 0.5243 - val_acc: 0.7898\n",
      "Epoch 320/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.4871 - acc: 0.8091 - val_loss: 0.5588 - val_acc: 0.7727\n",
      "Epoch 321/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4723 - acc: 0.8248 - val_loss: 0.5276 - val_acc: 0.7898\n",
      "Epoch 322/3000\n",
      "702/702 [==============================] - 0s 560us/sample - loss: 0.4723 - acc: 0.7977 - val_loss: 0.5625 - val_acc: 0.7841\n",
      "Epoch 323/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.5197 - acc: 0.8077 - val_loss: 0.5342 - val_acc: 0.7841\n",
      "Epoch 324/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.4640 - acc: 0.8077 - val_loss: 0.5847 - val_acc: 0.7557\n",
      "Epoch 325/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4694 - acc: 0.7906 - val_loss: 0.5318 - val_acc: 0.8011\n",
      "Epoch 326/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4530 - acc: 0.8162 - val_loss: 0.5615 - val_acc: 0.7841\n",
      "Epoch 327/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4535 - acc: 0.7991 - val_loss: 0.5540 - val_acc: 0.7955\n",
      "Epoch 328/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.4502 - acc: 0.8191 - val_loss: 0.5727 - val_acc: 0.7841\n",
      "Epoch 329/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.4815 - acc: 0.8034 - val_loss: 0.5604 - val_acc: 0.7841\n",
      "Epoch 330/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.4538 - acc: 0.8105 - val_loss: 0.5408 - val_acc: 0.7784\n",
      "Epoch 331/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4637 - acc: 0.7949 - val_loss: 0.5400 - val_acc: 0.7727\n",
      "Epoch 332/3000\n",
      "702/702 [==============================] - 0s 567us/sample - loss: 0.4601 - acc: 0.8162 - val_loss: 0.6259 - val_acc: 0.7557\n",
      "Epoch 333/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.5771 - acc: 0.7707 - val_loss: 0.5560 - val_acc: 0.7841\n",
      "Epoch 334/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.4738 - acc: 0.7892 - val_loss: 0.5688 - val_acc: 0.7727\n",
      "Epoch 335/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.5392 - acc: 0.7906 - val_loss: 0.5065 - val_acc: 0.7841\n",
      "Epoch 336/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4350 - acc: 0.8006 - val_loss: 0.5275 - val_acc: 0.7898\n",
      "Epoch 337/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4626 - acc: 0.8020 - val_loss: 0.5405 - val_acc: 0.8068\n",
      "Epoch 338/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4756 - acc: 0.8091 - val_loss: 0.5785 - val_acc: 0.8011\n",
      "Epoch 339/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.4623 - acc: 0.8048 - val_loss: 0.5322 - val_acc: 0.7898\n",
      "Epoch 340/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.4724 - acc: 0.8048 - val_loss: 0.5686 - val_acc: 0.8011\n",
      "Epoch 341/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4347 - acc: 0.8205 - val_loss: 0.5458 - val_acc: 0.8068\n",
      "Epoch 342/3000\n",
      "702/702 [==============================] - 0s 556us/sample - loss: 0.4560 - acc: 0.8148 - val_loss: 0.5771 - val_acc: 0.8011\n",
      "Epoch 343/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.4416 - acc: 0.8205 - val_loss: 0.5562 - val_acc: 0.8011\n",
      "Epoch 344/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.4607 - acc: 0.8191 - val_loss: 0.5767 - val_acc: 0.8295\n",
      "Epoch 345/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.4771 - acc: 0.7977 - val_loss: 0.5768 - val_acc: 0.8068\n",
      "Epoch 346/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.4732 - acc: 0.7991 - val_loss: 0.5859 - val_acc: 0.7898\n",
      "Epoch 347/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4634 - acc: 0.7991 - val_loss: 0.5796 - val_acc: 0.7443\n",
      "Epoch 348/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.4437 - acc: 0.8063 - val_loss: 0.5395 - val_acc: 0.7670\n",
      "Epoch 349/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4738 - acc: 0.8063 - val_loss: 0.5412 - val_acc: 0.7841\n",
      "Epoch 350/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.4803 - acc: 0.7963 - val_loss: 0.5383 - val_acc: 0.7784\n",
      "Epoch 351/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4371 - acc: 0.8191 - val_loss: 0.5246 - val_acc: 0.8011\n",
      "Epoch 352/3000\n",
      "702/702 [==============================] - 0s 568us/sample - loss: 0.4532 - acc: 0.8177 - val_loss: 0.6040 - val_acc: 0.7670\n",
      "Epoch 353/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.4918 - acc: 0.7977 - val_loss: 0.5620 - val_acc: 0.7955\n",
      "Epoch 354/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.4374 - acc: 0.7977 - val_loss: 0.5636 - val_acc: 0.7784\n",
      "Epoch 355/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.4606 - acc: 0.8020 - val_loss: 0.5387 - val_acc: 0.7841\n",
      "Epoch 356/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.4361 - acc: 0.8120 - val_loss: 0.5449 - val_acc: 0.7898\n",
      "Epoch 357/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.4183 - acc: 0.8205 - val_loss: 0.5430 - val_acc: 0.8352\n",
      "Epoch 358/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.4281 - acc: 0.8177 - val_loss: 0.5325 - val_acc: 0.8182\n",
      "Epoch 359/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.4415 - acc: 0.8219 - val_loss: 0.5453 - val_acc: 0.8239\n",
      "Epoch 360/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.4390 - acc: 0.8262 - val_loss: 0.5439 - val_acc: 0.8239\n",
      "Epoch 361/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4599 - acc: 0.8048 - val_loss: 0.5582 - val_acc: 0.8182\n",
      "Epoch 362/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 0.4589 - acc: 0.8248 - val_loss: 0.5287 - val_acc: 0.8295\n",
      "Epoch 363/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.4250 - acc: 0.8234 - val_loss: 0.5325 - val_acc: 0.7898\n",
      "Epoch 364/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.4351 - acc: 0.8077 - val_loss: 0.5609 - val_acc: 0.8295\n",
      "Epoch 365/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.4262 - acc: 0.8447 - val_loss: 0.5949 - val_acc: 0.8125\n",
      "Epoch 366/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.4633 - acc: 0.8134 - val_loss: 0.6061 - val_acc: 0.7841\n",
      "Epoch 367/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.4522 - acc: 0.8276 - val_loss: 0.5764 - val_acc: 0.8182\n",
      "Epoch 368/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.4174 - acc: 0.8276 - val_loss: 0.5650 - val_acc: 0.8352\n",
      "Epoch 369/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.4352 - acc: 0.8291 - val_loss: 0.5720 - val_acc: 0.8125\n",
      "Epoch 370/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.4526 - acc: 0.8063 - val_loss: 0.5574 - val_acc: 0.8295\n",
      "Epoch 371/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4326 - acc: 0.8191 - val_loss: 0.5646 - val_acc: 0.8409\n",
      "Epoch 372/3000\n",
      "702/702 [==============================] - 0s 564us/sample - loss: 0.4526 - acc: 0.8191 - val_loss: 0.6064 - val_acc: 0.7955\n",
      "Epoch 373/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4833 - acc: 0.8162 - val_loss: 0.5429 - val_acc: 0.8466\n",
      "Epoch 374/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.4309 - acc: 0.8177 - val_loss: 0.6073 - val_acc: 0.7670\n",
      "Epoch 375/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.4982 - acc: 0.8191 - val_loss: 0.5409 - val_acc: 0.8125\n",
      "Epoch 376/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.4431 - acc: 0.8177 - val_loss: 0.5762 - val_acc: 0.8182\n",
      "Epoch 377/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.4419 - acc: 0.8362 - val_loss: 0.5253 - val_acc: 0.8295\n",
      "Epoch 378/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4537 - acc: 0.8305 - val_loss: 0.5584 - val_acc: 0.8068\n",
      "Epoch 379/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4129 - acc: 0.8276 - val_loss: 0.5308 - val_acc: 0.8239\n",
      "Epoch 380/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.4537 - acc: 0.8162 - val_loss: 0.5800 - val_acc: 0.7898\n",
      "Epoch 381/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4756 - acc: 0.8177 - val_loss: 0.5198 - val_acc: 0.8182\n",
      "Epoch 382/3000\n",
      "702/702 [==============================] - 0s 552us/sample - loss: 0.4830 - acc: 0.8020 - val_loss: 0.6180 - val_acc: 0.7784\n",
      "Epoch 383/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4664 - acc: 0.8262 - val_loss: 0.5361 - val_acc: 0.8182\n",
      "Epoch 384/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.4275 - acc: 0.8291 - val_loss: 0.5692 - val_acc: 0.8125\n",
      "Epoch 385/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.4211 - acc: 0.8191 - val_loss: 0.5536 - val_acc: 0.8125\n",
      "Epoch 386/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.4644 - acc: 0.8063 - val_loss: 0.5848 - val_acc: 0.8182\n",
      "Epoch 387/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4305 - acc: 0.8376 - val_loss: 0.5784 - val_acc: 0.7898\n",
      "Epoch 388/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.4310 - acc: 0.8447 - val_loss: 0.5202 - val_acc: 0.8352\n",
      "Epoch 389/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.4361 - acc: 0.8276 - val_loss: 0.5509 - val_acc: 0.8068\n",
      "Epoch 390/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.4538 - acc: 0.8105 - val_loss: 0.5192 - val_acc: 0.8295\n",
      "Epoch 391/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4209 - acc: 0.8262 - val_loss: 0.5463 - val_acc: 0.8068\n",
      "Epoch 392/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 0.4441 - acc: 0.8205 - val_loss: 0.5641 - val_acc: 0.7955\n",
      "Epoch 393/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4635 - acc: 0.8219 - val_loss: 0.5056 - val_acc: 0.8580\n",
      "Epoch 394/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.4533 - acc: 0.8305 - val_loss: 0.5561 - val_acc: 0.7898\n",
      "Epoch 395/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.4230 - acc: 0.8276 - val_loss: 0.5037 - val_acc: 0.8295\n",
      "Epoch 396/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.4000 - acc: 0.8376 - val_loss: 0.5644 - val_acc: 0.8125\n",
      "Epoch 397/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.4024 - acc: 0.8419 - val_loss: 0.5522 - val_acc: 0.8352\n",
      "Epoch 398/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.4359 - acc: 0.8362 - val_loss: 0.6274 - val_acc: 0.7670\n",
      "Epoch 399/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.4550 - acc: 0.8177 - val_loss: 0.5152 - val_acc: 0.8182\n",
      "Epoch 400/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4584 - acc: 0.8148 - val_loss: 0.5672 - val_acc: 0.8011\n",
      "Epoch 401/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4461 - acc: 0.8162 - val_loss: 0.5129 - val_acc: 0.8295\n",
      "Epoch 402/3000\n",
      "702/702 [==============================] - 0s 559us/sample - loss: 0.3915 - acc: 0.8447 - val_loss: 0.5545 - val_acc: 0.8295\n",
      "Epoch 403/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.4488 - acc: 0.8234 - val_loss: 0.5374 - val_acc: 0.8125\n",
      "Epoch 404/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.4437 - acc: 0.8348 - val_loss: 0.5723 - val_acc: 0.8068\n",
      "Epoch 405/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.4418 - acc: 0.8234 - val_loss: 0.5453 - val_acc: 0.8295\n",
      "Epoch 406/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.4387 - acc: 0.8291 - val_loss: 0.5175 - val_acc: 0.8523\n",
      "Epoch 407/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.4053 - acc: 0.8376 - val_loss: 0.5344 - val_acc: 0.8011\n",
      "Epoch 408/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.4444 - acc: 0.8405 - val_loss: 0.5635 - val_acc: 0.7898\n",
      "Epoch 409/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.4568 - acc: 0.8262 - val_loss: 0.4979 - val_acc: 0.8409\n",
      "Epoch 410/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.5000 - acc: 0.7977 - val_loss: 0.5552 - val_acc: 0.8011\n",
      "Epoch 411/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4609 - acc: 0.8134 - val_loss: 0.5659 - val_acc: 0.8125\n",
      "Epoch 412/3000\n",
      "702/702 [==============================] - 0s 558us/sample - loss: 0.4814 - acc: 0.8048 - val_loss: 0.5529 - val_acc: 0.8466\n",
      "Epoch 413/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4774 - acc: 0.8105 - val_loss: 0.5612 - val_acc: 0.8182\n",
      "Epoch 414/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4306 - acc: 0.8433 - val_loss: 0.5567 - val_acc: 0.8182\n",
      "Epoch 415/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.3843 - acc: 0.8504 - val_loss: 0.5471 - val_acc: 0.8182\n",
      "Epoch 416/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.3936 - acc: 0.8405 - val_loss: 0.5738 - val_acc: 0.8182\n",
      "Epoch 417/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4477 - acc: 0.8248 - val_loss: 0.5839 - val_acc: 0.8068\n",
      "Epoch 418/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.4214 - acc: 0.8433 - val_loss: 0.5977 - val_acc: 0.7955\n",
      "Epoch 419/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.4566 - acc: 0.8575 - val_loss: 0.6362 - val_acc: 0.7841\n",
      "Epoch 420/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.4281 - acc: 0.8376 - val_loss: 0.6666 - val_acc: 0.8011\n",
      "Epoch 421/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4790 - acc: 0.8248 - val_loss: 0.6229 - val_acc: 0.7841\n",
      "Epoch 422/3000\n",
      "702/702 [==============================] - 0s 558us/sample - loss: 0.4585 - acc: 0.8248 - val_loss: 0.5910 - val_acc: 0.7898\n",
      "Epoch 423/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4605 - acc: 0.8234 - val_loss: 0.6578 - val_acc: 0.7727\n",
      "Epoch 424/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.4908 - acc: 0.7949 - val_loss: 0.6352 - val_acc: 0.7841\n",
      "Epoch 425/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.4428 - acc: 0.8291 - val_loss: 0.6110 - val_acc: 0.8068\n",
      "Epoch 426/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.4922 - acc: 0.7877 - val_loss: 0.5430 - val_acc: 0.8239\n",
      "Epoch 427/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.4311 - acc: 0.8433 - val_loss: 0.5789 - val_acc: 0.8125\n",
      "Epoch 428/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.4053 - acc: 0.8390 - val_loss: 0.5826 - val_acc: 0.8523\n",
      "Epoch 429/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4633 - acc: 0.8162 - val_loss: 0.6273 - val_acc: 0.8352\n",
      "Epoch 430/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.4544 - acc: 0.8191 - val_loss: 0.6153 - val_acc: 0.8295\n",
      "Epoch 431/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4013 - acc: 0.8547 - val_loss: 0.6015 - val_acc: 0.7955\n",
      "Epoch 432/3000\n",
      "702/702 [==============================] - 0s 556us/sample - loss: 0.4337 - acc: 0.8234 - val_loss: 0.5826 - val_acc: 0.7898\n",
      "Epoch 433/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.4296 - acc: 0.8433 - val_loss: 0.7269 - val_acc: 0.6989\n",
      "Epoch 434/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.5218 - acc: 0.7991 - val_loss: 0.5049 - val_acc: 0.8523\n",
      "Epoch 435/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4855 - acc: 0.8077 - val_loss: 0.6104 - val_acc: 0.7386\n",
      "Epoch 436/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.4523 - acc: 0.8105 - val_loss: 0.5070 - val_acc: 0.8409\n",
      "Epoch 437/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.4789 - acc: 0.8390 - val_loss: 0.5461 - val_acc: 0.8409\n",
      "Epoch 438/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4265 - acc: 0.8405 - val_loss: 0.5373 - val_acc: 0.8295\n",
      "Epoch 439/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.4128 - acc: 0.8390 - val_loss: 0.5494 - val_acc: 0.8068\n",
      "Epoch 440/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.4415 - acc: 0.8191 - val_loss: 0.5979 - val_acc: 0.7841\n",
      "Epoch 441/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4477 - acc: 0.8006 - val_loss: 0.5231 - val_acc: 0.8409\n",
      "Epoch 442/3000\n",
      "702/702 [==============================] - 0s 547us/sample - loss: 0.4546 - acc: 0.8276 - val_loss: 0.5961 - val_acc: 0.7841\n",
      "Epoch 443/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.4871 - acc: 0.8219 - val_loss: 0.5554 - val_acc: 0.7955\n",
      "Epoch 444/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.4486 - acc: 0.8333 - val_loss: 0.6297 - val_acc: 0.7670\n",
      "Epoch 445/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4996 - acc: 0.7877 - val_loss: 0.5163 - val_acc: 0.8295\n",
      "Epoch 446/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.4823 - acc: 0.8120 - val_loss: 0.5808 - val_acc: 0.7784\n",
      "Epoch 447/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.5267 - acc: 0.7906 - val_loss: 0.6560 - val_acc: 0.7557\n",
      "Epoch 448/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.4576 - acc: 0.8162 - val_loss: 0.5486 - val_acc: 0.8011\n",
      "Epoch 449/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.4848 - acc: 0.8319 - val_loss: 0.5524 - val_acc: 0.8295\n",
      "Epoch 450/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.4295 - acc: 0.8319 - val_loss: 0.5600 - val_acc: 0.8068\n",
      "Epoch 451/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4476 - acc: 0.8305 - val_loss: 0.6138 - val_acc: 0.7841\n",
      "Epoch 452/3000\n",
      "702/702 [==============================] - 0s 558us/sample - loss: 0.4264 - acc: 0.8305 - val_loss: 0.5140 - val_acc: 0.8352\n",
      "Epoch 453/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.4628 - acc: 0.8504 - val_loss: 0.5940 - val_acc: 0.7670\n",
      "Epoch 454/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.4383 - acc: 0.8362 - val_loss: 0.5648 - val_acc: 0.8125\n",
      "Epoch 455/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.4115 - acc: 0.8348 - val_loss: 0.5583 - val_acc: 0.7898\n",
      "Epoch 456/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.3643 - acc: 0.8490 - val_loss: 0.4788 - val_acc: 0.8807\n",
      "Epoch 457/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4421 - acc: 0.8362 - val_loss: 0.5258 - val_acc: 0.8239\n",
      "Epoch 458/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3794 - acc: 0.8575 - val_loss: 0.5059 - val_acc: 0.8125\n",
      "Epoch 459/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.4226 - acc: 0.8348 - val_loss: 0.5631 - val_acc: 0.7955\n",
      "Epoch 460/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.3682 - acc: 0.8504 - val_loss: 0.5090 - val_acc: 0.8182\n",
      "Epoch 461/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4080 - acc: 0.8533 - val_loss: 0.5529 - val_acc: 0.7955\n",
      "Epoch 462/3000\n",
      "702/702 [==============================] - 0s 561us/sample - loss: 0.4079 - acc: 0.8604 - val_loss: 0.5740 - val_acc: 0.8409\n",
      "Epoch 463/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.4274 - acc: 0.8405 - val_loss: 0.6238 - val_acc: 0.7955\n",
      "Epoch 464/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.4105 - acc: 0.8533 - val_loss: 0.5580 - val_acc: 0.8068\n",
      "Epoch 465/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.3806 - acc: 0.8689 - val_loss: 0.5114 - val_acc: 0.8409\n",
      "Epoch 466/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.4434 - acc: 0.8433 - val_loss: 0.6184 - val_acc: 0.7216\n",
      "Epoch 467/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.4423 - acc: 0.8419 - val_loss: 0.5016 - val_acc: 0.8125\n",
      "Epoch 468/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.4376 - acc: 0.8419 - val_loss: 0.6170 - val_acc: 0.7500\n",
      "Epoch 469/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.4873 - acc: 0.8148 - val_loss: 0.5200 - val_acc: 0.8409\n",
      "Epoch 470/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.4577 - acc: 0.8447 - val_loss: 0.6188 - val_acc: 0.8068\n",
      "Epoch 471/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4330 - acc: 0.8504 - val_loss: 0.5599 - val_acc: 0.8125\n",
      "Epoch 472/3000\n",
      "702/702 [==============================] - 0s 573us/sample - loss: 0.3989 - acc: 0.8675 - val_loss: 0.5482 - val_acc: 0.8125\n",
      "Epoch 473/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.4450 - acc: 0.8547 - val_loss: 0.4935 - val_acc: 0.8068\n",
      "Epoch 474/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.4019 - acc: 0.8561 - val_loss: 0.5455 - val_acc: 0.8011\n",
      "Epoch 475/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.4775 - acc: 0.8362 - val_loss: 0.6033 - val_acc: 0.7500\n",
      "Epoch 476/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4253 - acc: 0.8547 - val_loss: 0.5396 - val_acc: 0.8011\n",
      "Epoch 477/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.4511 - acc: 0.8561 - val_loss: 0.6211 - val_acc: 0.7670\n",
      "Epoch 478/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.4316 - acc: 0.8348 - val_loss: 0.5439 - val_acc: 0.8011\n",
      "Epoch 479/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.3944 - acc: 0.8447 - val_loss: 0.6512 - val_acc: 0.7614\n",
      "Epoch 480/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4063 - acc: 0.8276 - val_loss: 0.5068 - val_acc: 0.8409\n",
      "Epoch 481/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4000 - acc: 0.8504 - val_loss: 0.5495 - val_acc: 0.8068\n",
      "Epoch 482/3000\n",
      "702/702 [==============================] - 0s 554us/sample - loss: 0.3948 - acc: 0.8590 - val_loss: 0.5343 - val_acc: 0.8182\n",
      "Epoch 483/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.3958 - acc: 0.8575 - val_loss: 0.5040 - val_acc: 0.8068\n",
      "Epoch 484/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.3602 - acc: 0.8647 - val_loss: 0.5648 - val_acc: 0.7898\n",
      "Epoch 485/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4147 - acc: 0.8575 - val_loss: 0.5308 - val_acc: 0.8182\n",
      "Epoch 486/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.3943 - acc: 0.8647 - val_loss: 0.5388 - val_acc: 0.8011\n",
      "Epoch 487/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4015 - acc: 0.8533 - val_loss: 0.5047 - val_acc: 0.8125\n",
      "Epoch 488/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.3686 - acc: 0.8604 - val_loss: 0.5285 - val_acc: 0.8011\n",
      "Epoch 489/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.3564 - acc: 0.8718 - val_loss: 0.5559 - val_acc: 0.8011\n",
      "Epoch 490/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.3646 - acc: 0.8746 - val_loss: 0.5126 - val_acc: 0.8239\n",
      "Epoch 491/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3879 - acc: 0.8533 - val_loss: 0.5055 - val_acc: 0.8125\n",
      "Epoch 492/3000\n",
      "702/702 [==============================] - 0s 556us/sample - loss: 0.3591 - acc: 0.8647 - val_loss: 0.5501 - val_acc: 0.7841\n",
      "Epoch 493/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.4223 - acc: 0.8091 - val_loss: 0.4786 - val_acc: 0.8125\n",
      "Epoch 494/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.3876 - acc: 0.8575 - val_loss: 0.5526 - val_acc: 0.7614\n",
      "Epoch 495/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.4181 - acc: 0.8376 - val_loss: 0.4722 - val_acc: 0.8239\n",
      "Epoch 496/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.4391 - acc: 0.8419 - val_loss: 0.5941 - val_acc: 0.7216\n",
      "Epoch 497/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.4578 - acc: 0.8219 - val_loss: 0.5192 - val_acc: 0.8068\n",
      "Epoch 498/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4350 - acc: 0.8476 - val_loss: 0.5616 - val_acc: 0.7955\n",
      "Epoch 499/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.3888 - acc: 0.8789 - val_loss: 0.5274 - val_acc: 0.8125\n",
      "Epoch 500/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.3876 - acc: 0.8561 - val_loss: 0.4994 - val_acc: 0.8239\n",
      "Epoch 501/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3543 - acc: 0.8732 - val_loss: 0.5221 - val_acc: 0.8125\n",
      "Epoch 502/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 0.3571 - acc: 0.8675 - val_loss: 0.4937 - val_acc: 0.8352\n",
      "Epoch 503/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3927 - acc: 0.8661 - val_loss: 0.5111 - val_acc: 0.8125\n",
      "Epoch 504/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.3324 - acc: 0.8718 - val_loss: 0.5052 - val_acc: 0.8295\n",
      "Epoch 505/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.3475 - acc: 0.8889 - val_loss: 0.4791 - val_acc: 0.8466\n",
      "Epoch 506/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.3861 - acc: 0.8590 - val_loss: 0.5888 - val_acc: 0.7614\n",
      "Epoch 507/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.4398 - acc: 0.8262 - val_loss: 0.4932 - val_acc: 0.8466\n",
      "Epoch 508/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.3832 - acc: 0.8575 - val_loss: 0.6061 - val_acc: 0.7727\n",
      "Epoch 509/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.3936 - acc: 0.8419 - val_loss: 0.4880 - val_acc: 0.8295\n",
      "Epoch 510/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.4145 - acc: 0.8462 - val_loss: 0.5853 - val_acc: 0.7784\n",
      "Epoch 511/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3639 - acc: 0.8632 - val_loss: 0.5080 - val_acc: 0.8409\n",
      "Epoch 512/3000\n",
      "702/702 [==============================] - 0s 560us/sample - loss: 0.3945 - acc: 0.8632 - val_loss: 0.5747 - val_acc: 0.7727\n",
      "Epoch 513/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.3889 - acc: 0.8575 - val_loss: 0.4590 - val_acc: 0.8523\n",
      "Epoch 514/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.3895 - acc: 0.8547 - val_loss: 0.5959 - val_acc: 0.7727\n",
      "Epoch 515/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.3992 - acc: 0.8547 - val_loss: 0.5189 - val_acc: 0.8352\n",
      "Epoch 516/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.3527 - acc: 0.8789 - val_loss: 0.5593 - val_acc: 0.7784\n",
      "Epoch 517/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3687 - acc: 0.8818 - val_loss: 0.4804 - val_acc: 0.8239\n",
      "Epoch 518/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4013 - acc: 0.8618 - val_loss: 0.5501 - val_acc: 0.7670\n",
      "Epoch 519/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.3730 - acc: 0.8718 - val_loss: 0.4992 - val_acc: 0.8011\n",
      "Epoch 520/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3798 - acc: 0.8675 - val_loss: 0.4826 - val_acc: 0.8239\n",
      "Epoch 521/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3813 - acc: 0.8704 - val_loss: 0.5420 - val_acc: 0.8011\n",
      "Epoch 522/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 0.3693 - acc: 0.8761 - val_loss: 0.5418 - val_acc: 0.8182\n",
      "Epoch 523/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.3481 - acc: 0.8732 - val_loss: 0.5524 - val_acc: 0.8352\n",
      "Epoch 524/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.3835 - acc: 0.8647 - val_loss: 0.5606 - val_acc: 0.8125\n",
      "Epoch 525/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.3525 - acc: 0.8903 - val_loss: 0.5522 - val_acc: 0.7955\n",
      "Epoch 526/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.3963 - acc: 0.8490 - val_loss: 0.5334 - val_acc: 0.7898\n",
      "Epoch 527/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.4191 - acc: 0.8533 - val_loss: 0.5456 - val_acc: 0.8239\n",
      "Epoch 528/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.3899 - acc: 0.8533 - val_loss: 0.5370 - val_acc: 0.8239\n",
      "Epoch 529/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.3388 - acc: 0.8803 - val_loss: 0.5037 - val_acc: 0.8352\n",
      "Epoch 530/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.4198 - acc: 0.8647 - val_loss: 0.4759 - val_acc: 0.8523\n",
      "Epoch 531/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3605 - acc: 0.8732 - val_loss: 0.4772 - val_acc: 0.8523\n",
      "Epoch 532/3000\n",
      "702/702 [==============================] - 0s 559us/sample - loss: 0.3608 - acc: 0.8689 - val_loss: 0.5094 - val_acc: 0.8295\n",
      "Epoch 533/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.3911 - acc: 0.8661 - val_loss: 0.5448 - val_acc: 0.8068\n",
      "Epoch 534/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.3817 - acc: 0.8761 - val_loss: 0.5259 - val_acc: 0.8295\n",
      "Epoch 535/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4765 - acc: 0.8533 - val_loss: 0.5997 - val_acc: 0.7443\n",
      "Epoch 536/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.4052 - acc: 0.8490 - val_loss: 0.5434 - val_acc: 0.8182\n",
      "Epoch 537/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4614 - acc: 0.8433 - val_loss: 0.6411 - val_acc: 0.7386\n",
      "Epoch 538/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.5099 - acc: 0.8405 - val_loss: 0.5673 - val_acc: 0.8125\n",
      "Epoch 539/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.4129 - acc: 0.8604 - val_loss: 0.6436 - val_acc: 0.7955\n",
      "Epoch 540/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.4155 - acc: 0.8575 - val_loss: 0.5606 - val_acc: 0.8068\n",
      "Epoch 541/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3479 - acc: 0.8761 - val_loss: 0.5605 - val_acc: 0.7841\n",
      "Epoch 542/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 0.3566 - acc: 0.8689 - val_loss: 0.5616 - val_acc: 0.8011\n",
      "Epoch 543/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.3710 - acc: 0.8832 - val_loss: 0.5752 - val_acc: 0.8068\n",
      "Epoch 544/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.3742 - acc: 0.8704 - val_loss: 0.5524 - val_acc: 0.8125\n",
      "Epoch 545/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.3671 - acc: 0.8775 - val_loss: 0.5409 - val_acc: 0.8239\n",
      "Epoch 546/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.3492 - acc: 0.8932 - val_loss: 0.5569 - val_acc: 0.7898\n",
      "Epoch 547/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.4023 - acc: 0.8618 - val_loss: 0.5184 - val_acc: 0.8295\n",
      "Epoch 548/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.4211 - acc: 0.8561 - val_loss: 0.6364 - val_acc: 0.7500\n",
      "Epoch 549/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3737 - acc: 0.8590 - val_loss: 0.5218 - val_acc: 0.8409\n",
      "Epoch 550/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.4719 - acc: 0.8575 - val_loss: 0.6281 - val_acc: 0.7670\n",
      "Epoch 551/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4367 - acc: 0.8348 - val_loss: 0.5795 - val_acc: 0.7841\n",
      "Epoch 552/3000\n",
      "702/702 [==============================] - 0s 564us/sample - loss: 0.4287 - acc: 0.8248 - val_loss: 0.5476 - val_acc: 0.8125\n",
      "Epoch 553/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.4693 - acc: 0.8376 - val_loss: 0.6094 - val_acc: 0.7557\n",
      "Epoch 554/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.3713 - acc: 0.8490 - val_loss: 0.5016 - val_acc: 0.8352\n",
      "Epoch 555/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4022 - acc: 0.8704 - val_loss: 0.5841 - val_acc: 0.7670\n",
      "Epoch 556/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.4212 - acc: 0.8519 - val_loss: 0.4981 - val_acc: 0.8182\n",
      "Epoch 557/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.3827 - acc: 0.8647 - val_loss: 0.5689 - val_acc: 0.7727\n",
      "Epoch 558/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.3745 - acc: 0.8718 - val_loss: 0.5146 - val_acc: 0.8182\n",
      "Epoch 559/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.4029 - acc: 0.8533 - val_loss: 0.5290 - val_acc: 0.8182\n",
      "Epoch 560/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.4346 - acc: 0.8405 - val_loss: 0.5608 - val_acc: 0.8011\n",
      "Epoch 561/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3965 - acc: 0.8561 - val_loss: 0.5531 - val_acc: 0.8125\n",
      "Epoch 562/3000\n",
      "702/702 [==============================] - 0s 558us/sample - loss: 0.3755 - acc: 0.8746 - val_loss: 0.6358 - val_acc: 0.7670\n",
      "Epoch 563/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.3885 - acc: 0.8547 - val_loss: 0.5609 - val_acc: 0.8409\n",
      "Epoch 564/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.3821 - acc: 0.8647 - val_loss: 0.5917 - val_acc: 0.7784\n",
      "Epoch 565/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.3796 - acc: 0.8704 - val_loss: 0.5788 - val_acc: 0.7841\n",
      "Epoch 566/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.3823 - acc: 0.8661 - val_loss: 0.5285 - val_acc: 0.8011\n",
      "Epoch 567/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.3800 - acc: 0.8547 - val_loss: 0.6041 - val_acc: 0.7557\n",
      "Epoch 568/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.4001 - acc: 0.8376 - val_loss: 0.5474 - val_acc: 0.8011\n",
      "Epoch 569/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4007 - acc: 0.8632 - val_loss: 0.5175 - val_acc: 0.8182\n",
      "Epoch 570/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.3902 - acc: 0.8476 - val_loss: 0.5600 - val_acc: 0.8182\n",
      "Epoch 571/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3457 - acc: 0.8860 - val_loss: 0.5348 - val_acc: 0.8125\n",
      "Epoch 572/3000\n",
      "702/702 [==============================] - 0s 556us/sample - loss: 0.3874 - acc: 0.8590 - val_loss: 0.5442 - val_acc: 0.7955\n",
      "Epoch 573/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4362 - acc: 0.8490 - val_loss: 0.6026 - val_acc: 0.7216\n",
      "Epoch 574/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.4015 - acc: 0.8447 - val_loss: 0.4928 - val_acc: 0.8068\n",
      "Epoch 575/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.3656 - acc: 0.8661 - val_loss: 0.6389 - val_acc: 0.7045\n",
      "Epoch 576/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.3990 - acc: 0.8390 - val_loss: 0.5362 - val_acc: 0.8125\n",
      "Epoch 577/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3723 - acc: 0.8604 - val_loss: 0.6238 - val_acc: 0.7614\n",
      "Epoch 578/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.3914 - acc: 0.8632 - val_loss: 0.5939 - val_acc: 0.8239\n",
      "Epoch 579/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.4039 - acc: 0.8604 - val_loss: 0.6025 - val_acc: 0.7727\n",
      "Epoch 580/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.4023 - acc: 0.8675 - val_loss: 0.5151 - val_acc: 0.8295\n",
      "Epoch 581/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3732 - acc: 0.8746 - val_loss: 0.6224 - val_acc: 0.7443\n",
      "Epoch 582/3000\n",
      "702/702 [==============================] - 0s 556us/sample - loss: 0.3894 - acc: 0.8590 - val_loss: 0.5858 - val_acc: 0.7955\n",
      "Epoch 583/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.3996 - acc: 0.8519 - val_loss: 0.5591 - val_acc: 0.8068\n",
      "Epoch 584/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.3968 - acc: 0.8618 - val_loss: 0.5865 - val_acc: 0.7841\n",
      "Epoch 585/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.3885 - acc: 0.8632 - val_loss: 0.5880 - val_acc: 0.7898\n",
      "Epoch 586/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3703 - acc: 0.8704 - val_loss: 0.5364 - val_acc: 0.8182\n",
      "Epoch 587/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.3613 - acc: 0.8818 - val_loss: 0.5714 - val_acc: 0.7386\n",
      "Epoch 588/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.4003 - acc: 0.8519 - val_loss: 0.5559 - val_acc: 0.8182\n",
      "Epoch 589/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.3398 - acc: 0.8860 - val_loss: 0.5330 - val_acc: 0.8295\n",
      "Epoch 590/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.3963 - acc: 0.8718 - val_loss: 0.5923 - val_acc: 0.7727\n",
      "Epoch 591/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4100 - acc: 0.8533 - val_loss: 0.5978 - val_acc: 0.7841\n",
      "Epoch 592/3000\n",
      "702/702 [==============================] - 0s 555us/sample - loss: 0.3899 - acc: 0.8632 - val_loss: 0.5764 - val_acc: 0.7898\n",
      "Epoch 593/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.3669 - acc: 0.8689 - val_loss: 0.5721 - val_acc: 0.8125\n",
      "Epoch 594/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.3923 - acc: 0.8604 - val_loss: 0.5695 - val_acc: 0.8068\n",
      "Epoch 595/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.3429 - acc: 0.8832 - val_loss: 0.5503 - val_acc: 0.8182\n",
      "Epoch 596/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.3445 - acc: 0.8718 - val_loss: 0.5369 - val_acc: 0.8352\n",
      "Epoch 597/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.3559 - acc: 0.8746 - val_loss: 0.5982 - val_acc: 0.7784\n",
      "Epoch 598/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3471 - acc: 0.8704 - val_loss: 0.5436 - val_acc: 0.8182\n",
      "Epoch 599/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.3734 - acc: 0.8974 - val_loss: 0.5666 - val_acc: 0.7841\n",
      "Epoch 600/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3689 - acc: 0.8718 - val_loss: 0.4679 - val_acc: 0.8295\n",
      "Epoch 601/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3289 - acc: 0.8860 - val_loss: 0.5387 - val_acc: 0.7386\n",
      "Epoch 602/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 0.3655 - acc: 0.8533 - val_loss: 0.4463 - val_acc: 0.8295\n",
      "Epoch 603/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.3744 - acc: 0.8746 - val_loss: 0.5133 - val_acc: 0.7841\n",
      "Epoch 604/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.3321 - acc: 0.8875 - val_loss: 0.4693 - val_acc: 0.8239\n",
      "Epoch 605/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3690 - acc: 0.8732 - val_loss: 0.5305 - val_acc: 0.7500\n",
      "Epoch 606/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.3287 - acc: 0.8746 - val_loss: 0.4501 - val_acc: 0.8352\n",
      "Epoch 607/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.3429 - acc: 0.8946 - val_loss: 0.5530 - val_acc: 0.7386\n",
      "Epoch 608/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.3538 - acc: 0.8675 - val_loss: 0.4912 - val_acc: 0.8182\n",
      "Epoch 609/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.3618 - acc: 0.8761 - val_loss: 0.5044 - val_acc: 0.8011\n",
      "Epoch 610/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.3254 - acc: 0.8860 - val_loss: 0.4669 - val_acc: 0.8068\n",
      "Epoch 611/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3256 - acc: 0.8932 - val_loss: 0.4945 - val_acc: 0.7955\n",
      "Epoch 612/3000\n",
      "702/702 [==============================] - 0s 559us/sample - loss: 0.3192 - acc: 0.8889 - val_loss: 0.5222 - val_acc: 0.8011\n",
      "Epoch 613/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.3176 - acc: 0.8946 - val_loss: 0.5622 - val_acc: 0.8068\n",
      "Epoch 614/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.4030 - acc: 0.8732 - val_loss: 0.5514 - val_acc: 0.8068\n",
      "Epoch 615/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.3450 - acc: 0.8803 - val_loss: 0.5494 - val_acc: 0.7443\n",
      "Epoch 616/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3675 - acc: 0.8704 - val_loss: 0.5227 - val_acc: 0.7955\n",
      "Epoch 617/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4243 - acc: 0.8519 - val_loss: 0.5143 - val_acc: 0.8125\n",
      "Epoch 618/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.4420 - acc: 0.8604 - val_loss: 0.4537 - val_acc: 0.8466\n",
      "Epoch 619/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.3998 - acc: 0.8490 - val_loss: 0.5044 - val_acc: 0.7784\n",
      "Epoch 620/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.4251 - acc: 0.8390 - val_loss: 0.5044 - val_acc: 0.7841\n",
      "Epoch 621/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4040 - acc: 0.8604 - val_loss: 0.4651 - val_acc: 0.8239\n",
      "Epoch 622/3000\n",
      "702/702 [==============================] - 0s 561us/sample - loss: 0.3729 - acc: 0.8746 - val_loss: 0.4501 - val_acc: 0.8466\n",
      "Epoch 623/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.3549 - acc: 0.8889 - val_loss: 0.4845 - val_acc: 0.7898\n",
      "Epoch 624/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.3279 - acc: 0.8846 - val_loss: 0.4716 - val_acc: 0.8352\n",
      "Epoch 625/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.3862 - acc: 0.8704 - val_loss: 0.5904 - val_acc: 0.7727\n",
      "Epoch 626/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.3810 - acc: 0.8732 - val_loss: 0.6074 - val_acc: 0.7670\n",
      "Epoch 627/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.3677 - acc: 0.8732 - val_loss: 0.5757 - val_acc: 0.7670\n",
      "Epoch 628/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.3855 - acc: 0.8789 - val_loss: 0.5234 - val_acc: 0.8182\n",
      "Epoch 629/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.3528 - acc: 0.8832 - val_loss: 0.4969 - val_acc: 0.7898\n",
      "Epoch 630/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3017 - acc: 0.8903 - val_loss: 0.5671 - val_acc: 0.8068\n",
      "Epoch 631/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3516 - acc: 0.8661 - val_loss: 0.6149 - val_acc: 0.7500\n",
      "Epoch 632/3000\n",
      "702/702 [==============================] - 0s 553us/sample - loss: 0.3933 - acc: 0.8618 - val_loss: 0.5107 - val_acc: 0.7670\n",
      "Epoch 633/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.3551 - acc: 0.8732 - val_loss: 0.5111 - val_acc: 0.8295\n",
      "Epoch 634/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.3677 - acc: 0.8732 - val_loss: 0.6176 - val_acc: 0.7386\n",
      "Epoch 635/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.3962 - acc: 0.8490 - val_loss: 0.5487 - val_acc: 0.8125\n",
      "Epoch 636/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.3105 - acc: 0.8875 - val_loss: 0.4987 - val_acc: 0.8182\n",
      "Epoch 637/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3241 - acc: 0.8932 - val_loss: 0.5219 - val_acc: 0.7841\n",
      "Epoch 638/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.3288 - acc: 0.8832 - val_loss: 0.5488 - val_acc: 0.7841\n",
      "Epoch 639/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.3326 - acc: 0.8946 - val_loss: 0.5233 - val_acc: 0.7955\n",
      "Epoch 640/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.3317 - acc: 0.8946 - val_loss: 0.5278 - val_acc: 0.8011\n",
      "Epoch 641/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3636 - acc: 0.8575 - val_loss: 0.5216 - val_acc: 0.8182\n",
      "Epoch 642/3000\n",
      "702/702 [==============================] - 0s 550us/sample - loss: 0.3454 - acc: 0.8732 - val_loss: 0.5127 - val_acc: 0.7727\n",
      "Epoch 643/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.3285 - acc: 0.8875 - val_loss: 0.4910 - val_acc: 0.8068\n",
      "Epoch 644/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.3226 - acc: 0.8917 - val_loss: 0.5305 - val_acc: 0.7784\n",
      "Epoch 645/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.3258 - acc: 0.8818 - val_loss: 0.5044 - val_acc: 0.8011\n",
      "Epoch 646/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.3329 - acc: 0.8932 - val_loss: 0.5550 - val_acc: 0.7784\n",
      "Epoch 647/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.3601 - acc: 0.8803 - val_loss: 0.5003 - val_acc: 0.8466\n",
      "Epoch 648/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.3903 - acc: 0.8661 - val_loss: 0.5505 - val_acc: 0.7955\n",
      "Epoch 649/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.3131 - acc: 0.8989 - val_loss: 0.4906 - val_acc: 0.8011\n",
      "Epoch 650/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3158 - acc: 0.8932 - val_loss: 0.4908 - val_acc: 0.8239\n",
      "Epoch 651/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3147 - acc: 0.8889 - val_loss: 0.4796 - val_acc: 0.8125\n",
      "Epoch 652/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 0.4102 - acc: 0.8618 - val_loss: 0.5522 - val_acc: 0.7670\n",
      "Epoch 653/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.3651 - acc: 0.8761 - val_loss: 0.4548 - val_acc: 0.8466\n",
      "Epoch 654/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.3317 - acc: 0.8832 - val_loss: 0.4723 - val_acc: 0.8182\n",
      "Epoch 655/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.3828 - acc: 0.8561 - val_loss: 0.5403 - val_acc: 0.7557\n",
      "Epoch 656/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.3336 - acc: 0.8832 - val_loss: 0.5333 - val_acc: 0.8580\n",
      "Epoch 657/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.4242 - acc: 0.8675 - val_loss: 0.5844 - val_acc: 0.7614\n",
      "Epoch 658/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.3850 - acc: 0.8732 - val_loss: 0.4239 - val_acc: 0.8636\n",
      "Epoch 659/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.3985 - acc: 0.8604 - val_loss: 0.5792 - val_acc: 0.7557\n",
      "Epoch 660/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.3928 - acc: 0.8547 - val_loss: 0.5633 - val_acc: 0.8125\n",
      "Epoch 661/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3838 - acc: 0.8732 - val_loss: 0.6158 - val_acc: 0.7330\n",
      "Epoch 662/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.4089 - acc: 0.8561 - val_loss: 0.5252 - val_acc: 0.8352\n",
      "Epoch 663/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.3914 - acc: 0.8533 - val_loss: 0.5780 - val_acc: 0.7443\n",
      "Epoch 664/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.3505 - acc: 0.8746 - val_loss: 0.5569 - val_acc: 0.8466\n",
      "Epoch 665/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.3944 - acc: 0.8689 - val_loss: 0.6900 - val_acc: 0.7216\n",
      "Epoch 666/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.4710 - acc: 0.8604 - val_loss: 0.5335 - val_acc: 0.8239\n",
      "Epoch 667/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.4193 - acc: 0.8732 - val_loss: 0.6383 - val_acc: 0.7273\n",
      "Epoch 668/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.4377 - acc: 0.8575 - val_loss: 0.6425 - val_acc: 0.7727\n",
      "Epoch 669/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.3869 - acc: 0.8590 - val_loss: 0.6212 - val_acc: 0.7784\n",
      "Epoch 670/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.3956 - acc: 0.8732 - val_loss: 0.6243 - val_acc: 0.7727\n",
      "Epoch 671/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4375 - acc: 0.8604 - val_loss: 0.5632 - val_acc: 0.8466\n",
      "Epoch 672/3000\n",
      "702/702 [==============================] - 0s 551us/sample - loss: 0.5562 - acc: 0.8319 - val_loss: 0.5446 - val_acc: 0.8409\n",
      "Epoch 673/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4457 - acc: 0.8476 - val_loss: 0.6188 - val_acc: 0.7557\n",
      "Epoch 674/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.5357 - acc: 0.8077 - val_loss: 0.6122 - val_acc: 0.7102\n",
      "Epoch 675/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.4374 - acc: 0.8533 - val_loss: 0.5599 - val_acc: 0.7955\n",
      "Epoch 676/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.3996 - acc: 0.8632 - val_loss: 0.5117 - val_acc: 0.8182\n",
      "Epoch 677/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.4460 - acc: 0.8348 - val_loss: 0.7259 - val_acc: 0.6989\n",
      "Epoch 678/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.5593 - acc: 0.8048 - val_loss: 0.6672 - val_acc: 0.8011\n",
      "Epoch 679/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.4726 - acc: 0.8276 - val_loss: 0.6086 - val_acc: 0.7670\n",
      "Epoch 680/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.4549 - acc: 0.8333 - val_loss: 0.6265 - val_acc: 0.7273\n",
      "Epoch 681/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3969 - acc: 0.8547 - val_loss: 0.5797 - val_acc: 0.7784\n",
      "Epoch 682/3000\n",
      "702/702 [==============================] - 0s 551us/sample - loss: 0.4650 - acc: 0.8575 - val_loss: 0.7239 - val_acc: 0.6761\n",
      "Epoch 683/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.4847 - acc: 0.8248 - val_loss: 0.5600 - val_acc: 0.8352\n",
      "Epoch 684/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.4524 - acc: 0.8462 - val_loss: 0.7165 - val_acc: 0.6932\n",
      "Epoch 685/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.4698 - acc: 0.8305 - val_loss: 0.6269 - val_acc: 0.7273\n",
      "Epoch 686/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.3901 - acc: 0.8661 - val_loss: 0.5857 - val_acc: 0.7557\n",
      "Epoch 687/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.4102 - acc: 0.8675 - val_loss: 0.5900 - val_acc: 0.7500\n",
      "Epoch 688/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.4269 - acc: 0.8632 - val_loss: 0.6448 - val_acc: 0.7386\n",
      "Epoch 689/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3995 - acc: 0.8761 - val_loss: 0.6041 - val_acc: 0.7784\n",
      "Epoch 690/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3548 - acc: 0.8803 - val_loss: 0.6383 - val_acc: 0.7330\n",
      "Epoch 691/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3841 - acc: 0.8561 - val_loss: 0.5735 - val_acc: 0.7898\n",
      "Epoch 692/3000\n",
      "702/702 [==============================] - 0s 556us/sample - loss: 0.3396 - acc: 0.8775 - val_loss: 0.6802 - val_acc: 0.6989\n",
      "Epoch 693/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.3759 - acc: 0.8647 - val_loss: 0.6494 - val_acc: 0.7557\n",
      "Epoch 694/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.3624 - acc: 0.8604 - val_loss: 0.5937 - val_acc: 0.7443\n",
      "Epoch 695/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.3369 - acc: 0.8974 - val_loss: 0.5471 - val_acc: 0.7670\n",
      "Epoch 696/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.3244 - acc: 0.8803 - val_loss: 0.5278 - val_acc: 0.7784\n",
      "Epoch 697/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3538 - acc: 0.8789 - val_loss: 0.5222 - val_acc: 0.8011\n",
      "Epoch 698/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3154 - acc: 0.9046 - val_loss: 0.5166 - val_acc: 0.7841\n",
      "Epoch 699/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.3486 - acc: 0.8846 - val_loss: 0.5746 - val_acc: 0.7784\n",
      "Epoch 700/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.4039 - acc: 0.8746 - val_loss: 0.5541 - val_acc: 0.8068\n",
      "Epoch 701/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3527 - acc: 0.8846 - val_loss: 0.5422 - val_acc: 0.7955\n",
      "Epoch 702/3000\n",
      "702/702 [==============================] - 0s 549us/sample - loss: 0.3355 - acc: 0.8932 - val_loss: 0.5484 - val_acc: 0.7841\n",
      "Epoch 703/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.3295 - acc: 0.8889 - val_loss: 0.5187 - val_acc: 0.7898\n",
      "Epoch 704/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.3585 - acc: 0.8846 - val_loss: 0.5870 - val_acc: 0.7386\n",
      "Epoch 705/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.3466 - acc: 0.8803 - val_loss: 0.4858 - val_acc: 0.8523\n",
      "Epoch 706/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.3873 - acc: 0.8704 - val_loss: 0.6176 - val_acc: 0.7216\n",
      "Epoch 707/3000\n",
      "702/702 [==============================] - 0s 534us/sample - loss: 0.3809 - acc: 0.8533 - val_loss: 0.4996 - val_acc: 0.7955\n",
      "Epoch 708/3000\n",
      "702/702 [==============================] - 0s 542us/sample - loss: 0.3472 - acc: 0.8803 - val_loss: 0.5369 - val_acc: 0.7500\n",
      "Epoch 709/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.3426 - acc: 0.8832 - val_loss: 0.5011 - val_acc: 0.8182\n",
      "Epoch 710/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.3405 - acc: 0.8974 - val_loss: 0.6041 - val_acc: 0.7557\n",
      "Epoch 711/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3543 - acc: 0.8846 - val_loss: 0.6623 - val_acc: 0.7500\n",
      "Epoch 712/3000\n",
      "702/702 [==============================] - 0s 601us/sample - loss: 0.3273 - acc: 0.8803 - val_loss: 0.6016 - val_acc: 0.7727\n",
      "Epoch 713/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.3660 - acc: 0.8775 - val_loss: 0.5781 - val_acc: 0.7386\n",
      "Epoch 714/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.3593 - acc: 0.8647 - val_loss: 0.5625 - val_acc: 0.7670\n",
      "Epoch 715/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3405 - acc: 0.8832 - val_loss: 0.5319 - val_acc: 0.7955\n",
      "Epoch 716/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.3397 - acc: 0.8775 - val_loss: 0.6834 - val_acc: 0.6875\n",
      "Epoch 717/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.4150 - acc: 0.8419 - val_loss: 0.5731 - val_acc: 0.7670\n",
      "Epoch 718/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.3891 - acc: 0.8689 - val_loss: 0.6318 - val_acc: 0.7614\n",
      "Epoch 719/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.3861 - acc: 0.8675 - val_loss: 0.7110 - val_acc: 0.7045\n",
      "Epoch 720/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.3474 - acc: 0.9031 - val_loss: 0.6058 - val_acc: 0.8352\n",
      "Epoch 721/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3830 - acc: 0.8533 - val_loss: 0.7817 - val_acc: 0.7102\n",
      "Epoch 722/3000\n",
      "702/702 [==============================] - 0s 552us/sample - loss: 0.3222 - acc: 0.8932 - val_loss: 0.6722 - val_acc: 0.7670\n",
      "Epoch 723/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.5495 - acc: 0.8376 - val_loss: 0.9731 - val_acc: 0.6591\n",
      "Epoch 724/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.7842 - acc: 0.7778 - val_loss: 0.5839 - val_acc: 0.8239\n",
      "Epoch 725/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.5717 - acc: 0.8362 - val_loss: 0.6653 - val_acc: 0.7102\n",
      "Epoch 726/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.5063 - acc: 0.7920 - val_loss: 0.7233 - val_acc: 0.7102\n",
      "Epoch 727/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4928 - acc: 0.8519 - val_loss: 0.6860 - val_acc: 0.8239\n",
      "Epoch 728/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.4433 - acc: 0.8490 - val_loss: 0.7997 - val_acc: 0.6989\n",
      "Epoch 729/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.4473 - acc: 0.8462 - val_loss: 0.7510 - val_acc: 0.7330\n",
      "Epoch 730/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4139 - acc: 0.8547 - val_loss: 0.6690 - val_acc: 0.7670\n",
      "Epoch 731/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4241 - acc: 0.8561 - val_loss: 0.6723 - val_acc: 0.7557\n",
      "Epoch 732/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 0.3746 - acc: 0.8704 - val_loss: 0.6422 - val_acc: 0.7500\n",
      "Epoch 733/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.3980 - acc: 0.8661 - val_loss: 0.6354 - val_acc: 0.7500\n",
      "Epoch 734/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.4338 - acc: 0.8732 - val_loss: 0.6520 - val_acc: 0.7273\n",
      "Epoch 735/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.3600 - acc: 0.8718 - val_loss: 0.6870 - val_acc: 0.7216\n",
      "Epoch 736/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.3272 - acc: 0.8846 - val_loss: 0.6453 - val_acc: 0.7443\n",
      "Epoch 737/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.3227 - acc: 0.8974 - val_loss: 0.6613 - val_acc: 0.7102\n",
      "Epoch 738/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.3445 - acc: 0.8618 - val_loss: 0.6532 - val_acc: 0.7386\n",
      "Epoch 739/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.3731 - acc: 0.8704 - val_loss: 0.6119 - val_acc: 0.7784\n",
      "Epoch 740/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.3419 - acc: 0.8903 - val_loss: 0.6024 - val_acc: 0.7614\n",
      "Epoch 741/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3485 - acc: 0.8761 - val_loss: 0.5643 - val_acc: 0.7955\n",
      "Epoch 742/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.3579 - acc: 0.8761 - val_loss: 0.6157 - val_acc: 0.7614\n",
      "Epoch 743/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.4049 - acc: 0.8761 - val_loss: 0.6272 - val_acc: 0.7557\n",
      "Epoch 744/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.3649 - acc: 0.8704 - val_loss: 0.6201 - val_acc: 0.7216\n",
      "Epoch 745/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.3719 - acc: 0.8732 - val_loss: 0.6007 - val_acc: 0.7443\n",
      "Epoch 746/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.3837 - acc: 0.8618 - val_loss: 0.5182 - val_acc: 0.7955\n",
      "Epoch 747/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.3113 - acc: 0.8917 - val_loss: 0.6222 - val_acc: 0.7045\n",
      "Epoch 748/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.3843 - acc: 0.8775 - val_loss: 0.5715 - val_acc: 0.7784\n",
      "Epoch 749/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3966 - acc: 0.8775 - val_loss: 0.6791 - val_acc: 0.7443\n",
      "Epoch 750/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.3670 - acc: 0.8889 - val_loss: 0.6023 - val_acc: 0.7727\n",
      "Epoch 751/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3830 - acc: 0.8746 - val_loss: 0.5738 - val_acc: 0.7557\n",
      "Epoch 752/3000\n",
      "702/702 [==============================] - 0s 573us/sample - loss: 0.3237 - acc: 0.8903 - val_loss: 0.4611 - val_acc: 0.8409\n",
      "Epoch 753/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.3946 - acc: 0.8846 - val_loss: 0.6290 - val_acc: 0.7273\n",
      "Epoch 754/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.3952 - acc: 0.8604 - val_loss: 0.5429 - val_acc: 0.8182\n",
      "Epoch 755/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3479 - acc: 0.8718 - val_loss: 0.6007 - val_acc: 0.7500\n",
      "Epoch 756/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.3398 - acc: 0.8775 - val_loss: 0.5925 - val_acc: 0.7670\n",
      "Epoch 757/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.3805 - acc: 0.8575 - val_loss: 0.6601 - val_acc: 0.7045\n",
      "Epoch 758/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.3667 - acc: 0.8746 - val_loss: 0.5201 - val_acc: 0.8011\n",
      "Epoch 759/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.3529 - acc: 0.8860 - val_loss: 0.6182 - val_acc: 0.7386\n",
      "Epoch 760/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.3482 - acc: 0.8590 - val_loss: 0.5738 - val_acc: 0.7443\n",
      "Epoch 761/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3728 - acc: 0.8761 - val_loss: 0.5839 - val_acc: 0.7557\n",
      "Epoch 762/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.3620 - acc: 0.8689 - val_loss: 0.6468 - val_acc: 0.7386\n",
      "Epoch 763/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3637 - acc: 0.8746 - val_loss: 0.6215 - val_acc: 0.7500\n",
      "Epoch 764/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3083 - acc: 0.8889 - val_loss: 0.5876 - val_acc: 0.7500\n",
      "Epoch 765/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.3050 - acc: 0.8903 - val_loss: 0.5618 - val_acc: 0.7557\n",
      "Epoch 766/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3215 - acc: 0.9103 - val_loss: 0.5956 - val_acc: 0.7159\n",
      "Epoch 767/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3259 - acc: 0.8803 - val_loss: 0.5917 - val_acc: 0.7273\n",
      "Epoch 768/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3365 - acc: 0.8960 - val_loss: 0.5774 - val_acc: 0.7557\n",
      "Epoch 769/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.2585 - acc: 0.9117 - val_loss: 0.5452 - val_acc: 0.7557\n",
      "Epoch 770/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3045 - acc: 0.8917 - val_loss: 0.5487 - val_acc: 0.7898\n",
      "Epoch 771/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3396 - acc: 0.8989 - val_loss: 0.6155 - val_acc: 0.7557\n",
      "Epoch 772/3000\n",
      "702/702 [==============================] - 0s 585us/sample - loss: 0.3119 - acc: 0.8875 - val_loss: 0.6185 - val_acc: 0.7727\n",
      "Epoch 773/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.3096 - acc: 0.8889 - val_loss: 0.6204 - val_acc: 0.7330\n",
      "Epoch 774/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.3541 - acc: 0.8875 - val_loss: 0.4882 - val_acc: 0.8239\n",
      "Epoch 775/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.3505 - acc: 0.8875 - val_loss: 0.6334 - val_acc: 0.7159\n",
      "Epoch 776/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.3321 - acc: 0.8746 - val_loss: 0.6099 - val_acc: 0.7330\n",
      "Epoch 777/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.3721 - acc: 0.8761 - val_loss: 0.5803 - val_acc: 0.7841\n",
      "Epoch 778/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.3836 - acc: 0.8818 - val_loss: 0.5666 - val_acc: 0.7727\n",
      "Epoch 779/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.3052 - acc: 0.8946 - val_loss: 0.6393 - val_acc: 0.7557\n",
      "Epoch 780/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3485 - acc: 0.8689 - val_loss: 0.7098 - val_acc: 0.6932\n",
      "Epoch 781/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3625 - acc: 0.8632 - val_loss: 0.5641 - val_acc: 0.7727\n",
      "Epoch 782/3000\n",
      "702/702 [==============================] - 0s 560us/sample - loss: 0.3611 - acc: 0.8761 - val_loss: 0.6384 - val_acc: 0.7500\n",
      "Epoch 783/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.3812 - acc: 0.8761 - val_loss: 0.5995 - val_acc: 0.8011\n",
      "Epoch 784/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.3006 - acc: 0.9003 - val_loss: 0.6359 - val_acc: 0.7841\n",
      "Epoch 785/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.3240 - acc: 0.8789 - val_loss: 0.5203 - val_acc: 0.8295\n",
      "Epoch 786/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.3454 - acc: 0.8960 - val_loss: 0.6425 - val_acc: 0.7216\n",
      "Epoch 787/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.4011 - acc: 0.8547 - val_loss: 0.5434 - val_acc: 0.7784\n",
      "Epoch 788/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.4178 - acc: 0.8689 - val_loss: 0.5093 - val_acc: 0.8295\n",
      "Epoch 789/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3080 - acc: 0.8903 - val_loss: 0.6730 - val_acc: 0.7159\n",
      "Epoch 790/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3946 - acc: 0.8519 - val_loss: 0.6040 - val_acc: 0.7330\n",
      "Epoch 791/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3088 - acc: 0.8989 - val_loss: 0.5148 - val_acc: 0.8182\n",
      "Epoch 792/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 0.3530 - acc: 0.8989 - val_loss: 0.7077 - val_acc: 0.7045\n",
      "Epoch 793/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3515 - acc: 0.8647 - val_loss: 0.5459 - val_acc: 0.8125\n",
      "Epoch 794/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.4070 - acc: 0.8789 - val_loss: 0.7083 - val_acc: 0.7273\n",
      "Epoch 795/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.3244 - acc: 0.8832 - val_loss: 0.5511 - val_acc: 0.7784\n",
      "Epoch 796/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3067 - acc: 0.8989 - val_loss: 0.5654 - val_acc: 0.7898\n",
      "Epoch 797/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.3435 - acc: 0.8860 - val_loss: 0.6203 - val_acc: 0.7273\n",
      "Epoch 798/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.2847 - acc: 0.8960 - val_loss: 0.5181 - val_acc: 0.8011\n",
      "Epoch 799/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.2955 - acc: 0.8946 - val_loss: 0.5820 - val_acc: 0.7500\n",
      "Epoch 800/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.3375 - acc: 0.8860 - val_loss: 0.5929 - val_acc: 0.7500\n",
      "Epoch 801/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3186 - acc: 0.8818 - val_loss: 0.5968 - val_acc: 0.8125\n",
      "Epoch 802/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 0.3555 - acc: 0.8789 - val_loss: 0.5874 - val_acc: 0.7557\n",
      "Epoch 803/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.3459 - acc: 0.8960 - val_loss: 0.6124 - val_acc: 0.7898\n",
      "Epoch 804/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3358 - acc: 0.8803 - val_loss: 0.5834 - val_acc: 0.8011\n",
      "Epoch 805/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3440 - acc: 0.8946 - val_loss: 0.5859 - val_acc: 0.8239\n",
      "Epoch 806/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.2874 - acc: 0.9017 - val_loss: 0.6168 - val_acc: 0.7670\n",
      "Epoch 807/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.2835 - acc: 0.8903 - val_loss: 0.6636 - val_acc: 0.6989\n",
      "Epoch 808/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.3658 - acc: 0.8775 - val_loss: 0.5373 - val_acc: 0.8182\n",
      "Epoch 809/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.3151 - acc: 0.8832 - val_loss: 0.7441 - val_acc: 0.7159\n",
      "Epoch 810/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.3789 - acc: 0.8732 - val_loss: 0.6102 - val_acc: 0.7727\n",
      "Epoch 811/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.2949 - acc: 0.9046 - val_loss: 0.5764 - val_acc: 0.7784\n",
      "Epoch 812/3000\n",
      "702/702 [==============================] - 0s 567us/sample - loss: 0.3065 - acc: 0.8960 - val_loss: 0.5999 - val_acc: 0.7443\n",
      "Epoch 813/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3123 - acc: 0.8917 - val_loss: 0.6092 - val_acc: 0.7784\n",
      "Epoch 814/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.3428 - acc: 0.8846 - val_loss: 0.7226 - val_acc: 0.7159\n",
      "Epoch 815/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3330 - acc: 0.8789 - val_loss: 0.5727 - val_acc: 0.8011\n",
      "Epoch 816/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3003 - acc: 0.9060 - val_loss: 0.6258 - val_acc: 0.7443\n",
      "Epoch 817/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.3396 - acc: 0.8946 - val_loss: 0.5696 - val_acc: 0.8011\n",
      "Epoch 818/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.3609 - acc: 0.8803 - val_loss: 0.7002 - val_acc: 0.7045\n",
      "Epoch 819/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.3914 - acc: 0.8704 - val_loss: 0.6323 - val_acc: 0.8125\n",
      "Epoch 820/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.2956 - acc: 0.8917 - val_loss: 0.7367 - val_acc: 0.7159\n",
      "Epoch 821/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3403 - acc: 0.8661 - val_loss: 0.6365 - val_acc: 0.8352\n",
      "Epoch 822/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 0.3498 - acc: 0.8818 - val_loss: 0.6163 - val_acc: 0.7841\n",
      "Epoch 823/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.3233 - acc: 0.8974 - val_loss: 0.5303 - val_acc: 0.7557\n",
      "Epoch 824/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.3446 - acc: 0.8746 - val_loss: 0.5750 - val_acc: 0.7443\n",
      "Epoch 825/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.4025 - acc: 0.8746 - val_loss: 0.5479 - val_acc: 0.7784\n",
      "Epoch 826/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.3521 - acc: 0.8718 - val_loss: 0.7288 - val_acc: 0.7159\n",
      "Epoch 827/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3647 - acc: 0.8789 - val_loss: 0.7081 - val_acc: 0.7841\n",
      "Epoch 828/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.3804 - acc: 0.8632 - val_loss: 0.7174 - val_acc: 0.6989\n",
      "Epoch 829/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3475 - acc: 0.8746 - val_loss: 0.4861 - val_acc: 0.8409\n",
      "Epoch 830/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.3768 - acc: 0.8746 - val_loss: 0.6579 - val_acc: 0.7045\n",
      "Epoch 831/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4031 - acc: 0.8704 - val_loss: 0.5777 - val_acc: 0.7841\n",
      "Epoch 832/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.3698 - acc: 0.8675 - val_loss: 0.6794 - val_acc: 0.7273\n",
      "Epoch 833/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.3635 - acc: 0.8846 - val_loss: 0.5754 - val_acc: 0.7955\n",
      "Epoch 834/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3973 - acc: 0.8846 - val_loss: 0.6641 - val_acc: 0.7386\n",
      "Epoch 835/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.3340 - acc: 0.8718 - val_loss: 0.6668 - val_acc: 0.7443\n",
      "Epoch 836/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.3467 - acc: 0.8989 - val_loss: 0.6263 - val_acc: 0.7727\n",
      "Epoch 837/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.3938 - acc: 0.8803 - val_loss: 0.6648 - val_acc: 0.7500\n",
      "Epoch 838/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3754 - acc: 0.8889 - val_loss: 0.6384 - val_acc: 0.7557\n",
      "Epoch 839/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3644 - acc: 0.9031 - val_loss: 0.6323 - val_acc: 0.7330\n",
      "Epoch 840/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.3421 - acc: 0.8974 - val_loss: 0.6139 - val_acc: 0.7841\n",
      "Epoch 841/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3742 - acc: 0.8775 - val_loss: 0.6079 - val_acc: 0.7557\n",
      "Epoch 842/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.3623 - acc: 0.8704 - val_loss: 0.5874 - val_acc: 0.7841\n",
      "Epoch 843/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3344 - acc: 0.9003 - val_loss: 0.5807 - val_acc: 0.7557\n",
      "Epoch 844/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.3688 - acc: 0.8775 - val_loss: 0.6207 - val_acc: 0.7330\n",
      "Epoch 845/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.3909 - acc: 0.8732 - val_loss: 0.5851 - val_acc: 0.7841\n",
      "Epoch 846/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.3424 - acc: 0.8860 - val_loss: 0.6705 - val_acc: 0.7159\n",
      "Epoch 847/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.3166 - acc: 0.8989 - val_loss: 0.7003 - val_acc: 0.7557\n",
      "Epoch 848/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.2939 - acc: 0.9003 - val_loss: 0.6629 - val_acc: 0.7670\n",
      "Epoch 849/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.2775 - acc: 0.9103 - val_loss: 0.6773 - val_acc: 0.7386\n",
      "Epoch 850/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.3440 - acc: 0.8832 - val_loss: 0.6431 - val_acc: 0.8182\n",
      "Epoch 851/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3386 - acc: 0.8974 - val_loss: 0.6983 - val_acc: 0.7670\n",
      "Epoch 852/3000\n",
      "702/702 [==============================] - 0s 568us/sample - loss: 0.3265 - acc: 0.8960 - val_loss: 0.6800 - val_acc: 0.7443\n",
      "Epoch 853/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.3122 - acc: 0.8960 - val_loss: 0.6716 - val_acc: 0.7216\n",
      "Epoch 854/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.3669 - acc: 0.8789 - val_loss: 0.7548 - val_acc: 0.6818\n",
      "Epoch 855/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.3921 - acc: 0.8618 - val_loss: 0.6514 - val_acc: 0.7330\n",
      "Epoch 856/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3716 - acc: 0.8818 - val_loss: 0.7512 - val_acc: 0.7102\n",
      "Epoch 857/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.4050 - acc: 0.8746 - val_loss: 0.6566 - val_acc: 0.7898\n",
      "Epoch 858/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.3663 - acc: 0.8746 - val_loss: 0.6396 - val_acc: 0.7045\n",
      "Epoch 859/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.3269 - acc: 0.8903 - val_loss: 0.6279 - val_acc: 0.7216\n",
      "Epoch 860/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3670 - acc: 0.8661 - val_loss: 0.6185 - val_acc: 0.7330\n",
      "Epoch 861/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3250 - acc: 0.8946 - val_loss: 0.6986 - val_acc: 0.7159\n",
      "Epoch 862/3000\n",
      "702/702 [==============================] - 0s 568us/sample - loss: 0.3872 - acc: 0.8718 - val_loss: 0.6197 - val_acc: 0.7955\n",
      "Epoch 863/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.3166 - acc: 0.8889 - val_loss: 0.7017 - val_acc: 0.7159\n",
      "Epoch 864/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3267 - acc: 0.8846 - val_loss: 0.6512 - val_acc: 0.8125\n",
      "Epoch 865/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.3148 - acc: 0.8917 - val_loss: 0.6862 - val_acc: 0.7443\n",
      "Epoch 866/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.3114 - acc: 0.8989 - val_loss: 0.6075 - val_acc: 0.8182\n",
      "Epoch 867/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.3260 - acc: 0.9031 - val_loss: 0.7737 - val_acc: 0.7045\n",
      "Epoch 868/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.3034 - acc: 0.8974 - val_loss: 0.7902 - val_acc: 0.6989\n",
      "Epoch 869/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.3830 - acc: 0.8732 - val_loss: 0.5818 - val_acc: 0.8068\n",
      "Epoch 870/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.2965 - acc: 0.8875 - val_loss: 0.6765 - val_acc: 0.7216\n",
      "Epoch 871/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3180 - acc: 0.8875 - val_loss: 0.5761 - val_acc: 0.7727\n",
      "Epoch 872/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 0.3340 - acc: 0.8917 - val_loss: 0.7004 - val_acc: 0.7045\n",
      "Epoch 873/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3273 - acc: 0.8960 - val_loss: 0.6933 - val_acc: 0.7443\n",
      "Epoch 874/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.3711 - acc: 0.8875 - val_loss: 0.7387 - val_acc: 0.7273\n",
      "Epoch 875/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.3327 - acc: 0.8960 - val_loss: 0.6903 - val_acc: 0.7500\n",
      "Epoch 876/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.2632 - acc: 0.9074 - val_loss: 0.7670 - val_acc: 0.7159\n",
      "Epoch 877/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.3082 - acc: 0.8960 - val_loss: 0.6922 - val_acc: 0.7386\n",
      "Epoch 878/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.2967 - acc: 0.8989 - val_loss: 0.7248 - val_acc: 0.7273\n",
      "Epoch 879/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.2835 - acc: 0.9088 - val_loss: 0.6070 - val_acc: 0.8068\n",
      "Epoch 880/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.3671 - acc: 0.8789 - val_loss: 0.7172 - val_acc: 0.7216\n",
      "Epoch 881/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3424 - acc: 0.8632 - val_loss: 0.7432 - val_acc: 0.7386\n",
      "Epoch 882/3000\n",
      "702/702 [==============================] - 0s 569us/sample - loss: 0.2763 - acc: 0.9031 - val_loss: 0.6659 - val_acc: 0.7727\n",
      "Epoch 883/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.3104 - acc: 0.8989 - val_loss: 0.6982 - val_acc: 0.7159\n",
      "Epoch 884/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3097 - acc: 0.8917 - val_loss: 0.6109 - val_acc: 0.7784\n",
      "Epoch 885/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.2680 - acc: 0.9088 - val_loss: 0.6602 - val_acc: 0.7500\n",
      "Epoch 886/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.2756 - acc: 0.8974 - val_loss: 0.6458 - val_acc: 0.7557\n",
      "Epoch 887/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.2695 - acc: 0.9074 - val_loss: 0.6168 - val_acc: 0.8011\n",
      "Epoch 888/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.2981 - acc: 0.9031 - val_loss: 0.6018 - val_acc: 0.8068\n",
      "Epoch 889/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.2873 - acc: 0.9046 - val_loss: 0.7469 - val_acc: 0.7216\n",
      "Epoch 890/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.3788 - acc: 0.8419 - val_loss: 0.6648 - val_acc: 0.7784\n",
      "Epoch 891/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3536 - acc: 0.8761 - val_loss: 0.7163 - val_acc: 0.7386\n",
      "Epoch 892/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.3132 - acc: 0.8932 - val_loss: 0.6820 - val_acc: 0.7443\n",
      "Epoch 893/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3205 - acc: 0.9031 - val_loss: 0.7214 - val_acc: 0.7443\n",
      "Epoch 894/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.3362 - acc: 0.8960 - val_loss: 0.6675 - val_acc: 0.7614\n",
      "Epoch 895/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.3165 - acc: 0.8989 - val_loss: 0.6922 - val_acc: 0.7727\n",
      "Epoch 896/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.2992 - acc: 0.8974 - val_loss: 0.6602 - val_acc: 0.7727\n",
      "Epoch 897/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.2850 - acc: 0.9003 - val_loss: 0.7028 - val_acc: 0.7273\n",
      "Epoch 898/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3072 - acc: 0.9074 - val_loss: 0.6378 - val_acc: 0.7670\n",
      "Epoch 899/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3056 - acc: 0.8889 - val_loss: 0.6353 - val_acc: 0.7784\n",
      "Epoch 900/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3066 - acc: 0.9074 - val_loss: 0.6006 - val_acc: 0.7727\n",
      "Epoch 901/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3441 - acc: 0.8989 - val_loss: 0.6671 - val_acc: 0.7386\n",
      "Epoch 902/3000\n",
      "702/702 [==============================] - 0s 559us/sample - loss: 0.3418 - acc: 0.8917 - val_loss: 0.5671 - val_acc: 0.8068\n",
      "Epoch 903/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.2754 - acc: 0.8917 - val_loss: 0.6686 - val_acc: 0.7159\n",
      "Epoch 904/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3027 - acc: 0.8960 - val_loss: 0.8452 - val_acc: 0.7216\n",
      "Epoch 905/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.2920 - acc: 0.9003 - val_loss: 0.8320 - val_acc: 0.7898\n",
      "Epoch 906/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.3033 - acc: 0.9046 - val_loss: 0.9047 - val_acc: 0.7500\n",
      "Epoch 907/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.2781 - acc: 0.9160 - val_loss: 0.7947 - val_acc: 0.7557\n",
      "Epoch 908/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.2796 - acc: 0.9117 - val_loss: 0.8230 - val_acc: 0.7273\n",
      "Epoch 909/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.2742 - acc: 0.9031 - val_loss: 0.7326 - val_acc: 0.7614\n",
      "Epoch 910/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.3064 - acc: 0.8860 - val_loss: 0.7730 - val_acc: 0.7386\n",
      "Epoch 911/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3049 - acc: 0.8946 - val_loss: 0.8278 - val_acc: 0.7159\n",
      "Epoch 912/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 0.3319 - acc: 0.8932 - val_loss: 0.7811 - val_acc: 0.7784\n",
      "Epoch 913/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.2610 - acc: 0.9288 - val_loss: 0.9579 - val_acc: 0.6989\n",
      "Epoch 914/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3022 - acc: 0.8989 - val_loss: 0.6802 - val_acc: 0.7102\n",
      "Epoch 915/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.2933 - acc: 0.9031 - val_loss: 0.7466 - val_acc: 0.6818\n",
      "Epoch 916/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.3404 - acc: 0.8746 - val_loss: 0.5814 - val_acc: 0.7386\n",
      "Epoch 917/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.2966 - acc: 0.8917 - val_loss: 0.6507 - val_acc: 0.6989\n",
      "Epoch 918/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.2982 - acc: 0.8960 - val_loss: 0.6779 - val_acc: 0.7216\n",
      "Epoch 919/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.2856 - acc: 0.8974 - val_loss: 0.7496 - val_acc: 0.6989\n",
      "Epoch 920/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.2912 - acc: 0.9074 - val_loss: 0.6500 - val_acc: 0.7557\n",
      "Epoch 921/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.2835 - acc: 0.8989 - val_loss: 0.6584 - val_acc: 0.7330\n",
      "Epoch 922/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 0.2631 - acc: 0.9103 - val_loss: 0.6636 - val_acc: 0.7386\n",
      "Epoch 923/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.2901 - acc: 0.9031 - val_loss: 0.6507 - val_acc: 0.7557\n",
      "Epoch 924/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.2893 - acc: 0.9088 - val_loss: 0.7499 - val_acc: 0.7045\n",
      "Epoch 925/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.3066 - acc: 0.8974 - val_loss: 0.6018 - val_acc: 0.7784\n",
      "Epoch 926/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.3586 - acc: 0.8704 - val_loss: 0.6270 - val_acc: 0.7386\n",
      "Epoch 927/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.2960 - acc: 0.8917 - val_loss: 0.6035 - val_acc: 0.7216\n",
      "Epoch 928/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.2863 - acc: 0.9145 - val_loss: 0.6235 - val_acc: 0.7443\n",
      "Epoch 929/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.2841 - acc: 0.9074 - val_loss: 0.6281 - val_acc: 0.7386\n",
      "Epoch 930/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2727 - acc: 0.9145 - val_loss: 0.5997 - val_acc: 0.7727\n",
      "Epoch 931/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.2890 - acc: 0.8917 - val_loss: 0.5888 - val_acc: 0.7670\n",
      "Epoch 932/3000\n",
      "702/702 [==============================] - 0s 556us/sample - loss: 0.2904 - acc: 0.9060 - val_loss: 0.5322 - val_acc: 0.7784\n",
      "Epoch 933/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.2971 - acc: 0.8860 - val_loss: 0.6189 - val_acc: 0.7330\n",
      "Epoch 934/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.3249 - acc: 0.8960 - val_loss: 0.6476 - val_acc: 0.7557\n",
      "Epoch 935/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.2724 - acc: 0.9046 - val_loss: 0.6887 - val_acc: 0.7159\n",
      "Epoch 936/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.2668 - acc: 0.9060 - val_loss: 0.6312 - val_acc: 0.7273\n",
      "Epoch 937/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.2888 - acc: 0.8946 - val_loss: 0.5922 - val_acc: 0.7500\n",
      "Epoch 938/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.2879 - acc: 0.9046 - val_loss: 0.5984 - val_acc: 0.7670\n",
      "Epoch 939/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3187 - acc: 0.8860 - val_loss: 0.5877 - val_acc: 0.7727\n",
      "Epoch 940/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3238 - acc: 0.8989 - val_loss: 0.6413 - val_acc: 0.7557\n",
      "Epoch 941/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3109 - acc: 0.8803 - val_loss: 0.6487 - val_acc: 0.7386\n",
      "Epoch 942/3000\n",
      "702/702 [==============================] - 0s 561us/sample - loss: 0.2470 - acc: 0.9046 - val_loss: 0.5460 - val_acc: 0.8068\n",
      "Epoch 943/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3040 - acc: 0.8946 - val_loss: 0.6397 - val_acc: 0.7216\n",
      "Epoch 944/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.2715 - acc: 0.9074 - val_loss: 0.5606 - val_acc: 0.7784\n",
      "Epoch 945/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.2830 - acc: 0.9031 - val_loss: 0.6425 - val_acc: 0.7784\n",
      "Epoch 946/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.3082 - acc: 0.9031 - val_loss: 0.6629 - val_acc: 0.7614\n",
      "Epoch 947/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.3405 - acc: 0.8974 - val_loss: 0.6018 - val_acc: 0.7727\n",
      "Epoch 948/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.3449 - acc: 0.8917 - val_loss: 0.6517 - val_acc: 0.7557\n",
      "Epoch 949/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.3172 - acc: 0.8932 - val_loss: 0.7496 - val_acc: 0.6932\n",
      "Epoch 950/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3034 - acc: 0.8932 - val_loss: 0.6623 - val_acc: 0.7386\n",
      "Epoch 951/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3195 - acc: 0.8832 - val_loss: 0.6537 - val_acc: 0.7159\n",
      "Epoch 952/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.2965 - acc: 0.8818 - val_loss: 0.6133 - val_acc: 0.7330\n",
      "Epoch 953/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3177 - acc: 0.8860 - val_loss: 0.4988 - val_acc: 0.8068\n",
      "Epoch 954/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.3341 - acc: 0.8860 - val_loss: 0.7468 - val_acc: 0.6932\n",
      "Epoch 955/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4039 - acc: 0.8362 - val_loss: 0.7165 - val_acc: 0.7557\n",
      "Epoch 956/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.3823 - acc: 0.8732 - val_loss: 0.8213 - val_acc: 0.7159\n",
      "Epoch 957/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.3646 - acc: 0.8732 - val_loss: 0.7901 - val_acc: 0.7045\n",
      "Epoch 958/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.3302 - acc: 0.8860 - val_loss: 0.6260 - val_acc: 0.7614\n",
      "Epoch 959/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.4184 - acc: 0.8732 - val_loss: 0.7111 - val_acc: 0.6932\n",
      "Epoch 960/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.3356 - acc: 0.8803 - val_loss: 0.5394 - val_acc: 0.8352\n",
      "Epoch 961/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3463 - acc: 0.8846 - val_loss: 0.7646 - val_acc: 0.6932\n",
      "Epoch 962/3000\n",
      "702/702 [==============================] - 0s 560us/sample - loss: 0.3426 - acc: 0.8718 - val_loss: 0.7207 - val_acc: 0.7670\n",
      "Epoch 963/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.3416 - acc: 0.8746 - val_loss: 0.7760 - val_acc: 0.7330\n",
      "Epoch 964/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3575 - acc: 0.8732 - val_loss: 0.7023 - val_acc: 0.7102\n",
      "Epoch 965/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3753 - acc: 0.8917 - val_loss: 0.6114 - val_acc: 0.7443\n",
      "Epoch 966/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.3515 - acc: 0.8704 - val_loss: 0.7420 - val_acc: 0.7045\n",
      "Epoch 967/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3360 - acc: 0.8917 - val_loss: 0.7551 - val_acc: 0.6989\n",
      "Epoch 968/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.3315 - acc: 0.8818 - val_loss: 0.6891 - val_acc: 0.7273\n",
      "Epoch 969/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3208 - acc: 0.8903 - val_loss: 0.7674 - val_acc: 0.6818\n",
      "Epoch 970/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3087 - acc: 0.8903 - val_loss: 0.7037 - val_acc: 0.7443\n",
      "Epoch 971/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.2905 - acc: 0.9046 - val_loss: 0.6951 - val_acc: 0.6989\n",
      "Epoch 972/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 0.2939 - acc: 0.9088 - val_loss: 0.6369 - val_acc: 0.7273\n",
      "Epoch 973/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3554 - acc: 0.8803 - val_loss: 0.7102 - val_acc: 0.7045\n",
      "Epoch 974/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.3146 - acc: 0.9046 - val_loss: 0.6932 - val_acc: 0.7784\n",
      "Epoch 975/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.3013 - acc: 0.8903 - val_loss: 0.8185 - val_acc: 0.7216\n",
      "Epoch 976/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.2785 - acc: 0.9046 - val_loss: 0.6962 - val_acc: 0.7557\n",
      "Epoch 977/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.3344 - acc: 0.8917 - val_loss: 0.6497 - val_acc: 0.7500\n",
      "Epoch 978/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.2959 - acc: 0.8903 - val_loss: 0.6434 - val_acc: 0.7670\n",
      "Epoch 979/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3260 - acc: 0.8903 - val_loss: 0.6552 - val_acc: 0.7898\n",
      "Epoch 980/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.2888 - acc: 0.9103 - val_loss: 0.6979 - val_acc: 0.7102\n",
      "Epoch 981/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3108 - acc: 0.8875 - val_loss: 0.6416 - val_acc: 0.7784\n",
      "Epoch 982/3000\n",
      "702/702 [==============================] - 0s 588us/sample - loss: 0.3243 - acc: 0.9088 - val_loss: 0.7313 - val_acc: 0.7216\n",
      "Epoch 983/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.2950 - acc: 0.8960 - val_loss: 0.6975 - val_acc: 0.7614\n",
      "Epoch 984/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.2916 - acc: 0.9174 - val_loss: 0.7448 - val_acc: 0.7273\n",
      "Epoch 985/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.2628 - acc: 0.9174 - val_loss: 0.6258 - val_acc: 0.8011\n",
      "Epoch 986/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.2584 - acc: 0.9217 - val_loss: 0.7035 - val_acc: 0.7614\n",
      "Epoch 987/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.2458 - acc: 0.9217 - val_loss: 0.7193 - val_acc: 0.7898\n",
      "Epoch 988/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.2789 - acc: 0.9088 - val_loss: 0.6659 - val_acc: 0.7614\n",
      "Epoch 989/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.2765 - acc: 0.9145 - val_loss: 0.6201 - val_acc: 0.7955\n",
      "Epoch 990/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3801 - acc: 0.8818 - val_loss: 0.7987 - val_acc: 0.7330\n",
      "Epoch 991/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3436 - acc: 0.8932 - val_loss: 0.6701 - val_acc: 0.8011\n",
      "Epoch 992/3000\n",
      "702/702 [==============================] - 0s 552us/sample - loss: 0.4166 - acc: 0.8732 - val_loss: 0.9113 - val_acc: 0.6875\n",
      "Epoch 993/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.3286 - acc: 0.8974 - val_loss: 0.6607 - val_acc: 0.7841\n",
      "Epoch 994/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.3119 - acc: 0.8946 - val_loss: 0.6600 - val_acc: 0.7159\n",
      "Epoch 995/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.2961 - acc: 0.8875 - val_loss: 0.6353 - val_acc: 0.7045\n",
      "Epoch 996/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.2632 - acc: 0.9131 - val_loss: 0.5936 - val_acc: 0.7670\n",
      "Epoch 997/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3463 - acc: 0.8960 - val_loss: 0.7092 - val_acc: 0.6818\n",
      "Epoch 998/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.2908 - acc: 0.8932 - val_loss: 0.6048 - val_acc: 0.7614\n",
      "Epoch 999/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.3135 - acc: 0.8989 - val_loss: 0.6631 - val_acc: 0.7386\n",
      "Epoch 1000/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.3197 - acc: 0.8860 - val_loss: 0.7507 - val_acc: 0.6989\n",
      "Epoch 1001/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.2828 - acc: 0.9088 - val_loss: 0.6876 - val_acc: 0.7443\n",
      "Epoch 1002/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.2702 - acc: 0.9117 - val_loss: 0.7030 - val_acc: 0.7500\n",
      "Epoch 1003/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.2961 - acc: 0.8989 - val_loss: 0.6397 - val_acc: 0.7500\n",
      "Epoch 1004/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3078 - acc: 0.8974 - val_loss: 0.5872 - val_acc: 0.7898\n",
      "Epoch 1005/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.2617 - acc: 0.8989 - val_loss: 0.7168 - val_acc: 0.7045\n",
      "Epoch 1006/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.2970 - acc: 0.9017 - val_loss: 0.6712 - val_acc: 0.7727\n",
      "Epoch 1007/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.2765 - acc: 0.9316 - val_loss: 0.7700 - val_acc: 0.7045\n",
      "Epoch 1008/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.2547 - acc: 0.9160 - val_loss: 0.7234 - val_acc: 0.7443\n",
      "Epoch 1009/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2543 - acc: 0.9188 - val_loss: 0.7007 - val_acc: 0.7670\n",
      "Epoch 1010/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.2507 - acc: 0.9188 - val_loss: 0.7548 - val_acc: 0.7273\n",
      "Epoch 1011/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.2747 - acc: 0.9145 - val_loss: 0.7473 - val_acc: 0.7500\n",
      "Epoch 1012/3000\n",
      "702/702 [==============================] - 0s 569us/sample - loss: 0.2557 - acc: 0.9145 - val_loss: 0.8006 - val_acc: 0.7159\n",
      "Epoch 1013/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3122 - acc: 0.8960 - val_loss: 0.7016 - val_acc: 0.7784\n",
      "Epoch 1014/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.2788 - acc: 0.9160 - val_loss: 0.6807 - val_acc: 0.8011\n",
      "Epoch 1015/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.3318 - acc: 0.8775 - val_loss: 0.6524 - val_acc: 0.8068\n",
      "Epoch 1016/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.3133 - acc: 0.8818 - val_loss: 0.7476 - val_acc: 0.7443\n",
      "Epoch 1017/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.3243 - acc: 0.8917 - val_loss: 0.8406 - val_acc: 0.7386\n",
      "Epoch 1018/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.2922 - acc: 0.9031 - val_loss: 0.8683 - val_acc: 0.7273\n",
      "Epoch 1019/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.3028 - acc: 0.8932 - val_loss: 0.8304 - val_acc: 0.7614\n",
      "Epoch 1020/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.2991 - acc: 0.8989 - val_loss: 0.7750 - val_acc: 0.7443\n",
      "Epoch 1021/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3302 - acc: 0.8803 - val_loss: 0.6160 - val_acc: 0.7557\n",
      "Epoch 1022/3000\n",
      "702/702 [==============================] - 0s 582us/sample - loss: 0.3532 - acc: 0.8718 - val_loss: 0.7791 - val_acc: 0.6875\n",
      "Epoch 1023/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3739 - acc: 0.8533 - val_loss: 0.6389 - val_acc: 0.8068\n",
      "Epoch 1024/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.4036 - acc: 0.8803 - val_loss: 0.6450 - val_acc: 0.7784\n",
      "Epoch 1025/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.3269 - acc: 0.8818 - val_loss: 0.7920 - val_acc: 0.6818\n",
      "Epoch 1026/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3295 - acc: 0.8946 - val_loss: 0.6040 - val_acc: 0.7670\n",
      "Epoch 1027/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.2751 - acc: 0.9074 - val_loss: 0.7451 - val_acc: 0.7159\n",
      "Epoch 1028/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.3279 - acc: 0.8875 - val_loss: 0.6938 - val_acc: 0.7330\n",
      "Epoch 1029/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.2713 - acc: 0.9003 - val_loss: 0.6763 - val_acc: 0.6989\n",
      "Epoch 1030/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.3544 - acc: 0.8960 - val_loss: 0.6517 - val_acc: 0.7386\n",
      "Epoch 1031/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.2929 - acc: 0.8989 - val_loss: 0.6897 - val_acc: 0.7727\n",
      "Epoch 1032/3000\n",
      "702/702 [==============================] - 0s 561us/sample - loss: 0.3150 - acc: 0.8917 - val_loss: 0.7407 - val_acc: 0.7045\n",
      "Epoch 1033/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.3511 - acc: 0.8818 - val_loss: 0.6729 - val_acc: 0.7273\n",
      "Epoch 1034/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.2951 - acc: 0.8818 - val_loss: 0.6734 - val_acc: 0.7102\n",
      "Epoch 1035/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.2930 - acc: 0.9003 - val_loss: 0.7522 - val_acc: 0.6875\n",
      "Epoch 1036/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.2988 - acc: 0.8946 - val_loss: 0.7598 - val_acc: 0.6932\n",
      "Epoch 1037/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.2713 - acc: 0.9160 - val_loss: 0.7473 - val_acc: 0.6932\n",
      "Epoch 1038/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.2983 - acc: 0.9017 - val_loss: 0.7179 - val_acc: 0.7216\n",
      "Epoch 1039/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.2794 - acc: 0.9046 - val_loss: 0.6537 - val_acc: 0.7273\n",
      "Epoch 1040/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.2683 - acc: 0.9088 - val_loss: 0.6220 - val_acc: 0.7841\n",
      "Epoch 1041/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3009 - acc: 0.8974 - val_loss: 0.6757 - val_acc: 0.7045\n",
      "Epoch 1042/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 0.3006 - acc: 0.8917 - val_loss: 0.6373 - val_acc: 0.7727\n",
      "Epoch 1043/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.3134 - acc: 0.9088 - val_loss: 0.5972 - val_acc: 0.7443\n",
      "Epoch 1044/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.3528 - acc: 0.8960 - val_loss: 0.6116 - val_acc: 0.7614\n",
      "Epoch 1045/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.2820 - acc: 0.9031 - val_loss: 0.5804 - val_acc: 0.7557\n",
      "Epoch 1046/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3118 - acc: 0.8903 - val_loss: 0.5586 - val_acc: 0.8011\n",
      "Epoch 1047/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.2417 - acc: 0.9174 - val_loss: 0.5756 - val_acc: 0.8011\n",
      "Epoch 1048/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.2693 - acc: 0.9202 - val_loss: 0.6064 - val_acc: 0.7557\n",
      "Epoch 1049/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.3226 - acc: 0.8875 - val_loss: 0.6335 - val_acc: 0.7386\n",
      "Epoch 1050/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3310 - acc: 0.8803 - val_loss: 0.6319 - val_acc: 0.7614\n",
      "Epoch 1051/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3165 - acc: 0.8932 - val_loss: 0.7038 - val_acc: 0.6932\n",
      "Epoch 1052/3000\n",
      "702/702 [==============================] - 0s 569us/sample - loss: 0.3282 - acc: 0.8903 - val_loss: 0.6964 - val_acc: 0.7159\n",
      "Epoch 1053/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.3412 - acc: 0.8789 - val_loss: 0.6107 - val_acc: 0.7784\n",
      "Epoch 1054/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3124 - acc: 0.8875 - val_loss: 0.7556 - val_acc: 0.7045\n",
      "Epoch 1055/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.2736 - acc: 0.9088 - val_loss: 0.6934 - val_acc: 0.7273\n",
      "Epoch 1056/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.3161 - acc: 0.8917 - val_loss: 0.7560 - val_acc: 0.6818\n",
      "Epoch 1057/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.3390 - acc: 0.8846 - val_loss: 0.6700 - val_acc: 0.7670\n",
      "Epoch 1058/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.2933 - acc: 0.9031 - val_loss: 0.7872 - val_acc: 0.7557\n",
      "Epoch 1059/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.3056 - acc: 0.8974 - val_loss: 0.7632 - val_acc: 0.7159\n",
      "Epoch 1060/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.2846 - acc: 0.8946 - val_loss: 0.7672 - val_acc: 0.7159\n",
      "Epoch 1061/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3295 - acc: 0.8946 - val_loss: 0.8702 - val_acc: 0.6648\n",
      "Epoch 1062/3000\n",
      "702/702 [==============================] - 0s 567us/sample - loss: 0.3516 - acc: 0.8761 - val_loss: 0.6928 - val_acc: 0.7045\n",
      "Epoch 1063/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.3560 - acc: 0.8860 - val_loss: 0.6793 - val_acc: 0.7330\n",
      "Epoch 1064/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.2925 - acc: 0.8946 - val_loss: 0.6140 - val_acc: 0.7102\n",
      "Epoch 1065/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.2835 - acc: 0.9074 - val_loss: 0.5393 - val_acc: 0.7955\n",
      "Epoch 1066/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.3233 - acc: 0.9003 - val_loss: 0.5980 - val_acc: 0.7216\n",
      "Epoch 1067/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.2814 - acc: 0.8946 - val_loss: 0.5629 - val_acc: 0.7841\n",
      "Epoch 1068/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.2844 - acc: 0.8974 - val_loss: 0.6396 - val_acc: 0.7216\n",
      "Epoch 1069/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.3050 - acc: 0.8875 - val_loss: 0.6546 - val_acc: 0.7386\n",
      "Epoch 1070/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.3008 - acc: 0.9088 - val_loss: 0.7356 - val_acc: 0.7216\n",
      "Epoch 1071/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3470 - acc: 0.8718 - val_loss: 0.7324 - val_acc: 0.7443\n",
      "Epoch 1072/3000\n",
      "702/702 [==============================] - 0s 569us/sample - loss: 0.3412 - acc: 0.8960 - val_loss: 0.7141 - val_acc: 0.7330\n",
      "Epoch 1073/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3145 - acc: 0.8932 - val_loss: 0.5934 - val_acc: 0.7841\n",
      "Epoch 1074/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.2803 - acc: 0.9117 - val_loss: 0.6130 - val_acc: 0.7443\n",
      "Epoch 1075/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.2717 - acc: 0.9003 - val_loss: 0.6582 - val_acc: 0.7330\n",
      "Epoch 1076/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.2560 - acc: 0.9088 - val_loss: 0.6134 - val_acc: 0.7841\n",
      "Epoch 1077/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.2463 - acc: 0.9174 - val_loss: 0.5949 - val_acc: 0.7784\n",
      "Epoch 1078/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.2666 - acc: 0.9060 - val_loss: 0.5617 - val_acc: 0.7784\n",
      "Epoch 1079/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.2819 - acc: 0.9188 - val_loss: 0.5647 - val_acc: 0.7841\n",
      "Epoch 1080/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3040 - acc: 0.8946 - val_loss: 0.5380 - val_acc: 0.7614\n",
      "Epoch 1081/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.2596 - acc: 0.9031 - val_loss: 0.5595 - val_acc: 0.7386\n",
      "Epoch 1082/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 0.2881 - acc: 0.9131 - val_loss: 0.5778 - val_acc: 0.7898\n",
      "Epoch 1083/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.2276 - acc: 0.9145 - val_loss: 0.5979 - val_acc: 0.7670\n",
      "Epoch 1084/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.2663 - acc: 0.9174 - val_loss: 0.5843 - val_acc: 0.7727\n",
      "Epoch 1085/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.2359 - acc: 0.9160 - val_loss: 0.5658 - val_acc: 0.7955\n",
      "Epoch 1086/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2459 - acc: 0.9088 - val_loss: 0.5822 - val_acc: 0.7670\n",
      "Epoch 1087/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2695 - acc: 0.9145 - val_loss: 0.5190 - val_acc: 0.8182\n",
      "Epoch 1088/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3103 - acc: 0.9031 - val_loss: 0.6193 - val_acc: 0.7557\n",
      "Epoch 1089/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.2667 - acc: 0.9060 - val_loss: 0.5441 - val_acc: 0.7898\n",
      "Epoch 1090/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.2427 - acc: 0.9160 - val_loss: 0.5878 - val_acc: 0.7614\n",
      "Epoch 1091/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.2885 - acc: 0.8960 - val_loss: 0.6055 - val_acc: 0.7727\n",
      "Epoch 1092/3000\n",
      "702/702 [==============================] - 0s 561us/sample - loss: 0.2492 - acc: 0.9231 - val_loss: 0.5951 - val_acc: 0.8011\n",
      "Epoch 1093/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.2073 - acc: 0.9330 - val_loss: 0.7140 - val_acc: 0.7102\n",
      "Epoch 1094/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.2802 - acc: 0.9131 - val_loss: 0.5714 - val_acc: 0.7898\n",
      "Epoch 1095/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.3291 - acc: 0.9003 - val_loss: 0.6765 - val_acc: 0.7614\n",
      "Epoch 1096/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.2664 - acc: 0.9103 - val_loss: 0.7154 - val_acc: 0.7784\n",
      "Epoch 1097/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.2792 - acc: 0.9131 - val_loss: 0.5774 - val_acc: 0.7898\n",
      "Epoch 1098/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.2866 - acc: 0.9031 - val_loss: 0.6021 - val_acc: 0.7557\n",
      "Epoch 1099/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.3527 - acc: 0.8946 - val_loss: 0.6812 - val_acc: 0.7557\n",
      "Epoch 1100/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2935 - acc: 0.8960 - val_loss: 0.8101 - val_acc: 0.7273\n",
      "Epoch 1101/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3589 - acc: 0.8775 - val_loss: 0.8294 - val_acc: 0.6875\n",
      "Epoch 1102/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.2834 - acc: 0.9031 - val_loss: 0.7518 - val_acc: 0.6705\n",
      "Epoch 1103/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.3430 - acc: 0.8846 - val_loss: 0.6457 - val_acc: 0.7500\n",
      "Epoch 1104/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3631 - acc: 0.8875 - val_loss: 0.7819 - val_acc: 0.6932\n",
      "Epoch 1105/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.3456 - acc: 0.8818 - val_loss: 0.8410 - val_acc: 0.6875\n",
      "Epoch 1106/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.3501 - acc: 0.8718 - val_loss: 0.7235 - val_acc: 0.7614\n",
      "Epoch 1107/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.3737 - acc: 0.8803 - val_loss: 0.7739 - val_acc: 0.7557\n",
      "Epoch 1108/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.2826 - acc: 0.9031 - val_loss: 0.9446 - val_acc: 0.6705\n",
      "Epoch 1109/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.2808 - acc: 0.9003 - val_loss: 0.7532 - val_acc: 0.7273\n",
      "Epoch 1110/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.3086 - acc: 0.8846 - val_loss: 0.8308 - val_acc: 0.6818\n",
      "Epoch 1111/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.2873 - acc: 0.8875 - val_loss: 0.7796 - val_acc: 0.7330\n",
      "Epoch 1112/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 0.2600 - acc: 0.9074 - val_loss: 0.7541 - val_acc: 0.7443\n",
      "Epoch 1113/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.2912 - acc: 0.9017 - val_loss: 0.7221 - val_acc: 0.7670\n",
      "Epoch 1114/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.2768 - acc: 0.9160 - val_loss: 0.8097 - val_acc: 0.6989\n",
      "Epoch 1115/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.3065 - acc: 0.8860 - val_loss: 0.9301 - val_acc: 0.7330\n",
      "Epoch 1116/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.3267 - acc: 0.8846 - val_loss: 0.9629 - val_acc: 0.7330\n",
      "Epoch 1117/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.3652 - acc: 0.8661 - val_loss: 1.0113 - val_acc: 0.7614\n",
      "Epoch 1118/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3832 - acc: 0.8875 - val_loss: 0.7568 - val_acc: 0.7102\n",
      "Epoch 1119/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2771 - acc: 0.9088 - val_loss: 0.6541 - val_acc: 0.7386\n",
      "Epoch 1120/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.2929 - acc: 0.9003 - val_loss: 0.7000 - val_acc: 0.7216\n",
      "Epoch 1121/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.2639 - acc: 0.9188 - val_loss: 0.6765 - val_acc: 0.7557\n",
      "Epoch 1122/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 0.2923 - acc: 0.8903 - val_loss: 0.6780 - val_acc: 0.7330\n",
      "Epoch 1123/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3049 - acc: 0.9160 - val_loss: 0.7119 - val_acc: 0.7443\n",
      "Epoch 1124/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.2684 - acc: 0.9088 - val_loss: 0.6245 - val_acc: 0.7955\n",
      "Epoch 1125/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.2998 - acc: 0.8917 - val_loss: 0.7133 - val_acc: 0.6989\n",
      "Epoch 1126/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.3611 - acc: 0.9017 - val_loss: 0.6723 - val_acc: 0.7330\n",
      "Epoch 1127/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.3395 - acc: 0.9017 - val_loss: 0.6633 - val_acc: 0.6875\n",
      "Epoch 1128/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.3165 - acc: 0.9031 - val_loss: 0.6295 - val_acc: 0.7557\n",
      "Epoch 1129/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.3539 - acc: 0.8832 - val_loss: 0.5875 - val_acc: 0.7727\n",
      "Epoch 1130/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3677 - acc: 0.8917 - val_loss: 0.6488 - val_acc: 0.7330\n",
      "Epoch 1131/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.2987 - acc: 0.9074 - val_loss: 0.5280 - val_acc: 0.8068\n",
      "Epoch 1132/3000\n",
      "702/702 [==============================] - 0s 555us/sample - loss: 0.3085 - acc: 0.9003 - val_loss: 0.6629 - val_acc: 0.7216\n",
      "Epoch 1133/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3089 - acc: 0.8761 - val_loss: 0.6737 - val_acc: 0.6989\n",
      "Epoch 1134/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.3438 - acc: 0.8689 - val_loss: 0.5908 - val_acc: 0.7443\n",
      "Epoch 1135/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2479 - acc: 0.9088 - val_loss: 0.5706 - val_acc: 0.7727\n",
      "Epoch 1136/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.2735 - acc: 0.9060 - val_loss: 0.5413 - val_acc: 0.8182\n",
      "Epoch 1137/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3322 - acc: 0.8889 - val_loss: 0.6541 - val_acc: 0.7330\n",
      "Epoch 1138/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3709 - acc: 0.8832 - val_loss: 0.6278 - val_acc: 0.7330\n",
      "Epoch 1139/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.3686 - acc: 0.8718 - val_loss: 0.6494 - val_acc: 0.7273\n",
      "Epoch 1140/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.3740 - acc: 0.8775 - val_loss: 0.6321 - val_acc: 0.7273\n",
      "Epoch 1141/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3664 - acc: 0.8917 - val_loss: 0.6555 - val_acc: 0.7159\n",
      "Epoch 1142/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 0.3380 - acc: 0.8889 - val_loss: 0.7051 - val_acc: 0.6989\n",
      "Epoch 1143/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.3177 - acc: 0.9046 - val_loss: 0.7111 - val_acc: 0.6932\n",
      "Epoch 1144/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.2924 - acc: 0.9117 - val_loss: 0.6609 - val_acc: 0.7045\n",
      "Epoch 1145/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.2832 - acc: 0.8989 - val_loss: 0.6087 - val_acc: 0.7330\n",
      "Epoch 1146/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.3183 - acc: 0.8974 - val_loss: 0.6401 - val_acc: 0.7216\n",
      "Epoch 1147/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.3105 - acc: 0.9003 - val_loss: 0.6256 - val_acc: 0.7614\n",
      "Epoch 1148/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.3108 - acc: 0.8889 - val_loss: 0.6413 - val_acc: 0.7386\n",
      "Epoch 1149/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.3193 - acc: 0.8917 - val_loss: 0.6607 - val_acc: 0.6989\n",
      "Epoch 1150/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2560 - acc: 0.9117 - val_loss: 0.6520 - val_acc: 0.7386\n",
      "Epoch 1151/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.2644 - acc: 0.9217 - val_loss: 0.5667 - val_acc: 0.7670\n",
      "Epoch 1152/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.2505 - acc: 0.9202 - val_loss: 0.5289 - val_acc: 0.7727\n",
      "Epoch 1153/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.2480 - acc: 0.9188 - val_loss: 0.5982 - val_acc: 0.7727\n",
      "Epoch 1154/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.2297 - acc: 0.9302 - val_loss: 0.5641 - val_acc: 0.8011\n",
      "Epoch 1155/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.2664 - acc: 0.9217 - val_loss: 0.6520 - val_acc: 0.7557\n",
      "Epoch 1156/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.3178 - acc: 0.8818 - val_loss: 1.6789 - val_acc: 0.6193\n",
      "Epoch 1157/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 2.8154 - acc: 0.5442 - val_loss: 2.4455 - val_acc: 0.4375\n",
      "Epoch 1158/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 2.5528 - acc: 0.4202 - val_loss: 1.4624 - val_acc: 0.5682\n",
      "Epoch 1159/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.6508 - acc: 0.5513 - val_loss: 1.7695 - val_acc: 0.5341\n",
      "Epoch 1160/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 3.2673 - acc: 0.4644 - val_loss: 2.7698 - val_acc: 0.3693\n",
      "Epoch 1161/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 2.6921 - acc: 0.3889 - val_loss: 1.9781 - val_acc: 0.4545\n",
      "Epoch 1162/3000\n",
      "702/702 [==============================] - 0s 552us/sample - loss: 2.9767 - acc: 0.4117 - val_loss: 2.2105 - val_acc: 0.4943\n",
      "Epoch 1163/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 3.0031 - acc: 0.2877 - val_loss: 2.3734 - val_acc: 0.2898\n",
      "Epoch 1164/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 2.8039 - acc: 0.3333 - val_loss: 2.2369 - val_acc: 0.3352\n",
      "Epoch 1165/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 3.3267 - acc: 0.2764 - val_loss: 2.8722 - val_acc: 0.3011\n",
      "Epoch 1166/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 4.6966 - acc: 0.2365 - val_loss: 13.0716 - val_acc: 0.1193\n",
      "Epoch 1167/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 14.4370 - acc: 0.2037 - val_loss: 2.9845 - val_acc: 0.2500\n",
      "Epoch 1168/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 6.3020 - acc: 0.1795 - val_loss: 4.1537 - val_acc: 0.0795\n",
      "Epoch 1169/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 5.0545 - acc: 0.1140 - val_loss: 4.2990 - val_acc: 0.0795\n",
      "Epoch 1170/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 4.0802 - acc: 0.0840 - val_loss: 3.0016 - val_acc: 0.1705\n",
      "Epoch 1171/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 3.8798 - acc: 0.1040 - val_loss: 2.5496 - val_acc: 0.2784\n",
      "Epoch 1172/3000\n",
      "702/702 [==============================] - 0s 570us/sample - loss: 3.1499 - acc: 0.2094 - val_loss: 2.0974 - val_acc: 0.3125\n",
      "Epoch 1173/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 3.8817 - acc: 0.1553 - val_loss: 3.1515 - val_acc: 0.2614\n",
      "Epoch 1174/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 3.7375 - acc: 0.2094 - val_loss: 3.0862 - val_acc: 0.1932\n",
      "Epoch 1175/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 3.0896 - acc: 0.1809 - val_loss: 2.3838 - val_acc: 0.2670\n",
      "Epoch 1176/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 2.7701 - acc: 0.2322 - val_loss: 2.3165 - val_acc: 0.2500\n",
      "Epoch 1177/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 2.5675 - acc: 0.2137 - val_loss: 2.0594 - val_acc: 0.2784\n",
      "Epoch 1178/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 2.4595 - acc: 0.2450 - val_loss: 1.8276 - val_acc: 0.3693\n",
      "Epoch 1179/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 2.5173 - acc: 0.2308 - val_loss: 2.3567 - val_acc: 0.2557\n",
      "Epoch 1180/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 2.4573 - acc: 0.2407 - val_loss: 1.8686 - val_acc: 0.3011\n",
      "Epoch 1181/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 2.7783 - acc: 0.2707 - val_loss: 1.8534 - val_acc: 0.3239\n",
      "Epoch 1182/3000\n",
      "702/702 [==============================] - 0s 566us/sample - loss: 2.1856 - acc: 0.2080 - val_loss: 1.7700 - val_acc: 0.3693\n",
      "Epoch 1183/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 2.1235 - acc: 0.2963 - val_loss: 1.8822 - val_acc: 0.3466\n",
      "Epoch 1184/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 2.1335 - acc: 0.2550 - val_loss: 1.6873 - val_acc: 0.3750\n",
      "Epoch 1185/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 2.0255 - acc: 0.2678 - val_loss: 1.6560 - val_acc: 0.3750\n",
      "Epoch 1186/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 2.0036 - acc: 0.3077 - val_loss: 1.6688 - val_acc: 0.3693\n",
      "Epoch 1187/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.9891 - acc: 0.2607 - val_loss: 1.7821 - val_acc: 0.3125\n",
      "Epoch 1188/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.8723 - acc: 0.2806 - val_loss: 1.6333 - val_acc: 0.3750\n",
      "Epoch 1189/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.9259 - acc: 0.2806 - val_loss: 1.6554 - val_acc: 0.3693\n",
      "Epoch 1190/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 1.9434 - acc: 0.2764 - val_loss: 1.7501 - val_acc: 0.3295\n",
      "Epoch 1191/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.9483 - acc: 0.2877 - val_loss: 1.6318 - val_acc: 0.3750\n",
      "Epoch 1192/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 1.8983 - acc: 0.2877 - val_loss: 1.6870 - val_acc: 0.3693\n",
      "Epoch 1193/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.8471 - acc: 0.2906 - val_loss: 1.7131 - val_acc: 0.3636\n",
      "Epoch 1194/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 1.7705 - acc: 0.3305 - val_loss: 1.6285 - val_acc: 0.3750\n",
      "Epoch 1195/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 1.7472 - acc: 0.3575 - val_loss: 1.8178 - val_acc: 0.2273\n",
      "Epoch 1196/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 1.7364 - acc: 0.3248 - val_loss: 1.7807 - val_acc: 0.2727\n",
      "Epoch 1197/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.6910 - acc: 0.3590 - val_loss: 1.8440 - val_acc: 0.2159\n",
      "Epoch 1198/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.7088 - acc: 0.3248 - val_loss: 1.6227 - val_acc: 0.3977\n",
      "Epoch 1199/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 1.7486 - acc: 0.3191 - val_loss: 1.6832 - val_acc: 0.3580\n",
      "Epoch 1200/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 1.6853 - acc: 0.3305 - val_loss: 1.6604 - val_acc: 0.3523\n",
      "Epoch 1201/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.7227 - acc: 0.3148 - val_loss: 1.9165 - val_acc: 0.1989\n",
      "Epoch 1202/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 1.7155 - acc: 0.3148 - val_loss: 1.6401 - val_acc: 0.3523\n",
      "Epoch 1203/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 1.6692 - acc: 0.3390 - val_loss: 1.6845 - val_acc: 0.3409\n",
      "Epoch 1204/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 1.6075 - acc: 0.3618 - val_loss: 1.7641 - val_acc: 0.2330\n",
      "Epoch 1205/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 1.6153 - acc: 0.3732 - val_loss: 1.7255 - val_acc: 0.3182\n",
      "Epoch 1206/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 1.6199 - acc: 0.3504 - val_loss: 1.7520 - val_acc: 0.3182\n",
      "Epoch 1207/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.6054 - acc: 0.3447 - val_loss: 1.7928 - val_acc: 0.2045\n",
      "Epoch 1208/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 1.5712 - acc: 0.3661 - val_loss: 1.6789 - val_acc: 0.3580\n",
      "Epoch 1209/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 1.5660 - acc: 0.3832 - val_loss: 1.7355 - val_acc: 0.3125\n",
      "Epoch 1210/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 1.5616 - acc: 0.3689 - val_loss: 1.8520 - val_acc: 0.2102\n",
      "Epoch 1211/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.5799 - acc: 0.3376 - val_loss: 1.7394 - val_acc: 0.3182\n",
      "Epoch 1212/3000\n",
      "702/702 [==============================] - 0s 555us/sample - loss: 1.5505 - acc: 0.3932 - val_loss: 1.7059 - val_acc: 0.3182\n",
      "Epoch 1213/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.5116 - acc: 0.3974 - val_loss: 1.8003 - val_acc: 0.2443\n",
      "Epoch 1214/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 1.5057 - acc: 0.4145 - val_loss: 1.7259 - val_acc: 0.3239\n",
      "Epoch 1215/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.5099 - acc: 0.4074 - val_loss: 1.8087 - val_acc: 0.2500\n",
      "Epoch 1216/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.5113 - acc: 0.4245 - val_loss: 1.8720 - val_acc: 0.2330\n",
      "Epoch 1217/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.6034 - acc: 0.3419 - val_loss: 1.5879 - val_acc: 0.3920\n",
      "Epoch 1218/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 1.6163 - acc: 0.3504 - val_loss: 1.9098 - val_acc: 0.2614\n",
      "Epoch 1219/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 1.7246 - acc: 0.3105 - val_loss: 1.8672 - val_acc: 0.2898\n",
      "Epoch 1220/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.6676 - acc: 0.3433 - val_loss: 1.4379 - val_acc: 0.4489\n",
      "Epoch 1221/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.6211 - acc: 0.3618 - val_loss: 1.8216 - val_acc: 0.2500\n",
      "Epoch 1222/3000\n",
      "702/702 [==============================] - 0s 555us/sample - loss: 1.6081 - acc: 0.3504 - val_loss: 1.7468 - val_acc: 0.2727\n",
      "Epoch 1223/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 1.5403 - acc: 0.4060 - val_loss: 1.8848 - val_acc: 0.2557\n",
      "Epoch 1224/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.6098 - acc: 0.3362 - val_loss: 1.7271 - val_acc: 0.3352\n",
      "Epoch 1225/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 1.5222 - acc: 0.3974 - val_loss: 1.7822 - val_acc: 0.2614\n",
      "Epoch 1226/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.5077 - acc: 0.4017 - val_loss: 1.7856 - val_acc: 0.2273\n",
      "Epoch 1227/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.4694 - acc: 0.4345 - val_loss: 1.7099 - val_acc: 0.3295\n",
      "Epoch 1228/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 1.4556 - acc: 0.4231 - val_loss: 1.6562 - val_acc: 0.3523\n",
      "Epoch 1229/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 1.4371 - acc: 0.4231 - val_loss: 1.7611 - val_acc: 0.2614\n",
      "Epoch 1230/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.4393 - acc: 0.4573 - val_loss: 1.6439 - val_acc: 0.4148\n",
      "Epoch 1231/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.4674 - acc: 0.4259 - val_loss: 1.7264 - val_acc: 0.3068\n",
      "Epoch 1232/3000\n",
      "702/702 [==============================] - 0s 564us/sample - loss: 1.4794 - acc: 0.4217 - val_loss: 1.6650 - val_acc: 0.3352\n",
      "Epoch 1233/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.4376 - acc: 0.4103 - val_loss: 1.7654 - val_acc: 0.2841\n",
      "Epoch 1234/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 1.4171 - acc: 0.4672 - val_loss: 1.7314 - val_acc: 0.2557\n",
      "Epoch 1235/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 1.4270 - acc: 0.4644 - val_loss: 1.7032 - val_acc: 0.3068\n",
      "Epoch 1236/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 1.4383 - acc: 0.4544 - val_loss: 1.6804 - val_acc: 0.3125\n",
      "Epoch 1237/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.3981 - acc: 0.4530 - val_loss: 1.6623 - val_acc: 0.2841\n",
      "Epoch 1238/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.4359 - acc: 0.4615 - val_loss: 1.7383 - val_acc: 0.3068\n",
      "Epoch 1239/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 1.4270 - acc: 0.4288 - val_loss: 1.6890 - val_acc: 0.3239\n",
      "Epoch 1240/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.4751 - acc: 0.4060 - val_loss: 1.7664 - val_acc: 0.2898\n",
      "Epoch 1241/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.5044 - acc: 0.3946 - val_loss: 1.5289 - val_acc: 0.4375\n",
      "Epoch 1242/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 1.4726 - acc: 0.4601 - val_loss: 1.7586 - val_acc: 0.2386\n",
      "Epoch 1243/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 1.4773 - acc: 0.4117 - val_loss: 1.7324 - val_acc: 0.2727\n",
      "Epoch 1244/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 1.4508 - acc: 0.4672 - val_loss: 1.8227 - val_acc: 0.2898\n",
      "Epoch 1245/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 1.4357 - acc: 0.4316 - val_loss: 1.6459 - val_acc: 0.3011\n",
      "Epoch 1246/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 1.4089 - acc: 0.4772 - val_loss: 1.7414 - val_acc: 0.2670\n",
      "Epoch 1247/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.4021 - acc: 0.4715 - val_loss: 1.7937 - val_acc: 0.2670\n",
      "Epoch 1248/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 1.4172 - acc: 0.4687 - val_loss: 1.7895 - val_acc: 0.2557\n",
      "Epoch 1249/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.4058 - acc: 0.4601 - val_loss: 1.7408 - val_acc: 0.2557\n",
      "Epoch 1250/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 1.4054 - acc: 0.4943 - val_loss: 1.7263 - val_acc: 0.2614\n",
      "Epoch 1251/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.3916 - acc: 0.4900 - val_loss: 1.7796 - val_acc: 0.2443\n",
      "Epoch 1252/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 1.4002 - acc: 0.4872 - val_loss: 1.7714 - val_acc: 0.2727\n",
      "Epoch 1253/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.3747 - acc: 0.5128 - val_loss: 1.8492 - val_acc: 0.2216\n",
      "Epoch 1254/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 1.3566 - acc: 0.5043 - val_loss: 1.7455 - val_acc: 0.2841\n",
      "Epoch 1255/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 1.3998 - acc: 0.4701 - val_loss: 1.7834 - val_acc: 0.2841\n",
      "Epoch 1256/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.3859 - acc: 0.4744 - val_loss: 1.7389 - val_acc: 0.2500\n",
      "Epoch 1257/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 1.3670 - acc: 0.4900 - val_loss: 1.7476 - val_acc: 0.2330\n",
      "Epoch 1258/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 1.3525 - acc: 0.4815 - val_loss: 1.6217 - val_acc: 0.3523\n",
      "Epoch 1259/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.3499 - acc: 0.4886 - val_loss: 1.6053 - val_acc: 0.3580\n",
      "Epoch 1260/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.3517 - acc: 0.4986 - val_loss: 1.7307 - val_acc: 0.2727\n",
      "Epoch 1261/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.3870 - acc: 0.4786 - val_loss: 1.6123 - val_acc: 0.3807\n",
      "Epoch 1262/3000\n",
      "702/702 [==============================] - 0s 558us/sample - loss: 1.3383 - acc: 0.4972 - val_loss: 1.6392 - val_acc: 0.3523\n",
      "Epoch 1263/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.3529 - acc: 0.4900 - val_loss: 1.6377 - val_acc: 0.3750\n",
      "Epoch 1264/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 1.3662 - acc: 0.4986 - val_loss: 1.6187 - val_acc: 0.3807\n",
      "Epoch 1265/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 1.3627 - acc: 0.4687 - val_loss: 1.5618 - val_acc: 0.3807\n",
      "Epoch 1266/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.3673 - acc: 0.4886 - val_loss: 1.5790 - val_acc: 0.3636\n",
      "Epoch 1267/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 1.3433 - acc: 0.5014 - val_loss: 1.6577 - val_acc: 0.2614\n",
      "Epoch 1268/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 1.3672 - acc: 0.4872 - val_loss: 1.5599 - val_acc: 0.3523\n",
      "Epoch 1269/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.3417 - acc: 0.4986 - val_loss: 1.6545 - val_acc: 0.3125\n",
      "Epoch 1270/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 1.3377 - acc: 0.5028 - val_loss: 1.5717 - val_acc: 0.3352\n",
      "Epoch 1271/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.3916 - acc: 0.4644 - val_loss: 1.6492 - val_acc: 0.3239\n",
      "Epoch 1272/3000\n",
      "702/702 [==============================] - 0s 559us/sample - loss: 1.3268 - acc: 0.5028 - val_loss: 1.6753 - val_acc: 0.2898\n",
      "Epoch 1273/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 1.3575 - acc: 0.4772 - val_loss: 1.6662 - val_acc: 0.3011\n",
      "Epoch 1274/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 1.3545 - acc: 0.4886 - val_loss: 1.5459 - val_acc: 0.4545\n",
      "Epoch 1275/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 1.2840 - acc: 0.5071 - val_loss: 1.6384 - val_acc: 0.3182\n",
      "Epoch 1276/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.3197 - acc: 0.5028 - val_loss: 1.5759 - val_acc: 0.4148\n",
      "Epoch 1277/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.3436 - acc: 0.5014 - val_loss: 1.6113 - val_acc: 0.3580\n",
      "Epoch 1278/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.3159 - acc: 0.4929 - val_loss: 1.5598 - val_acc: 0.3693\n",
      "Epoch 1279/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.3557 - acc: 0.4915 - val_loss: 1.7080 - val_acc: 0.2841\n",
      "Epoch 1280/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 1.3388 - acc: 0.5100 - val_loss: 1.5446 - val_acc: 0.3750\n",
      "Epoch 1281/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.3578 - acc: 0.4872 - val_loss: 1.6410 - val_acc: 0.3182\n",
      "Epoch 1282/3000\n",
      "702/702 [==============================] - 0s 567us/sample - loss: 1.3271 - acc: 0.5100 - val_loss: 1.5632 - val_acc: 0.3864\n",
      "Epoch 1283/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.3269 - acc: 0.5014 - val_loss: 1.7449 - val_acc: 0.2273\n",
      "Epoch 1284/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 1.3085 - acc: 0.5100 - val_loss: 1.5706 - val_acc: 0.3352\n",
      "Epoch 1285/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.3215 - acc: 0.4872 - val_loss: 1.6119 - val_acc: 0.3352\n",
      "Epoch 1286/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.3321 - acc: 0.4986 - val_loss: 1.6355 - val_acc: 0.3182\n",
      "Epoch 1287/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.2784 - acc: 0.5285 - val_loss: 1.5926 - val_acc: 0.3693\n",
      "Epoch 1288/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 1.3100 - acc: 0.4943 - val_loss: 1.6250 - val_acc: 0.3409\n",
      "Epoch 1289/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.3457 - acc: 0.5057 - val_loss: 1.6118 - val_acc: 0.3182\n",
      "Epoch 1290/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.3163 - acc: 0.5014 - val_loss: 1.7393 - val_acc: 0.2443\n",
      "Epoch 1291/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.2992 - acc: 0.5171 - val_loss: 1.6236 - val_acc: 0.3409\n",
      "Epoch 1292/3000\n",
      "702/702 [==============================] - 0s 561us/sample - loss: 1.3239 - acc: 0.5142 - val_loss: 1.6405 - val_acc: 0.3466\n",
      "Epoch 1293/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 1.3205 - acc: 0.5057 - val_loss: 1.6123 - val_acc: 0.3466\n",
      "Epoch 1294/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 1.3507 - acc: 0.4929 - val_loss: 1.6488 - val_acc: 0.3466\n",
      "Epoch 1295/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.3134 - acc: 0.5100 - val_loss: 1.5850 - val_acc: 0.4148\n",
      "Epoch 1296/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 1.3414 - acc: 0.5043 - val_loss: 1.6144 - val_acc: 0.3523\n",
      "Epoch 1297/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.3555 - acc: 0.4758 - val_loss: 1.6531 - val_acc: 0.3466\n",
      "Epoch 1298/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 1.3138 - acc: 0.5114 - val_loss: 1.6383 - val_acc: 0.3295\n",
      "Epoch 1299/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 1.3084 - acc: 0.4957 - val_loss: 1.5903 - val_acc: 0.3580\n",
      "Epoch 1300/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.2909 - acc: 0.5028 - val_loss: 1.5882 - val_acc: 0.3636\n",
      "Epoch 1301/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.2836 - acc: 0.5199 - val_loss: 1.6135 - val_acc: 0.3239\n",
      "Epoch 1302/3000\n",
      "702/702 [==============================] - 0s 556us/sample - loss: 1.3012 - acc: 0.4943 - val_loss: 1.6371 - val_acc: 0.3409\n",
      "Epoch 1303/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 1.3391 - acc: 0.4957 - val_loss: 1.6752 - val_acc: 0.3125\n",
      "Epoch 1304/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 1.2971 - acc: 0.5128 - val_loss: 1.6057 - val_acc: 0.3352\n",
      "Epoch 1305/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 1.3298 - acc: 0.5043 - val_loss: 1.6388 - val_acc: 0.3295\n",
      "Epoch 1306/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 1.3062 - acc: 0.5014 - val_loss: 1.6433 - val_acc: 0.3239\n",
      "Epoch 1307/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 1.2938 - acc: 0.5114 - val_loss: 1.6248 - val_acc: 0.2841\n",
      "Epoch 1308/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.2909 - acc: 0.5014 - val_loss: 1.6724 - val_acc: 0.2841\n",
      "Epoch 1309/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.3041 - acc: 0.5043 - val_loss: 1.5814 - val_acc: 0.3693\n",
      "Epoch 1310/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 1.3067 - acc: 0.5171 - val_loss: 1.6772 - val_acc: 0.2898\n",
      "Epoch 1311/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.3153 - acc: 0.4943 - val_loss: 1.6028 - val_acc: 0.3352\n",
      "Epoch 1312/3000\n",
      "702/702 [==============================] - 0s 550us/sample - loss: 1.3282 - acc: 0.5285 - val_loss: 1.6085 - val_acc: 0.3409\n",
      "Epoch 1313/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 1.2979 - acc: 0.4972 - val_loss: 1.5761 - val_acc: 0.3523\n",
      "Epoch 1314/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 1.3255 - acc: 0.5071 - val_loss: 1.6434 - val_acc: 0.3239\n",
      "Epoch 1315/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.2912 - acc: 0.5214 - val_loss: 1.6734 - val_acc: 0.2727\n",
      "Epoch 1316/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.3253 - acc: 0.5043 - val_loss: 1.6015 - val_acc: 0.3409\n",
      "Epoch 1317/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.2869 - acc: 0.5356 - val_loss: 1.6288 - val_acc: 0.3352\n",
      "Epoch 1318/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.2940 - acc: 0.5342 - val_loss: 1.5605 - val_acc: 0.3636\n",
      "Epoch 1319/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.2758 - acc: 0.5385 - val_loss: 1.6115 - val_acc: 0.3239\n",
      "Epoch 1320/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 1.2772 - acc: 0.5242 - val_loss: 1.5740 - val_acc: 0.3295\n",
      "Epoch 1321/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.2468 - acc: 0.5399 - val_loss: 1.5817 - val_acc: 0.3409\n",
      "Epoch 1322/3000\n",
      "702/702 [==============================] - 0s 578us/sample - loss: 1.2487 - acc: 0.5313 - val_loss: 1.4861 - val_acc: 0.3750\n",
      "Epoch 1323/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 1.2386 - acc: 0.5456 - val_loss: 1.5356 - val_acc: 0.3636\n",
      "Epoch 1324/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 1.2756 - acc: 0.5484 - val_loss: 1.5265 - val_acc: 0.3693\n",
      "Epoch 1325/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 1.3187 - acc: 0.5214 - val_loss: 1.6358 - val_acc: 0.2955\n",
      "Epoch 1326/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 1.2923 - acc: 0.5313 - val_loss: 1.6467 - val_acc: 0.2841\n",
      "Epoch 1327/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 1.2794 - acc: 0.5399 - val_loss: 1.5988 - val_acc: 0.2955\n",
      "Epoch 1328/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.2752 - acc: 0.5313 - val_loss: 1.6419 - val_acc: 0.2727\n",
      "Epoch 1329/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.2967 - acc: 0.5399 - val_loss: 1.5661 - val_acc: 0.3636\n",
      "Epoch 1330/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.2445 - acc: 0.5370 - val_loss: 1.5892 - val_acc: 0.3693\n",
      "Epoch 1331/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.3107 - acc: 0.5356 - val_loss: 1.6699 - val_acc: 0.3182\n",
      "Epoch 1332/3000\n",
      "702/702 [==============================] - 0s 558us/sample - loss: 1.2758 - acc: 0.5356 - val_loss: 1.7086 - val_acc: 0.2898\n",
      "Epoch 1333/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 1.2481 - acc: 0.5385 - val_loss: 1.6558 - val_acc: 0.3295\n",
      "Epoch 1334/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.2580 - acc: 0.5199 - val_loss: 1.5460 - val_acc: 0.3864\n",
      "Epoch 1335/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.2796 - acc: 0.5085 - val_loss: 1.6309 - val_acc: 0.3068\n",
      "Epoch 1336/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.2422 - acc: 0.5527 - val_loss: 1.5678 - val_acc: 0.3125\n",
      "Epoch 1337/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.2653 - acc: 0.5385 - val_loss: 1.6171 - val_acc: 0.3068\n",
      "Epoch 1338/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.2716 - acc: 0.5256 - val_loss: 1.4668 - val_acc: 0.4148\n",
      "Epoch 1339/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.3075 - acc: 0.5313 - val_loss: 1.5602 - val_acc: 0.3295\n",
      "Epoch 1340/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 1.2917 - acc: 0.5427 - val_loss: 1.6790 - val_acc: 0.2614\n",
      "Epoch 1341/3000\n",
      "702/702 [==============================] - 1s 1ms/sample - loss: 1.2432 - acc: 0.5328 - val_loss: 1.5806 - val_acc: 0.3409\n",
      "Epoch 1342/3000\n",
      "702/702 [==============================] - 0s 556us/sample - loss: 1.2622 - acc: 0.5413 - val_loss: 1.6783 - val_acc: 0.2727\n",
      "Epoch 1343/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.2765 - acc: 0.5399 - val_loss: 1.5493 - val_acc: 0.3182\n",
      "Epoch 1344/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.2669 - acc: 0.5271 - val_loss: 1.6302 - val_acc: 0.3011\n",
      "Epoch 1345/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.2612 - acc: 0.5356 - val_loss: 1.5549 - val_acc: 0.3750\n",
      "Epoch 1346/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.2903 - acc: 0.5313 - val_loss: 1.6143 - val_acc: 0.3011\n",
      "Epoch 1347/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 1.2572 - acc: 0.5370 - val_loss: 1.6078 - val_acc: 0.2955\n",
      "Epoch 1348/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 1.2441 - acc: 0.5670 - val_loss: 1.5273 - val_acc: 0.4148\n",
      "Epoch 1349/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 1.2315 - acc: 0.5427 - val_loss: 1.5958 - val_acc: 0.2841\n",
      "Epoch 1350/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 1.2618 - acc: 0.5199 - val_loss: 1.6009 - val_acc: 0.2955\n",
      "Epoch 1351/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.2492 - acc: 0.5228 - val_loss: 1.6940 - val_acc: 0.2784\n",
      "Epoch 1352/3000\n",
      "702/702 [==============================] - 0s 560us/sample - loss: 1.2725 - acc: 0.5513 - val_loss: 1.6906 - val_acc: 0.2670\n",
      "Epoch 1353/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 1.2680 - acc: 0.5256 - val_loss: 1.4996 - val_acc: 0.4148\n",
      "Epoch 1354/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 1.2500 - acc: 0.5484 - val_loss: 1.5255 - val_acc: 0.4205\n",
      "Epoch 1355/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 1.2729 - acc: 0.5157 - val_loss: 1.6260 - val_acc: 0.2955\n",
      "Epoch 1356/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 1.2741 - acc: 0.5228 - val_loss: 1.6791 - val_acc: 0.2841\n",
      "Epoch 1357/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.2121 - acc: 0.5399 - val_loss: 1.6441 - val_acc: 0.2955\n",
      "Epoch 1358/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 1.2884 - acc: 0.5285 - val_loss: 1.4690 - val_acc: 0.4148\n",
      "Epoch 1359/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.2418 - acc: 0.5442 - val_loss: 1.5781 - val_acc: 0.3182\n",
      "Epoch 1360/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.2571 - acc: 0.5413 - val_loss: 1.6441 - val_acc: 0.2784\n",
      "Epoch 1361/3000\n",
      "702/702 [==============================] - 1s 1ms/sample - loss: 1.2642 - acc: 0.5185 - val_loss: 1.5989 - val_acc: 0.2955\n",
      "Epoch 1362/3000\n",
      "702/702 [==============================] - 0s 549us/sample - loss: 1.2731 - acc: 0.5299 - val_loss: 1.7369 - val_acc: 0.2386\n",
      "Epoch 1363/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 1.2890 - acc: 0.5399 - val_loss: 1.5852 - val_acc: 0.3125\n",
      "Epoch 1364/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 1.2772 - acc: 0.5285 - val_loss: 1.7530 - val_acc: 0.2386\n",
      "Epoch 1365/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 1.2529 - acc: 0.5456 - val_loss: 1.7247 - val_acc: 0.2727\n",
      "Epoch 1366/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 1.2413 - acc: 0.5513 - val_loss: 1.6128 - val_acc: 0.4091\n",
      "Epoch 1367/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.2657 - acc: 0.5199 - val_loss: 1.5367 - val_acc: 0.4205\n",
      "Epoch 1368/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 1.2350 - acc: 0.5484 - val_loss: 1.5815 - val_acc: 0.3864\n",
      "Epoch 1369/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.2488 - acc: 0.5513 - val_loss: 1.6266 - val_acc: 0.2841\n",
      "Epoch 1370/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.2729 - acc: 0.5399 - val_loss: 1.6713 - val_acc: 0.2727\n",
      "Epoch 1371/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.2303 - acc: 0.5413 - val_loss: 1.6522 - val_acc: 0.3011\n",
      "Epoch 1372/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 1.2297 - acc: 0.5242 - val_loss: 1.6172 - val_acc: 0.2784\n",
      "Epoch 1373/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 1.2717 - acc: 0.5385 - val_loss: 1.7149 - val_acc: 0.2557\n",
      "Epoch 1374/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.2446 - acc: 0.5499 - val_loss: 1.6579 - val_acc: 0.2841\n",
      "Epoch 1375/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.2480 - acc: 0.5598 - val_loss: 1.6583 - val_acc: 0.2670\n",
      "Epoch 1376/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.2442 - acc: 0.5370 - val_loss: 1.6685 - val_acc: 0.2784\n",
      "Epoch 1377/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.2640 - acc: 0.5128 - val_loss: 1.6069 - val_acc: 0.2955\n",
      "Epoch 1378/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.2447 - acc: 0.5427 - val_loss: 1.5535 - val_acc: 0.3182\n",
      "Epoch 1379/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.2515 - acc: 0.5484 - val_loss: 1.6792 - val_acc: 0.2670\n",
      "Epoch 1380/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 1.2273 - acc: 0.5413 - val_loss: 1.6216 - val_acc: 0.2841\n",
      "Epoch 1381/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.2450 - acc: 0.5613 - val_loss: 1.5669 - val_acc: 0.3580\n",
      "Epoch 1382/3000\n",
      "702/702 [==============================] - 0s 558us/sample - loss: 1.2654 - acc: 0.5399 - val_loss: 1.6499 - val_acc: 0.3125\n",
      "Epoch 1383/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.2788 - acc: 0.5484 - val_loss: 1.6575 - val_acc: 0.2898\n",
      "Epoch 1384/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 1.2478 - acc: 0.5570 - val_loss: 1.6204 - val_acc: 0.3125\n",
      "Epoch 1385/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.2123 - acc: 0.5499 - val_loss: 1.7095 - val_acc: 0.2898\n",
      "Epoch 1386/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.2394 - acc: 0.5356 - val_loss: 1.7234 - val_acc: 0.2614\n",
      "Epoch 1387/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 1.2466 - acc: 0.5242 - val_loss: 1.6565 - val_acc: 0.2670\n",
      "Epoch 1388/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 1.2717 - acc: 0.5214 - val_loss: 1.8122 - val_acc: 0.2557\n",
      "Epoch 1389/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 1.2824 - acc: 0.5271 - val_loss: 1.7308 - val_acc: 0.2557\n",
      "Epoch 1390/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 1.2613 - acc: 0.5385 - val_loss: 1.5507 - val_acc: 0.3352\n",
      "Epoch 1391/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.2906 - acc: 0.5256 - val_loss: 1.6504 - val_acc: 0.2955\n",
      "Epoch 1392/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 1.2279 - acc: 0.5285 - val_loss: 1.6045 - val_acc: 0.3239\n",
      "Epoch 1393/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.2455 - acc: 0.5199 - val_loss: 1.5325 - val_acc: 0.3523\n",
      "Epoch 1394/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.2183 - acc: 0.5385 - val_loss: 1.6588 - val_acc: 0.2898\n",
      "Epoch 1395/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.2377 - acc: 0.5370 - val_loss: 1.6125 - val_acc: 0.3182\n",
      "Epoch 1396/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 1.2851 - acc: 0.5328 - val_loss: 1.5609 - val_acc: 0.3409\n",
      "Epoch 1397/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.2462 - acc: 0.5313 - val_loss: 1.6580 - val_acc: 0.2841\n",
      "Epoch 1398/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 1.2250 - acc: 0.5299 - val_loss: 1.5185 - val_acc: 0.3864\n",
      "Epoch 1399/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.2453 - acc: 0.5214 - val_loss: 1.5755 - val_acc: 0.3636\n",
      "Epoch 1400/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.2714 - acc: 0.5157 - val_loss: 1.6131 - val_acc: 0.3239\n",
      "Epoch 1401/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.2596 - acc: 0.5171 - val_loss: 1.5690 - val_acc: 0.3068\n",
      "Epoch 1402/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 1.2481 - acc: 0.5399 - val_loss: 1.6704 - val_acc: 0.2841\n",
      "Epoch 1403/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 1.2788 - acc: 0.5128 - val_loss: 1.6046 - val_acc: 0.3466\n",
      "Epoch 1404/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.2268 - acc: 0.5385 - val_loss: 1.5350 - val_acc: 0.3466\n",
      "Epoch 1405/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.2890 - acc: 0.5342 - val_loss: 1.7731 - val_acc: 0.2500\n",
      "Epoch 1406/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.2298 - acc: 0.5242 - val_loss: 1.6653 - val_acc: 0.2670\n",
      "Epoch 1407/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 1.2119 - acc: 0.5442 - val_loss: 1.4492 - val_acc: 0.4091\n",
      "Epoch 1408/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.2541 - acc: 0.5057 - val_loss: 1.6589 - val_acc: 0.2727\n",
      "Epoch 1409/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.1961 - acc: 0.5570 - val_loss: 1.6456 - val_acc: 0.2841\n",
      "Epoch 1410/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.2099 - acc: 0.5684 - val_loss: 1.4552 - val_acc: 0.4432\n",
      "Epoch 1411/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.2168 - acc: 0.5385 - val_loss: 1.4521 - val_acc: 0.4148\n",
      "Epoch 1412/3000\n",
      "702/702 [==============================] - 0s 550us/sample - loss: 1.2198 - acc: 0.5527 - val_loss: 1.5471 - val_acc: 0.3295\n",
      "Epoch 1413/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.2675 - acc: 0.5470 - val_loss: 1.6490 - val_acc: 0.2670\n",
      "Epoch 1414/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 1.2048 - acc: 0.5627 - val_loss: 1.5727 - val_acc: 0.2784\n",
      "Epoch 1415/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.2286 - acc: 0.5299 - val_loss: 1.5824 - val_acc: 0.3068\n",
      "Epoch 1416/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.2505 - acc: 0.5256 - val_loss: 1.4887 - val_acc: 0.3466\n",
      "Epoch 1417/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 1.2709 - acc: 0.5313 - val_loss: 1.6710 - val_acc: 0.2557\n",
      "Epoch 1418/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.1876 - acc: 0.5513 - val_loss: 1.5403 - val_acc: 0.3125\n",
      "Epoch 1419/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 1.2332 - acc: 0.5256 - val_loss: 1.4696 - val_acc: 0.4261\n",
      "Epoch 1420/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.2372 - acc: 0.5470 - val_loss: 1.5042 - val_acc: 0.4318\n",
      "Epoch 1421/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.1962 - acc: 0.5613 - val_loss: 1.6048 - val_acc: 0.3011\n",
      "Epoch 1422/3000\n",
      "702/702 [==============================] - 0s 564us/sample - loss: 1.2552 - acc: 0.5584 - val_loss: 1.5281 - val_acc: 0.3750\n",
      "Epoch 1423/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.2232 - acc: 0.5584 - val_loss: 1.6037 - val_acc: 0.2955\n",
      "Epoch 1424/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.2270 - acc: 0.5570 - val_loss: 1.5471 - val_acc: 0.3352\n",
      "Epoch 1425/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.2205 - acc: 0.5541 - val_loss: 1.5262 - val_acc: 0.3750\n",
      "Epoch 1426/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 1.2037 - acc: 0.5570 - val_loss: 1.6070 - val_acc: 0.2841\n",
      "Epoch 1427/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 1.1853 - acc: 0.5570 - val_loss: 1.5520 - val_acc: 0.3295\n",
      "Epoch 1428/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.1793 - acc: 0.5641 - val_loss: 1.5633 - val_acc: 0.3011\n",
      "Epoch 1429/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 1.2427 - acc: 0.5598 - val_loss: 1.6028 - val_acc: 0.2784\n",
      "Epoch 1430/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.2376 - acc: 0.5527 - val_loss: 1.6163 - val_acc: 0.2784\n",
      "Epoch 1431/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.1924 - acc: 0.5427 - val_loss: 1.5282 - val_acc: 0.3239\n",
      "Epoch 1432/3000\n",
      "702/702 [==============================] - 0s 564us/sample - loss: 1.2234 - acc: 0.5513 - val_loss: 1.6199 - val_acc: 0.2841\n",
      "Epoch 1433/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 1.2148 - acc: 0.5513 - val_loss: 1.5819 - val_acc: 0.2955\n",
      "Epoch 1434/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.1923 - acc: 0.5399 - val_loss: 1.6024 - val_acc: 0.2955\n",
      "Epoch 1435/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.1879 - acc: 0.5584 - val_loss: 1.5570 - val_acc: 0.3239\n",
      "Epoch 1436/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.2201 - acc: 0.5427 - val_loss: 1.5941 - val_acc: 0.3182\n",
      "Epoch 1437/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.1900 - acc: 0.5556 - val_loss: 1.4929 - val_acc: 0.3636\n",
      "Epoch 1438/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.2153 - acc: 0.5484 - val_loss: 1.7131 - val_acc: 0.2500\n",
      "Epoch 1439/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 1.2244 - acc: 0.5684 - val_loss: 1.6298 - val_acc: 0.2727\n",
      "Epoch 1440/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 1.2405 - acc: 0.5470 - val_loss: 1.5642 - val_acc: 0.2955\n",
      "Epoch 1441/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.2185 - acc: 0.5670 - val_loss: 1.5734 - val_acc: 0.2955\n",
      "Epoch 1442/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 1.2130 - acc: 0.5584 - val_loss: 1.5503 - val_acc: 0.3352\n",
      "Epoch 1443/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 1.2149 - acc: 0.5413 - val_loss: 1.4498 - val_acc: 0.4318\n",
      "Epoch 1444/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.2153 - acc: 0.5527 - val_loss: 1.6690 - val_acc: 0.3011\n",
      "Epoch 1445/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.2549 - acc: 0.5399 - val_loss: 1.5471 - val_acc: 0.3920\n",
      "Epoch 1446/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 1.1745 - acc: 0.5670 - val_loss: 1.4406 - val_acc: 0.4318\n",
      "Epoch 1447/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 1.2186 - acc: 0.5484 - val_loss: 1.5779 - val_acc: 0.3011\n",
      "Epoch 1448/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.1678 - acc: 0.5598 - val_loss: 1.5064 - val_acc: 0.3295\n",
      "Epoch 1449/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 1.2326 - acc: 0.5484 - val_loss: 1.6752 - val_acc: 0.2614\n",
      "Epoch 1450/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 1.2275 - acc: 0.5470 - val_loss: 1.5371 - val_acc: 0.3352\n",
      "Epoch 1451/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.2474 - acc: 0.5442 - val_loss: 1.4443 - val_acc: 0.4318\n",
      "Epoch 1452/3000\n",
      "702/702 [==============================] - 0s 586us/sample - loss: 1.2360 - acc: 0.5655 - val_loss: 1.5656 - val_acc: 0.2784\n",
      "Epoch 1453/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.2198 - acc: 0.5499 - val_loss: 1.6011 - val_acc: 0.2898\n",
      "Epoch 1454/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.2052 - acc: 0.5684 - val_loss: 1.6160 - val_acc: 0.2898\n",
      "Epoch 1455/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 1.2639 - acc: 0.5342 - val_loss: 1.7237 - val_acc: 0.2670\n",
      "Epoch 1456/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 1.2314 - acc: 0.5370 - val_loss: 1.4000 - val_acc: 0.4773\n",
      "Epoch 1457/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 1.1952 - acc: 0.5541 - val_loss: 1.5539 - val_acc: 0.3864\n",
      "Epoch 1458/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 1.2306 - acc: 0.5527 - val_loss: 1.6059 - val_acc: 0.3011\n",
      "Epoch 1459/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 1.2145 - acc: 0.5527 - val_loss: 1.5728 - val_acc: 0.3295\n",
      "Epoch 1460/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 1.1865 - acc: 0.5556 - val_loss: 1.4393 - val_acc: 0.4205\n",
      "Epoch 1461/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.2436 - acc: 0.5370 - val_loss: 1.6320 - val_acc: 0.2727\n",
      "Epoch 1462/3000\n",
      "702/702 [==============================] - 0s 589us/sample - loss: 1.2430 - acc: 0.5456 - val_loss: 1.6127 - val_acc: 0.2784\n",
      "Epoch 1463/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.2086 - acc: 0.5570 - val_loss: 1.5256 - val_acc: 0.3125\n",
      "Epoch 1464/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 1.1805 - acc: 0.5741 - val_loss: 1.5611 - val_acc: 0.3523\n",
      "Epoch 1465/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 1.2211 - acc: 0.5385 - val_loss: 1.4480 - val_acc: 0.4205\n",
      "Epoch 1466/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 1.2142 - acc: 0.5499 - val_loss: 1.4750 - val_acc: 0.4489\n",
      "Epoch 1467/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 1.2527 - acc: 0.5385 - val_loss: 1.6487 - val_acc: 0.2955\n",
      "Epoch 1468/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 1.2020 - acc: 0.5499 - val_loss: 1.5862 - val_acc: 0.3068\n",
      "Epoch 1469/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 1.2078 - acc: 0.5655 - val_loss: 1.4447 - val_acc: 0.4034\n",
      "Epoch 1470/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 1.2185 - acc: 0.5584 - val_loss: 1.4578 - val_acc: 0.4091\n",
      "Epoch 1471/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.2048 - acc: 0.5712 - val_loss: 1.5604 - val_acc: 0.3580\n",
      "Epoch 1472/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 1.1883 - acc: 0.5513 - val_loss: 1.5520 - val_acc: 0.3750\n",
      "Epoch 1473/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 1.2064 - acc: 0.5499 - val_loss: 1.4993 - val_acc: 0.4205\n",
      "Epoch 1474/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 1.1618 - acc: 0.5541 - val_loss: 1.4807 - val_acc: 0.3977\n",
      "Epoch 1475/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 1.1732 - acc: 0.5783 - val_loss: 1.5752 - val_acc: 0.2955\n",
      "Epoch 1476/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.2179 - acc: 0.5513 - val_loss: 1.6262 - val_acc: 0.2841\n",
      "Epoch 1477/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.2103 - acc: 0.5755 - val_loss: 1.5429 - val_acc: 0.3409\n",
      "Epoch 1478/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 1.1897 - acc: 0.5627 - val_loss: 1.4718 - val_acc: 0.4091\n",
      "Epoch 1479/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 1.2046 - acc: 0.5442 - val_loss: 1.4809 - val_acc: 0.4261\n",
      "Epoch 1480/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.1644 - acc: 0.5570 - val_loss: 1.5654 - val_acc: 0.3182\n",
      "Epoch 1481/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.2407 - acc: 0.5513 - val_loss: 1.5458 - val_acc: 0.3239\n",
      "Epoch 1482/3000\n",
      "702/702 [==============================] - 0s 556us/sample - loss: 1.1848 - acc: 0.5670 - val_loss: 1.5312 - val_acc: 0.3352\n",
      "Epoch 1483/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 1.2139 - acc: 0.5584 - val_loss: 1.5666 - val_acc: 0.3125\n",
      "Epoch 1484/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 1.1627 - acc: 0.5712 - val_loss: 1.5400 - val_acc: 0.3352\n",
      "Epoch 1485/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 1.1636 - acc: 0.5755 - val_loss: 1.4505 - val_acc: 0.3977\n",
      "Epoch 1486/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.1560 - acc: 0.5755 - val_loss: 1.5258 - val_acc: 0.3693\n",
      "Epoch 1487/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 1.1803 - acc: 0.5684 - val_loss: 1.4993 - val_acc: 0.3409\n",
      "Epoch 1488/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 1.1873 - acc: 0.5513 - val_loss: 1.5164 - val_acc: 0.3295\n",
      "Epoch 1489/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.1908 - acc: 0.5613 - val_loss: 1.5598 - val_acc: 0.2955\n",
      "Epoch 1490/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 1.1698 - acc: 0.5627 - val_loss: 1.5585 - val_acc: 0.3125\n",
      "Epoch 1491/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.1348 - acc: 0.5684 - val_loss: 1.3623 - val_acc: 0.5000\n",
      "Epoch 1492/3000\n",
      "702/702 [==============================] - 0s 555us/sample - loss: 1.2029 - acc: 0.5741 - val_loss: 1.4662 - val_acc: 0.4205\n",
      "Epoch 1493/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.2156 - acc: 0.5584 - val_loss: 1.4857 - val_acc: 0.3693\n",
      "Epoch 1494/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 1.1823 - acc: 0.5698 - val_loss: 1.5020 - val_acc: 0.3352\n",
      "Epoch 1495/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.2141 - acc: 0.5670 - val_loss: 1.4938 - val_acc: 0.3125\n",
      "Epoch 1496/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 1.2234 - acc: 0.5527 - val_loss: 1.5584 - val_acc: 0.3011\n",
      "Epoch 1497/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 1.2031 - acc: 0.5570 - val_loss: 1.4938 - val_acc: 0.3409\n",
      "Epoch 1498/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 1.1518 - acc: 0.5755 - val_loss: 1.4541 - val_acc: 0.4091\n",
      "Epoch 1499/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.1583 - acc: 0.5912 - val_loss: 1.5088 - val_acc: 0.3750\n",
      "Epoch 1500/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.1919 - acc: 0.5655 - val_loss: 1.5405 - val_acc: 0.3295\n",
      "Epoch 1501/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.1937 - acc: 0.5613 - val_loss: 1.6213 - val_acc: 0.3011\n",
      "Epoch 1502/3000\n",
      "702/702 [==============================] - 0s 556us/sample - loss: 1.1527 - acc: 0.5726 - val_loss: 1.4050 - val_acc: 0.4205\n",
      "Epoch 1503/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.2167 - acc: 0.5456 - val_loss: 1.6464 - val_acc: 0.2670\n",
      "Epoch 1504/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 1.2333 - acc: 0.5556 - val_loss: 1.5626 - val_acc: 0.3125\n",
      "Epoch 1505/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 1.1620 - acc: 0.5584 - val_loss: 1.5009 - val_acc: 0.3409\n",
      "Epoch 1506/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.1632 - acc: 0.5584 - val_loss: 1.5240 - val_acc: 0.3295\n",
      "Epoch 1507/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.1871 - acc: 0.5840 - val_loss: 1.4296 - val_acc: 0.4205\n",
      "Epoch 1508/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 1.1677 - acc: 0.5712 - val_loss: 1.6134 - val_acc: 0.2841\n",
      "Epoch 1509/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.1841 - acc: 0.5598 - val_loss: 1.5774 - val_acc: 0.3182\n",
      "Epoch 1510/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.1582 - acc: 0.5755 - val_loss: 1.3129 - val_acc: 0.5170\n",
      "Epoch 1511/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.1945 - acc: 0.5712 - val_loss: 1.5711 - val_acc: 0.3466\n",
      "Epoch 1512/3000\n",
      "702/702 [==============================] - 0s 566us/sample - loss: 1.1925 - acc: 0.5527 - val_loss: 1.5877 - val_acc: 0.3068\n",
      "Epoch 1513/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 1.2218 - acc: 0.5598 - val_loss: 1.6059 - val_acc: 0.3182\n",
      "Epoch 1514/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.1868 - acc: 0.5598 - val_loss: 1.5704 - val_acc: 0.2955\n",
      "Epoch 1515/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 1.2144 - acc: 0.5556 - val_loss: 1.5435 - val_acc: 0.3466\n",
      "Epoch 1516/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.2066 - acc: 0.5541 - val_loss: 1.4900 - val_acc: 0.3807\n",
      "Epoch 1517/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.1954 - acc: 0.5627 - val_loss: 1.6040 - val_acc: 0.3125\n",
      "Epoch 1518/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 1.1866 - acc: 0.5670 - val_loss: 1.5173 - val_acc: 0.3636\n",
      "Epoch 1519/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 1.1801 - acc: 0.5513 - val_loss: 1.3940 - val_acc: 0.4318\n",
      "Epoch 1520/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 1.1388 - acc: 0.5783 - val_loss: 1.5343 - val_acc: 0.3295\n",
      "Epoch 1521/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.1764 - acc: 0.5741 - val_loss: 1.4535 - val_acc: 0.3864\n",
      "Epoch 1522/3000\n",
      "702/702 [==============================] - 0s 556us/sample - loss: 1.2073 - acc: 0.5556 - val_loss: 1.6229 - val_acc: 0.2955\n",
      "Epoch 1523/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.1733 - acc: 0.5840 - val_loss: 1.5819 - val_acc: 0.3011\n",
      "Epoch 1524/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.1430 - acc: 0.5527 - val_loss: 1.5053 - val_acc: 0.3750\n",
      "Epoch 1525/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.1868 - acc: 0.5627 - val_loss: 1.6396 - val_acc: 0.3011\n",
      "Epoch 1526/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.1800 - acc: 0.5627 - val_loss: 1.4878 - val_acc: 0.3864\n",
      "Epoch 1527/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.1562 - acc: 0.5570 - val_loss: 1.4079 - val_acc: 0.4318\n",
      "Epoch 1528/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.2475 - acc: 0.5456 - val_loss: 1.5274 - val_acc: 0.3466\n",
      "Epoch 1529/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 1.1688 - acc: 0.5798 - val_loss: 1.6191 - val_acc: 0.2955\n",
      "Epoch 1530/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.1660 - acc: 0.5826 - val_loss: 1.4533 - val_acc: 0.4489\n",
      "Epoch 1531/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.1543 - acc: 0.5755 - val_loss: 1.4568 - val_acc: 0.4261\n",
      "Epoch 1532/3000\n",
      "702/702 [==============================] - 0s 554us/sample - loss: 1.1624 - acc: 0.5826 - val_loss: 1.5662 - val_acc: 0.3068\n",
      "Epoch 1533/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 1.2139 - acc: 0.5499 - val_loss: 1.5291 - val_acc: 0.3466\n",
      "Epoch 1534/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.1684 - acc: 0.5613 - val_loss: 1.4721 - val_acc: 0.4091\n",
      "Epoch 1535/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 1.1902 - acc: 0.5442 - val_loss: 1.5423 - val_acc: 0.3693\n",
      "Epoch 1536/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 1.1653 - acc: 0.5698 - val_loss: 1.4985 - val_acc: 0.3352\n",
      "Epoch 1537/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.2066 - acc: 0.5613 - val_loss: 1.6218 - val_acc: 0.2898\n",
      "Epoch 1538/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 1.1690 - acc: 0.5741 - val_loss: 1.5652 - val_acc: 0.3466\n",
      "Epoch 1539/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.1566 - acc: 0.5684 - val_loss: 1.5292 - val_acc: 0.3693\n",
      "Epoch 1540/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 1.1722 - acc: 0.5883 - val_loss: 1.5043 - val_acc: 0.3636\n",
      "Epoch 1541/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.1727 - acc: 0.5598 - val_loss: 1.6212 - val_acc: 0.3011\n",
      "Epoch 1542/3000\n",
      "702/702 [==============================] - 0s 558us/sample - loss: 1.1629 - acc: 0.5684 - val_loss: 1.5114 - val_acc: 0.3750\n",
      "Epoch 1543/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.2223 - acc: 0.5826 - val_loss: 1.5918 - val_acc: 0.3011\n",
      "Epoch 1544/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.1773 - acc: 0.5769 - val_loss: 1.4910 - val_acc: 0.3920\n",
      "Epoch 1545/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.1837 - acc: 0.5712 - val_loss: 1.5394 - val_acc: 0.3466\n",
      "Epoch 1546/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.1879 - acc: 0.5484 - val_loss: 1.6554 - val_acc: 0.2955\n",
      "Epoch 1547/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.1522 - acc: 0.5783 - val_loss: 1.3822 - val_acc: 0.4886\n",
      "Epoch 1548/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.2099 - acc: 0.5499 - val_loss: 1.4753 - val_acc: 0.4375\n",
      "Epoch 1549/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.1620 - acc: 0.5926 - val_loss: 1.5898 - val_acc: 0.3352\n",
      "Epoch 1550/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.1680 - acc: 0.5556 - val_loss: 1.4699 - val_acc: 0.4318\n",
      "Epoch 1551/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.1956 - acc: 0.5570 - val_loss: 1.4903 - val_acc: 0.3580\n",
      "Epoch 1552/3000\n",
      "702/702 [==============================] - 0s 568us/sample - loss: 1.1557 - acc: 0.5812 - val_loss: 1.6067 - val_acc: 0.3239\n",
      "Epoch 1553/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.2000 - acc: 0.5741 - val_loss: 1.4688 - val_acc: 0.4375\n",
      "Epoch 1554/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 1.1867 - acc: 0.5613 - val_loss: 1.3880 - val_acc: 0.5000\n",
      "Epoch 1555/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 1.1271 - acc: 0.5855 - val_loss: 1.5947 - val_acc: 0.3523\n",
      "Epoch 1556/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 1.2023 - acc: 0.5883 - val_loss: 1.7272 - val_acc: 0.3068\n",
      "Epoch 1557/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.1957 - acc: 0.5912 - val_loss: 1.4075 - val_acc: 0.4659\n",
      "Epoch 1558/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.1843 - acc: 0.5769 - val_loss: 1.5682 - val_acc: 0.4034\n",
      "Epoch 1559/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.1348 - acc: 0.6011 - val_loss: 1.6811 - val_acc: 0.3125\n",
      "Epoch 1560/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.1182 - acc: 0.5869 - val_loss: 1.6065 - val_acc: 0.3693\n",
      "Epoch 1561/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.0892 - acc: 0.5954 - val_loss: 1.6577 - val_acc: 0.4034\n",
      "Epoch 1562/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 1.1589 - acc: 0.5926 - val_loss: 1.5930 - val_acc: 0.3807\n",
      "Epoch 1563/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 1.2385 - acc: 0.5627 - val_loss: 1.4778 - val_acc: 0.4034\n",
      "Epoch 1564/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.1012 - acc: 0.5969 - val_loss: 1.6951 - val_acc: 0.3977\n",
      "Epoch 1565/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.1516 - acc: 0.6011 - val_loss: 1.4762 - val_acc: 0.4091\n",
      "Epoch 1566/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.0906 - acc: 0.6125 - val_loss: 1.5913 - val_acc: 0.3636\n",
      "Epoch 1567/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.1100 - acc: 0.6026 - val_loss: 1.5840 - val_acc: 0.3409\n",
      "Epoch 1568/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.0875 - acc: 0.6068 - val_loss: 1.5532 - val_acc: 0.4034\n",
      "Epoch 1569/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 1.0504 - acc: 0.6083 - val_loss: 1.5707 - val_acc: 0.4318\n",
      "Epoch 1570/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.0797 - acc: 0.6054 - val_loss: 1.4648 - val_acc: 0.4489\n",
      "Epoch 1571/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.0924 - acc: 0.6097 - val_loss: 1.4405 - val_acc: 0.4375\n",
      "Epoch 1572/3000\n",
      "702/702 [==============================] - 0s 566us/sample - loss: 1.1024 - acc: 0.6140 - val_loss: 1.3750 - val_acc: 0.4205\n",
      "Epoch 1573/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 1.0520 - acc: 0.6140 - val_loss: 1.5524 - val_acc: 0.4091\n",
      "Epoch 1574/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.0284 - acc: 0.6225 - val_loss: 1.3247 - val_acc: 0.4602\n",
      "Epoch 1575/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.0506 - acc: 0.6197 - val_loss: 1.4459 - val_acc: 0.3977\n",
      "Epoch 1576/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 1.0150 - acc: 0.6325 - val_loss: 1.4370 - val_acc: 0.4034\n",
      "Epoch 1577/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 1.0309 - acc: 0.6282 - val_loss: 1.3722 - val_acc: 0.4375\n",
      "Epoch 1578/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.0197 - acc: 0.6111 - val_loss: 1.4256 - val_acc: 0.4205\n",
      "Epoch 1579/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.0208 - acc: 0.6353 - val_loss: 1.2765 - val_acc: 0.5057\n",
      "Epoch 1580/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.9787 - acc: 0.6325 - val_loss: 1.4016 - val_acc: 0.4375\n",
      "Epoch 1581/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.0141 - acc: 0.6282 - val_loss: 1.3641 - val_acc: 0.4602\n",
      "Epoch 1582/3000\n",
      "702/702 [==============================] - 0s 558us/sample - loss: 1.0224 - acc: 0.6197 - val_loss: 1.4749 - val_acc: 0.4205\n",
      "Epoch 1583/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 1.0264 - acc: 0.6239 - val_loss: 1.4015 - val_acc: 0.4261\n",
      "Epoch 1584/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.0283 - acc: 0.6254 - val_loss: 1.5145 - val_acc: 0.4318\n",
      "Epoch 1585/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.0223 - acc: 0.6154 - val_loss: 1.3241 - val_acc: 0.4773\n",
      "Epoch 1586/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 1.0030 - acc: 0.6396 - val_loss: 1.2928 - val_acc: 0.5114\n",
      "Epoch 1587/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.0031 - acc: 0.6353 - val_loss: 1.3523 - val_acc: 0.4432\n",
      "Epoch 1588/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.9913 - acc: 0.6410 - val_loss: 1.2290 - val_acc: 0.5227\n",
      "Epoch 1589/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 1.0302 - acc: 0.6339 - val_loss: 1.4162 - val_acc: 0.4432\n",
      "Epoch 1590/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.9851 - acc: 0.6225 - val_loss: 1.3407 - val_acc: 0.4602\n",
      "Epoch 1591/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.9546 - acc: 0.6510 - val_loss: 1.2825 - val_acc: 0.4943\n",
      "Epoch 1592/3000\n",
      "702/702 [==============================] - 0s 558us/sample - loss: 0.9870 - acc: 0.6425 - val_loss: 1.2532 - val_acc: 0.5000\n",
      "Epoch 1593/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.9678 - acc: 0.6339 - val_loss: 1.3036 - val_acc: 0.4773\n",
      "Epoch 1594/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.9939 - acc: 0.6325 - val_loss: 1.2789 - val_acc: 0.5057\n",
      "Epoch 1595/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.9762 - acc: 0.6368 - val_loss: 1.2479 - val_acc: 0.5114\n",
      "Epoch 1596/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.9728 - acc: 0.6453 - val_loss: 1.2913 - val_acc: 0.4943\n",
      "Epoch 1597/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.0203 - acc: 0.6254 - val_loss: 1.4731 - val_acc: 0.4318\n",
      "Epoch 1598/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.9998 - acc: 0.6197 - val_loss: 1.0953 - val_acc: 0.6023\n",
      "Epoch 1599/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.0055 - acc: 0.6282 - val_loss: 1.2858 - val_acc: 0.5000\n",
      "Epoch 1600/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.0139 - acc: 0.6239 - val_loss: 1.2627 - val_acc: 0.5000\n",
      "Epoch 1601/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.0555 - acc: 0.6211 - val_loss: 1.3833 - val_acc: 0.4545\n",
      "Epoch 1602/3000\n",
      "702/702 [==============================] - 0s 568us/sample - loss: 0.9819 - acc: 0.6368 - val_loss: 1.2557 - val_acc: 0.5341\n",
      "Epoch 1603/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.9717 - acc: 0.6439 - val_loss: 1.2518 - val_acc: 0.5114\n",
      "Epoch 1604/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.0061 - acc: 0.6339 - val_loss: 1.1712 - val_acc: 0.5398\n",
      "Epoch 1605/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.9684 - acc: 0.6339 - val_loss: 1.1990 - val_acc: 0.5284\n",
      "Epoch 1606/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.9957 - acc: 0.6339 - val_loss: 1.1836 - val_acc: 0.5341\n",
      "Epoch 1607/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.9539 - acc: 0.6368 - val_loss: 1.2149 - val_acc: 0.5455\n",
      "Epoch 1608/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.0031 - acc: 0.6453 - val_loss: 1.1238 - val_acc: 0.5682\n",
      "Epoch 1609/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.9655 - acc: 0.6353 - val_loss: 1.2231 - val_acc: 0.5341\n",
      "Epoch 1610/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.9476 - acc: 0.6524 - val_loss: 1.1641 - val_acc: 0.5625\n",
      "Epoch 1611/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.9892 - acc: 0.6239 - val_loss: 1.2290 - val_acc: 0.5284\n",
      "Epoch 1612/3000\n",
      "702/702 [==============================] - 0s 553us/sample - loss: 1.0232 - acc: 0.6382 - val_loss: 1.2071 - val_acc: 0.5398\n",
      "Epoch 1613/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.9818 - acc: 0.6396 - val_loss: 1.2875 - val_acc: 0.5057\n",
      "Epoch 1614/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.0014 - acc: 0.6268 - val_loss: 1.2434 - val_acc: 0.5227\n",
      "Epoch 1615/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.0124 - acc: 0.6296 - val_loss: 1.1376 - val_acc: 0.5625\n",
      "Epoch 1616/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.0312 - acc: 0.6396 - val_loss: 1.4674 - val_acc: 0.4375\n",
      "Epoch 1617/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.0318 - acc: 0.6182 - val_loss: 1.2565 - val_acc: 0.5398\n",
      "Epoch 1618/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.9831 - acc: 0.6083 - val_loss: 1.3206 - val_acc: 0.5000\n",
      "Epoch 1619/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.9407 - acc: 0.6538 - val_loss: 1.2608 - val_acc: 0.5114\n",
      "Epoch 1620/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.9467 - acc: 0.6396 - val_loss: 1.1287 - val_acc: 0.5568\n",
      "Epoch 1621/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.9989 - acc: 0.6382 - val_loss: 1.1851 - val_acc: 0.5227\n",
      "Epoch 1622/3000\n",
      "702/702 [==============================] - 0s 549us/sample - loss: 0.9805 - acc: 0.6353 - val_loss: 1.2372 - val_acc: 0.5284\n",
      "Epoch 1623/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.9790 - acc: 0.6268 - val_loss: 1.1731 - val_acc: 0.5568\n",
      "Epoch 1624/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 1.0473 - acc: 0.6182 - val_loss: 1.2607 - val_acc: 0.5227\n",
      "Epoch 1625/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.9893 - acc: 0.6410 - val_loss: 1.1579 - val_acc: 0.5511\n",
      "Epoch 1626/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.9686 - acc: 0.6439 - val_loss: 1.2196 - val_acc: 0.5284\n",
      "Epoch 1627/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 1.0332 - acc: 0.6325 - val_loss: 1.1752 - val_acc: 0.5568\n",
      "Epoch 1628/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.9700 - acc: 0.6325 - val_loss: 1.1704 - val_acc: 0.5455\n",
      "Epoch 1629/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.9712 - acc: 0.6168 - val_loss: 1.1558 - val_acc: 0.5795\n",
      "Epoch 1630/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.9882 - acc: 0.6396 - val_loss: 1.1447 - val_acc: 0.5682\n",
      "Epoch 1631/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.9782 - acc: 0.6453 - val_loss: 1.1236 - val_acc: 0.5852\n",
      "Epoch 1632/3000\n",
      "702/702 [==============================] - 0s 548us/sample - loss: 1.0111 - acc: 0.6197 - val_loss: 1.2855 - val_acc: 0.5114\n",
      "Epoch 1633/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 1.0063 - acc: 0.6197 - val_loss: 1.1841 - val_acc: 0.5625\n",
      "Epoch 1634/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.9991 - acc: 0.6254 - val_loss: 1.1173 - val_acc: 0.5795\n",
      "Epoch 1635/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.9584 - acc: 0.6453 - val_loss: 1.0829 - val_acc: 0.5909\n",
      "Epoch 1636/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 1.0045 - acc: 0.6339 - val_loss: 1.0348 - val_acc: 0.6250\n",
      "Epoch 1637/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 1.0123 - acc: 0.6339 - val_loss: 1.2523 - val_acc: 0.5170\n",
      "Epoch 1638/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 1.0111 - acc: 0.6353 - val_loss: 1.2451 - val_acc: 0.5341\n",
      "Epoch 1639/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.0169 - acc: 0.6325 - val_loss: 1.2871 - val_acc: 0.5057\n",
      "Epoch 1640/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.9877 - acc: 0.6296 - val_loss: 1.2101 - val_acc: 0.5284\n",
      "Epoch 1641/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.9784 - acc: 0.6268 - val_loss: 1.3750 - val_acc: 0.4602\n",
      "Epoch 1642/3000\n",
      "702/702 [==============================] - 0s 542us/sample - loss: 0.9706 - acc: 0.6125 - val_loss: 1.0941 - val_acc: 0.6307\n",
      "Epoch 1643/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.0352 - acc: 0.6382 - val_loss: 1.2236 - val_acc: 0.5455\n",
      "Epoch 1644/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 1.0256 - acc: 0.6268 - val_loss: 1.1373 - val_acc: 0.5739\n",
      "Epoch 1645/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.9541 - acc: 0.6353 - val_loss: 1.1599 - val_acc: 0.5909\n",
      "Epoch 1646/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.0233 - acc: 0.6111 - val_loss: 1.3381 - val_acc: 0.4602\n",
      "Epoch 1647/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.9802 - acc: 0.6325 - val_loss: 1.2187 - val_acc: 0.5170\n",
      "Epoch 1648/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.0152 - acc: 0.6168 - val_loss: 1.1576 - val_acc: 0.5625\n",
      "Epoch 1649/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.9802 - acc: 0.6311 - val_loss: 1.1239 - val_acc: 0.5909\n",
      "Epoch 1650/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 1.0019 - acc: 0.6254 - val_loss: 1.2374 - val_acc: 0.5170\n",
      "Epoch 1651/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.9766 - acc: 0.6254 - val_loss: 1.2030 - val_acc: 0.5511\n",
      "Epoch 1652/3000\n",
      "702/702 [==============================] - 0s 558us/sample - loss: 0.9989 - acc: 0.6311 - val_loss: 1.2532 - val_acc: 0.5057\n",
      "Epoch 1653/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.9622 - acc: 0.6425 - val_loss: 1.2363 - val_acc: 0.5284\n",
      "Epoch 1654/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 1.0064 - acc: 0.6353 - val_loss: 1.2510 - val_acc: 0.5227\n",
      "Epoch 1655/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 1.0225 - acc: 0.6182 - val_loss: 1.1986 - val_acc: 0.5398\n",
      "Epoch 1656/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.9530 - acc: 0.6652 - val_loss: 1.2528 - val_acc: 0.5000\n",
      "Epoch 1657/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.9547 - acc: 0.6396 - val_loss: 1.3083 - val_acc: 0.5057\n",
      "Epoch 1658/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 1.0075 - acc: 0.6111 - val_loss: 1.4712 - val_acc: 0.4375\n",
      "Epoch 1659/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.9497 - acc: 0.6581 - val_loss: 1.2988 - val_acc: 0.5170\n",
      "Epoch 1660/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.9497 - acc: 0.6268 - val_loss: 1.2533 - val_acc: 0.5341\n",
      "Epoch 1661/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.9851 - acc: 0.6453 - val_loss: 1.1517 - val_acc: 0.5739\n",
      "Epoch 1662/3000\n",
      "702/702 [==============================] - 0s 559us/sample - loss: 0.9015 - acc: 0.6439 - val_loss: 1.2340 - val_acc: 0.5398\n",
      "Epoch 1663/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.9217 - acc: 0.6467 - val_loss: 1.2769 - val_acc: 0.5284\n",
      "Epoch 1664/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.9330 - acc: 0.6510 - val_loss: 1.2846 - val_acc: 0.5114\n",
      "Epoch 1665/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.9130 - acc: 0.6510 - val_loss: 1.1123 - val_acc: 0.5852\n",
      "Epoch 1666/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.9962 - acc: 0.6339 - val_loss: 1.1909 - val_acc: 0.5341\n",
      "Epoch 1667/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.9381 - acc: 0.6724 - val_loss: 1.1132 - val_acc: 0.5852\n",
      "Epoch 1668/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.9350 - acc: 0.6510 - val_loss: 1.3111 - val_acc: 0.4943\n",
      "Epoch 1669/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.8941 - acc: 0.6624 - val_loss: 1.2132 - val_acc: 0.5284\n",
      "Epoch 1670/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.8740 - acc: 0.6695 - val_loss: 1.1629 - val_acc: 0.5739\n",
      "Epoch 1671/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.8984 - acc: 0.6510 - val_loss: 1.1037 - val_acc: 0.5909\n",
      "Epoch 1672/3000\n",
      "702/702 [==============================] - 0s 554us/sample - loss: 0.8808 - acc: 0.6695 - val_loss: 1.2484 - val_acc: 0.5625\n",
      "Epoch 1673/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.8964 - acc: 0.6652 - val_loss: 1.3117 - val_acc: 0.4830\n",
      "Epoch 1674/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.9025 - acc: 0.6624 - val_loss: 1.3882 - val_acc: 0.5057\n",
      "Epoch 1675/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.8850 - acc: 0.6652 - val_loss: 1.2518 - val_acc: 0.5625\n",
      "Epoch 1676/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.8877 - acc: 0.6823 - val_loss: 1.1492 - val_acc: 0.5795\n",
      "Epoch 1677/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.8671 - acc: 0.6709 - val_loss: 1.2878 - val_acc: 0.5511\n",
      "Epoch 1678/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.8814 - acc: 0.6738 - val_loss: 1.1952 - val_acc: 0.5852\n",
      "Epoch 1679/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.8942 - acc: 0.6510 - val_loss: 1.2781 - val_acc: 0.5625\n",
      "Epoch 1680/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.8461 - acc: 0.6752 - val_loss: 1.1871 - val_acc: 0.5682\n",
      "Epoch 1681/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.8402 - acc: 0.6994 - val_loss: 1.1800 - val_acc: 0.5625\n",
      "Epoch 1682/3000\n",
      "702/702 [==============================] - 0s 559us/sample - loss: 0.8190 - acc: 0.6866 - val_loss: 1.1557 - val_acc: 0.5625\n",
      "Epoch 1683/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.8561 - acc: 0.7009 - val_loss: 1.1434 - val_acc: 0.5739\n",
      "Epoch 1684/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.8518 - acc: 0.6966 - val_loss: 1.0587 - val_acc: 0.5966\n",
      "Epoch 1685/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.8324 - acc: 0.6923 - val_loss: 1.1283 - val_acc: 0.5739\n",
      "Epoch 1686/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.8005 - acc: 0.6980 - val_loss: 1.1041 - val_acc: 0.5966\n",
      "Epoch 1687/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.8712 - acc: 0.6823 - val_loss: 1.2065 - val_acc: 0.5568\n",
      "Epoch 1688/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.8121 - acc: 0.6937 - val_loss: 1.0659 - val_acc: 0.5852\n",
      "Epoch 1689/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.8420 - acc: 0.6980 - val_loss: 1.0075 - val_acc: 0.6250\n",
      "Epoch 1690/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.8667 - acc: 0.6980 - val_loss: 1.1529 - val_acc: 0.5739\n",
      "Epoch 1691/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.8424 - acc: 0.6852 - val_loss: 1.1312 - val_acc: 0.5909\n",
      "Epoch 1692/3000\n",
      "702/702 [==============================] - 0s 560us/sample - loss: 0.8488 - acc: 0.6923 - val_loss: 1.3240 - val_acc: 0.5341\n",
      "Epoch 1693/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.8850 - acc: 0.6695 - val_loss: 1.1540 - val_acc: 0.5625\n",
      "Epoch 1694/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.8108 - acc: 0.6752 - val_loss: 1.1422 - val_acc: 0.5795\n",
      "Epoch 1695/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.8200 - acc: 0.6966 - val_loss: 1.0345 - val_acc: 0.6023\n",
      "Epoch 1696/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.8265 - acc: 0.6923 - val_loss: 1.0802 - val_acc: 0.5852\n",
      "Epoch 1697/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.8191 - acc: 0.6838 - val_loss: 1.1847 - val_acc: 0.5625\n",
      "Epoch 1698/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.7665 - acc: 0.7080 - val_loss: 1.2101 - val_acc: 0.5455\n",
      "Epoch 1699/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.7759 - acc: 0.7037 - val_loss: 1.1576 - val_acc: 0.5625\n",
      "Epoch 1700/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.8199 - acc: 0.7051 - val_loss: 1.0563 - val_acc: 0.5909\n",
      "Epoch 1701/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.8079 - acc: 0.6980 - val_loss: 1.1625 - val_acc: 0.5682\n",
      "Epoch 1702/3000\n",
      "702/702 [==============================] - 0s 552us/sample - loss: 0.8267 - acc: 0.7051 - val_loss: 1.1513 - val_acc: 0.5739\n",
      "Epoch 1703/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.8319 - acc: 0.7051 - val_loss: 1.2469 - val_acc: 0.5227\n",
      "Epoch 1704/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.8306 - acc: 0.6809 - val_loss: 1.2398 - val_acc: 0.5455\n",
      "Epoch 1705/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.7990 - acc: 0.7023 - val_loss: 1.0982 - val_acc: 0.5795\n",
      "Epoch 1706/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.8114 - acc: 0.6852 - val_loss: 1.3053 - val_acc: 0.5341\n",
      "Epoch 1707/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.8675 - acc: 0.6781 - val_loss: 0.9883 - val_acc: 0.6080\n",
      "Epoch 1708/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.8234 - acc: 0.6966 - val_loss: 1.2751 - val_acc: 0.5568\n",
      "Epoch 1709/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.8015 - acc: 0.6923 - val_loss: 1.1972 - val_acc: 0.5455\n",
      "Epoch 1710/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.7845 - acc: 0.7094 - val_loss: 1.2482 - val_acc: 0.5284\n",
      "Epoch 1711/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.8762 - acc: 0.6838 - val_loss: 1.0517 - val_acc: 0.5909\n",
      "Epoch 1712/3000\n",
      "702/702 [==============================] - 0s 568us/sample - loss: 0.8177 - acc: 0.7066 - val_loss: 1.1199 - val_acc: 0.5795\n",
      "Epoch 1713/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.8445 - acc: 0.6937 - val_loss: 1.1095 - val_acc: 0.5739\n",
      "Epoch 1714/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.8526 - acc: 0.6909 - val_loss: 1.1240 - val_acc: 0.5739\n",
      "Epoch 1715/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.8019 - acc: 0.7009 - val_loss: 1.1841 - val_acc: 0.5568\n",
      "Epoch 1716/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.8140 - acc: 0.6838 - val_loss: 1.0018 - val_acc: 0.6023\n",
      "Epoch 1717/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.2513 - acc: 0.5584 - val_loss: 1.1508 - val_acc: 0.5511\n",
      "Epoch 1718/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 1.1715 - acc: 0.6581 - val_loss: 1.3065 - val_acc: 0.4943\n",
      "Epoch 1719/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.0318 - acc: 0.6439 - val_loss: 1.7999 - val_acc: 0.4091\n",
      "Epoch 1720/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 1.8766 - acc: 0.4088 - val_loss: 2.2094 - val_acc: 0.3466\n",
      "Epoch 1721/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 2.0382 - acc: 0.3390 - val_loss: 1.5762 - val_acc: 0.4659\n",
      "Epoch 1722/3000\n",
      "702/702 [==============================] - 0s 546us/sample - loss: 1.4178 - acc: 0.5142 - val_loss: 1.5369 - val_acc: 0.4886\n",
      "Epoch 1723/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 1.8740 - acc: 0.4074 - val_loss: 2.0398 - val_acc: 0.4091\n",
      "Epoch 1724/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 2.2721 - acc: 0.3205 - val_loss: 2.0843 - val_acc: 0.3693\n",
      "Epoch 1725/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 2.0444 - acc: 0.3390 - val_loss: 1.9635 - val_acc: 0.4318\n",
      "Epoch 1726/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.8470 - acc: 0.3746 - val_loss: 1.9124 - val_acc: 0.4205\n",
      "Epoch 1727/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 1.8976 - acc: 0.3689 - val_loss: 2.0080 - val_acc: 0.3750\n",
      "Epoch 1728/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 1.9364 - acc: 0.3490 - val_loss: 1.9810 - val_acc: 0.3864\n",
      "Epoch 1729/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 1.8950 - acc: 0.3533 - val_loss: 2.0875 - val_acc: 0.3864\n",
      "Epoch 1730/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 1.8135 - acc: 0.3889 - val_loss: 1.8458 - val_acc: 0.4432\n",
      "Epoch 1731/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.9561 - acc: 0.3462 - val_loss: 2.4119 - val_acc: 0.3352\n",
      "Epoch 1732/3000\n",
      "702/702 [==============================] - 0s 550us/sample - loss: 2.0176 - acc: 0.3205 - val_loss: 2.3715 - val_acc: 0.3409\n",
      "Epoch 1733/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 1.8576 - acc: 0.3433 - val_loss: 1.9434 - val_acc: 0.3068\n",
      "Epoch 1734/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 1.8896 - acc: 0.3205 - val_loss: 2.0055 - val_acc: 0.2898\n",
      "Epoch 1735/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 1.7972 - acc: 0.3476 - val_loss: 2.0239 - val_acc: 0.3068\n",
      "Epoch 1736/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 1.7541 - acc: 0.3390 - val_loss: 1.9113 - val_acc: 0.3352\n",
      "Epoch 1737/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.8840 - acc: 0.3504 - val_loss: 1.9915 - val_acc: 0.2841\n",
      "Epoch 1738/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.7215 - acc: 0.3561 - val_loss: 1.9110 - val_acc: 0.3295\n",
      "Epoch 1739/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.7607 - acc: 0.3390 - val_loss: 1.8883 - val_acc: 0.3466\n",
      "Epoch 1740/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 1.7503 - acc: 0.3618 - val_loss: 2.1637 - val_acc: 0.2898\n",
      "Epoch 1741/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.7417 - acc: 0.3575 - val_loss: 1.8336 - val_acc: 0.3523\n",
      "Epoch 1742/3000\n",
      "702/702 [==============================] - 0s 553us/sample - loss: 1.7142 - acc: 0.3761 - val_loss: 1.9351 - val_acc: 0.3352\n",
      "Epoch 1743/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.5940 - acc: 0.3903 - val_loss: 1.8929 - val_acc: 0.3580\n",
      "Epoch 1744/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.7207 - acc: 0.3433 - val_loss: 1.9644 - val_acc: 0.3523\n",
      "Epoch 1745/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 1.7057 - acc: 0.3946 - val_loss: 2.2181 - val_acc: 0.3068\n",
      "Epoch 1746/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.7415 - acc: 0.3618 - val_loss: 1.8904 - val_acc: 0.3068\n",
      "Epoch 1747/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 1.6927 - acc: 0.3818 - val_loss: 1.8769 - val_acc: 0.3068\n",
      "Epoch 1748/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.6729 - acc: 0.3604 - val_loss: 1.9500 - val_acc: 0.2898\n",
      "Epoch 1749/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 3.1622 - acc: 0.3547 - val_loss: 1.9385 - val_acc: 0.3182\n",
      "Epoch 1750/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 1.6558 - acc: 0.3889 - val_loss: 1.9624 - val_acc: 0.3182\n",
      "Epoch 1751/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.7021 - acc: 0.3504 - val_loss: 1.9148 - val_acc: 0.3011\n",
      "Epoch 1752/3000\n",
      "702/702 [==============================] - 0s 559us/sample - loss: 1.6797 - acc: 0.3533 - val_loss: 1.8895 - val_acc: 0.2898\n",
      "Epoch 1753/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 1.6364 - acc: 0.3604 - val_loss: 1.8477 - val_acc: 0.3011\n",
      "Epoch 1754/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.5941 - acc: 0.3846 - val_loss: 1.7909 - val_acc: 0.3239\n",
      "Epoch 1755/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.7289 - acc: 0.3561 - val_loss: 2.0229 - val_acc: 0.3068\n",
      "Epoch 1756/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 2.2070 - acc: 0.3390 - val_loss: 1.7494 - val_acc: 0.3523\n",
      "Epoch 1757/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 1.7143 - acc: 0.3875 - val_loss: 1.8640 - val_acc: 0.3352\n",
      "Epoch 1758/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.7882 - acc: 0.3575 - val_loss: 1.7635 - val_acc: 0.3466\n",
      "Epoch 1759/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.6606 - acc: 0.3846 - val_loss: 1.8694 - val_acc: 0.3068\n",
      "Epoch 1760/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 1.5900 - acc: 0.3875 - val_loss: 1.8115 - val_acc: 0.3125\n",
      "Epoch 1761/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.6269 - acc: 0.3818 - val_loss: 1.7526 - val_acc: 0.3409\n",
      "Epoch 1762/3000\n",
      "702/702 [==============================] - 0s 552us/sample - loss: 1.6567 - acc: 0.3846 - val_loss: 1.7231 - val_acc: 0.3409\n",
      "Epoch 1763/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.6335 - acc: 0.3860 - val_loss: 1.7575 - val_acc: 0.3580\n",
      "Epoch 1764/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.6252 - acc: 0.3775 - val_loss: 1.8435 - val_acc: 0.3352\n",
      "Epoch 1765/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.6678 - acc: 0.3860 - val_loss: 1.7356 - val_acc: 0.3409\n",
      "Epoch 1766/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.6151 - acc: 0.3803 - val_loss: 1.7298 - val_acc: 0.3636\n",
      "Epoch 1767/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 1.6508 - acc: 0.3761 - val_loss: 1.8445 - val_acc: 0.3352\n",
      "Epoch 1768/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 1.6598 - acc: 0.3675 - val_loss: 1.7977 - val_acc: 0.3011\n",
      "Epoch 1769/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 1.6339 - acc: 0.3632 - val_loss: 1.8340 - val_acc: 0.2557\n",
      "Epoch 1770/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.6864 - acc: 0.3675 - val_loss: 1.7582 - val_acc: 0.3182\n",
      "Epoch 1771/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.6045 - acc: 0.3632 - val_loss: 1.7332 - val_acc: 0.3352\n",
      "Epoch 1772/3000\n",
      "702/702 [==============================] - 0s 564us/sample - loss: 1.6378 - acc: 0.3960 - val_loss: 1.7255 - val_acc: 0.3409\n",
      "Epoch 1773/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 1.6083 - acc: 0.3960 - val_loss: 1.7594 - val_acc: 0.3125\n",
      "Epoch 1774/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.6068 - acc: 0.3846 - val_loss: 1.7634 - val_acc: 0.3182\n",
      "Epoch 1775/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.5959 - acc: 0.3846 - val_loss: 1.7291 - val_acc: 0.3352\n",
      "Epoch 1776/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 1.5751 - acc: 0.4074 - val_loss: 1.7471 - val_acc: 0.3295\n",
      "Epoch 1777/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.5818 - acc: 0.3889 - val_loss: 1.7619 - val_acc: 0.3182\n",
      "Epoch 1778/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 1.5584 - acc: 0.3775 - val_loss: 1.7015 - val_acc: 0.3068\n",
      "Epoch 1779/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.4957 - acc: 0.4288 - val_loss: 1.5752 - val_acc: 0.3523\n",
      "Epoch 1780/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.4294 - acc: 0.4558 - val_loss: 1.5819 - val_acc: 0.3239\n",
      "Epoch 1781/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.3700 - acc: 0.4701 - val_loss: 1.6479 - val_acc: 0.2955\n",
      "Epoch 1782/3000\n",
      "702/702 [==============================] - 0s 555us/sample - loss: 1.3894 - acc: 0.4744 - val_loss: 1.6873 - val_acc: 0.3011\n",
      "Epoch 1783/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 1.3207 - acc: 0.4943 - val_loss: 1.6814 - val_acc: 0.2727\n",
      "Epoch 1784/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.2398 - acc: 0.5399 - val_loss: 1.6915 - val_acc: 0.4830\n",
      "Epoch 1785/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 1.2378 - acc: 0.5256 - val_loss: 1.8001 - val_acc: 0.3750\n",
      "Epoch 1786/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.2323 - acc: 0.5641 - val_loss: 1.6838 - val_acc: 0.4375\n",
      "Epoch 1787/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.4172 - acc: 0.4929 - val_loss: 1.7935 - val_acc: 0.3977\n",
      "Epoch 1788/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 1.6658 - acc: 0.3746 - val_loss: 1.7746 - val_acc: 0.4205\n",
      "Epoch 1789/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 1.6985 - acc: 0.3704 - val_loss: 1.7147 - val_acc: 0.4318\n",
      "Epoch 1790/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 1.6971 - acc: 0.3903 - val_loss: 1.6663 - val_acc: 0.4545\n",
      "Epoch 1791/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.7085 - acc: 0.3661 - val_loss: 1.6664 - val_acc: 0.4659\n",
      "Epoch 1792/3000\n",
      "702/702 [==============================] - 0s 551us/sample - loss: 1.6954 - acc: 0.3789 - val_loss: 1.6312 - val_acc: 0.4545\n",
      "Epoch 1793/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 1.6347 - acc: 0.3775 - val_loss: 1.6932 - val_acc: 0.4375\n",
      "Epoch 1794/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.6074 - acc: 0.3732 - val_loss: 1.7116 - val_acc: 0.3977\n",
      "Epoch 1795/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 1.6006 - acc: 0.3775 - val_loss: 1.6592 - val_acc: 0.4261\n",
      "Epoch 1796/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 1.5857 - acc: 0.3974 - val_loss: 1.6348 - val_acc: 0.4432\n",
      "Epoch 1797/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 1.6051 - acc: 0.3803 - val_loss: 1.6192 - val_acc: 0.4375\n",
      "Epoch 1798/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.5725 - acc: 0.3860 - val_loss: 1.6093 - val_acc: 0.4432\n",
      "Epoch 1799/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.5854 - acc: 0.3775 - val_loss: 1.5885 - val_acc: 0.4432\n",
      "Epoch 1800/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 1.5395 - acc: 0.3875 - val_loss: 1.5847 - val_acc: 0.4432\n",
      "Epoch 1801/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.5467 - acc: 0.4060 - val_loss: 1.5647 - val_acc: 0.4545\n",
      "Epoch 1802/3000\n",
      "702/702 [==============================] - 0s 550us/sample - loss: 1.5994 - acc: 0.3704 - val_loss: 1.5687 - val_acc: 0.4432\n",
      "Epoch 1803/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 1.5288 - acc: 0.3818 - val_loss: 1.6205 - val_acc: 0.4318\n",
      "Epoch 1804/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 1.5827 - acc: 0.3718 - val_loss: 1.5647 - val_acc: 0.4602\n",
      "Epoch 1805/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.5653 - acc: 0.3946 - val_loss: 1.5547 - val_acc: 0.4432\n",
      "Epoch 1806/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 1.5808 - acc: 0.3803 - val_loss: 1.5700 - val_acc: 0.4375\n",
      "Epoch 1807/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 1.5652 - acc: 0.3832 - val_loss: 1.5812 - val_acc: 0.4318\n",
      "Epoch 1808/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 1.5157 - acc: 0.4060 - val_loss: 1.5619 - val_acc: 0.4432\n",
      "Epoch 1809/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 1.5474 - acc: 0.3946 - val_loss: 1.5225 - val_acc: 0.4432\n",
      "Epoch 1810/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 1.5562 - acc: 0.3903 - val_loss: 1.5515 - val_acc: 0.4318\n",
      "Epoch 1811/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.5332 - acc: 0.3846 - val_loss: 1.5597 - val_acc: 0.4375\n",
      "Epoch 1812/3000\n",
      "702/702 [==============================] - 0s 542us/sample - loss: 1.5146 - acc: 0.4074 - val_loss: 1.5202 - val_acc: 0.4432\n",
      "Epoch 1813/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 1.5377 - acc: 0.4046 - val_loss: 1.5054 - val_acc: 0.4489\n",
      "Epoch 1814/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.5440 - acc: 0.4160 - val_loss: 1.5842 - val_acc: 0.4318\n",
      "Epoch 1815/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.5151 - acc: 0.4117 - val_loss: 1.6585 - val_acc: 0.4205\n",
      "Epoch 1816/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 1.5284 - acc: 0.3903 - val_loss: 1.6195 - val_acc: 0.4261\n",
      "Epoch 1817/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.4916 - acc: 0.4046 - val_loss: 1.6043 - val_acc: 0.4318\n",
      "Epoch 1818/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 1.5078 - acc: 0.3989 - val_loss: 1.6009 - val_acc: 0.4489\n",
      "Epoch 1819/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.4798 - acc: 0.4231 - val_loss: 1.5899 - val_acc: 0.4716\n",
      "Epoch 1820/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 1.4064 - acc: 0.4487 - val_loss: 1.6904 - val_acc: 0.4091\n",
      "Epoch 1821/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.4312 - acc: 0.4516 - val_loss: 1.6589 - val_acc: 0.4432\n",
      "Epoch 1822/3000\n",
      "702/702 [==============================] - 0s 557us/sample - loss: 1.4716 - acc: 0.4373 - val_loss: 1.4681 - val_acc: 0.4659\n",
      "Epoch 1823/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 1.3706 - acc: 0.4786 - val_loss: 1.3526 - val_acc: 0.5341\n",
      "Epoch 1824/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.3817 - acc: 0.4986 - val_loss: 1.6039 - val_acc: 0.4602\n",
      "Epoch 1825/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.3846 - acc: 0.4601 - val_loss: 1.5022 - val_acc: 0.4659\n",
      "Epoch 1826/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 1.4566 - acc: 0.4729 - val_loss: 1.5529 - val_acc: 0.4659\n",
      "Epoch 1827/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.3085 - acc: 0.4801 - val_loss: 1.5641 - val_acc: 0.4716\n",
      "Epoch 1828/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.3062 - acc: 0.4701 - val_loss: 1.4019 - val_acc: 0.5057\n",
      "Epoch 1829/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 1.3555 - acc: 0.4786 - val_loss: 1.6845 - val_acc: 0.4148\n",
      "Epoch 1830/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 1.3420 - acc: 0.4715 - val_loss: 1.6588 - val_acc: 0.4432\n",
      "Epoch 1831/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.3205 - acc: 0.5071 - val_loss: 1.6095 - val_acc: 0.4375\n",
      "Epoch 1832/3000\n",
      "702/702 [==============================] - 0s 566us/sample - loss: 1.3152 - acc: 0.4644 - val_loss: 1.4788 - val_acc: 0.4545\n",
      "Epoch 1833/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 1.3230 - acc: 0.4772 - val_loss: 1.3248 - val_acc: 0.5114\n",
      "Epoch 1834/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 1.3373 - acc: 0.4872 - val_loss: 1.3543 - val_acc: 0.5057\n",
      "Epoch 1835/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.2771 - acc: 0.4858 - val_loss: 1.5442 - val_acc: 0.4659\n",
      "Epoch 1836/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 1.2594 - acc: 0.4957 - val_loss: 1.3420 - val_acc: 0.5000\n",
      "Epoch 1837/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 1.2673 - acc: 0.4900 - val_loss: 1.4098 - val_acc: 0.4886\n",
      "Epoch 1838/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.2439 - acc: 0.4957 - val_loss: 1.2973 - val_acc: 0.5568\n",
      "Epoch 1839/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.2809 - acc: 0.5014 - val_loss: 1.4924 - val_acc: 0.4830\n",
      "Epoch 1840/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 1.2655 - acc: 0.4943 - val_loss: 1.4997 - val_acc: 0.4716\n",
      "Epoch 1841/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.2675 - acc: 0.4915 - val_loss: 1.5044 - val_acc: 0.4773\n",
      "Epoch 1842/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 1.3576 - acc: 0.5214 - val_loss: 1.5183 - val_acc: 0.4432\n",
      "Epoch 1843/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 1.2827 - acc: 0.4915 - val_loss: 1.4975 - val_acc: 0.4318\n",
      "Epoch 1844/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.2775 - acc: 0.4744 - val_loss: 1.4142 - val_acc: 0.4886\n",
      "Epoch 1845/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 1.2563 - acc: 0.5000 - val_loss: 1.3419 - val_acc: 0.5341\n",
      "Epoch 1846/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 1.3207 - acc: 0.5427 - val_loss: 1.3626 - val_acc: 0.5114\n",
      "Epoch 1847/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.2915 - acc: 0.4986 - val_loss: 1.4180 - val_acc: 0.4716\n",
      "Epoch 1848/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 1.2886 - acc: 0.4872 - val_loss: 1.4384 - val_acc: 0.4830\n",
      "Epoch 1849/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 1.2609 - acc: 0.5499 - val_loss: 1.4562 - val_acc: 0.5227\n",
      "Epoch 1850/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 1.2274 - acc: 0.5513 - val_loss: 1.4245 - val_acc: 0.5341\n",
      "Epoch 1851/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.1782 - acc: 0.5670 - val_loss: 1.5217 - val_acc: 0.4489\n",
      "Epoch 1852/3000\n",
      "702/702 [==============================] - 0s 570us/sample - loss: 1.1944 - acc: 0.5328 - val_loss: 1.4732 - val_acc: 0.5114\n",
      "Epoch 1853/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.1735 - acc: 0.5983 - val_loss: 1.4668 - val_acc: 0.4545\n",
      "Epoch 1854/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.2090 - acc: 0.5171 - val_loss: 1.4071 - val_acc: 0.4830\n",
      "Epoch 1855/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 1.2285 - acc: 0.5242 - val_loss: 1.3540 - val_acc: 0.5284\n",
      "Epoch 1856/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 1.1781 - acc: 0.5926 - val_loss: 1.4964 - val_acc: 0.4943\n",
      "Epoch 1857/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 1.2264 - acc: 0.5271 - val_loss: 1.5411 - val_acc: 0.4830\n",
      "Epoch 1858/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 1.2170 - acc: 0.5370 - val_loss: 1.5598 - val_acc: 0.4432\n",
      "Epoch 1859/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 1.4746 - acc: 0.4444 - val_loss: 1.6796 - val_acc: 0.4375\n",
      "Epoch 1860/3000\n",
      "702/702 [==============================] - 0s 536us/sample - loss: 1.6989 - acc: 0.3704 - val_loss: 1.7184 - val_acc: 0.4545\n",
      "Epoch 1861/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.6644 - acc: 0.3761 - val_loss: 1.7396 - val_acc: 0.4545\n",
      "Epoch 1862/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 1.6656 - acc: 0.3860 - val_loss: 1.7151 - val_acc: 0.4716\n",
      "Epoch 1863/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 1.6671 - acc: 0.3832 - val_loss: 1.6679 - val_acc: 0.4432\n",
      "Epoch 1864/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 1.5861 - acc: 0.3903 - val_loss: 1.6210 - val_acc: 0.4318\n",
      "Epoch 1865/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 1.4982 - acc: 0.3946 - val_loss: 1.3821 - val_acc: 0.4716\n",
      "Epoch 1866/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.3252 - acc: 0.4701 - val_loss: 1.4419 - val_acc: 0.4943\n",
      "Epoch 1867/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 1.2345 - acc: 0.5456 - val_loss: 1.4968 - val_acc: 0.4943\n",
      "Epoch 1868/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.2140 - acc: 0.5712 - val_loss: 1.4632 - val_acc: 0.5284\n",
      "Epoch 1869/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 1.2644 - acc: 0.5541 - val_loss: 1.3906 - val_acc: 0.5170\n",
      "Epoch 1870/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 1.1753 - acc: 0.6054 - val_loss: 1.4303 - val_acc: 0.5000\n",
      "Epoch 1871/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.1782 - acc: 0.5641 - val_loss: 1.4621 - val_acc: 0.4545\n",
      "Epoch 1872/3000\n",
      "702/702 [==============================] - 0s 552us/sample - loss: 1.2044 - acc: 0.5328 - val_loss: 1.3242 - val_acc: 0.5625\n",
      "Epoch 1873/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 1.2091 - acc: 0.6182 - val_loss: 1.2891 - val_acc: 0.5909\n",
      "Epoch 1874/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.1461 - acc: 0.5755 - val_loss: 1.3352 - val_acc: 0.5170\n",
      "Epoch 1875/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 1.1579 - acc: 0.5641 - val_loss: 1.2968 - val_acc: 0.5682\n",
      "Epoch 1876/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 1.1692 - acc: 0.6111 - val_loss: 1.2838 - val_acc: 0.5682\n",
      "Epoch 1877/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.1238 - acc: 0.6097 - val_loss: 1.2490 - val_acc: 0.5909\n",
      "Epoch 1878/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 1.1051 - acc: 0.6296 - val_loss: 1.3158 - val_acc: 0.5398\n",
      "Epoch 1879/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 1.0713 - acc: 0.6282 - val_loss: 1.3137 - val_acc: 0.5568\n",
      "Epoch 1880/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 1.0537 - acc: 0.6453 - val_loss: 1.3288 - val_acc: 0.5341\n",
      "Epoch 1881/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.0793 - acc: 0.6040 - val_loss: 1.3383 - val_acc: 0.5568\n",
      "Epoch 1882/3000\n",
      "702/702 [==============================] - 0s 588us/sample - loss: 1.0948 - acc: 0.6197 - val_loss: 1.3218 - val_acc: 0.5568\n",
      "Epoch 1883/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 1.0658 - acc: 0.6311 - val_loss: 1.3210 - val_acc: 0.5625\n",
      "Epoch 1884/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 1.1373 - acc: 0.6097 - val_loss: 1.4589 - val_acc: 0.5057\n",
      "Epoch 1885/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.2627 - acc: 0.4943 - val_loss: 1.6174 - val_acc: 0.4432\n",
      "Epoch 1886/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 1.5234 - acc: 0.4174 - val_loss: 1.5590 - val_acc: 0.4432\n",
      "Epoch 1887/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 1.3415 - acc: 0.4672 - val_loss: 1.2948 - val_acc: 0.5455\n",
      "Epoch 1888/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 1.2499 - acc: 0.5299 - val_loss: 1.3380 - val_acc: 0.5284\n",
      "Epoch 1889/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 1.1386 - acc: 0.6111 - val_loss: 1.3830 - val_acc: 0.5000\n",
      "Epoch 1890/3000\n",
      "702/702 [==============================] - 0s 550us/sample - loss: 1.1217 - acc: 0.5741 - val_loss: 1.3620 - val_acc: 0.5284\n",
      "Epoch 1891/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.1232 - acc: 0.6197 - val_loss: 1.3231 - val_acc: 0.5682\n",
      "Epoch 1892/3000\n",
      "702/702 [==============================] - 0s 587us/sample - loss: 1.1370 - acc: 0.5897 - val_loss: 1.4766 - val_acc: 0.5000\n",
      "Epoch 1893/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 1.0864 - acc: 0.6125 - val_loss: 1.4171 - val_acc: 0.5057\n",
      "Epoch 1894/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.0762 - acc: 0.6182 - val_loss: 1.3224 - val_acc: 0.5455\n",
      "Epoch 1895/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 1.0678 - acc: 0.6339 - val_loss: 1.3564 - val_acc: 0.5398\n",
      "Epoch 1896/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 1.0156 - acc: 0.6496 - val_loss: 1.3633 - val_acc: 0.5511\n",
      "Epoch 1897/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 1.0627 - acc: 0.6368 - val_loss: 1.2671 - val_acc: 0.5966\n",
      "Epoch 1898/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 1.0895 - acc: 0.6353 - val_loss: 1.3238 - val_acc: 0.5682\n",
      "Epoch 1899/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 1.0355 - acc: 0.6239 - val_loss: 1.3549 - val_acc: 0.5625\n",
      "Epoch 1900/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.0533 - acc: 0.6211 - val_loss: 1.4065 - val_acc: 0.5000\n",
      "Epoch 1901/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.2308 - acc: 0.5370 - val_loss: 1.4505 - val_acc: 0.4602\n",
      "Epoch 1902/3000\n",
      "702/702 [==============================] - 0s 564us/sample - loss: 1.3181 - acc: 0.4801 - val_loss: 1.4358 - val_acc: 0.4489\n",
      "Epoch 1903/3000\n",
      "702/702 [==============================] - 0s 474us/sample - loss: 1.1931 - acc: 0.5328 - val_loss: 1.3560 - val_acc: 0.4943\n",
      "Epoch 1904/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 1.1967 - acc: 0.5826 - val_loss: 1.2908 - val_acc: 0.5511\n",
      "Epoch 1905/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 1.1051 - acc: 0.6140 - val_loss: 1.2169 - val_acc: 0.6136\n",
      "Epoch 1906/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.0888 - acc: 0.5997 - val_loss: 1.2902 - val_acc: 0.5455\n",
      "Epoch 1907/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.0672 - acc: 0.6068 - val_loss: 1.3621 - val_acc: 0.5114\n",
      "Epoch 1908/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.1152 - acc: 0.5926 - val_loss: 1.2641 - val_acc: 0.5739\n",
      "Epoch 1909/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.0447 - acc: 0.6168 - val_loss: 1.3369 - val_acc: 0.5170\n",
      "Epoch 1910/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.0598 - acc: 0.5926 - val_loss: 1.1721 - val_acc: 0.6420\n",
      "Epoch 1911/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.0405 - acc: 0.6425 - val_loss: 1.3139 - val_acc: 0.5568\n",
      "Epoch 1912/3000\n",
      "702/702 [==============================] - 0s 566us/sample - loss: 1.0403 - acc: 0.6168 - val_loss: 1.2578 - val_acc: 0.5966\n",
      "Epoch 1913/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 1.0029 - acc: 0.6538 - val_loss: 1.2565 - val_acc: 0.5966\n",
      "Epoch 1914/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.9725 - acc: 0.6410 - val_loss: 1.3443 - val_acc: 0.5511\n",
      "Epoch 1915/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 1.0288 - acc: 0.6254 - val_loss: 1.3043 - val_acc: 0.5852\n",
      "Epoch 1916/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.9672 - acc: 0.6567 - val_loss: 1.3620 - val_acc: 0.5625\n",
      "Epoch 1917/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.9626 - acc: 0.6567 - val_loss: 1.2638 - val_acc: 0.5966\n",
      "Epoch 1918/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.9555 - acc: 0.6638 - val_loss: 1.1821 - val_acc: 0.6023\n",
      "Epoch 1919/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.9742 - acc: 0.6453 - val_loss: 1.1854 - val_acc: 0.6023\n",
      "Epoch 1920/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.9497 - acc: 0.6410 - val_loss: 1.2507 - val_acc: 0.5682\n",
      "Epoch 1921/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.0158 - acc: 0.6282 - val_loss: 1.3955 - val_acc: 0.5398\n",
      "Epoch 1922/3000\n",
      "702/702 [==============================] - 0s 573us/sample - loss: 0.9926 - acc: 0.6097 - val_loss: 1.2421 - val_acc: 0.5966\n",
      "Epoch 1923/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.9352 - acc: 0.6353 - val_loss: 1.1957 - val_acc: 0.6193\n",
      "Epoch 1924/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 1.0162 - acc: 0.6140 - val_loss: 1.4439 - val_acc: 0.5057\n",
      "Epoch 1925/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 1.0728 - acc: 0.6168 - val_loss: 1.2343 - val_acc: 0.5909\n",
      "Epoch 1926/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.9784 - acc: 0.6396 - val_loss: 1.4743 - val_acc: 0.5000\n",
      "Epoch 1927/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 1.0904 - acc: 0.6011 - val_loss: 1.2468 - val_acc: 0.5852\n",
      "Epoch 1928/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 1.0965 - acc: 0.6268 - val_loss: 1.4475 - val_acc: 0.5057\n",
      "Epoch 1929/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.1571 - acc: 0.5584 - val_loss: 1.4733 - val_acc: 0.4659\n",
      "Epoch 1930/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.1669 - acc: 0.5783 - val_loss: 1.2558 - val_acc: 0.5795\n",
      "Epoch 1931/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.0628 - acc: 0.6168 - val_loss: 1.3436 - val_acc: 0.4943\n",
      "Epoch 1932/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 1.1036 - acc: 0.5940 - val_loss: 1.3925 - val_acc: 0.4716\n",
      "Epoch 1933/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.1039 - acc: 0.6011 - val_loss: 1.4116 - val_acc: 0.4886\n",
      "Epoch 1934/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 1.0353 - acc: 0.6197 - val_loss: 1.4509 - val_acc: 0.4773\n",
      "Epoch 1935/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.0361 - acc: 0.6239 - val_loss: 1.3285 - val_acc: 0.5000\n",
      "Epoch 1936/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.9747 - acc: 0.6425 - val_loss: 1.3285 - val_acc: 0.5682\n",
      "Epoch 1937/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.9958 - acc: 0.6382 - val_loss: 1.4613 - val_acc: 0.5057\n",
      "Epoch 1938/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.9418 - acc: 0.6652 - val_loss: 1.2952 - val_acc: 0.5625\n",
      "Epoch 1939/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.9600 - acc: 0.6467 - val_loss: 1.3291 - val_acc: 0.5341\n",
      "Epoch 1940/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.9614 - acc: 0.6467 - val_loss: 1.2583 - val_acc: 0.5682\n",
      "Epoch 1941/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.9595 - acc: 0.6638 - val_loss: 1.2308 - val_acc: 0.5739\n",
      "Epoch 1942/3000\n",
      "702/702 [==============================] - 0s 568us/sample - loss: 0.9120 - acc: 0.6553 - val_loss: 1.3909 - val_acc: 0.5114\n",
      "Epoch 1943/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.9486 - acc: 0.6581 - val_loss: 1.3039 - val_acc: 0.5511\n",
      "Epoch 1944/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.9026 - acc: 0.6695 - val_loss: 1.2294 - val_acc: 0.5739\n",
      "Epoch 1945/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.9295 - acc: 0.6567 - val_loss: 1.1645 - val_acc: 0.5909\n",
      "Epoch 1946/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.8950 - acc: 0.6695 - val_loss: 1.1683 - val_acc: 0.6023\n",
      "Epoch 1947/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.9627 - acc: 0.6467 - val_loss: 1.2259 - val_acc: 0.5682\n",
      "Epoch 1948/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.9471 - acc: 0.6510 - val_loss: 1.2712 - val_acc: 0.5739\n",
      "Epoch 1949/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.9231 - acc: 0.6595 - val_loss: 1.2323 - val_acc: 0.5795\n",
      "Epoch 1950/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.9859 - acc: 0.6368 - val_loss: 1.5121 - val_acc: 0.4830\n",
      "Epoch 1951/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 1.2183 - acc: 0.5584 - val_loss: 1.3042 - val_acc: 0.5114\n",
      "Epoch 1952/3000\n",
      "702/702 [==============================] - 0s 577us/sample - loss: 1.0601 - acc: 0.6026 - val_loss: 1.2577 - val_acc: 0.5568\n",
      "Epoch 1953/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.0421 - acc: 0.6111 - val_loss: 1.4775 - val_acc: 0.4602\n",
      "Epoch 1954/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 1.2248 - acc: 0.5370 - val_loss: 1.4585 - val_acc: 0.4659\n",
      "Epoch 1955/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 1.0355 - acc: 0.6097 - val_loss: 1.2881 - val_acc: 0.5625\n",
      "Epoch 1956/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.9868 - acc: 0.6382 - val_loss: 1.3644 - val_acc: 0.5000\n",
      "Epoch 1957/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.9863 - acc: 0.6268 - val_loss: 1.3157 - val_acc: 0.5170\n",
      "Epoch 1958/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.9428 - acc: 0.6368 - val_loss: 1.1692 - val_acc: 0.5966\n",
      "Epoch 1959/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.9838 - acc: 0.6311 - val_loss: 1.3941 - val_acc: 0.4886\n",
      "Epoch 1960/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.9720 - acc: 0.6239 - val_loss: 1.1934 - val_acc: 0.5739\n",
      "Epoch 1961/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.9595 - acc: 0.6467 - val_loss: 1.2026 - val_acc: 0.5795\n",
      "Epoch 1962/3000\n",
      "702/702 [==============================] - 0s 556us/sample - loss: 0.9038 - acc: 0.6681 - val_loss: 1.3508 - val_acc: 0.5170\n",
      "Epoch 1963/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.9375 - acc: 0.6553 - val_loss: 1.0869 - val_acc: 0.6307\n",
      "Epoch 1964/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.9469 - acc: 0.6268 - val_loss: 1.3188 - val_acc: 0.5227\n",
      "Epoch 1965/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.0001 - acc: 0.6254 - val_loss: 1.3217 - val_acc: 0.5227\n",
      "Epoch 1966/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.8550 - acc: 0.6823 - val_loss: 1.1445 - val_acc: 0.5909\n",
      "Epoch 1967/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.9547 - acc: 0.6439 - val_loss: 1.2981 - val_acc: 0.5227\n",
      "Epoch 1968/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.9835 - acc: 0.6396 - val_loss: 1.2256 - val_acc: 0.5511\n",
      "Epoch 1969/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.8732 - acc: 0.6681 - val_loss: 1.1938 - val_acc: 0.5682\n",
      "Epoch 1970/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.8984 - acc: 0.6439 - val_loss: 1.4189 - val_acc: 0.5227\n",
      "Epoch 1971/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.9013 - acc: 0.6638 - val_loss: 1.2272 - val_acc: 0.5739\n",
      "Epoch 1972/3000\n",
      "702/702 [==============================] - 0s 557us/sample - loss: 0.8961 - acc: 0.6553 - val_loss: 1.2292 - val_acc: 0.5568\n",
      "Epoch 1973/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.8603 - acc: 0.6781 - val_loss: 1.1792 - val_acc: 0.5909\n",
      "Epoch 1974/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.8449 - acc: 0.6795 - val_loss: 1.1896 - val_acc: 0.5966\n",
      "Epoch 1975/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.8436 - acc: 0.6809 - val_loss: 1.2528 - val_acc: 0.5795\n",
      "Epoch 1976/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.7966 - acc: 0.6823 - val_loss: 1.1989 - val_acc: 0.5739\n",
      "Epoch 1977/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.8240 - acc: 0.6752 - val_loss: 1.2839 - val_acc: 0.5455\n",
      "Epoch 1978/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.8328 - acc: 0.6724 - val_loss: 1.1811 - val_acc: 0.5966\n",
      "Epoch 1979/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.8791 - acc: 0.6638 - val_loss: 1.3109 - val_acc: 0.5284\n",
      "Epoch 1980/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.8572 - acc: 0.6581 - val_loss: 1.1183 - val_acc: 0.6364\n",
      "Epoch 1981/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.8196 - acc: 0.6624 - val_loss: 1.0881 - val_acc: 0.6364\n",
      "Epoch 1982/3000\n",
      "702/702 [==============================] - 0s 567us/sample - loss: 0.8282 - acc: 0.6766 - val_loss: 1.1885 - val_acc: 0.6023\n",
      "Epoch 1983/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.7741 - acc: 0.6766 - val_loss: 1.0495 - val_acc: 0.6477\n",
      "Epoch 1984/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.8116 - acc: 0.6781 - val_loss: 1.1482 - val_acc: 0.6193\n",
      "Epoch 1985/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.8072 - acc: 0.6852 - val_loss: 1.0959 - val_acc: 0.6193\n",
      "Epoch 1986/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.7915 - acc: 0.6966 - val_loss: 1.1002 - val_acc: 0.6364\n",
      "Epoch 1987/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.7912 - acc: 0.6823 - val_loss: 0.9609 - val_acc: 0.6648\n",
      "Epoch 1988/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.8423 - acc: 0.6781 - val_loss: 1.1118 - val_acc: 0.6080\n",
      "Epoch 1989/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.7810 - acc: 0.6980 - val_loss: 1.0478 - val_acc: 0.6364\n",
      "Epoch 1990/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.7727 - acc: 0.6966 - val_loss: 1.0787 - val_acc: 0.6250\n",
      "Epoch 1991/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.8124 - acc: 0.6952 - val_loss: 1.0183 - val_acc: 0.6591\n",
      "Epoch 1992/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 0.7568 - acc: 0.6966 - val_loss: 1.0297 - val_acc: 0.6250\n",
      "Epoch 1993/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.7660 - acc: 0.6952 - val_loss: 1.1341 - val_acc: 0.6193\n",
      "Epoch 1994/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.7823 - acc: 0.6909 - val_loss: 1.0310 - val_acc: 0.6080\n",
      "Epoch 1995/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.7584 - acc: 0.7051 - val_loss: 1.0770 - val_acc: 0.6420\n",
      "Epoch 1996/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.7964 - acc: 0.6952 - val_loss: 1.0358 - val_acc: 0.6534\n",
      "Epoch 1997/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.8169 - acc: 0.6895 - val_loss: 1.1092 - val_acc: 0.6307\n",
      "Epoch 1998/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.7620 - acc: 0.6937 - val_loss: 1.1623 - val_acc: 0.6136\n",
      "Epoch 1999/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.8304 - acc: 0.6809 - val_loss: 1.2275 - val_acc: 0.5341\n",
      "Epoch 2000/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.8086 - acc: 0.6795 - val_loss: 1.0924 - val_acc: 0.6023\n",
      "Epoch 2001/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.7947 - acc: 0.6937 - val_loss: 1.0003 - val_acc: 0.6364\n",
      "Epoch 2002/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.7388 - acc: 0.7066 - val_loss: 1.0410 - val_acc: 0.6364\n",
      "Epoch 2003/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.8064 - acc: 0.6923 - val_loss: 1.0348 - val_acc: 0.6250\n",
      "Epoch 2004/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.7795 - acc: 0.6809 - val_loss: 1.0638 - val_acc: 0.6307\n",
      "Epoch 2005/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.8040 - acc: 0.6895 - val_loss: 1.0561 - val_acc: 0.5966\n",
      "Epoch 2006/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.8145 - acc: 0.6752 - val_loss: 1.0041 - val_acc: 0.6250\n",
      "Epoch 2007/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.8669 - acc: 0.6638 - val_loss: 1.4104 - val_acc: 0.4943\n",
      "Epoch 2008/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.0604 - acc: 0.6410 - val_loss: 1.2058 - val_acc: 0.5625\n",
      "Epoch 2009/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.8639 - acc: 0.6695 - val_loss: 1.3124 - val_acc: 0.5625\n",
      "Epoch 2010/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.8938 - acc: 0.6538 - val_loss: 1.2689 - val_acc: 0.5682\n",
      "Epoch 2011/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.8257 - acc: 0.6823 - val_loss: 1.3274 - val_acc: 0.5341\n",
      "Epoch 2012/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.7677 - acc: 0.7037 - val_loss: 1.4035 - val_acc: 0.5114\n",
      "Epoch 2013/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.8772 - acc: 0.6638 - val_loss: 1.1524 - val_acc: 0.5966\n",
      "Epoch 2014/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.7912 - acc: 0.6852 - val_loss: 1.2568 - val_acc: 0.5682\n",
      "Epoch 2015/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.7966 - acc: 0.6809 - val_loss: 1.4468 - val_acc: 0.5170\n",
      "Epoch 2016/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.8016 - acc: 0.6952 - val_loss: 1.2947 - val_acc: 0.5227\n",
      "Epoch 2017/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.7405 - acc: 0.7023 - val_loss: 1.2298 - val_acc: 0.5625\n",
      "Epoch 2018/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.7074 - acc: 0.7123 - val_loss: 1.2298 - val_acc: 0.5795\n",
      "Epoch 2019/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.7413 - acc: 0.6923 - val_loss: 1.2273 - val_acc: 0.5398\n",
      "Epoch 2020/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.7359 - acc: 0.7037 - val_loss: 1.0865 - val_acc: 0.5909\n",
      "Epoch 2021/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.7512 - acc: 0.6994 - val_loss: 1.2063 - val_acc: 0.5455\n",
      "Epoch 2022/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 0.7475 - acc: 0.6923 - val_loss: 1.0923 - val_acc: 0.5795\n",
      "Epoch 2023/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.7671 - acc: 0.7080 - val_loss: 1.1723 - val_acc: 0.5682\n",
      "Epoch 2024/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.7204 - acc: 0.7080 - val_loss: 1.1137 - val_acc: 0.5966\n",
      "Epoch 2025/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.7183 - acc: 0.7066 - val_loss: 1.0806 - val_acc: 0.5852\n",
      "Epoch 2026/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.7296 - acc: 0.7137 - val_loss: 1.1679 - val_acc: 0.5568\n",
      "Epoch 2027/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.6828 - acc: 0.7322 - val_loss: 1.0911 - val_acc: 0.5739\n",
      "Epoch 2028/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.7578 - acc: 0.7080 - val_loss: 1.2525 - val_acc: 0.5568\n",
      "Epoch 2029/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.7394 - acc: 0.6980 - val_loss: 1.0850 - val_acc: 0.5966\n",
      "Epoch 2030/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.7591 - acc: 0.7066 - val_loss: 1.1225 - val_acc: 0.5739\n",
      "Epoch 2031/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.7116 - acc: 0.7009 - val_loss: 1.1274 - val_acc: 0.5795\n",
      "Epoch 2032/3000\n",
      "702/702 [==============================] - 0s 558us/sample - loss: 0.7447 - acc: 0.7051 - val_loss: 1.0042 - val_acc: 0.6136\n",
      "Epoch 2033/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.7180 - acc: 0.7080 - val_loss: 1.1508 - val_acc: 0.5852\n",
      "Epoch 2034/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.7505 - acc: 0.7037 - val_loss: 1.1718 - val_acc: 0.5795\n",
      "Epoch 2035/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.7077 - acc: 0.7137 - val_loss: 1.0358 - val_acc: 0.6136\n",
      "Epoch 2036/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.7002 - acc: 0.7194 - val_loss: 1.0986 - val_acc: 0.6080\n",
      "Epoch 2037/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.7137 - acc: 0.7179 - val_loss: 0.9931 - val_acc: 0.6364\n",
      "Epoch 2038/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.6457 - acc: 0.7422 - val_loss: 1.0090 - val_acc: 0.5966\n",
      "Epoch 2039/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.6899 - acc: 0.7379 - val_loss: 0.9813 - val_acc: 0.6364\n",
      "Epoch 2040/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.6321 - acc: 0.7621 - val_loss: 0.9508 - val_acc: 0.6534\n",
      "Epoch 2041/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.6823 - acc: 0.7479 - val_loss: 0.9780 - val_acc: 0.6420\n",
      "Epoch 2042/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.6420 - acc: 0.7379 - val_loss: 0.9863 - val_acc: 0.6307\n",
      "Epoch 2043/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.6705 - acc: 0.7265 - val_loss: 1.0440 - val_acc: 0.6080\n",
      "Epoch 2044/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.6890 - acc: 0.7293 - val_loss: 0.9627 - val_acc: 0.6364\n",
      "Epoch 2045/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.6576 - acc: 0.7350 - val_loss: 0.9794 - val_acc: 0.6307\n",
      "Epoch 2046/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.6937 - acc: 0.7236 - val_loss: 0.9730 - val_acc: 0.6364\n",
      "Epoch 2047/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.6700 - acc: 0.7393 - val_loss: 0.9381 - val_acc: 0.6364\n",
      "Epoch 2048/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.6636 - acc: 0.7265 - val_loss: 1.0692 - val_acc: 0.5966\n",
      "Epoch 2049/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.7096 - acc: 0.7194 - val_loss: 0.8876 - val_acc: 0.6818\n",
      "Epoch 2050/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.7168 - acc: 0.7051 - val_loss: 0.9555 - val_acc: 0.6420\n",
      "Epoch 2051/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.6562 - acc: 0.7393 - val_loss: 1.0287 - val_acc: 0.5852\n",
      "Epoch 2052/3000\n",
      "702/702 [==============================] - 0s 587us/sample - loss: 0.6730 - acc: 0.7265 - val_loss: 1.1297 - val_acc: 0.6080\n",
      "Epoch 2053/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.7065 - acc: 0.7037 - val_loss: 1.0177 - val_acc: 0.6136\n",
      "Epoch 2054/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.7692 - acc: 0.7023 - val_loss: 0.9925 - val_acc: 0.6250\n",
      "Epoch 2055/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.7395 - acc: 0.7251 - val_loss: 1.2376 - val_acc: 0.5682\n",
      "Epoch 2056/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.7049 - acc: 0.7066 - val_loss: 1.1039 - val_acc: 0.5909\n",
      "Epoch 2057/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.6939 - acc: 0.7279 - val_loss: 1.0913 - val_acc: 0.5909\n",
      "Epoch 2058/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.7206 - acc: 0.7080 - val_loss: 1.1343 - val_acc: 0.5795\n",
      "Epoch 2059/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.6443 - acc: 0.7365 - val_loss: 0.9576 - val_acc: 0.6307\n",
      "Epoch 2060/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.7729 - acc: 0.7066 - val_loss: 1.1803 - val_acc: 0.5852\n",
      "Epoch 2061/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.7754 - acc: 0.6852 - val_loss: 1.1807 - val_acc: 0.5682\n",
      "Epoch 2062/3000\n",
      "702/702 [==============================] - 0s 566us/sample - loss: 0.6553 - acc: 0.7393 - val_loss: 1.0880 - val_acc: 0.5795\n",
      "Epoch 2063/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.6851 - acc: 0.7350 - val_loss: 1.1319 - val_acc: 0.5852\n",
      "Epoch 2064/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.6496 - acc: 0.7479 - val_loss: 1.0491 - val_acc: 0.5909\n",
      "Epoch 2065/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.6981 - acc: 0.7293 - val_loss: 1.1316 - val_acc: 0.5682\n",
      "Epoch 2066/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.7264 - acc: 0.7222 - val_loss: 1.3468 - val_acc: 0.5057\n",
      "Epoch 2067/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.7083 - acc: 0.7123 - val_loss: 1.1270 - val_acc: 0.5739\n",
      "Epoch 2068/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.7652 - acc: 0.7066 - val_loss: 1.1775 - val_acc: 0.5852\n",
      "Epoch 2069/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.6802 - acc: 0.7165 - val_loss: 1.2557 - val_acc: 0.5739\n",
      "Epoch 2070/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.6944 - acc: 0.7279 - val_loss: 1.1370 - val_acc: 0.6023\n",
      "Epoch 2071/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.6755 - acc: 0.7422 - val_loss: 1.2203 - val_acc: 0.5568\n",
      "Epoch 2072/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.6918 - acc: 0.7350 - val_loss: 1.2160 - val_acc: 0.5682\n",
      "Epoch 2073/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.6902 - acc: 0.7393 - val_loss: 1.1112 - val_acc: 0.5739\n",
      "Epoch 2074/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.7396 - acc: 0.7236 - val_loss: 1.2321 - val_acc: 0.5625\n",
      "Epoch 2075/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.7269 - acc: 0.6909 - val_loss: 1.1215 - val_acc: 0.5682\n",
      "Epoch 2076/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.7127 - acc: 0.7222 - val_loss: 1.0196 - val_acc: 0.5966\n",
      "Epoch 2077/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.8509 - acc: 0.6994 - val_loss: 1.5099 - val_acc: 0.5170\n",
      "Epoch 2078/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.9103 - acc: 0.6638 - val_loss: 1.4560 - val_acc: 0.5455\n",
      "Epoch 2079/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.7792 - acc: 0.6909 - val_loss: 1.3199 - val_acc: 0.5625\n",
      "Epoch 2080/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.8429 - acc: 0.6923 - val_loss: 1.3957 - val_acc: 0.5398\n",
      "Epoch 2081/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.7598 - acc: 0.6980 - val_loss: 1.3671 - val_acc: 0.5568\n",
      "Epoch 2082/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.8477 - acc: 0.6638 - val_loss: 1.1438 - val_acc: 0.5909\n",
      "Epoch 2083/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.6846 - acc: 0.7137 - val_loss: 1.0571 - val_acc: 0.6023\n",
      "Epoch 2084/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.6481 - acc: 0.7521 - val_loss: 1.1695 - val_acc: 0.5341\n",
      "Epoch 2085/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.6596 - acc: 0.7407 - val_loss: 1.1965 - val_acc: 0.5511\n",
      "Epoch 2086/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.6544 - acc: 0.7336 - val_loss: 1.0630 - val_acc: 0.5966\n",
      "Epoch 2087/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.7071 - acc: 0.7365 - val_loss: 1.0110 - val_acc: 0.6080\n",
      "Epoch 2088/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.7096 - acc: 0.7137 - val_loss: 1.2296 - val_acc: 0.5568\n",
      "Epoch 2089/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.7371 - acc: 0.7023 - val_loss: 1.0966 - val_acc: 0.5852\n",
      "Epoch 2090/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.6372 - acc: 0.7422 - val_loss: 1.0133 - val_acc: 0.6023\n",
      "Epoch 2091/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.6863 - acc: 0.7365 - val_loss: 1.0476 - val_acc: 0.5966\n",
      "Epoch 2092/3000\n",
      "702/702 [==============================] - 0s 581us/sample - loss: 0.6675 - acc: 0.7422 - val_loss: 1.0903 - val_acc: 0.5909\n",
      "Epoch 2093/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.6234 - acc: 0.7536 - val_loss: 1.0235 - val_acc: 0.6023\n",
      "Epoch 2094/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.6454 - acc: 0.7593 - val_loss: 1.0406 - val_acc: 0.6080\n",
      "Epoch 2095/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.5819 - acc: 0.7536 - val_loss: 1.0674 - val_acc: 0.6136\n",
      "Epoch 2096/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.5993 - acc: 0.7692 - val_loss: 0.9715 - val_acc: 0.6193\n",
      "Epoch 2097/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.6419 - acc: 0.7564 - val_loss: 1.1198 - val_acc: 0.5625\n",
      "Epoch 2098/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.6736 - acc: 0.7293 - val_loss: 1.1986 - val_acc: 0.5852\n",
      "Epoch 2099/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.6768 - acc: 0.7080 - val_loss: 1.1455 - val_acc: 0.6080\n",
      "Epoch 2100/3000\n",
      "702/702 [==============================] - 0s 520us/sample - loss: 0.6627 - acc: 0.7350 - val_loss: 0.9685 - val_acc: 0.6477\n",
      "Epoch 2101/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.6382 - acc: 0.7536 - val_loss: 1.1764 - val_acc: 0.5795\n",
      "Epoch 2102/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 0.6761 - acc: 0.7051 - val_loss: 1.2088 - val_acc: 0.5625\n",
      "Epoch 2103/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.6486 - acc: 0.7265 - val_loss: 1.0013 - val_acc: 0.6250\n",
      "Epoch 2104/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.6417 - acc: 0.7550 - val_loss: 1.0475 - val_acc: 0.6080\n",
      "Epoch 2105/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.6696 - acc: 0.7308 - val_loss: 1.0577 - val_acc: 0.6080\n",
      "Epoch 2106/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.6727 - acc: 0.7279 - val_loss: 0.8926 - val_acc: 0.6705\n",
      "Epoch 2107/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.6846 - acc: 0.7336 - val_loss: 1.0424 - val_acc: 0.6420\n",
      "Epoch 2108/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.6588 - acc: 0.7293 - val_loss: 1.0286 - val_acc: 0.6420\n",
      "Epoch 2109/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.6320 - acc: 0.7422 - val_loss: 1.0286 - val_acc: 0.6193\n",
      "Epoch 2110/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.6411 - acc: 0.7379 - val_loss: 1.1990 - val_acc: 0.5852\n",
      "Epoch 2111/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.7295 - acc: 0.7123 - val_loss: 1.1097 - val_acc: 0.6193\n",
      "Epoch 2112/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.6380 - acc: 0.7507 - val_loss: 0.9471 - val_acc: 0.6648\n",
      "Epoch 2113/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.6388 - acc: 0.7578 - val_loss: 1.0274 - val_acc: 0.6250\n",
      "Epoch 2114/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.5973 - acc: 0.7550 - val_loss: 0.9981 - val_acc: 0.6307\n",
      "Epoch 2115/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.6214 - acc: 0.7493 - val_loss: 0.9559 - val_acc: 0.6477\n",
      "Epoch 2116/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.6129 - acc: 0.7493 - val_loss: 0.9977 - val_acc: 0.6420\n",
      "Epoch 2117/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.6494 - acc: 0.7293 - val_loss: 0.9305 - val_acc: 0.6534\n",
      "Epoch 2118/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.5754 - acc: 0.7664 - val_loss: 0.9337 - val_acc: 0.6477\n",
      "Epoch 2119/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.5854 - acc: 0.7749 - val_loss: 1.0114 - val_acc: 0.6307\n",
      "Epoch 2120/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.6005 - acc: 0.7550 - val_loss: 0.9276 - val_acc: 0.6420\n",
      "Epoch 2121/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.6365 - acc: 0.7507 - val_loss: 0.9719 - val_acc: 0.6420\n",
      "Epoch 2122/3000\n",
      "702/702 [==============================] - 0s 567us/sample - loss: 0.6235 - acc: 0.7550 - val_loss: 0.9824 - val_acc: 0.6364\n",
      "Epoch 2123/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.5871 - acc: 0.7564 - val_loss: 0.9414 - val_acc: 0.6477\n",
      "Epoch 2124/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.5687 - acc: 0.7721 - val_loss: 0.9429 - val_acc: 0.6648\n",
      "Epoch 2125/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.5742 - acc: 0.7650 - val_loss: 0.9455 - val_acc: 0.6591\n",
      "Epoch 2126/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.5912 - acc: 0.7849 - val_loss: 0.9724 - val_acc: 0.6477\n",
      "Epoch 2127/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.5669 - acc: 0.7764 - val_loss: 0.9776 - val_acc: 0.6477\n",
      "Epoch 2128/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.5929 - acc: 0.7863 - val_loss: 0.9865 - val_acc: 0.6364\n",
      "Epoch 2129/3000\n",
      "702/702 [==============================] - 0s 527us/sample - loss: 0.6018 - acc: 0.7735 - val_loss: 0.9956 - val_acc: 0.6307\n",
      "Epoch 2130/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.5934 - acc: 0.7564 - val_loss: 0.8877 - val_acc: 0.6648\n",
      "Epoch 2131/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5856 - acc: 0.7792 - val_loss: 1.0227 - val_acc: 0.6250\n",
      "Epoch 2132/3000\n",
      "702/702 [==============================] - 0s 646us/sample - loss: 0.6794 - acc: 0.7251 - val_loss: 1.0195 - val_acc: 0.6250\n",
      "Epoch 2133/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.6324 - acc: 0.7721 - val_loss: 0.8384 - val_acc: 0.6875\n",
      "Epoch 2134/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.6693 - acc: 0.7593 - val_loss: 0.9826 - val_acc: 0.6420\n",
      "Epoch 2135/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.6678 - acc: 0.7165 - val_loss: 0.9436 - val_acc: 0.6307\n",
      "Epoch 2136/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.5740 - acc: 0.7792 - val_loss: 0.8759 - val_acc: 0.6477\n",
      "Epoch 2137/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.5555 - acc: 0.7707 - val_loss: 0.9405 - val_acc: 0.6307\n",
      "Epoch 2138/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.6161 - acc: 0.7607 - val_loss: 0.9535 - val_acc: 0.6136\n",
      "Epoch 2139/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.5713 - acc: 0.7863 - val_loss: 0.9472 - val_acc: 0.6193\n",
      "Epoch 2140/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.6017 - acc: 0.7635 - val_loss: 0.8860 - val_acc: 0.6534\n",
      "Epoch 2141/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5741 - acc: 0.7764 - val_loss: 0.9161 - val_acc: 0.6591\n",
      "Epoch 2142/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.5954 - acc: 0.7650 - val_loss: 0.8938 - val_acc: 0.6420\n",
      "Epoch 2143/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.5926 - acc: 0.7749 - val_loss: 0.9340 - val_acc: 0.6250\n",
      "Epoch 2144/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.5741 - acc: 0.7735 - val_loss: 0.8851 - val_acc: 0.6591\n",
      "Epoch 2145/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.6041 - acc: 0.7735 - val_loss: 0.9676 - val_acc: 0.6420\n",
      "Epoch 2146/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.6102 - acc: 0.7493 - val_loss: 0.8464 - val_acc: 0.6705\n",
      "Epoch 2147/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.5708 - acc: 0.7692 - val_loss: 0.9109 - val_acc: 0.6534\n",
      "Epoch 2148/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.5459 - acc: 0.7920 - val_loss: 0.9137 - val_acc: 0.6193\n",
      "Epoch 2149/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.5606 - acc: 0.7906 - val_loss: 0.9098 - val_acc: 0.6364\n",
      "Epoch 2150/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.5544 - acc: 0.7963 - val_loss: 0.8878 - val_acc: 0.6705\n",
      "Epoch 2151/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5791 - acc: 0.7806 - val_loss: 0.8637 - val_acc: 0.6875\n",
      "Epoch 2152/3000\n",
      "702/702 [==============================] - 0s 589us/sample - loss: 0.5222 - acc: 0.8063 - val_loss: 0.8667 - val_acc: 0.6705\n",
      "Epoch 2153/3000\n",
      "702/702 [==============================] - 0s 531us/sample - loss: 0.5391 - acc: 0.7934 - val_loss: 0.8659 - val_acc: 0.6477\n",
      "Epoch 2154/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.5349 - acc: 0.8063 - val_loss: 0.8915 - val_acc: 0.6648\n",
      "Epoch 2155/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.5460 - acc: 0.7778 - val_loss: 0.8748 - val_acc: 0.6761\n",
      "Epoch 2156/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.5415 - acc: 0.7977 - val_loss: 0.8303 - val_acc: 0.6648\n",
      "Epoch 2157/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.5548 - acc: 0.7977 - val_loss: 0.8905 - val_acc: 0.6591\n",
      "Epoch 2158/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.5574 - acc: 0.8091 - val_loss: 0.8714 - val_acc: 0.6648\n",
      "Epoch 2159/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.6205 - acc: 0.7692 - val_loss: 0.9736 - val_acc: 0.6136\n",
      "Epoch 2160/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.7560 - acc: 0.7692 - val_loss: 0.9320 - val_acc: 0.6534\n",
      "Epoch 2161/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5678 - acc: 0.7892 - val_loss: 0.9139 - val_acc: 0.6818\n",
      "Epoch 2162/3000\n",
      "702/702 [==============================] - 0s 596us/sample - loss: 0.5741 - acc: 0.7835 - val_loss: 1.0173 - val_acc: 0.6136\n",
      "Epoch 2163/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.5398 - acc: 0.8063 - val_loss: 1.0078 - val_acc: 0.6136\n",
      "Epoch 2164/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.5289 - acc: 0.8191 - val_loss: 0.9369 - val_acc: 0.6477\n",
      "Epoch 2165/3000\n",
      "702/702 [==============================] - 0s 533us/sample - loss: 0.5570 - acc: 0.7821 - val_loss: 0.8149 - val_acc: 0.6818\n",
      "Epoch 2166/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.5634 - acc: 0.8020 - val_loss: 0.9412 - val_acc: 0.6364\n",
      "Epoch 2167/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.5466 - acc: 0.7920 - val_loss: 0.8953 - val_acc: 0.6591\n",
      "Epoch 2168/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.5216 - acc: 0.8034 - val_loss: 0.9296 - val_acc: 0.6420\n",
      "Epoch 2169/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.5653 - acc: 0.7749 - val_loss: 0.9395 - val_acc: 0.6477\n",
      "Epoch 2170/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.5831 - acc: 0.7849 - val_loss: 0.9297 - val_acc: 0.6477\n",
      "Epoch 2171/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.6442 - acc: 0.7222 - val_loss: 0.9549 - val_acc: 0.6591\n",
      "Epoch 2172/3000\n",
      "702/702 [==============================] - 0s 554us/sample - loss: 0.5856 - acc: 0.7564 - val_loss: 0.7637 - val_acc: 0.7273\n",
      "Epoch 2173/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.7590 - acc: 0.7080 - val_loss: 1.0714 - val_acc: 0.6307\n",
      "Epoch 2174/3000\n",
      "702/702 [==============================] - 0s 474us/sample - loss: 0.7596 - acc: 0.6866 - val_loss: 0.8976 - val_acc: 0.6534\n",
      "Epoch 2175/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.6878 - acc: 0.7536 - val_loss: 1.0081 - val_acc: 0.6307\n",
      "Epoch 2176/3000\n",
      "702/702 [==============================] - 0s 474us/sample - loss: 0.7636 - acc: 0.7051 - val_loss: 1.3369 - val_acc: 0.5625\n",
      "Epoch 2177/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.7973 - acc: 0.6937 - val_loss: 0.9482 - val_acc: 0.6250\n",
      "Epoch 2178/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.8042 - acc: 0.6994 - val_loss: 1.0234 - val_acc: 0.6193\n",
      "Epoch 2179/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.6839 - acc: 0.7265 - val_loss: 1.2458 - val_acc: 0.5511\n",
      "Epoch 2180/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.7718 - acc: 0.7009 - val_loss: 1.1178 - val_acc: 0.5966\n",
      "Epoch 2181/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.6289 - acc: 0.7479 - val_loss: 0.8433 - val_acc: 0.6818\n",
      "Epoch 2182/3000\n",
      "702/702 [==============================] - 0s 558us/sample - loss: 0.6135 - acc: 0.7564 - val_loss: 0.9196 - val_acc: 0.6591\n",
      "Epoch 2183/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.6138 - acc: 0.7564 - val_loss: 0.9166 - val_acc: 0.6307\n",
      "Epoch 2184/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.6119 - acc: 0.7564 - val_loss: 0.9554 - val_acc: 0.6080\n",
      "Epoch 2185/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 0.5935 - acc: 0.7735 - val_loss: 1.0094 - val_acc: 0.6193\n",
      "Epoch 2186/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.5788 - acc: 0.7621 - val_loss: 0.9016 - val_acc: 0.6477\n",
      "Epoch 2187/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.6219 - acc: 0.7578 - val_loss: 0.9606 - val_acc: 0.6250\n",
      "Epoch 2188/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.6283 - acc: 0.7507 - val_loss: 1.0127 - val_acc: 0.6250\n",
      "Epoch 2189/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.6228 - acc: 0.7521 - val_loss: 0.8895 - val_acc: 0.6534\n",
      "Epoch 2190/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.5788 - acc: 0.7678 - val_loss: 0.8832 - val_acc: 0.6591\n",
      "Epoch 2191/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.6026 - acc: 0.7635 - val_loss: 1.0116 - val_acc: 0.6250\n",
      "Epoch 2192/3000\n",
      "702/702 [==============================] - 0s 555us/sample - loss: 0.5607 - acc: 0.7664 - val_loss: 0.9775 - val_acc: 0.6080\n",
      "Epoch 2193/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.5844 - acc: 0.7607 - val_loss: 0.9113 - val_acc: 0.6364\n",
      "Epoch 2194/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.5560 - acc: 0.7749 - val_loss: 0.9382 - val_acc: 0.6420\n",
      "Epoch 2195/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.5475 - acc: 0.7735 - val_loss: 0.8687 - val_acc: 0.6534\n",
      "Epoch 2196/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.5715 - acc: 0.7664 - val_loss: 0.8736 - val_acc: 0.6534\n",
      "Epoch 2197/3000\n",
      "702/702 [==============================] - 0s 474us/sample - loss: 0.6200 - acc: 0.7678 - val_loss: 0.9941 - val_acc: 0.6136\n",
      "Epoch 2198/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.5911 - acc: 0.7621 - val_loss: 0.8770 - val_acc: 0.6534\n",
      "Epoch 2199/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.5527 - acc: 0.7920 - val_loss: 0.9546 - val_acc: 0.6364\n",
      "Epoch 2200/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.5767 - acc: 0.7621 - val_loss: 1.0162 - val_acc: 0.6136\n",
      "Epoch 2201/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5556 - acc: 0.7977 - val_loss: 0.9760 - val_acc: 0.6250\n",
      "Epoch 2202/3000\n",
      "702/702 [==============================] - 0s 553us/sample - loss: 0.5893 - acc: 0.7735 - val_loss: 0.9980 - val_acc: 0.6364\n",
      "Epoch 2203/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.5528 - acc: 0.7692 - val_loss: 0.8965 - val_acc: 0.6420\n",
      "Epoch 2204/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.5555 - acc: 0.7977 - val_loss: 1.0437 - val_acc: 0.6250\n",
      "Epoch 2205/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.5283 - acc: 0.8006 - val_loss: 0.9447 - val_acc: 0.6080\n",
      "Epoch 2206/3000\n",
      "702/702 [==============================] - 0s 473us/sample - loss: 0.6211 - acc: 0.7650 - val_loss: 0.9704 - val_acc: 0.6420\n",
      "Epoch 2207/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.5642 - acc: 0.7863 - val_loss: 0.9006 - val_acc: 0.6648\n",
      "Epoch 2208/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.5718 - acc: 0.8077 - val_loss: 0.9803 - val_acc: 0.6477\n",
      "Epoch 2209/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.5927 - acc: 0.7664 - val_loss: 1.0472 - val_acc: 0.6136\n",
      "Epoch 2210/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.5671 - acc: 0.7806 - val_loss: 0.8632 - val_acc: 0.6761\n",
      "Epoch 2211/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5467 - acc: 0.7934 - val_loss: 0.8271 - val_acc: 0.6875\n",
      "Epoch 2212/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 0.5331 - acc: 0.7991 - val_loss: 0.8981 - val_acc: 0.6591\n",
      "Epoch 2213/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.5719 - acc: 0.7792 - val_loss: 0.8733 - val_acc: 0.6591\n",
      "Epoch 2214/3000\n",
      "702/702 [==============================] - 0s 474us/sample - loss: 0.5628 - acc: 0.8063 - val_loss: 0.9115 - val_acc: 0.6534\n",
      "Epoch 2215/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.5388 - acc: 0.8020 - val_loss: 0.8785 - val_acc: 0.6534\n",
      "Epoch 2216/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.5553 - acc: 0.8034 - val_loss: 0.9114 - val_acc: 0.6875\n",
      "Epoch 2217/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 0.5670 - acc: 0.8020 - val_loss: 0.8454 - val_acc: 0.7102\n",
      "Epoch 2218/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.5225 - acc: 0.8091 - val_loss: 0.9132 - val_acc: 0.6591\n",
      "Epoch 2219/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.5336 - acc: 0.8134 - val_loss: 0.7517 - val_acc: 0.7557\n",
      "Epoch 2220/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.5594 - acc: 0.7949 - val_loss: 0.8694 - val_acc: 0.7102\n",
      "Epoch 2221/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5667 - acc: 0.7934 - val_loss: 0.8401 - val_acc: 0.7216\n",
      "Epoch 2222/3000\n",
      "702/702 [==============================] - 0s 558us/sample - loss: 0.5559 - acc: 0.8006 - val_loss: 0.8565 - val_acc: 0.7216\n",
      "Epoch 2223/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.5519 - acc: 0.8020 - val_loss: 0.7498 - val_acc: 0.7557\n",
      "Epoch 2224/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.6052 - acc: 0.7806 - val_loss: 0.9204 - val_acc: 0.6761\n",
      "Epoch 2225/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.6619 - acc: 0.7322 - val_loss: 0.8810 - val_acc: 0.7045\n",
      "Epoch 2226/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.5581 - acc: 0.7821 - val_loss: 0.8661 - val_acc: 0.6989\n",
      "Epoch 2227/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.5368 - acc: 0.8134 - val_loss: 0.8713 - val_acc: 0.7102\n",
      "Epoch 2228/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.5102 - acc: 0.8191 - val_loss: 0.7971 - val_acc: 0.7330\n",
      "Epoch 2229/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.6082 - acc: 0.7963 - val_loss: 0.8488 - val_acc: 0.6818\n",
      "Epoch 2230/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.5525 - acc: 0.8063 - val_loss: 0.9031 - val_acc: 0.6705\n",
      "Epoch 2231/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5687 - acc: 0.7963 - val_loss: 0.8538 - val_acc: 0.6989\n",
      "Epoch 2232/3000\n",
      "702/702 [==============================] - 0s 561us/sample - loss: 0.5654 - acc: 0.7892 - val_loss: 0.9296 - val_acc: 0.6818\n",
      "Epoch 2233/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.5361 - acc: 0.7949 - val_loss: 0.7799 - val_acc: 0.7500\n",
      "Epoch 2234/3000\n",
      "702/702 [==============================] - 0s 474us/sample - loss: 0.5519 - acc: 0.7934 - val_loss: 0.9918 - val_acc: 0.6023\n",
      "Epoch 2235/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.6213 - acc: 0.7664 - val_loss: 0.8240 - val_acc: 0.7273\n",
      "Epoch 2236/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.5886 - acc: 0.7778 - val_loss: 0.9889 - val_acc: 0.5795\n",
      "Epoch 2237/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.7138 - acc: 0.6823 - val_loss: 1.0148 - val_acc: 0.5682\n",
      "Epoch 2238/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.6431 - acc: 0.7350 - val_loss: 0.8039 - val_acc: 0.7273\n",
      "Epoch 2239/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.5752 - acc: 0.7692 - val_loss: 0.9941 - val_acc: 0.6307\n",
      "Epoch 2240/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.5661 - acc: 0.7934 - val_loss: 0.9168 - val_acc: 0.6989\n",
      "Epoch 2241/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5176 - acc: 0.8205 - val_loss: 0.8185 - val_acc: 0.6989\n",
      "Epoch 2242/3000\n",
      "702/702 [==============================] - 0s 557us/sample - loss: 0.5423 - acc: 0.7963 - val_loss: 1.0285 - val_acc: 0.6250\n",
      "Epoch 2243/3000\n",
      "702/702 [==============================] - 0s 473us/sample - loss: 0.5901 - acc: 0.7635 - val_loss: 0.8864 - val_acc: 0.6761\n",
      "Epoch 2244/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.5294 - acc: 0.7977 - val_loss: 0.9298 - val_acc: 0.7102\n",
      "Epoch 2245/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.5599 - acc: 0.7949 - val_loss: 0.8875 - val_acc: 0.7159\n",
      "Epoch 2246/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.5384 - acc: 0.8006 - val_loss: 0.9022 - val_acc: 0.6989\n",
      "Epoch 2247/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.5684 - acc: 0.7863 - val_loss: 0.9437 - val_acc: 0.6705\n",
      "Epoch 2248/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.5502 - acc: 0.8006 - val_loss: 0.7711 - val_acc: 0.7216\n",
      "Epoch 2249/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.5292 - acc: 0.7877 - val_loss: 0.8998 - val_acc: 0.6875\n",
      "Epoch 2250/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.5372 - acc: 0.8020 - val_loss: 0.8453 - val_acc: 0.7330\n",
      "Epoch 2251/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5190 - acc: 0.8034 - val_loss: 0.9094 - val_acc: 0.6989\n",
      "Epoch 2252/3000\n",
      "702/702 [==============================] - 0s 564us/sample - loss: 0.5569 - acc: 0.7806 - val_loss: 0.9381 - val_acc: 0.6648\n",
      "Epoch 2253/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.5265 - acc: 0.8048 - val_loss: 0.7960 - val_acc: 0.7216\n",
      "Epoch 2254/3000\n",
      "702/702 [==============================] - 0s 473us/sample - loss: 0.5677 - acc: 0.7934 - val_loss: 0.9031 - val_acc: 0.6875\n",
      "Epoch 2255/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.5553 - acc: 0.7863 - val_loss: 0.8567 - val_acc: 0.7045\n",
      "Epoch 2256/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.5198 - acc: 0.8148 - val_loss: 0.8504 - val_acc: 0.6932\n",
      "Epoch 2257/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.5244 - acc: 0.7963 - val_loss: 0.8580 - val_acc: 0.7045\n",
      "Epoch 2258/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.5314 - acc: 0.8006 - val_loss: 0.8723 - val_acc: 0.6932\n",
      "Epoch 2259/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.5250 - acc: 0.8191 - val_loss: 0.8016 - val_acc: 0.7500\n",
      "Epoch 2260/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.5331 - acc: 0.8276 - val_loss: 0.7343 - val_acc: 0.7727\n",
      "Epoch 2261/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5250 - acc: 0.8020 - val_loss: 0.9056 - val_acc: 0.6648\n",
      "Epoch 2262/3000\n",
      "702/702 [==============================] - 0s 554us/sample - loss: 0.5590 - acc: 0.7792 - val_loss: 0.7444 - val_acc: 0.7500\n",
      "Epoch 2263/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.4994 - acc: 0.8034 - val_loss: 0.9193 - val_acc: 0.6477\n",
      "Epoch 2264/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.6613 - acc: 0.7222 - val_loss: 0.9708 - val_acc: 0.6136\n",
      "Epoch 2265/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.5300 - acc: 0.7991 - val_loss: 0.8038 - val_acc: 0.7614\n",
      "Epoch 2266/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.5128 - acc: 0.8006 - val_loss: 0.9118 - val_acc: 0.6648\n",
      "Epoch 2267/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.5718 - acc: 0.7749 - val_loss: 1.0033 - val_acc: 0.6136\n",
      "Epoch 2268/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.5621 - acc: 0.7991 - val_loss: 0.8455 - val_acc: 0.7386\n",
      "Epoch 2269/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.5326 - acc: 0.8034 - val_loss: 0.9164 - val_acc: 0.7045\n",
      "Epoch 2270/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.5631 - acc: 0.7849 - val_loss: 0.9354 - val_acc: 0.6705\n",
      "Epoch 2271/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5508 - acc: 0.8034 - val_loss: 0.7966 - val_acc: 0.7216\n",
      "Epoch 2272/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.5587 - acc: 0.7963 - val_loss: 0.9614 - val_acc: 0.6818\n",
      "Epoch 2273/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.5225 - acc: 0.8077 - val_loss: 0.8034 - val_acc: 0.7216\n",
      "Epoch 2274/3000\n",
      "702/702 [==============================] - 0s 471us/sample - loss: 0.5770 - acc: 0.7949 - val_loss: 0.8572 - val_acc: 0.7330\n",
      "Epoch 2275/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.4936 - acc: 0.8191 - val_loss: 0.8484 - val_acc: 0.7216\n",
      "Epoch 2276/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.5084 - acc: 0.8134 - val_loss: 0.8590 - val_acc: 0.7159\n",
      "Epoch 2277/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 0.4658 - acc: 0.8462 - val_loss: 0.8746 - val_acc: 0.7045\n",
      "Epoch 2278/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.5086 - acc: 0.8091 - val_loss: 0.8446 - val_acc: 0.7216\n",
      "Epoch 2279/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.5257 - acc: 0.8048 - val_loss: 0.8541 - val_acc: 0.7102\n",
      "Epoch 2280/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.4790 - acc: 0.8276 - val_loss: 0.8330 - val_acc: 0.7273\n",
      "Epoch 2281/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4938 - acc: 0.8262 - val_loss: 0.8437 - val_acc: 0.7216\n",
      "Epoch 2282/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.5171 - acc: 0.8034 - val_loss: 0.7626 - val_acc: 0.7386\n",
      "Epoch 2283/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.4815 - acc: 0.8276 - val_loss: 0.8767 - val_acc: 0.6875\n",
      "Epoch 2284/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.4989 - acc: 0.8034 - val_loss: 0.7540 - val_acc: 0.7614\n",
      "Epoch 2285/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.4962 - acc: 0.8177 - val_loss: 0.7594 - val_acc: 0.7386\n",
      "Epoch 2286/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.5268 - acc: 0.7877 - val_loss: 0.9038 - val_acc: 0.6591\n",
      "Epoch 2287/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.5402 - acc: 0.7892 - val_loss: 0.7591 - val_acc: 0.7557\n",
      "Epoch 2288/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.5330 - acc: 0.8048 - val_loss: 0.9268 - val_acc: 0.6193\n",
      "Epoch 2289/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.5949 - acc: 0.7678 - val_loss: 0.6721 - val_acc: 0.7500\n",
      "Epoch 2290/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.6480 - acc: 0.7578 - val_loss: 0.9934 - val_acc: 0.5852\n",
      "Epoch 2291/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5897 - acc: 0.7664 - val_loss: 0.8493 - val_acc: 0.7045\n",
      "Epoch 2292/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 0.5255 - acc: 0.8091 - val_loss: 0.8757 - val_acc: 0.6875\n",
      "Epoch 2293/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.5230 - acc: 0.8120 - val_loss: 0.7624 - val_acc: 0.7500\n",
      "Epoch 2294/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.5437 - acc: 0.7991 - val_loss: 0.9915 - val_acc: 0.5966\n",
      "Epoch 2295/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.5481 - acc: 0.7863 - val_loss: 0.8364 - val_acc: 0.6932\n",
      "Epoch 2296/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.5287 - acc: 0.7977 - val_loss: 0.9516 - val_acc: 0.6250\n",
      "Epoch 2297/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.5524 - acc: 0.7892 - val_loss: 0.8532 - val_acc: 0.7045\n",
      "Epoch 2298/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.5244 - acc: 0.8091 - val_loss: 0.9210 - val_acc: 0.6591\n",
      "Epoch 2299/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.4935 - acc: 0.8305 - val_loss: 0.9292 - val_acc: 0.6648\n",
      "Epoch 2300/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.4916 - acc: 0.8120 - val_loss: 0.8039 - val_acc: 0.7159\n",
      "Epoch 2301/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5593 - acc: 0.7877 - val_loss: 0.9478 - val_acc: 0.6364\n",
      "Epoch 2302/3000\n",
      "702/702 [==============================] - 0s 554us/sample - loss: 0.5056 - acc: 0.8063 - val_loss: 0.7278 - val_acc: 0.7614\n",
      "Epoch 2303/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.5870 - acc: 0.7749 - val_loss: 0.9519 - val_acc: 0.6193\n",
      "Epoch 2304/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.5416 - acc: 0.7877 - val_loss: 0.8348 - val_acc: 0.7102\n",
      "Epoch 2305/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.5683 - acc: 0.7977 - val_loss: 1.0558 - val_acc: 0.6023\n",
      "Epoch 2306/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.5166 - acc: 0.7863 - val_loss: 0.8385 - val_acc: 0.7443\n",
      "Epoch 2307/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.5617 - acc: 0.8077 - val_loss: 1.0098 - val_acc: 0.5966\n",
      "Epoch 2308/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.5346 - acc: 0.7849 - val_loss: 0.7864 - val_acc: 0.7386\n",
      "Epoch 2309/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.4958 - acc: 0.8120 - val_loss: 0.8901 - val_acc: 0.6648\n",
      "Epoch 2310/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.5533 - acc: 0.7877 - val_loss: 0.8221 - val_acc: 0.7159\n",
      "Epoch 2311/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5186 - acc: 0.8148 - val_loss: 0.8999 - val_acc: 0.6761\n",
      "Epoch 2312/3000\n",
      "702/702 [==============================] - 0s 560us/sample - loss: 0.5341 - acc: 0.8063 - val_loss: 1.0215 - val_acc: 0.6591\n",
      "Epoch 2313/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.5585 - acc: 0.7892 - val_loss: 0.9122 - val_acc: 0.6932\n",
      "Epoch 2314/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.5474 - acc: 0.7977 - val_loss: 0.9509 - val_acc: 0.6591\n",
      "Epoch 2315/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.5766 - acc: 0.7806 - val_loss: 0.7497 - val_acc: 0.7386\n",
      "Epoch 2316/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.5023 - acc: 0.7863 - val_loss: 0.8490 - val_acc: 0.6932\n",
      "Epoch 2317/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.5275 - acc: 0.8105 - val_loss: 0.7991 - val_acc: 0.7557\n",
      "Epoch 2318/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.5002 - acc: 0.8048 - val_loss: 0.9445 - val_acc: 0.6591\n",
      "Epoch 2319/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.5672 - acc: 0.7906 - val_loss: 1.0160 - val_acc: 0.6477\n",
      "Epoch 2320/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.5183 - acc: 0.8034 - val_loss: 1.0270 - val_acc: 0.6307\n",
      "Epoch 2321/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.7761 - acc: 0.6980 - val_loss: 1.0779 - val_acc: 0.5739\n",
      "Epoch 2322/3000\n",
      "702/702 [==============================] - 0s 566us/sample - loss: 0.6186 - acc: 0.7678 - val_loss: 0.7764 - val_acc: 0.6705\n",
      "Epoch 2323/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.6929 - acc: 0.7621 - val_loss: 1.1680 - val_acc: 0.5000\n",
      "Epoch 2324/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.5759 - acc: 0.7479 - val_loss: 0.8844 - val_acc: 0.6818\n",
      "Epoch 2325/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.6207 - acc: 0.7963 - val_loss: 0.9561 - val_acc: 0.6136\n",
      "Epoch 2326/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 0.5771 - acc: 0.7735 - val_loss: 0.9492 - val_acc: 0.6080\n",
      "Epoch 2327/3000\n",
      "702/702 [==============================] - 0s 473us/sample - loss: 0.5311 - acc: 0.8105 - val_loss: 0.7324 - val_acc: 0.7330\n",
      "Epoch 2328/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.5577 - acc: 0.8048 - val_loss: 0.9476 - val_acc: 0.6136\n",
      "Epoch 2329/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.5224 - acc: 0.7892 - val_loss: 0.8411 - val_acc: 0.6875\n",
      "Epoch 2330/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.5440 - acc: 0.8077 - val_loss: 0.9044 - val_acc: 0.6648\n",
      "Epoch 2331/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5514 - acc: 0.7977 - val_loss: 0.9717 - val_acc: 0.6420\n",
      "Epoch 2332/3000\n",
      "702/702 [==============================] - 0s 596us/sample - loss: 0.5321 - acc: 0.8006 - val_loss: 0.8573 - val_acc: 0.7216\n",
      "Epoch 2333/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.5030 - acc: 0.8048 - val_loss: 0.9840 - val_acc: 0.6420\n",
      "Epoch 2334/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.4901 - acc: 0.8077 - val_loss: 0.8762 - val_acc: 0.7045\n",
      "Epoch 2335/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.5126 - acc: 0.8162 - val_loss: 0.8727 - val_acc: 0.6875\n",
      "Epoch 2336/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.4643 - acc: 0.8248 - val_loss: 0.9322 - val_acc: 0.6591\n",
      "Epoch 2337/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.4744 - acc: 0.8120 - val_loss: 0.8143 - val_acc: 0.7216\n",
      "Epoch 2338/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.5040 - acc: 0.8105 - val_loss: 0.8270 - val_acc: 0.7045\n",
      "Epoch 2339/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.4811 - acc: 0.8162 - val_loss: 0.8030 - val_acc: 0.7443\n",
      "Epoch 2340/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.5018 - acc: 0.8262 - val_loss: 0.9512 - val_acc: 0.6648\n",
      "Epoch 2341/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4839 - acc: 0.8362 - val_loss: 0.9361 - val_acc: 0.7273\n",
      "Epoch 2342/3000\n",
      "702/702 [==============================] - 0s 569us/sample - loss: 0.5281 - acc: 0.7977 - val_loss: 1.0483 - val_acc: 0.6193\n",
      "Epoch 2343/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.5002 - acc: 0.8134 - val_loss: 0.9134 - val_acc: 0.6818\n",
      "Epoch 2344/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.4948 - acc: 0.8120 - val_loss: 0.9532 - val_acc: 0.6534\n",
      "Epoch 2345/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.5024 - acc: 0.7963 - val_loss: 0.8358 - val_acc: 0.7159\n",
      "Epoch 2346/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.4946 - acc: 0.8219 - val_loss: 1.0083 - val_acc: 0.6250\n",
      "Epoch 2347/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.5165 - acc: 0.8091 - val_loss: 0.9726 - val_acc: 0.6818\n",
      "Epoch 2348/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.5086 - acc: 0.8134 - val_loss: 1.0553 - val_acc: 0.6364\n",
      "Epoch 2349/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.5096 - acc: 0.8034 - val_loss: 0.9085 - val_acc: 0.7330\n",
      "Epoch 2350/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.4881 - acc: 0.8034 - val_loss: 0.9755 - val_acc: 0.6193\n",
      "Epoch 2351/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5147 - acc: 0.8020 - val_loss: 0.8538 - val_acc: 0.6818\n",
      "Epoch 2352/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.4926 - acc: 0.8191 - val_loss: 0.8184 - val_acc: 0.6932\n",
      "Epoch 2353/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.5146 - acc: 0.8120 - val_loss: 0.8862 - val_acc: 0.6705\n",
      "Epoch 2354/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.4865 - acc: 0.8219 - val_loss: 0.8748 - val_acc: 0.6932\n",
      "Epoch 2355/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.4774 - acc: 0.8234 - val_loss: 0.8530 - val_acc: 0.7273\n",
      "Epoch 2356/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 0.5792 - acc: 0.7906 - val_loss: 0.9290 - val_acc: 0.6705\n",
      "Epoch 2357/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.5336 - acc: 0.7977 - val_loss: 0.8705 - val_acc: 0.7330\n",
      "Epoch 2358/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.5709 - acc: 0.7764 - val_loss: 1.0328 - val_acc: 0.6477\n",
      "Epoch 2359/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.5771 - acc: 0.8006 - val_loss: 0.8986 - val_acc: 0.6591\n",
      "Epoch 2360/3000\n",
      "702/702 [==============================] - 0s 474us/sample - loss: 0.5176 - acc: 0.7977 - val_loss: 0.9639 - val_acc: 0.5966\n",
      "Epoch 2361/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5664 - acc: 0.7593 - val_loss: 0.7065 - val_acc: 0.7386\n",
      "Epoch 2362/3000\n",
      "702/702 [==============================] - 0s 552us/sample - loss: 0.5794 - acc: 0.7963 - val_loss: 1.0664 - val_acc: 0.5909\n",
      "Epoch 2363/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.5928 - acc: 0.7450 - val_loss: 0.9125 - val_acc: 0.6875\n",
      "Epoch 2364/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.5405 - acc: 0.7977 - val_loss: 0.9128 - val_acc: 0.6705\n",
      "Epoch 2365/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.5805 - acc: 0.7593 - val_loss: 1.0003 - val_acc: 0.6193\n",
      "Epoch 2366/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.5307 - acc: 0.8120 - val_loss: 0.7368 - val_acc: 0.7273\n",
      "Epoch 2367/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.5655 - acc: 0.7920 - val_loss: 1.0005 - val_acc: 0.5966\n",
      "Epoch 2368/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.6130 - acc: 0.7293 - val_loss: 0.9577 - val_acc: 0.6136\n",
      "Epoch 2369/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.5705 - acc: 0.7920 - val_loss: 0.7341 - val_acc: 0.7443\n",
      "Epoch 2370/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.4832 - acc: 0.8105 - val_loss: 0.9048 - val_acc: 0.6420\n",
      "Epoch 2371/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5048 - acc: 0.8020 - val_loss: 0.7902 - val_acc: 0.7216\n",
      "Epoch 2372/3000\n",
      "702/702 [==============================] - 0s 556us/sample - loss: 0.4752 - acc: 0.8120 - val_loss: 0.8238 - val_acc: 0.6989\n",
      "Epoch 2373/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.4863 - acc: 0.8219 - val_loss: 0.9126 - val_acc: 0.6477\n",
      "Epoch 2374/3000\n",
      "702/702 [==============================] - 0s 470us/sample - loss: 0.5276 - acc: 0.7991 - val_loss: 0.7394 - val_acc: 0.7670\n",
      "Epoch 2375/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.4819 - acc: 0.8248 - val_loss: 0.8681 - val_acc: 0.6705\n",
      "Epoch 2376/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.4576 - acc: 0.8319 - val_loss: 0.7657 - val_acc: 0.7557\n",
      "Epoch 2377/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.4741 - acc: 0.8376 - val_loss: 0.8915 - val_acc: 0.6818\n",
      "Epoch 2378/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.5229 - acc: 0.8177 - val_loss: 0.8386 - val_acc: 0.6989\n",
      "Epoch 2379/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.4724 - acc: 0.8063 - val_loss: 0.8076 - val_acc: 0.6989\n",
      "Epoch 2380/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.4931 - acc: 0.8219 - val_loss: 0.7905 - val_acc: 0.7216\n",
      "Epoch 2381/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5011 - acc: 0.8234 - val_loss: 0.9354 - val_acc: 0.6250\n",
      "Epoch 2382/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.5698 - acc: 0.7564 - val_loss: 0.8548 - val_acc: 0.6932\n",
      "Epoch 2383/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.5356 - acc: 0.8091 - val_loss: 0.8305 - val_acc: 0.7045\n",
      "Epoch 2384/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.6290 - acc: 0.7664 - val_loss: 1.0240 - val_acc: 0.5852\n",
      "Epoch 2385/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.5501 - acc: 0.7863 - val_loss: 0.7631 - val_acc: 0.7500\n",
      "Epoch 2386/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.5029 - acc: 0.8162 - val_loss: 1.0078 - val_acc: 0.5795\n",
      "Epoch 2387/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.6186 - acc: 0.7450 - val_loss: 0.7916 - val_acc: 0.7216\n",
      "Epoch 2388/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.5417 - acc: 0.8177 - val_loss: 0.8762 - val_acc: 0.6705\n",
      "Epoch 2389/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.5283 - acc: 0.7991 - val_loss: 0.9179 - val_acc: 0.6477\n",
      "Epoch 2390/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.5330 - acc: 0.7963 - val_loss: 0.7193 - val_acc: 0.7614\n",
      "Epoch 2391/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4938 - acc: 0.8262 - val_loss: 0.9034 - val_acc: 0.6648\n",
      "Epoch 2392/3000\n",
      "702/702 [==============================] - 0s 558us/sample - loss: 0.5006 - acc: 0.7949 - val_loss: 0.7989 - val_acc: 0.7102\n",
      "Epoch 2393/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 0.4771 - acc: 0.8333 - val_loss: 0.8141 - val_acc: 0.7273\n",
      "Epoch 2394/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.5011 - acc: 0.8105 - val_loss: 0.8285 - val_acc: 0.7045\n",
      "Epoch 2395/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.4964 - acc: 0.8134 - val_loss: 0.6794 - val_acc: 0.7670\n",
      "Epoch 2396/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.5098 - acc: 0.8148 - val_loss: 0.8743 - val_acc: 0.6761\n",
      "Epoch 2397/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.5289 - acc: 0.8120 - val_loss: 0.9275 - val_acc: 0.6534\n",
      "Epoch 2398/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.4685 - acc: 0.8376 - val_loss: 0.9063 - val_acc: 0.6875\n",
      "Epoch 2399/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 0.5036 - acc: 0.8276 - val_loss: 1.0109 - val_acc: 0.6648\n",
      "Epoch 2400/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.5137 - acc: 0.8120 - val_loss: 0.8281 - val_acc: 0.7330\n",
      "Epoch 2401/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5110 - acc: 0.8134 - val_loss: 0.8958 - val_acc: 0.6420\n",
      "Epoch 2402/3000\n",
      "702/702 [==============================] - 0s 581us/sample - loss: 0.5163 - acc: 0.8034 - val_loss: 0.6671 - val_acc: 0.7443\n",
      "Epoch 2403/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.5065 - acc: 0.8148 - val_loss: 0.7368 - val_acc: 0.7216\n",
      "Epoch 2404/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.4810 - acc: 0.8219 - val_loss: 0.7167 - val_acc: 0.7614\n",
      "Epoch 2405/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.5326 - acc: 0.8248 - val_loss: 0.8032 - val_acc: 0.7273\n",
      "Epoch 2406/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.4859 - acc: 0.8205 - val_loss: 0.8014 - val_acc: 0.7386\n",
      "Epoch 2407/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.4688 - acc: 0.8447 - val_loss: 0.7633 - val_acc: 0.7386\n",
      "Epoch 2408/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.4595 - acc: 0.8191 - val_loss: 0.8183 - val_acc: 0.6875\n",
      "Epoch 2409/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.4857 - acc: 0.7963 - val_loss: 0.7917 - val_acc: 0.7273\n",
      "Epoch 2410/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.4483 - acc: 0.8490 - val_loss: 0.8116 - val_acc: 0.6989\n",
      "Epoch 2411/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4978 - acc: 0.8219 - val_loss: 0.7866 - val_acc: 0.7330\n",
      "Epoch 2412/3000\n",
      "702/702 [==============================] - 0s 581us/sample - loss: 0.5231 - acc: 0.8191 - val_loss: 0.8023 - val_acc: 0.7330\n",
      "Epoch 2413/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.4624 - acc: 0.8276 - val_loss: 0.8049 - val_acc: 0.7386\n",
      "Epoch 2414/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.4390 - acc: 0.8405 - val_loss: 0.7971 - val_acc: 0.7557\n",
      "Epoch 2415/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.4469 - acc: 0.8447 - val_loss: 0.7612 - val_acc: 0.7670\n",
      "Epoch 2416/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.4750 - acc: 0.8205 - val_loss: 0.8095 - val_acc: 0.7045\n",
      "Epoch 2417/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.4924 - acc: 0.8433 - val_loss: 0.8785 - val_acc: 0.6250\n",
      "Epoch 2418/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.4733 - acc: 0.8333 - val_loss: 0.7254 - val_acc: 0.7443\n",
      "Epoch 2419/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.5397 - acc: 0.8148 - val_loss: 0.9396 - val_acc: 0.6136\n",
      "Epoch 2420/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.5106 - acc: 0.7949 - val_loss: 0.7720 - val_acc: 0.7727\n",
      "Epoch 2421/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 2.9569 - acc: 0.6026 - val_loss: 0.9377 - val_acc: 0.6193\n",
      "Epoch 2422/3000\n",
      "702/702 [==============================] - 0s 561us/sample - loss: 0.6564 - acc: 0.7479 - val_loss: 1.0032 - val_acc: 0.6193\n",
      "Epoch 2423/3000\n",
      "702/702 [==============================] - 0s 474us/sample - loss: 0.7072 - acc: 0.7493 - val_loss: 0.8716 - val_acc: 0.6591\n",
      "Epoch 2424/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.7559 - acc: 0.7108 - val_loss: 0.9645 - val_acc: 0.6080\n",
      "Epoch 2425/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.6830 - acc: 0.7293 - val_loss: 1.0795 - val_acc: 0.5455\n",
      "Epoch 2426/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.6888 - acc: 0.7265 - val_loss: 0.9302 - val_acc: 0.6591\n",
      "Epoch 2427/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.6479 - acc: 0.7764 - val_loss: 0.9623 - val_acc: 0.6307\n",
      "Epoch 2428/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.5796 - acc: 0.7778 - val_loss: 1.0647 - val_acc: 0.5625\n",
      "Epoch 2429/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.5972 - acc: 0.7934 - val_loss: 0.8441 - val_acc: 0.6818\n",
      "Epoch 2430/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.5666 - acc: 0.7963 - val_loss: 0.8868 - val_acc: 0.6648\n",
      "Epoch 2431/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5618 - acc: 0.7835 - val_loss: 1.2475 - val_acc: 0.4886\n",
      "Epoch 2432/3000\n",
      "702/702 [==============================] - 0s 556us/sample - loss: 0.6639 - acc: 0.7066 - val_loss: 1.0948 - val_acc: 0.5625\n",
      "Epoch 2433/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.6074 - acc: 0.7578 - val_loss: 0.9500 - val_acc: 0.7159\n",
      "Epoch 2434/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.5518 - acc: 0.8034 - val_loss: 0.9698 - val_acc: 0.6477\n",
      "Epoch 2435/3000\n",
      "702/702 [==============================] - 0s 473us/sample - loss: 0.5705 - acc: 0.7863 - val_loss: 0.8005 - val_acc: 0.7443\n",
      "Epoch 2436/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.4901 - acc: 0.8248 - val_loss: 0.7652 - val_acc: 0.7330\n",
      "Epoch 2437/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.5608 - acc: 0.8191 - val_loss: 0.7828 - val_acc: 0.7216\n",
      "Epoch 2438/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.4677 - acc: 0.8405 - val_loss: 0.8077 - val_acc: 0.6875\n",
      "Epoch 2439/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.5112 - acc: 0.8234 - val_loss: 0.8649 - val_acc: 0.6761\n",
      "Epoch 2440/3000\n",
      "702/702 [==============================] - 0s 474us/sample - loss: 0.5078 - acc: 0.8162 - val_loss: 0.8943 - val_acc: 0.7216\n",
      "Epoch 2441/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5330 - acc: 0.7977 - val_loss: 1.0573 - val_acc: 0.5966\n",
      "Epoch 2442/3000\n",
      "702/702 [==============================] - 0s 559us/sample - loss: 0.5665 - acc: 0.7778 - val_loss: 0.9942 - val_acc: 0.6250\n",
      "Epoch 2443/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.5018 - acc: 0.8234 - val_loss: 0.7906 - val_acc: 0.7159\n",
      "Epoch 2444/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.5162 - acc: 0.8048 - val_loss: 0.7832 - val_acc: 0.6534\n",
      "Epoch 2445/3000\n",
      "702/702 [==============================] - 0s 474us/sample - loss: 0.5605 - acc: 0.8006 - val_loss: 0.6832 - val_acc: 0.7727\n",
      "Epoch 2446/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.5008 - acc: 0.8177 - val_loss: 0.8596 - val_acc: 0.6420\n",
      "Epoch 2447/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.5226 - acc: 0.8234 - val_loss: 0.8327 - val_acc: 0.7159\n",
      "Epoch 2448/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.4925 - acc: 0.8205 - val_loss: 0.8300 - val_acc: 0.7102\n",
      "Epoch 2449/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.4867 - acc: 0.8248 - val_loss: 0.9055 - val_acc: 0.6705\n",
      "Epoch 2450/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.5098 - acc: 0.8205 - val_loss: 0.8736 - val_acc: 0.6705\n",
      "Epoch 2451/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4962 - acc: 0.8291 - val_loss: 0.8945 - val_acc: 0.6591\n",
      "Epoch 2452/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 0.4751 - acc: 0.8177 - val_loss: 0.8732 - val_acc: 0.6420\n",
      "Epoch 2453/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.4946 - acc: 0.8077 - val_loss: 0.7612 - val_acc: 0.7102\n",
      "Epoch 2454/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.4837 - acc: 0.8205 - val_loss: 0.7776 - val_acc: 0.6932\n",
      "Epoch 2455/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.4832 - acc: 0.8319 - val_loss: 0.7052 - val_acc: 0.7500\n",
      "Epoch 2456/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.5219 - acc: 0.8105 - val_loss: 0.9221 - val_acc: 0.5966\n",
      "Epoch 2457/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.5092 - acc: 0.8020 - val_loss: 0.7503 - val_acc: 0.7273\n",
      "Epoch 2458/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4808 - acc: 0.8234 - val_loss: 0.9053 - val_acc: 0.6420\n",
      "Epoch 2459/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.5015 - acc: 0.8091 - val_loss: 0.8554 - val_acc: 0.6705\n",
      "Epoch 2460/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.4652 - acc: 0.8248 - val_loss: 0.8456 - val_acc: 0.7102\n",
      "Epoch 2461/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4655 - acc: 0.8305 - val_loss: 0.7675 - val_acc: 0.7500\n",
      "Epoch 2462/3000\n",
      "702/702 [==============================] - 0s 550us/sample - loss: 0.5153 - acc: 0.8020 - val_loss: 0.8678 - val_acc: 0.6477\n",
      "Epoch 2463/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.4800 - acc: 0.8234 - val_loss: 0.7624 - val_acc: 0.7443\n",
      "Epoch 2464/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 0.5036 - acc: 0.8148 - val_loss: 0.8734 - val_acc: 0.6477\n",
      "Epoch 2465/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 0.4733 - acc: 0.8162 - val_loss: 0.7582 - val_acc: 0.7273\n",
      "Epoch 2466/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.5125 - acc: 0.8162 - val_loss: 0.9708 - val_acc: 0.5739\n",
      "Epoch 2467/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.6288 - acc: 0.7165 - val_loss: 0.9202 - val_acc: 0.6023\n",
      "Epoch 2468/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.5486 - acc: 0.7906 - val_loss: 0.6364 - val_acc: 0.8011\n",
      "Epoch 2469/3000\n",
      "702/702 [==============================] - 0s 470us/sample - loss: 0.5489 - acc: 0.8034 - val_loss: 0.9516 - val_acc: 0.6023\n",
      "Epoch 2470/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.5367 - acc: 0.7778 - val_loss: 0.8575 - val_acc: 0.6761\n",
      "Epoch 2471/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4943 - acc: 0.8048 - val_loss: 0.8285 - val_acc: 0.6932\n",
      "Epoch 2472/3000\n",
      "702/702 [==============================] - 0s 556us/sample - loss: 0.5032 - acc: 0.7906 - val_loss: 0.9038 - val_acc: 0.6023\n",
      "Epoch 2473/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.5298 - acc: 0.7892 - val_loss: 0.6835 - val_acc: 0.7784\n",
      "Epoch 2474/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.5134 - acc: 0.8077 - val_loss: 0.9496 - val_acc: 0.6080\n",
      "Epoch 2475/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.5437 - acc: 0.7792 - val_loss: 0.8332 - val_acc: 0.6875\n",
      "Epoch 2476/3000\n",
      "702/702 [==============================] - 0s 471us/sample - loss: 0.5159 - acc: 0.8205 - val_loss: 0.7430 - val_acc: 0.7614\n",
      "Epoch 2477/3000\n",
      "702/702 [==============================] - 0s 472us/sample - loss: 0.4796 - acc: 0.8177 - val_loss: 0.8542 - val_acc: 0.6477\n",
      "Epoch 2478/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.4657 - acc: 0.8191 - val_loss: 0.7134 - val_acc: 0.7614\n",
      "Epoch 2479/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.5205 - acc: 0.8034 - val_loss: 0.7883 - val_acc: 0.6932\n",
      "Epoch 2480/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.4426 - acc: 0.8362 - val_loss: 0.7332 - val_acc: 0.7330\n",
      "Epoch 2481/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4582 - acc: 0.8433 - val_loss: 0.7847 - val_acc: 0.6761\n",
      "Epoch 2482/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 0.4285 - acc: 0.8462 - val_loss: 0.7205 - val_acc: 0.7216\n",
      "Epoch 2483/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.4773 - acc: 0.8291 - val_loss: 0.7454 - val_acc: 0.7102\n",
      "Epoch 2484/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.4767 - acc: 0.8348 - val_loss: 0.6596 - val_acc: 0.7670\n",
      "Epoch 2485/3000\n",
      "702/702 [==============================] - 0s 474us/sample - loss: 0.4519 - acc: 0.8390 - val_loss: 0.7256 - val_acc: 0.7273\n",
      "Epoch 2486/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.4559 - acc: 0.8276 - val_loss: 0.7508 - val_acc: 0.7045\n",
      "Epoch 2487/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.4809 - acc: 0.8105 - val_loss: 0.7582 - val_acc: 0.6932\n",
      "Epoch 2488/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.4523 - acc: 0.8234 - val_loss: 0.7288 - val_acc: 0.7216\n",
      "Epoch 2489/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4403 - acc: 0.8504 - val_loss: 0.7642 - val_acc: 0.6705\n",
      "Epoch 2490/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 0.4436 - acc: 0.8348 - val_loss: 0.6375 - val_acc: 0.7727\n",
      "Epoch 2491/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4391 - acc: 0.8390 - val_loss: 0.7784 - val_acc: 0.6705\n",
      "Epoch 2492/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 0.4443 - acc: 0.8305 - val_loss: 0.6497 - val_acc: 0.7784\n",
      "Epoch 2493/3000\n",
      "702/702 [==============================] - 0s 473us/sample - loss: 0.4456 - acc: 0.8319 - val_loss: 0.7993 - val_acc: 0.6420\n",
      "Epoch 2494/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.4780 - acc: 0.8034 - val_loss: 0.6230 - val_acc: 0.7955\n",
      "Epoch 2495/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 0.4382 - acc: 0.8333 - val_loss: 0.7748 - val_acc: 0.6534\n",
      "Epoch 2496/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.4947 - acc: 0.7977 - val_loss: 0.6212 - val_acc: 0.7898\n",
      "Epoch 2497/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4573 - acc: 0.8291 - val_loss: 0.7002 - val_acc: 0.7443\n",
      "Epoch 2498/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4730 - acc: 0.8262 - val_loss: 0.6385 - val_acc: 0.7898\n",
      "Epoch 2499/3000\n",
      "702/702 [==============================] - 0s 470us/sample - loss: 0.5039 - acc: 0.8348 - val_loss: 0.7645 - val_acc: 0.7045\n",
      "Epoch 2500/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 0.4411 - acc: 0.8291 - val_loss: 0.6688 - val_acc: 0.7670\n",
      "Epoch 2501/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4319 - acc: 0.8490 - val_loss: 0.7510 - val_acc: 0.6989\n",
      "Epoch 2502/3000\n",
      "702/702 [==============================] - 0s 555us/sample - loss: 0.4191 - acc: 0.8362 - val_loss: 0.6969 - val_acc: 0.7443\n",
      "Epoch 2503/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.4721 - acc: 0.8305 - val_loss: 0.8285 - val_acc: 0.6420\n",
      "Epoch 2504/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.4395 - acc: 0.8291 - val_loss: 0.7038 - val_acc: 0.7386\n",
      "Epoch 2505/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.4286 - acc: 0.8519 - val_loss: 0.7031 - val_acc: 0.7330\n",
      "Epoch 2506/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.4714 - acc: 0.8405 - val_loss: 0.7526 - val_acc: 0.7159\n",
      "Epoch 2507/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.5117 - acc: 0.8120 - val_loss: 0.6369 - val_acc: 0.7898\n",
      "Epoch 2508/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4386 - acc: 0.8305 - val_loss: 0.7669 - val_acc: 0.6705\n",
      "Epoch 2509/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.4659 - acc: 0.8134 - val_loss: 0.5910 - val_acc: 0.7841\n",
      "Epoch 2510/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.5183 - acc: 0.8006 - val_loss: 0.7876 - val_acc: 0.6705\n",
      "Epoch 2511/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4718 - acc: 0.8077 - val_loss: 0.8227 - val_acc: 0.6989\n",
      "Epoch 2512/3000\n",
      "702/702 [==============================] - 0s 566us/sample - loss: 0.4599 - acc: 0.8248 - val_loss: 0.8376 - val_acc: 0.6989\n",
      "Epoch 2513/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.4518 - acc: 0.8291 - val_loss: 0.8143 - val_acc: 0.6989\n",
      "Epoch 2514/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4574 - acc: 0.8362 - val_loss: 0.7393 - val_acc: 0.7330\n",
      "Epoch 2515/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.4427 - acc: 0.8362 - val_loss: 0.8145 - val_acc: 0.6477\n",
      "Epoch 2516/3000\n",
      "702/702 [==============================] - 0s 472us/sample - loss: 0.4320 - acc: 0.8405 - val_loss: 0.8121 - val_acc: 0.6420\n",
      "Epoch 2517/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.4812 - acc: 0.8205 - val_loss: 0.6779 - val_acc: 0.7443\n",
      "Epoch 2518/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.4470 - acc: 0.8376 - val_loss: 0.7663 - val_acc: 0.6818\n",
      "Epoch 2519/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.4454 - acc: 0.8319 - val_loss: 0.7136 - val_acc: 0.7159\n",
      "Epoch 2520/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.4576 - acc: 0.8348 - val_loss: 0.7131 - val_acc: 0.7216\n",
      "Epoch 2521/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5155 - acc: 0.8191 - val_loss: 0.7644 - val_acc: 0.6818\n",
      "Epoch 2522/3000\n",
      "702/702 [==============================] - 0s 561us/sample - loss: 0.4480 - acc: 0.8405 - val_loss: 0.6455 - val_acc: 0.7614\n",
      "Epoch 2523/3000\n",
      "702/702 [==============================] - 0s 474us/sample - loss: 0.4699 - acc: 0.8191 - val_loss: 0.8974 - val_acc: 0.6250\n",
      "Epoch 2524/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.4608 - acc: 0.8177 - val_loss: 0.6443 - val_acc: 0.7670\n",
      "Epoch 2525/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.5558 - acc: 0.8020 - val_loss: 0.8740 - val_acc: 0.6193\n",
      "Epoch 2526/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.4835 - acc: 0.8077 - val_loss: 0.6504 - val_acc: 0.7955\n",
      "Epoch 2527/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.4959 - acc: 0.8191 - val_loss: 0.8125 - val_acc: 0.6534\n",
      "Epoch 2528/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.5142 - acc: 0.7806 - val_loss: 0.7895 - val_acc: 0.6705\n",
      "Epoch 2529/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.4640 - acc: 0.8319 - val_loss: 0.6163 - val_acc: 0.7898\n",
      "Epoch 2530/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.4776 - acc: 0.8234 - val_loss: 0.8803 - val_acc: 0.6250\n",
      "Epoch 2531/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4727 - acc: 0.8248 - val_loss: 0.6672 - val_acc: 0.7784\n",
      "Epoch 2532/3000\n",
      "702/702 [==============================] - 0s 560us/sample - loss: 0.5084 - acc: 0.8077 - val_loss: 0.8212 - val_acc: 0.6705\n",
      "Epoch 2533/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.4988 - acc: 0.8077 - val_loss: 0.7176 - val_acc: 0.7386\n",
      "Epoch 2534/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.4987 - acc: 0.8234 - val_loss: 0.8843 - val_acc: 0.6420\n",
      "Epoch 2535/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 0.5289 - acc: 0.7749 - val_loss: 0.7957 - val_acc: 0.6818\n",
      "Epoch 2536/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.5572 - acc: 0.8091 - val_loss: 0.6723 - val_acc: 0.7670\n",
      "Epoch 2537/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.4767 - acc: 0.8162 - val_loss: 0.8166 - val_acc: 0.6534\n",
      "Epoch 2538/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.4644 - acc: 0.8134 - val_loss: 0.6777 - val_acc: 0.7670\n",
      "Epoch 2539/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.4346 - acc: 0.8376 - val_loss: 0.7747 - val_acc: 0.7045\n",
      "Epoch 2540/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.4201 - acc: 0.8647 - val_loss: 0.6267 - val_acc: 0.8011\n",
      "Epoch 2541/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5034 - acc: 0.8333 - val_loss: 0.8284 - val_acc: 0.6420\n",
      "Epoch 2542/3000\n",
      "702/702 [==============================] - 0s 564us/sample - loss: 0.4714 - acc: 0.8077 - val_loss: 0.6355 - val_acc: 0.7841\n",
      "Epoch 2543/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.4715 - acc: 0.8063 - val_loss: 0.7811 - val_acc: 0.6705\n",
      "Epoch 2544/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.4577 - acc: 0.8219 - val_loss: 0.6318 - val_acc: 0.7841\n",
      "Epoch 2545/3000\n",
      "702/702 [==============================] - 0s 470us/sample - loss: 0.5020 - acc: 0.8348 - val_loss: 0.6865 - val_acc: 0.7443\n",
      "Epoch 2546/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.4262 - acc: 0.8447 - val_loss: 0.6913 - val_acc: 0.7727\n",
      "Epoch 2547/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4334 - acc: 0.8333 - val_loss: 0.8766 - val_acc: 0.6875\n",
      "Epoch 2548/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.4456 - acc: 0.8248 - val_loss: 0.8942 - val_acc: 0.6932\n",
      "Epoch 2549/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4452 - acc: 0.8333 - val_loss: 0.8322 - val_acc: 0.6932\n",
      "Epoch 2550/3000\n",
      "702/702 [==============================] - 0s 470us/sample - loss: 0.4686 - acc: 0.8333 - val_loss: 0.7100 - val_acc: 0.7500\n",
      "Epoch 2551/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4399 - acc: 0.8462 - val_loss: 0.7657 - val_acc: 0.6989\n",
      "Epoch 2552/3000\n",
      "702/702 [==============================] - 0s 557us/sample - loss: 0.4184 - acc: 0.8604 - val_loss: 0.7402 - val_acc: 0.7102\n",
      "Epoch 2553/3000\n",
      "702/702 [==============================] - 0s 472us/sample - loss: 0.4177 - acc: 0.8504 - val_loss: 0.6962 - val_acc: 0.7784\n",
      "Epoch 2554/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.3997 - acc: 0.8618 - val_loss: 0.7871 - val_acc: 0.7216\n",
      "Epoch 2555/3000\n",
      "702/702 [==============================] - 0s 469us/sample - loss: 0.4275 - acc: 0.8319 - val_loss: 0.6796 - val_acc: 0.7955\n",
      "Epoch 2556/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.4756 - acc: 0.8248 - val_loss: 0.8720 - val_acc: 0.6080\n",
      "Epoch 2557/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.4782 - acc: 0.8105 - val_loss: 0.6389 - val_acc: 0.8011\n",
      "Epoch 2558/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.4404 - acc: 0.8405 - val_loss: 0.6654 - val_acc: 0.7898\n",
      "Epoch 2559/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.4581 - acc: 0.8348 - val_loss: 0.7357 - val_acc: 0.7216\n",
      "Epoch 2560/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.4504 - acc: 0.8433 - val_loss: 0.7016 - val_acc: 0.7159\n",
      "Epoch 2561/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4206 - acc: 0.8433 - val_loss: 0.5975 - val_acc: 0.7955\n",
      "Epoch 2562/3000\n",
      "702/702 [==============================] - 0s 553us/sample - loss: 0.4100 - acc: 0.8590 - val_loss: 0.6737 - val_acc: 0.7273\n",
      "Epoch 2563/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.4364 - acc: 0.8504 - val_loss: 0.6671 - val_acc: 0.7727\n",
      "Epoch 2564/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.4297 - acc: 0.8333 - val_loss: 0.8032 - val_acc: 0.6932\n",
      "Epoch 2565/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.4104 - acc: 0.8547 - val_loss: 0.7353 - val_acc: 0.7500\n",
      "Epoch 2566/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.4562 - acc: 0.8205 - val_loss: 0.7826 - val_acc: 0.6818\n",
      "Epoch 2567/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.4756 - acc: 0.8248 - val_loss: 0.7640 - val_acc: 0.6875\n",
      "Epoch 2568/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4812 - acc: 0.8177 - val_loss: 0.7431 - val_acc: 0.7159\n",
      "Epoch 2569/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.4501 - acc: 0.8462 - val_loss: 0.7656 - val_acc: 0.6989\n",
      "Epoch 2570/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.4369 - acc: 0.8405 - val_loss: 0.7165 - val_acc: 0.7670\n",
      "Epoch 2571/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4173 - acc: 0.8433 - val_loss: 0.7917 - val_acc: 0.7102\n",
      "Epoch 2572/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 0.4675 - acc: 0.8291 - val_loss: 0.6682 - val_acc: 0.7841\n",
      "Epoch 2573/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.4736 - acc: 0.8319 - val_loss: 0.8435 - val_acc: 0.6364\n",
      "Epoch 2574/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4252 - acc: 0.8319 - val_loss: 0.6358 - val_acc: 0.8068\n",
      "Epoch 2575/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.4589 - acc: 0.8305 - val_loss: 0.7555 - val_acc: 0.7102\n",
      "Epoch 2576/3000\n",
      "702/702 [==============================] - 0s 473us/sample - loss: 0.4713 - acc: 0.8276 - val_loss: 0.7666 - val_acc: 0.7102\n",
      "Epoch 2577/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.4938 - acc: 0.7991 - val_loss: 0.8149 - val_acc: 0.6761\n",
      "Epoch 2578/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.5008 - acc: 0.8120 - val_loss: 0.7830 - val_acc: 0.7102\n",
      "Epoch 2579/3000\n",
      "702/702 [==============================] - 0s 473us/sample - loss: 0.5576 - acc: 0.7621 - val_loss: 1.1060 - val_acc: 0.5682\n",
      "Epoch 2580/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.6014 - acc: 0.7322 - val_loss: 0.7131 - val_acc: 0.7727\n",
      "Epoch 2581/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5100 - acc: 0.8120 - val_loss: 0.6578 - val_acc: 0.8068\n",
      "Epoch 2582/3000\n",
      "702/702 [==============================] - 0s 566us/sample - loss: 0.4635 - acc: 0.8490 - val_loss: 0.7805 - val_acc: 0.6705\n",
      "Epoch 2583/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.5019 - acc: 0.8191 - val_loss: 0.6497 - val_acc: 0.7784\n",
      "Epoch 2584/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.4140 - acc: 0.8390 - val_loss: 0.6531 - val_acc: 0.7898\n",
      "Epoch 2585/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.4363 - acc: 0.8419 - val_loss: 0.7874 - val_acc: 0.6761\n",
      "Epoch 2586/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.4527 - acc: 0.8248 - val_loss: 0.6594 - val_acc: 0.7841\n",
      "Epoch 2587/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.4609 - acc: 0.8333 - val_loss: 0.6433 - val_acc: 0.7443\n",
      "Epoch 2588/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.4363 - acc: 0.8362 - val_loss: 0.6936 - val_acc: 0.7330\n",
      "Epoch 2589/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.4130 - acc: 0.8533 - val_loss: 0.6403 - val_acc: 0.7500\n",
      "Epoch 2590/3000\n",
      "702/702 [==============================] - 0s 472us/sample - loss: 0.3929 - acc: 0.8604 - val_loss: 0.7335 - val_acc: 0.7159\n",
      "Epoch 2591/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4227 - acc: 0.8376 - val_loss: 0.6722 - val_acc: 0.7500\n",
      "Epoch 2592/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.4134 - acc: 0.8447 - val_loss: 0.7041 - val_acc: 0.7216\n",
      "Epoch 2593/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.4588 - acc: 0.8305 - val_loss: 0.6734 - val_acc: 0.7500\n",
      "Epoch 2594/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.4188 - acc: 0.8462 - val_loss: 0.7051 - val_acc: 0.7273\n",
      "Epoch 2595/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.4817 - acc: 0.7977 - val_loss: 0.7625 - val_acc: 0.6477\n",
      "Epoch 2596/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.4408 - acc: 0.8362 - val_loss: 0.5433 - val_acc: 0.8239\n",
      "Epoch 2597/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.4984 - acc: 0.8148 - val_loss: 0.8063 - val_acc: 0.6477\n",
      "Epoch 2598/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.4698 - acc: 0.8048 - val_loss: 0.5763 - val_acc: 0.8011\n",
      "Epoch 2599/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.5123 - acc: 0.8191 - val_loss: 0.8083 - val_acc: 0.6818\n",
      "Epoch 2600/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.5271 - acc: 0.7892 - val_loss: 0.8021 - val_acc: 0.6989\n",
      "Epoch 2601/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4601 - acc: 0.8262 - val_loss: 0.6146 - val_acc: 0.7898\n",
      "Epoch 2602/3000\n",
      "702/702 [==============================] - 0s 555us/sample - loss: 0.5589 - acc: 0.7721 - val_loss: 1.0202 - val_acc: 0.6080\n",
      "Epoch 2603/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.6152 - acc: 0.7094 - val_loss: 0.7269 - val_acc: 0.6932\n",
      "Epoch 2604/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.5582 - acc: 0.8148 - val_loss: 0.7625 - val_acc: 0.6648\n",
      "Epoch 2605/3000\n",
      "702/702 [==============================] - 0s 468us/sample - loss: 0.5418 - acc: 0.7507 - val_loss: 1.0215 - val_acc: 0.5966\n",
      "Epoch 2606/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.7140 - acc: 0.6524 - val_loss: 0.9292 - val_acc: 0.6136\n",
      "Epoch 2607/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.5512 - acc: 0.7849 - val_loss: 0.5650 - val_acc: 0.8352\n",
      "Epoch 2608/3000\n",
      "702/702 [==============================] - 0s 468us/sample - loss: 0.5365 - acc: 0.7877 - val_loss: 0.9433 - val_acc: 0.6080\n",
      "Epoch 2609/3000\n",
      "702/702 [==============================] - 0s 472us/sample - loss: 0.5402 - acc: 0.7692 - val_loss: 0.9994 - val_acc: 0.6080\n",
      "Epoch 2610/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.4860 - acc: 0.8162 - val_loss: 0.6903 - val_acc: 0.7670\n",
      "Epoch 2611/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5112 - acc: 0.8105 - val_loss: 0.9061 - val_acc: 0.6250\n",
      "Epoch 2612/3000\n",
      "702/702 [==============================] - 0s 550us/sample - loss: 0.5405 - acc: 0.7764 - val_loss: 0.9078 - val_acc: 0.6307\n",
      "Epoch 2613/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.4657 - acc: 0.8177 - val_loss: 0.6563 - val_acc: 0.7727\n",
      "Epoch 2614/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4690 - acc: 0.8148 - val_loss: 0.6578 - val_acc: 0.7784\n",
      "Epoch 2615/3000\n",
      "702/702 [==============================] - 0s 468us/sample - loss: 0.4608 - acc: 0.8348 - val_loss: 0.8230 - val_acc: 0.6534\n",
      "Epoch 2616/3000\n",
      "702/702 [==============================] - 0s 468us/sample - loss: 0.4620 - acc: 0.8319 - val_loss: 0.7012 - val_acc: 0.7557\n",
      "Epoch 2617/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.5353 - acc: 0.8162 - val_loss: 0.7279 - val_acc: 0.7500\n",
      "Epoch 2618/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4199 - acc: 0.8504 - val_loss: 0.7996 - val_acc: 0.7045\n",
      "Epoch 2619/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 0.4402 - acc: 0.8419 - val_loss: 0.7040 - val_acc: 0.7557\n",
      "Epoch 2620/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.4532 - acc: 0.8262 - val_loss: 0.7537 - val_acc: 0.6932\n",
      "Epoch 2621/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4904 - acc: 0.7977 - val_loss: 0.7613 - val_acc: 0.6591\n",
      "Epoch 2622/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.4307 - acc: 0.8376 - val_loss: 0.5782 - val_acc: 0.8182\n",
      "Epoch 2623/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.4455 - acc: 0.8276 - val_loss: 0.8264 - val_acc: 0.6534\n",
      "Epoch 2624/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.4948 - acc: 0.8134 - val_loss: 0.6992 - val_acc: 0.7784\n",
      "Epoch 2625/3000\n",
      "702/702 [==============================] - 0s 474us/sample - loss: 0.4915 - acc: 0.8148 - val_loss: 0.8039 - val_acc: 0.6705\n",
      "Epoch 2626/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.5072 - acc: 0.7949 - val_loss: 0.8223 - val_acc: 0.6477\n",
      "Epoch 2627/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.4462 - acc: 0.8462 - val_loss: 0.6009 - val_acc: 0.7841\n",
      "Epoch 2628/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 0.5115 - acc: 0.8120 - val_loss: 0.8250 - val_acc: 0.6534\n",
      "Epoch 2629/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.4605 - acc: 0.8205 - val_loss: 0.6567 - val_acc: 0.7614\n",
      "Epoch 2630/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4492 - acc: 0.8348 - val_loss: 0.6626 - val_acc: 0.7727\n",
      "Epoch 2631/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4110 - acc: 0.8504 - val_loss: 0.6626 - val_acc: 0.7500\n",
      "Epoch 2632/3000\n",
      "702/702 [==============================] - 0s 557us/sample - loss: 0.4159 - acc: 0.8575 - val_loss: 0.7520 - val_acc: 0.6875\n",
      "Epoch 2633/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.4560 - acc: 0.8362 - val_loss: 0.7747 - val_acc: 0.7386\n",
      "Epoch 2634/3000\n",
      "702/702 [==============================] - 0s 470us/sample - loss: 0.4109 - acc: 0.8561 - val_loss: 0.6844 - val_acc: 0.7727\n",
      "Epoch 2635/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4094 - acc: 0.8390 - val_loss: 0.7619 - val_acc: 0.6818\n",
      "Epoch 2636/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.4428 - acc: 0.8362 - val_loss: 0.6421 - val_acc: 0.7614\n",
      "Epoch 2637/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.4074 - acc: 0.8490 - val_loss: 0.6514 - val_acc: 0.7614\n",
      "Epoch 2638/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.4192 - acc: 0.8632 - val_loss: 0.6711 - val_acc: 0.7557\n",
      "Epoch 2639/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.3953 - acc: 0.8647 - val_loss: 0.6257 - val_acc: 0.8011\n",
      "Epoch 2640/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.4395 - acc: 0.8533 - val_loss: 0.7280 - val_acc: 0.7102\n",
      "Epoch 2641/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4122 - acc: 0.8447 - val_loss: 0.6262 - val_acc: 0.8068\n",
      "Epoch 2642/3000\n",
      "702/702 [==============================] - 0s 552us/sample - loss: 0.4072 - acc: 0.8561 - val_loss: 0.7257 - val_acc: 0.7159\n",
      "Epoch 2643/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.4188 - acc: 0.8405 - val_loss: 0.6262 - val_acc: 0.8011\n",
      "Epoch 2644/3000\n",
      "702/702 [==============================] - 0s 472us/sample - loss: 0.3673 - acc: 0.8704 - val_loss: 0.7140 - val_acc: 0.7273\n",
      "Epoch 2645/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 0.4028 - acc: 0.8504 - val_loss: 0.5893 - val_acc: 0.8011\n",
      "Epoch 2646/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.4145 - acc: 0.8647 - val_loss: 0.6726 - val_acc: 0.7500\n",
      "Epoch 2647/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4120 - acc: 0.8504 - val_loss: 0.6174 - val_acc: 0.7841\n",
      "Epoch 2648/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.4585 - acc: 0.8120 - val_loss: 0.8553 - val_acc: 0.6420\n",
      "Epoch 2649/3000\n",
      "702/702 [==============================] - 0s 473us/sample - loss: 0.4528 - acc: 0.8120 - val_loss: 0.5645 - val_acc: 0.7614\n",
      "Epoch 2650/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.5785 - acc: 0.8063 - val_loss: 0.9823 - val_acc: 0.6136\n",
      "Epoch 2651/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.6820 - acc: 0.6909 - val_loss: 0.9092 - val_acc: 0.6080\n",
      "Epoch 2652/3000\n",
      "702/702 [==============================] - 0s 547us/sample - loss: 0.4896 - acc: 0.7934 - val_loss: 0.5700 - val_acc: 0.8125\n",
      "Epoch 2653/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 0.5190 - acc: 0.8091 - val_loss: 0.7975 - val_acc: 0.6420\n",
      "Epoch 2654/3000\n",
      "702/702 [==============================] - 0s 474us/sample - loss: 0.4653 - acc: 0.8105 - val_loss: 0.6996 - val_acc: 0.7273\n",
      "Epoch 2655/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4557 - acc: 0.8262 - val_loss: 0.6772 - val_acc: 0.7500\n",
      "Epoch 2656/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.4187 - acc: 0.8362 - val_loss: 0.7879 - val_acc: 0.7102\n",
      "Epoch 2657/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.4348 - acc: 0.8390 - val_loss: 0.6640 - val_acc: 0.7273\n",
      "Epoch 2658/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.4002 - acc: 0.8575 - val_loss: 0.6315 - val_acc: 0.7614\n",
      "Epoch 2659/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.3903 - acc: 0.8547 - val_loss: 0.6994 - val_acc: 0.7159\n",
      "Epoch 2660/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.4285 - acc: 0.8504 - val_loss: 0.6874 - val_acc: 0.7386\n",
      "Epoch 2661/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4673 - acc: 0.8390 - val_loss: 0.6660 - val_acc: 0.7330\n",
      "Epoch 2662/3000\n",
      "702/702 [==============================] - 0s 542us/sample - loss: 0.4443 - acc: 0.8419 - val_loss: 0.7254 - val_acc: 0.6818\n",
      "Epoch 2663/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.4424 - acc: 0.8333 - val_loss: 0.5974 - val_acc: 0.7727\n",
      "Epoch 2664/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.4187 - acc: 0.8348 - val_loss: 0.6597 - val_acc: 0.7443\n",
      "Epoch 2665/3000\n",
      "702/702 [==============================] - 0s 474us/sample - loss: 0.4116 - acc: 0.8547 - val_loss: 0.7052 - val_acc: 0.7045\n",
      "Epoch 2666/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.4023 - acc: 0.8547 - val_loss: 0.6194 - val_acc: 0.7955\n",
      "Epoch 2667/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.4005 - acc: 0.8533 - val_loss: 0.7360 - val_acc: 0.6818\n",
      "Epoch 2668/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.4191 - acc: 0.8462 - val_loss: 0.6533 - val_acc: 0.7386\n",
      "Epoch 2669/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.4327 - acc: 0.8405 - val_loss: 0.6816 - val_acc: 0.7330\n",
      "Epoch 2670/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.4159 - acc: 0.8433 - val_loss: 0.8155 - val_acc: 0.6534\n",
      "Epoch 2671/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4560 - acc: 0.8248 - val_loss: 0.5556 - val_acc: 0.8239\n",
      "Epoch 2672/3000\n",
      "702/702 [==============================] - 0s 551us/sample - loss: 0.4205 - acc: 0.8348 - val_loss: 0.7262 - val_acc: 0.7102\n",
      "Epoch 2673/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.4550 - acc: 0.8120 - val_loss: 0.6142 - val_acc: 0.7727\n",
      "Epoch 2674/3000\n",
      "702/702 [==============================] - 0s 472us/sample - loss: 0.4262 - acc: 0.8519 - val_loss: 0.6711 - val_acc: 0.7330\n",
      "Epoch 2675/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 0.4206 - acc: 0.8447 - val_loss: 0.7721 - val_acc: 0.6705\n",
      "Epoch 2676/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.3771 - acc: 0.8604 - val_loss: 0.6357 - val_acc: 0.7841\n",
      "Epoch 2677/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.4654 - acc: 0.8490 - val_loss: 0.7571 - val_acc: 0.6705\n",
      "Epoch 2678/3000\n",
      "702/702 [==============================] - 0s 472us/sample - loss: 0.3872 - acc: 0.8618 - val_loss: 0.5946 - val_acc: 0.7841\n",
      "Epoch 2679/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.4174 - acc: 0.8604 - val_loss: 0.7563 - val_acc: 0.6932\n",
      "Epoch 2680/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.4053 - acc: 0.8476 - val_loss: 0.7890 - val_acc: 0.6989\n",
      "Epoch 2681/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4139 - acc: 0.8447 - val_loss: 0.7365 - val_acc: 0.7557\n",
      "Epoch 2682/3000\n",
      "702/702 [==============================] - 0s 561us/sample - loss: 0.4112 - acc: 0.8490 - val_loss: 0.7045 - val_acc: 0.7557\n",
      "Epoch 2683/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.4309 - acc: 0.8291 - val_loss: 0.7355 - val_acc: 0.7216\n",
      "Epoch 2684/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.4003 - acc: 0.8618 - val_loss: 0.7307 - val_acc: 0.7102\n",
      "Epoch 2685/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.3995 - acc: 0.8604 - val_loss: 0.7737 - val_acc: 0.7386\n",
      "Epoch 2686/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.3805 - acc: 0.8661 - val_loss: 0.8800 - val_acc: 0.6591\n",
      "Epoch 2687/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.4850 - acc: 0.8177 - val_loss: 0.6205 - val_acc: 0.7898\n",
      "Epoch 2688/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.4438 - acc: 0.8348 - val_loss: 0.6512 - val_acc: 0.6989\n",
      "Epoch 2689/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.4280 - acc: 0.8362 - val_loss: 0.6631 - val_acc: 0.7159\n",
      "Epoch 2690/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.4573 - acc: 0.8333 - val_loss: 0.7519 - val_acc: 0.6818\n",
      "Epoch 2691/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5236 - acc: 0.7692 - val_loss: 0.9542 - val_acc: 0.5909\n",
      "Epoch 2692/3000\n",
      "702/702 [==============================] - 0s 540us/sample - loss: 0.4954 - acc: 0.7806 - val_loss: 0.5672 - val_acc: 0.8125\n",
      "Epoch 2693/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.4552 - acc: 0.8376 - val_loss: 0.7509 - val_acc: 0.6705\n",
      "Epoch 2694/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.4571 - acc: 0.8291 - val_loss: 0.6298 - val_acc: 0.7784\n",
      "Epoch 2695/3000\n",
      "702/702 [==============================] - 0s 474us/sample - loss: 0.3948 - acc: 0.8561 - val_loss: 0.9161 - val_acc: 0.6307\n",
      "Epoch 2696/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.5190 - acc: 0.8134 - val_loss: 0.7846 - val_acc: 0.7159\n",
      "Epoch 2697/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.5025 - acc: 0.8048 - val_loss: 0.8333 - val_acc: 0.6193\n",
      "Epoch 2698/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.5837 - acc: 0.7792 - val_loss: 0.7940 - val_acc: 0.6591\n",
      "Epoch 2699/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.4808 - acc: 0.8319 - val_loss: 0.5441 - val_acc: 0.8295\n",
      "Epoch 2700/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4990 - acc: 0.8319 - val_loss: 0.8813 - val_acc: 0.6364\n",
      "Epoch 2701/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4401 - acc: 0.8120 - val_loss: 0.6548 - val_acc: 0.7557\n",
      "Epoch 2702/3000\n",
      "702/702 [==============================] - 0s 566us/sample - loss: 0.5090 - acc: 0.8234 - val_loss: 0.8516 - val_acc: 0.6364\n",
      "Epoch 2703/3000\n",
      "702/702 [==============================] - 0s 470us/sample - loss: 0.6009 - acc: 0.7422 - val_loss: 1.1072 - val_acc: 0.5739\n",
      "Epoch 2704/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.5988 - acc: 0.7308 - val_loss: 0.7673 - val_acc: 0.7102\n",
      "Epoch 2705/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.5554 - acc: 0.8205 - val_loss: 0.7655 - val_acc: 0.7045\n",
      "Epoch 2706/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.5308 - acc: 0.7735 - val_loss: 1.0049 - val_acc: 0.5852\n",
      "Epoch 2707/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.6380 - acc: 0.7265 - val_loss: 0.8220 - val_acc: 0.6648\n",
      "Epoch 2708/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.4772 - acc: 0.8262 - val_loss: 0.6033 - val_acc: 0.8125\n",
      "Epoch 2709/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.4296 - acc: 0.8348 - val_loss: 0.7032 - val_acc: 0.7500\n",
      "Epoch 2710/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.4209 - acc: 0.8390 - val_loss: 0.7518 - val_acc: 0.7386\n",
      "Epoch 2711/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4644 - acc: 0.8390 - val_loss: 0.6572 - val_acc: 0.7670\n",
      "Epoch 2712/3000\n",
      "702/702 [==============================] - 0s 553us/sample - loss: 0.4316 - acc: 0.8490 - val_loss: 0.6911 - val_acc: 0.7159\n",
      "Epoch 2713/3000\n",
      "702/702 [==============================] - 0s 474us/sample - loss: 0.3955 - acc: 0.8504 - val_loss: 0.6277 - val_acc: 0.7784\n",
      "Epoch 2714/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.4093 - acc: 0.8447 - val_loss: 0.6316 - val_acc: 0.7784\n",
      "Epoch 2715/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.4525 - acc: 0.8419 - val_loss: 0.7259 - val_acc: 0.7159\n",
      "Epoch 2716/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.4198 - acc: 0.8604 - val_loss: 0.6447 - val_acc: 0.8011\n",
      "Epoch 2717/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.3895 - acc: 0.8561 - val_loss: 0.7315 - val_acc: 0.7330\n",
      "Epoch 2718/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.4251 - acc: 0.8376 - val_loss: 0.6991 - val_acc: 0.7273\n",
      "Epoch 2719/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 0.3971 - acc: 0.8590 - val_loss: 0.6314 - val_acc: 0.8068\n",
      "Epoch 2720/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4474 - acc: 0.8305 - val_loss: 0.8050 - val_acc: 0.6705\n",
      "Epoch 2721/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4389 - acc: 0.8319 - val_loss: 0.6074 - val_acc: 0.8239\n",
      "Epoch 2722/3000\n",
      "702/702 [==============================] - 0s 558us/sample - loss: 0.4582 - acc: 0.8419 - val_loss: 0.7611 - val_acc: 0.6761\n",
      "Epoch 2723/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.4503 - acc: 0.8234 - val_loss: 0.6747 - val_acc: 0.7500\n",
      "Epoch 2724/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.4144 - acc: 0.8575 - val_loss: 0.6555 - val_acc: 0.7727\n",
      "Epoch 2725/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4198 - acc: 0.8447 - val_loss: 0.6563 - val_acc: 0.7557\n",
      "Epoch 2726/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.4182 - acc: 0.8405 - val_loss: 0.6639 - val_acc: 0.7670\n",
      "Epoch 2727/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.3876 - acc: 0.8561 - val_loss: 0.7468 - val_acc: 0.6932\n",
      "Epoch 2728/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4116 - acc: 0.8533 - val_loss: 0.5642 - val_acc: 0.8580\n",
      "Epoch 2729/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.4424 - acc: 0.8447 - val_loss: 0.6385 - val_acc: 0.7614\n",
      "Epoch 2730/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.4105 - acc: 0.8590 - val_loss: 0.6089 - val_acc: 0.7670\n",
      "Epoch 2731/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3804 - acc: 0.8519 - val_loss: 0.7016 - val_acc: 0.7330\n",
      "Epoch 2732/3000\n",
      "702/702 [==============================] - 0s 556us/sample - loss: 0.3719 - acc: 0.8689 - val_loss: 0.6985 - val_acc: 0.7727\n",
      "Epoch 2733/3000\n",
      "702/702 [==============================] - 0s 474us/sample - loss: 0.4145 - acc: 0.8433 - val_loss: 0.7808 - val_acc: 0.7045\n",
      "Epoch 2734/3000\n",
      "702/702 [==============================] - 0s 474us/sample - loss: 0.4205 - acc: 0.8405 - val_loss: 0.6176 - val_acc: 0.8239\n",
      "Epoch 2735/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.3897 - acc: 0.8533 - val_loss: 0.6377 - val_acc: 0.7841\n",
      "Epoch 2736/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.3861 - acc: 0.8718 - val_loss: 0.6544 - val_acc: 0.7386\n",
      "Epoch 2737/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.3652 - acc: 0.8661 - val_loss: 0.6266 - val_acc: 0.7443\n",
      "Epoch 2738/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.3920 - acc: 0.8547 - val_loss: 0.6302 - val_acc: 0.7727\n",
      "Epoch 2739/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.3978 - acc: 0.8547 - val_loss: 0.6680 - val_acc: 0.7330\n",
      "Epoch 2740/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.3793 - acc: 0.8604 - val_loss: 0.6144 - val_acc: 0.7841\n",
      "Epoch 2741/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3961 - acc: 0.8504 - val_loss: 0.7467 - val_acc: 0.6648\n",
      "Epoch 2742/3000\n",
      "702/702 [==============================] - 0s 548us/sample - loss: 0.4283 - acc: 0.8362 - val_loss: 0.5326 - val_acc: 0.8295\n",
      "Epoch 2743/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.4056 - acc: 0.8519 - val_loss: 0.6556 - val_acc: 0.7045\n",
      "Epoch 2744/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.4328 - acc: 0.8490 - val_loss: 0.5750 - val_acc: 0.8068\n",
      "Epoch 2745/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.4237 - acc: 0.8519 - val_loss: 0.6876 - val_acc: 0.6989\n",
      "Epoch 2746/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4856 - acc: 0.8091 - val_loss: 0.7224 - val_acc: 0.6932\n",
      "Epoch 2747/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.4570 - acc: 0.8447 - val_loss: 0.6472 - val_acc: 0.7443\n",
      "Epoch 2748/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.4175 - acc: 0.8547 - val_loss: 0.6075 - val_acc: 0.7784\n",
      "Epoch 2749/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.4360 - acc: 0.8462 - val_loss: 0.7064 - val_acc: 0.6989\n",
      "Epoch 2750/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.3813 - acc: 0.8704 - val_loss: 0.6315 - val_acc: 0.7614\n",
      "Epoch 2751/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3950 - acc: 0.8604 - val_loss: 0.6686 - val_acc: 0.7443\n",
      "Epoch 2752/3000\n",
      "702/702 [==============================] - 0s 555us/sample - loss: 0.4233 - acc: 0.8533 - val_loss: 0.7070 - val_acc: 0.6932\n",
      "Epoch 2753/3000\n",
      "702/702 [==============================] - 0s 472us/sample - loss: 0.3908 - acc: 0.8575 - val_loss: 0.5868 - val_acc: 0.8352\n",
      "Epoch 2754/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.4007 - acc: 0.8561 - val_loss: 0.7707 - val_acc: 0.6705\n",
      "Epoch 2755/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.4093 - acc: 0.8462 - val_loss: 0.6087 - val_acc: 0.8239\n",
      "Epoch 2756/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4313 - acc: 0.8462 - val_loss: 0.7767 - val_acc: 0.6875\n",
      "Epoch 2757/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.4249 - acc: 0.8405 - val_loss: 0.5860 - val_acc: 0.8239\n",
      "Epoch 2758/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4209 - acc: 0.8390 - val_loss: 0.6563 - val_acc: 0.7500\n",
      "Epoch 2759/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.3846 - acc: 0.8547 - val_loss: 0.5933 - val_acc: 0.8239\n",
      "Epoch 2760/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4056 - acc: 0.8447 - val_loss: 0.6920 - val_acc: 0.7216\n",
      "Epoch 2761/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3788 - acc: 0.8675 - val_loss: 0.6383 - val_acc: 0.7841\n",
      "Epoch 2762/3000\n",
      "702/702 [==============================] - 0s 558us/sample - loss: 0.3813 - acc: 0.8476 - val_loss: 0.6127 - val_acc: 0.8011\n",
      "Epoch 2763/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.3656 - acc: 0.8718 - val_loss: 0.6274 - val_acc: 0.7614\n",
      "Epoch 2764/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.3666 - acc: 0.8746 - val_loss: 0.5990 - val_acc: 0.7841\n",
      "Epoch 2765/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3423 - acc: 0.8732 - val_loss: 0.6120 - val_acc: 0.7727\n",
      "Epoch 2766/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.4029 - acc: 0.8462 - val_loss: 0.5542 - val_acc: 0.8295\n",
      "Epoch 2767/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.3772 - acc: 0.8632 - val_loss: 0.6198 - val_acc: 0.7557\n",
      "Epoch 2768/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.4292 - acc: 0.8604 - val_loss: 0.5431 - val_acc: 0.8295\n",
      "Epoch 2769/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.3495 - acc: 0.8818 - val_loss: 0.6199 - val_acc: 0.7614\n",
      "Epoch 2770/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.3444 - acc: 0.8803 - val_loss: 0.5805 - val_acc: 0.8068\n",
      "Epoch 2771/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3354 - acc: 0.8704 - val_loss: 0.6887 - val_acc: 0.7273\n",
      "Epoch 2772/3000\n",
      "702/702 [==============================] - 0s 545us/sample - loss: 0.3974 - acc: 0.8519 - val_loss: 0.5659 - val_acc: 0.8352\n",
      "Epoch 2773/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.4214 - acc: 0.8547 - val_loss: 0.6435 - val_acc: 0.7955\n",
      "Epoch 2774/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.3895 - acc: 0.8561 - val_loss: 0.6806 - val_acc: 0.7727\n",
      "Epoch 2775/3000\n",
      "702/702 [==============================] - 0s 471us/sample - loss: 0.4267 - acc: 0.8647 - val_loss: 0.6690 - val_acc: 0.7614\n",
      "Epoch 2776/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3797 - acc: 0.8590 - val_loss: 0.6594 - val_acc: 0.7614\n",
      "Epoch 2777/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.3713 - acc: 0.8647 - val_loss: 0.6034 - val_acc: 0.7898\n",
      "Epoch 2778/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.5219 - acc: 0.8063 - val_loss: 0.8206 - val_acc: 0.6364\n",
      "Epoch 2779/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.4592 - acc: 0.8191 - val_loss: 0.6547 - val_acc: 0.7500\n",
      "Epoch 2780/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.4408 - acc: 0.8219 - val_loss: 0.7532 - val_acc: 0.6648\n",
      "Epoch 2781/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4678 - acc: 0.8305 - val_loss: 0.6031 - val_acc: 0.7727\n",
      "Epoch 2782/3000\n",
      "702/702 [==============================] - 0s 555us/sample - loss: 0.4662 - acc: 0.8048 - val_loss: 0.7453 - val_acc: 0.6705\n",
      "Epoch 2783/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.4344 - acc: 0.8348 - val_loss: 0.6272 - val_acc: 0.7784\n",
      "Epoch 2784/3000\n",
      "702/702 [==============================] - 0s 473us/sample - loss: 0.3970 - acc: 0.8462 - val_loss: 0.8223 - val_acc: 0.6477\n",
      "Epoch 2785/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.4114 - acc: 0.8504 - val_loss: 0.6050 - val_acc: 0.8011\n",
      "Epoch 2786/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.3670 - acc: 0.8661 - val_loss: 0.7916 - val_acc: 0.6591\n",
      "Epoch 2787/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.4579 - acc: 0.8419 - val_loss: 0.5945 - val_acc: 0.8125\n",
      "Epoch 2788/3000\n",
      "702/702 [==============================] - 0s 523us/sample - loss: 0.3929 - acc: 0.8547 - val_loss: 0.6083 - val_acc: 0.7614\n",
      "Epoch 2789/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.4033 - acc: 0.8632 - val_loss: 0.6035 - val_acc: 0.7727\n",
      "Epoch 2790/3000\n",
      "702/702 [==============================] - 0s 472us/sample - loss: 0.3609 - acc: 0.8832 - val_loss: 0.7278 - val_acc: 0.7045\n",
      "Epoch 2791/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3961 - acc: 0.8561 - val_loss: 0.5639 - val_acc: 0.8125\n",
      "Epoch 2792/3000\n",
      "702/702 [==============================] - 0s 569us/sample - loss: 0.4173 - acc: 0.8433 - val_loss: 0.7756 - val_acc: 0.6591\n",
      "Epoch 2793/3000\n",
      "702/702 [==============================] - 0s 472us/sample - loss: 0.4285 - acc: 0.8362 - val_loss: 0.5355 - val_acc: 0.7955\n",
      "Epoch 2794/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4486 - acc: 0.8006 - val_loss: 0.8412 - val_acc: 0.6364\n",
      "Epoch 2795/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4796 - acc: 0.7963 - val_loss: 0.5300 - val_acc: 0.8295\n",
      "Epoch 2796/3000\n",
      "702/702 [==============================] - 0s 470us/sample - loss: 0.3775 - acc: 0.8647 - val_loss: 0.6423 - val_acc: 0.7216\n",
      "Epoch 2797/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.4417 - acc: 0.8618 - val_loss: 0.5948 - val_acc: 0.7614\n",
      "Epoch 2798/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.3606 - acc: 0.8704 - val_loss: 0.8470 - val_acc: 0.7216\n",
      "Epoch 2799/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.6013 - acc: 0.8832 - val_loss: 0.7162 - val_acc: 0.7784\n",
      "Epoch 2800/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4360 - acc: 0.8447 - val_loss: 0.7847 - val_acc: 0.6420\n",
      "Epoch 2801/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4486 - acc: 0.8433 - val_loss: 0.6454 - val_acc: 0.7898\n",
      "Epoch 2802/3000\n",
      "702/702 [==============================] - 0s 552us/sample - loss: 0.4979 - acc: 0.8191 - val_loss: 0.8390 - val_acc: 0.6420\n",
      "Epoch 2803/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.4114 - acc: 0.8462 - val_loss: 0.5700 - val_acc: 0.8068\n",
      "Epoch 2804/3000\n",
      "702/702 [==============================] - 0s 466us/sample - loss: 0.4144 - acc: 0.8447 - val_loss: 0.6657 - val_acc: 0.7159\n",
      "Epoch 2805/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.4386 - acc: 0.8419 - val_loss: 0.6282 - val_acc: 0.7557\n",
      "Epoch 2806/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.3601 - acc: 0.8661 - val_loss: 0.5963 - val_acc: 0.7955\n",
      "Epoch 2807/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.4009 - acc: 0.8604 - val_loss: 0.7090 - val_acc: 0.7216\n",
      "Epoch 2808/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.3684 - acc: 0.8604 - val_loss: 0.5419 - val_acc: 0.8295\n",
      "Epoch 2809/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.4897 - acc: 0.8148 - val_loss: 0.8085 - val_acc: 0.6477\n",
      "Epoch 2810/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.4418 - acc: 0.8319 - val_loss: 0.5832 - val_acc: 0.7955\n",
      "Epoch 2811/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3525 - acc: 0.8689 - val_loss: 0.6532 - val_acc: 0.7330\n",
      "Epoch 2812/3000\n",
      "702/702 [==============================] - 0s 547us/sample - loss: 0.3900 - acc: 0.8704 - val_loss: 0.5960 - val_acc: 0.7784\n",
      "Epoch 2813/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.3941 - acc: 0.8547 - val_loss: 0.6890 - val_acc: 0.7102\n",
      "Epoch 2814/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.4137 - acc: 0.8675 - val_loss: 0.6306 - val_acc: 0.7784\n",
      "Epoch 2815/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.3560 - acc: 0.8818 - val_loss: 0.6460 - val_acc: 0.7727\n",
      "Epoch 2816/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.4208 - acc: 0.8590 - val_loss: 0.6401 - val_acc: 0.7784\n",
      "Epoch 2817/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.4235 - acc: 0.8476 - val_loss: 0.7217 - val_acc: 0.7216\n",
      "Epoch 2818/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.3994 - acc: 0.8490 - val_loss: 0.5886 - val_acc: 0.7841\n",
      "Epoch 2819/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.3869 - acc: 0.8604 - val_loss: 0.7061 - val_acc: 0.7159\n",
      "Epoch 2820/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.3841 - acc: 0.8632 - val_loss: 0.4981 - val_acc: 0.8352\n",
      "Epoch 2821/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3599 - acc: 0.8704 - val_loss: 0.5758 - val_acc: 0.7898\n",
      "Epoch 2822/3000\n",
      "702/702 [==============================] - 0s 577us/sample - loss: 0.3471 - acc: 0.8704 - val_loss: 0.5325 - val_acc: 0.8182\n",
      "Epoch 2823/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.3616 - acc: 0.8632 - val_loss: 0.6090 - val_acc: 0.7727\n",
      "Epoch 2824/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.4059 - acc: 0.8462 - val_loss: 0.8071 - val_acc: 0.6477\n",
      "Epoch 2825/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4505 - acc: 0.8276 - val_loss: 0.5269 - val_acc: 0.8295\n",
      "Epoch 2826/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3870 - acc: 0.8561 - val_loss: 0.7168 - val_acc: 0.6989\n",
      "Epoch 2827/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 0.4051 - acc: 0.8447 - val_loss: 0.5546 - val_acc: 0.7955\n",
      "Epoch 2828/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.3436 - acc: 0.8718 - val_loss: 0.5448 - val_acc: 0.8182\n",
      "Epoch 2829/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3669 - acc: 0.8775 - val_loss: 0.6402 - val_acc: 0.7330\n",
      "Epoch 2830/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.3561 - acc: 0.8846 - val_loss: 0.6553 - val_acc: 0.7443\n",
      "Epoch 2831/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3725 - acc: 0.8732 - val_loss: 0.6859 - val_acc: 0.7273\n",
      "Epoch 2832/3000\n",
      "702/702 [==============================] - 0s 548us/sample - loss: 0.3584 - acc: 0.8647 - val_loss: 0.6150 - val_acc: 0.7841\n",
      "Epoch 2833/3000\n",
      "702/702 [==============================] - 0s 472us/sample - loss: 0.3665 - acc: 0.8718 - val_loss: 0.6343 - val_acc: 0.7727\n",
      "Epoch 2834/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.3489 - acc: 0.8846 - val_loss: 0.6577 - val_acc: 0.7386\n",
      "Epoch 2835/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.3392 - acc: 0.8718 - val_loss: 0.5858 - val_acc: 0.7955\n",
      "Epoch 2836/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.3687 - acc: 0.8718 - val_loss: 0.6694 - val_acc: 0.7273\n",
      "Epoch 2837/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.3753 - acc: 0.8704 - val_loss: 0.5198 - val_acc: 0.8352\n",
      "Epoch 2838/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.4148 - acc: 0.8333 - val_loss: 0.8111 - val_acc: 0.6420\n",
      "Epoch 2839/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.4256 - acc: 0.8262 - val_loss: 0.5700 - val_acc: 0.8182\n",
      "Epoch 2840/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.4808 - acc: 0.7934 - val_loss: 0.7777 - val_acc: 0.6477\n",
      "Epoch 2841/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4452 - acc: 0.8234 - val_loss: 0.6131 - val_acc: 0.7841\n",
      "Epoch 2842/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.4488 - acc: 0.8120 - val_loss: 0.7183 - val_acc: 0.6705\n",
      "Epoch 2843/3000\n",
      "702/702 [==============================] - 0s 471us/sample - loss: 0.4406 - acc: 0.8276 - val_loss: 0.6240 - val_acc: 0.7670\n",
      "Epoch 2844/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.4755 - acc: 0.8120 - val_loss: 0.8790 - val_acc: 0.6193\n",
      "Epoch 2845/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.5249 - acc: 0.7806 - val_loss: 0.5880 - val_acc: 0.8068\n",
      "Epoch 2846/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.4750 - acc: 0.8447 - val_loss: 0.7014 - val_acc: 0.7159\n",
      "Epoch 2847/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4519 - acc: 0.8305 - val_loss: 0.7102 - val_acc: 0.7216\n",
      "Epoch 2848/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.4623 - acc: 0.8348 - val_loss: 0.7316 - val_acc: 0.7045\n",
      "Epoch 2849/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.5198 - acc: 0.7749 - val_loss: 0.9726 - val_acc: 0.6080\n",
      "Epoch 2850/3000\n",
      "702/702 [==============================] - 0s 474us/sample - loss: 0.5665 - acc: 0.7650 - val_loss: 0.5492 - val_acc: 0.8182\n",
      "Epoch 2851/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4978 - acc: 0.8148 - val_loss: 0.6637 - val_acc: 0.7102\n",
      "Epoch 2852/3000\n",
      "702/702 [==============================] - 0s 558us/sample - loss: 0.4306 - acc: 0.8376 - val_loss: 0.7497 - val_acc: 0.6648\n",
      "Epoch 2853/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.4793 - acc: 0.8462 - val_loss: 0.6455 - val_acc: 0.7898\n",
      "Epoch 2854/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.4068 - acc: 0.8405 - val_loss: 0.8056 - val_acc: 0.6648\n",
      "Epoch 2855/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4273 - acc: 0.8234 - val_loss: 0.6581 - val_acc: 0.7841\n",
      "Epoch 2856/3000\n",
      "702/702 [==============================] - 0s 470us/sample - loss: 0.4522 - acc: 0.8447 - val_loss: 0.6109 - val_acc: 0.7898\n",
      "Epoch 2857/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.3807 - acc: 0.8604 - val_loss: 0.6886 - val_acc: 0.6761\n",
      "Epoch 2858/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.3723 - acc: 0.8604 - val_loss: 0.5245 - val_acc: 0.8239\n",
      "Epoch 2859/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3893 - acc: 0.8675 - val_loss: 0.6139 - val_acc: 0.7216\n",
      "Epoch 2860/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.3780 - acc: 0.8618 - val_loss: 0.5517 - val_acc: 0.8125\n",
      "Epoch 2861/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3849 - acc: 0.8718 - val_loss: 0.7190 - val_acc: 0.6648\n",
      "Epoch 2862/3000\n",
      "702/702 [==============================] - 0s 554us/sample - loss: 0.4311 - acc: 0.8305 - val_loss: 0.6283 - val_acc: 0.7557\n",
      "Epoch 2863/3000\n",
      "702/702 [==============================] - 0s 473us/sample - loss: 0.3937 - acc: 0.8689 - val_loss: 0.7263 - val_acc: 0.6534\n",
      "Epoch 2864/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.4213 - acc: 0.8319 - val_loss: 0.6980 - val_acc: 0.6818\n",
      "Epoch 2865/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.3971 - acc: 0.8632 - val_loss: 0.6571 - val_acc: 0.7670\n",
      "Epoch 2866/3000\n",
      "702/702 [==============================] - 0s 479us/sample - loss: 0.3757 - acc: 0.8661 - val_loss: 0.6531 - val_acc: 0.7500\n",
      "Epoch 2867/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.3739 - acc: 0.8732 - val_loss: 0.6273 - val_acc: 0.7386\n",
      "Epoch 2868/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.3749 - acc: 0.8618 - val_loss: 0.6325 - val_acc: 0.7330\n",
      "Epoch 2869/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.4001 - acc: 0.8604 - val_loss: 0.5752 - val_acc: 0.8011\n",
      "Epoch 2870/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.3703 - acc: 0.8732 - val_loss: 0.6550 - val_acc: 0.7614\n",
      "Epoch 2871/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3701 - acc: 0.8917 - val_loss: 0.7174 - val_acc: 0.7159\n",
      "Epoch 2872/3000\n",
      "702/702 [==============================] - 0s 553us/sample - loss: 0.3489 - acc: 0.8789 - val_loss: 0.6637 - val_acc: 0.7670\n",
      "Epoch 2873/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.3258 - acc: 0.9003 - val_loss: 0.5753 - val_acc: 0.7898\n",
      "Epoch 2874/3000\n",
      "702/702 [==============================] - 0s 470us/sample - loss: 0.3667 - acc: 0.8746 - val_loss: 0.6264 - val_acc: 0.7784\n",
      "Epoch 2875/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.3354 - acc: 0.8903 - val_loss: 0.5733 - val_acc: 0.8068\n",
      "Epoch 2876/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.5219 - acc: 0.8590 - val_loss: 0.6238 - val_acc: 0.7727\n",
      "Epoch 2877/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.3232 - acc: 0.8846 - val_loss: 0.6549 - val_acc: 0.7670\n",
      "Epoch 2878/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.3841 - acc: 0.8732 - val_loss: 0.6387 - val_acc: 0.7898\n",
      "Epoch 2879/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.5116 - acc: 0.8219 - val_loss: 1.0101 - val_acc: 0.5852\n",
      "Epoch 2880/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.5387 - acc: 0.8006 - val_loss: 0.7265 - val_acc: 0.7273\n",
      "Epoch 2881/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4165 - acc: 0.8476 - val_loss: 0.8352 - val_acc: 0.6364\n",
      "Epoch 2882/3000\n",
      "702/702 [==============================] - 0s 550us/sample - loss: 0.4439 - acc: 0.8348 - val_loss: 0.5293 - val_acc: 0.8125\n",
      "Epoch 2883/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.4950 - acc: 0.8305 - val_loss: 0.9230 - val_acc: 0.6080\n",
      "Epoch 2884/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.5128 - acc: 0.7991 - val_loss: 0.5662 - val_acc: 0.8239\n",
      "Epoch 2885/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.4626 - acc: 0.8376 - val_loss: 0.7801 - val_acc: 0.6932\n",
      "Epoch 2886/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.4192 - acc: 0.8490 - val_loss: 0.6190 - val_acc: 0.7841\n",
      "Epoch 2887/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.4409 - acc: 0.8447 - val_loss: 0.6760 - val_acc: 0.7216\n",
      "Epoch 2888/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.5121 - acc: 0.8447 - val_loss: 0.7090 - val_acc: 0.6932\n",
      "Epoch 2889/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.4726 - acc: 0.8248 - val_loss: 0.6433 - val_acc: 0.7898\n",
      "Epoch 2890/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.4338 - acc: 0.8575 - val_loss: 0.7800 - val_acc: 0.6534\n",
      "Epoch 2891/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4815 - acc: 0.8291 - val_loss: 0.6458 - val_acc: 0.7784\n",
      "Epoch 2892/3000\n",
      "702/702 [==============================] - 0s 564us/sample - loss: 0.4125 - acc: 0.8689 - val_loss: 0.7180 - val_acc: 0.7045\n",
      "Epoch 2893/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.3862 - acc: 0.8604 - val_loss: 0.6278 - val_acc: 0.7841\n",
      "Epoch 2894/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 0.4557 - acc: 0.8419 - val_loss: 0.8224 - val_acc: 0.6477\n",
      "Epoch 2895/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.4332 - acc: 0.8476 - val_loss: 0.6953 - val_acc: 0.7784\n",
      "Epoch 2896/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.3956 - acc: 0.8647 - val_loss: 0.8728 - val_acc: 0.6420\n",
      "Epoch 2897/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.4348 - acc: 0.8462 - val_loss: 0.6298 - val_acc: 0.7727\n",
      "Epoch 2898/3000\n",
      "702/702 [==============================] - 0s 475us/sample - loss: 0.3464 - acc: 0.8746 - val_loss: 0.6209 - val_acc: 0.7557\n",
      "Epoch 2899/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.3613 - acc: 0.8547 - val_loss: 0.5320 - val_acc: 0.8182\n",
      "Epoch 2900/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.4415 - acc: 0.8419 - val_loss: 0.6386 - val_acc: 0.7159\n",
      "Epoch 2901/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4477 - acc: 0.8390 - val_loss: 0.6805 - val_acc: 0.6989\n",
      "Epoch 2902/3000\n",
      "702/702 [==============================] - 0s 546us/sample - loss: 0.5527 - acc: 0.7621 - val_loss: 0.8916 - val_acc: 0.6193\n",
      "Epoch 2903/3000\n",
      "702/702 [==============================] - 0s 471us/sample - loss: 0.4937 - acc: 0.8006 - val_loss: 0.4977 - val_acc: 0.8580\n",
      "Epoch 2904/3000\n",
      "702/702 [==============================] - 0s 467us/sample - loss: 0.4350 - acc: 0.8476 - val_loss: 0.7112 - val_acc: 0.7159\n",
      "Epoch 2905/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.3983 - acc: 0.8547 - val_loss: 0.6863 - val_acc: 0.7386\n",
      "Epoch 2906/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.4063 - acc: 0.8476 - val_loss: 0.7651 - val_acc: 0.6932\n",
      "Epoch 2907/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.4167 - acc: 0.8376 - val_loss: 0.6952 - val_acc: 0.7386\n",
      "Epoch 2908/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.4181 - acc: 0.8390 - val_loss: 0.8077 - val_acc: 0.6534\n",
      "Epoch 2909/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.4307 - acc: 0.8405 - val_loss: 0.5289 - val_acc: 0.8125\n",
      "Epoch 2910/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.3963 - acc: 0.8519 - val_loss: 0.6032 - val_acc: 0.7898\n",
      "Epoch 2911/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3573 - acc: 0.8818 - val_loss: 0.6531 - val_acc: 0.7614\n",
      "Epoch 2912/3000\n",
      "702/702 [==============================] - 0s 554us/sample - loss: 0.3948 - acc: 0.8675 - val_loss: 0.6694 - val_acc: 0.7443\n",
      "Epoch 2913/3000\n",
      "702/702 [==============================] - 0s 474us/sample - loss: 0.3670 - acc: 0.8647 - val_loss: 0.6284 - val_acc: 0.7898\n",
      "Epoch 2914/3000\n",
      "702/702 [==============================] - 0s 473us/sample - loss: 0.3740 - acc: 0.8775 - val_loss: 0.6140 - val_acc: 0.7955\n",
      "Epoch 2915/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.4152 - acc: 0.8547 - val_loss: 0.7999 - val_acc: 0.6591\n",
      "Epoch 2916/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.4246 - acc: 0.8433 - val_loss: 0.5777 - val_acc: 0.7898\n",
      "Epoch 2917/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.4291 - acc: 0.8405 - val_loss: 0.8999 - val_acc: 0.6193\n",
      "Epoch 2918/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.4696 - acc: 0.8034 - val_loss: 0.5958 - val_acc: 0.8011\n",
      "Epoch 2919/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4966 - acc: 0.8419 - val_loss: 0.9601 - val_acc: 0.6136\n",
      "Epoch 2920/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.5456 - acc: 0.7564 - val_loss: 0.9727 - val_acc: 0.6080\n",
      "Epoch 2921/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.5200 - acc: 0.7934 - val_loss: 0.5389 - val_acc: 0.8295\n",
      "Epoch 2922/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.5933 - acc: 0.7906 - val_loss: 1.0253 - val_acc: 0.5852\n",
      "Epoch 2923/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.6244 - acc: 0.7023 - val_loss: 0.9637 - val_acc: 0.6023\n",
      "Epoch 2924/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.5151 - acc: 0.7792 - val_loss: 0.6314 - val_acc: 0.7898\n",
      "Epoch 2925/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.4680 - acc: 0.8248 - val_loss: 0.7148 - val_acc: 0.7386\n",
      "Epoch 2926/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.4477 - acc: 0.8262 - val_loss: 0.8506 - val_acc: 0.6534\n",
      "Epoch 2927/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.4526 - acc: 0.8305 - val_loss: 0.6674 - val_acc: 0.7614\n",
      "Epoch 2928/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.4401 - acc: 0.8504 - val_loss: 0.7605 - val_acc: 0.7330\n",
      "Epoch 2929/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.4334 - acc: 0.8547 - val_loss: 0.7433 - val_acc: 0.7557\n",
      "Epoch 2930/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.3712 - acc: 0.8689 - val_loss: 0.6919 - val_acc: 0.7784\n",
      "Epoch 2931/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3946 - acc: 0.8604 - val_loss: 0.7598 - val_acc: 0.7500\n",
      "Epoch 2932/3000\n",
      "702/702 [==============================] - 0s 601us/sample - loss: 0.4198 - acc: 0.8433 - val_loss: 0.7016 - val_acc: 0.7614\n",
      "Epoch 2933/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.3611 - acc: 0.8761 - val_loss: 0.6140 - val_acc: 0.8068\n",
      "Epoch 2934/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.4099 - acc: 0.8333 - val_loss: 0.8644 - val_acc: 0.6307\n",
      "Epoch 2935/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.4450 - acc: 0.8077 - val_loss: 0.6067 - val_acc: 0.8011\n",
      "Epoch 2936/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.3983 - acc: 0.8647 - val_loss: 0.7746 - val_acc: 0.6705\n",
      "Epoch 2937/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.5926 - acc: 0.8447 - val_loss: 0.7006 - val_acc: 0.7614\n",
      "Epoch 2938/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.4140 - acc: 0.8718 - val_loss: 0.7117 - val_acc: 0.7614\n",
      "Epoch 2939/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.4345 - acc: 0.8476 - val_loss: 0.7818 - val_acc: 0.7216\n",
      "Epoch 2940/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.3902 - acc: 0.8704 - val_loss: 0.6817 - val_acc: 0.7898\n",
      "Epoch 2941/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4289 - acc: 0.8490 - val_loss: 0.7224 - val_acc: 0.7330\n",
      "Epoch 2942/3000\n",
      "702/702 [==============================] - 0s 583us/sample - loss: 0.3619 - acc: 0.8632 - val_loss: 0.5925 - val_acc: 0.8011\n",
      "Epoch 2943/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.3493 - acc: 0.8704 - val_loss: 0.7357 - val_acc: 0.7102\n",
      "Epoch 2944/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.3919 - acc: 0.8405 - val_loss: 0.5810 - val_acc: 0.8011\n",
      "Epoch 2945/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.4282 - acc: 0.8476 - val_loss: 0.9403 - val_acc: 0.6080\n",
      "Epoch 2946/3000\n",
      "702/702 [==============================] - 0s 470us/sample - loss: 0.4343 - acc: 0.8162 - val_loss: 0.6610 - val_acc: 0.7670\n",
      "Epoch 2947/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.3644 - acc: 0.8689 - val_loss: 0.8063 - val_acc: 0.6761\n",
      "Epoch 2948/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.4046 - acc: 0.8390 - val_loss: 0.5844 - val_acc: 0.7898\n",
      "Epoch 2949/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.3724 - acc: 0.8803 - val_loss: 0.5785 - val_acc: 0.7898\n",
      "Epoch 2950/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.3505 - acc: 0.8661 - val_loss: 0.7013 - val_acc: 0.7216\n",
      "Epoch 2951/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3803 - acc: 0.8590 - val_loss: 0.6055 - val_acc: 0.7784\n",
      "Epoch 2952/3000\n",
      "702/702 [==============================] - 0s 557us/sample - loss: 0.3741 - acc: 0.8746 - val_loss: 0.6049 - val_acc: 0.7500\n",
      "Epoch 2953/3000\n",
      "702/702 [==============================] - 0s 473us/sample - loss: 0.3805 - acc: 0.8575 - val_loss: 0.6369 - val_acc: 0.7557\n",
      "Epoch 2954/3000\n",
      "702/702 [==============================] - 0s 471us/sample - loss: 0.3626 - acc: 0.8732 - val_loss: 0.5628 - val_acc: 0.8125\n",
      "Epoch 2955/3000\n",
      "702/702 [==============================] - 0s 478us/sample - loss: 0.4035 - acc: 0.8632 - val_loss: 0.6785 - val_acc: 0.7443\n",
      "Epoch 2956/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.3429 - acc: 0.8789 - val_loss: 0.6615 - val_acc: 0.7727\n",
      "Epoch 2957/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 0.3420 - acc: 0.8775 - val_loss: 0.6629 - val_acc: 0.7670\n",
      "Epoch 2958/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.3657 - acc: 0.8832 - val_loss: 0.6803 - val_acc: 0.7273\n",
      "Epoch 2959/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.3555 - acc: 0.8903 - val_loss: 0.6812 - val_acc: 0.7670\n",
      "Epoch 2960/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.3833 - acc: 0.8647 - val_loss: 0.6736 - val_acc: 0.7727\n",
      "Epoch 2961/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.3757 - acc: 0.8761 - val_loss: 0.6402 - val_acc: 0.7727\n",
      "Epoch 2962/3000\n",
      "702/702 [==============================] - 0s 554us/sample - loss: 0.3152 - acc: 0.8946 - val_loss: 0.6543 - val_acc: 0.7443\n",
      "Epoch 2963/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.3413 - acc: 0.8789 - val_loss: 0.5823 - val_acc: 0.8068\n",
      "Epoch 2964/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.3628 - acc: 0.8604 - val_loss: 0.6661 - val_acc: 0.7500\n",
      "Epoch 2965/3000\n",
      "702/702 [==============================] - 0s 477us/sample - loss: 0.4108 - acc: 0.8433 - val_loss: 0.8691 - val_acc: 0.6477\n",
      "Epoch 2966/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.4048 - acc: 0.8447 - val_loss: 0.5821 - val_acc: 0.8182\n",
      "Epoch 2967/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.4114 - acc: 0.8519 - val_loss: 0.7184 - val_acc: 0.7273\n",
      "Epoch 2968/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3900 - acc: 0.8547 - val_loss: 0.8192 - val_acc: 0.6477\n",
      "Epoch 2969/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.5413 - acc: 0.7521 - val_loss: 0.8374 - val_acc: 0.6591\n",
      "Epoch 2970/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.4622 - acc: 0.8362 - val_loss: 0.5927 - val_acc: 0.7955\n",
      "Epoch 2971/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4431 - acc: 0.8561 - val_loss: 0.7769 - val_acc: 0.6591\n",
      "Epoch 2972/3000\n",
      "702/702 [==============================] - 0s 545us/sample - loss: 0.4298 - acc: 0.8333 - val_loss: 0.5984 - val_acc: 0.8011\n",
      "Epoch 2973/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.4041 - acc: 0.8689 - val_loss: 0.8027 - val_acc: 0.6591\n",
      "Epoch 2974/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.4263 - acc: 0.8276 - val_loss: 0.5881 - val_acc: 0.8011\n",
      "Epoch 2975/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 0.4424 - acc: 0.8405 - val_loss: 0.8723 - val_acc: 0.6250\n",
      "Epoch 2976/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.5206 - acc: 0.7821 - val_loss: 0.8919 - val_acc: 0.6136\n",
      "Epoch 2977/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.4454 - acc: 0.8319 - val_loss: 0.6226 - val_acc: 0.8068\n",
      "Epoch 2978/3000\n",
      "702/702 [==============================] - 0s 481us/sample - loss: 0.4289 - acc: 0.8490 - val_loss: 0.9342 - val_acc: 0.6193\n",
      "Epoch 2979/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.4132 - acc: 0.8276 - val_loss: 0.5431 - val_acc: 0.8352\n",
      "Epoch 2980/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.4831 - acc: 0.8276 - val_loss: 0.8203 - val_acc: 0.6705\n",
      "Epoch 2981/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4443 - acc: 0.8276 - val_loss: 0.8058 - val_acc: 0.6818\n",
      "Epoch 2982/3000\n",
      "702/702 [==============================] - 0s 558us/sample - loss: 0.3953 - acc: 0.8618 - val_loss: 0.7155 - val_acc: 0.7500\n",
      "Epoch 2983/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.3244 - acc: 0.8917 - val_loss: 0.7846 - val_acc: 0.6989\n",
      "Epoch 2984/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.3885 - acc: 0.8661 - val_loss: 0.7051 - val_acc: 0.7614\n",
      "Epoch 2985/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.3482 - acc: 0.8803 - val_loss: 0.8471 - val_acc: 0.6364\n",
      "Epoch 2986/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.3924 - acc: 0.8490 - val_loss: 0.6211 - val_acc: 0.8239\n",
      "Epoch 2987/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.3979 - acc: 0.8590 - val_loss: 0.7030 - val_acc: 0.7557\n",
      "Epoch 2988/3000\n",
      "702/702 [==============================] - 0s 476us/sample - loss: 0.3637 - acc: 0.8789 - val_loss: 0.6922 - val_acc: 0.7500\n",
      "Epoch 2989/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3341 - acc: 0.8803 - val_loss: 0.5719 - val_acc: 0.8182\n",
      "Epoch 2990/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.3535 - acc: 0.8575 - val_loss: 0.8412 - val_acc: 0.6420\n",
      "Epoch 2991/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 0.4079 - acc: 0.8305 - val_loss: 0.5744 - val_acc: 0.8352\n",
      "Epoch 2992/3000\n",
      "702/702 [==============================] - 0s 549us/sample - loss: 0.3582 - acc: 0.8789 - val_loss: 0.6915 - val_acc: 0.7500\n",
      "Epoch 2993/3000\n",
      "702/702 [==============================] - 0s 474us/sample - loss: 0.3579 - acc: 0.8746 - val_loss: 0.6236 - val_acc: 0.7727\n",
      "Epoch 2994/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.3250 - acc: 0.8960 - val_loss: 0.7087 - val_acc: 0.7216\n",
      "Epoch 2995/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.3893 - acc: 0.8647 - val_loss: 0.6591 - val_acc: 0.7670\n",
      "Epoch 2996/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.3741 - acc: 0.8889 - val_loss: 0.6883 - val_acc: 0.7955\n",
      "Epoch 2997/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.3482 - acc: 0.8746 - val_loss: 0.7984 - val_acc: 0.7159\n",
      "Epoch 2998/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.3440 - acc: 0.8718 - val_loss: 0.6649 - val_acc: 0.8125\n",
      "Epoch 2999/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.3969 - acc: 0.8561 - val_loss: 0.8403 - val_acc: 0.6705\n",
      "Epoch 3000/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.3870 - acc: 0.8476 - val_loss: 0.5208 - val_acc: 0.8182\n"
     ]
    }
   ],
   "source": [
    "model = Mynet_squeeze()\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=optimizers.Adam(lr=0.006, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(trainX,trainY, \n",
    "                    epochs=3000, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    verbose=1, \n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[tf.keras.callbacks.TensorBoard(log_dir='../logs/'+IN_DIR_PATH+'/events', histogram_freq=10)#,\n",
    "#                                tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=700,verbose=0,mode='auto')\n",
    "                              ]\n",
    "                   )\n",
    "\n",
    "score = model.evaluate(valX, valY, batch_size=BATCH_SIZE, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "# loss\n",
    "def plot_history_loss(fit):\n",
    "    # Plot the loss in the history\n",
    "    axL.plot(fit.history['loss'],label=\"loss for training\")\n",
    "    axL.plot(fit.history['val_loss'],label=\"loss for validation\")\n",
    "    axL.set_title('model loss')\n",
    "    axL.set_xlabel('epoch')\n",
    "    axL.set_ylabel('loss')\n",
    "    axL.set_ylim([0,3])\n",
    "    axL.legend(loc='upper right')\n",
    "\n",
    "# acc\n",
    "def plot_history_acc(fit):\n",
    "    # Plot the loss in the history\n",
    "    axR.plot(fit.history['acc'],label=\"loss for training\")\n",
    "    axR.plot(fit.history['val_acc'],label=\"loss for validation\")\n",
    "    axR.set_title('model accuracy')\n",
    "    axR.set_xlabel('epoch')\n",
    "    axR.set_ylabel('accuracy')\n",
    "    axR.legend(loc='upper right')\n",
    "\n",
    "plot_history_loss(history)\n",
    "plot_history_acc(history)\n",
    "plt.show()\n",
    "fig.savefig('../logs/'+IN_DIR_PATH+'/loss_acc.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[ 7  0  0  0  0  0  0  0  0]\n",
      " [ 0 15  0  0  0  0  0  0  0]\n",
      " [ 0  0 21  0  0  0  0  0  0]\n",
      " [ 0  0  0  3  0  0  0  0  0]\n",
      " [ 1  0  0  0  4  0  1  0  1]\n",
      " [ 0  0  0  0  6  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  8  0  0]\n",
      " [ 0  0  0  0  3  0  0  5  3]\n",
      " [ 2  0  0  0  0  0  0  0 12]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/numpy/lib/arraysetops.py:564: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n",
      "/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "71949"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y = model.predict(valX)\n",
    "pred_y_c = np.argmax(pred_y,axis=1)\n",
    "# pred_y_one_hot = np.identity(len(class_list))[pred_y_c]\n",
    "true_y = np.argmax(valY,axis=1)\n",
    "confusion_mtx = confusion_matrix(true_y, pred_y_c) \n",
    "plot_confusion_matrix(confusion_mtx, classes=class_list)\n",
    "plt.show()\n",
    "cr = classification_report(true_y,pred_y_c,labels=class_list)\n",
    "f = open('../logs/'+IN_DIR_PATH+'/cr.txt','w')\n",
    "f.write(cr)\n",
    "f.close()\n",
    "\n",
    "keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 0, 1, 0, 2, 8, 8, 7, 3, 2, 1, 4, 2, 0, 2, 1, 2, 8, 6, 4, 4, 7,\n",
       "       1, 1, 0, 7, 7, 1, 2, 1, 1, 4, 0, 4, 3, 2, 0, 4, 1, 0, 3, 4, 7, 0,\n",
       "       4, 1, 6, 6, 8, 0, 0, 1, 6, 4, 8, 2, 2, 2, 4, 2, 1, 0, 8, 8, 7, 2,\n",
       "       8, 3, 1, 3, 2, 2, 6, 8, 3, 8, 8, 3, 3, 6, 8, 4, 0, 4, 7, 6, 6, 0,\n",
       "       2, 8, 3, 7, 3, 2, 6, 2, 7, 7])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6, 0, 1, 0, 2, 8, 8, 7, 3, 2, 1, 7, 2, 0, 2, 1, 2, 8, 6, 5, 7, 7,\n",
       "        1, 1, 4, 7, 7, 1, 2, 1, 1, 5, 8, 7, 3, 2, 0, 7, 1, 0, 3, 5, 7, 4,\n",
       "        5, 1, 6, 6, 4, 4, 8, 1, 6, 5, 8, 2, 2, 2, 5, 2, 1, 0, 8, 8, 5, 2,\n",
       "        5, 3, 1, 3, 2, 2, 6, 8, 3, 8, 8, 6, 3, 6, 8, 4, 0, 5, 7, 6, 6, 0,\n",
       "        2, 8, 3, 7, 3, 2, 6, 2, 5, 7]),)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98,)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -3,  0,  0,  0,  0,  0,\n",
       "        0,  0, -1, -3,  0,  0,  0, -4,  0,  0,  0,  0,  0,  0, -1, -8, -3,\n",
       "        0,  0,  0, -3,  0,  0,  0, -1,  0, -4, -1,  0,  0,  0,  4, -4, -8,\n",
       "        0,  0, -1,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  2,  0,  3,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0, -3,  0,  0,  0,  0,  0, -1,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2,  0])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.3.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
