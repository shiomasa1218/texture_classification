{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory growth: True\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import Input,Model,optimizers\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Conv1D, UpSampling1D,Activation\n",
    "from tensorflow.keras.layers import MaxPooling1D, BatchNormalization, Flatten,GlobalMaxPooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "from tensorflow.keras.callbacks import Callback, TensorBoard\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, classification_report,plot_confusion_matrix\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print('memory growth:', tf.config.experimental.get_memory_growth(physical_devices[0]))\n",
    "else:\n",
    "    print(\"Not enough GPU hardware devices available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_RATE = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define\n",
    "EPOCH = 3000\n",
    "BATCH_SIZE = 128\n",
    "MINIBATCH = 64\n",
    "DROP_RATE = 0.5\n",
    "\n",
    "FC_SIZE = 50\n",
    "FILTER_SIZE = 64\n",
    "\n",
    "DATA_START = 250\n",
    "DATA_LEN = 4096\n",
    "\n",
    "IN_DIR_PATH = \"dft321\"\n",
    "\n",
    "os.makedirs('../logs/'+IN_DIR_PATH+'/events',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/976 [00:00<?, ?it/s]/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/ipykernel_launcher.py:22: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "100%|██████████| 976/976 [00:42<00:00, 22.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# input_dir and out_dir\n",
    "input_dir_path = \"../preprocessed/train_\"+IN_DIR_PATH+\"/*/*.csv\"\n",
    "input_dir = glob.glob(input_dir_path)\n",
    "input_num = len(input_dir)\n",
    "\n",
    "class_list = np.array([])\n",
    "all_data = np.array([])\n",
    "all_label = np.array([])\n",
    "\n",
    "init_flg = True\n",
    "\n",
    "for d in tqdm(input_dir):\n",
    "    # print(d)\n",
    "    if \".DS_Store\" in d:\n",
    "        os.remove(d)\n",
    "        input_dir.remove(d)\n",
    "        continue\n",
    "    \n",
    "    end_index = d.rfind('/')\n",
    "    start_index = d[:end_index].rfind('/')\n",
    "    class_name = d[start_index+1:end_index]\n",
    "    if class_name not in class_list:\n",
    "        class_list = np.append(class_list,class_name)\n",
    "        # print(class_name)\n",
    "    \n",
    "    label = np.where(class_list == class_name)\n",
    "    all_label = np.append(all_label,label)\n",
    "    \n",
    "    data = np.loadtxt(d, delimiter=\",\")\n",
    "    \n",
    "    # axis_time = np.vstack(data[DATA_START:DATA_START+DATA_LEN,0])\n",
    "    axis_value = np.hstack([data[DATA_START:DATA_START+DATA_LEN, 1]])\n",
    "    \n",
    "    # preprocessed = np.hstack([axis_time,axis_value])\n",
    "    if init_flg:\n",
    "        all_data = axis_value\n",
    "        init_flg = False\n",
    "    else:\n",
    "        all_data = np.vstack([all_data,axis_value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_label = to_categorical(all_label,class_list.shape[0])\n",
    "\n",
    "p = np.random.permutation(input_num)\n",
    "shuffled_data = all_data[p]\n",
    "shuffled_label = one_hot_label[p]\n",
    "\n",
    "trainX = shuffled_data[:int(input_num*VAL_RATE)]\n",
    "valX = shuffled_data[int(input_num*VAL_RATE):]\n",
    "trainY = shuffled_label[:int(input_num*VAL_RATE)]\n",
    "valY = shuffled_label[int(input_num*VAL_RATE):]\n",
    "\n",
    "trainX = np.reshape(trainX,(int(input_num*VAL_RATE),DATA_LEN,1)).astype(np.float32)\n",
    "valX = np.reshape(valX,(input_num-int(input_num*VAL_RATE),DATA_LEN,1)).astype(np.float32)\n",
    "# trainY = np.reshape(trainY,(int(input_num*VAL_RATE),class_list.shape[0],1))\n",
    "# valY = np.reshape(valY,(input_num-int(input_num*VAL_RATE),class_list.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878, 4096, 1)\n",
      "(878, 9)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mynet():\n",
    "    inputs = Input(shape=(DATA_LEN,1))\n",
    "    # Due to memory limitation, images will resized on-the-fly.\n",
    "    x = Conv1D(FILTER_SIZE, 5, padding='same', input_shape=(DATA_LEN,1), activation=None)(inputs)\n",
    "    x = Conv1D(FILTER_SIZE, 5, padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Dropout(DROP_RATE)(x)\n",
    "\n",
    "    x = Conv1D(FILTER_SIZE*2, 5, padding='same')(x)\n",
    "    x = Conv1D(FILTER_SIZE*2, 5, padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Dropout(DROP_RATE)(x)\n",
    "    \n",
    "    x = Conv1D(FILTER_SIZE*4, 5, padding='same')(x)\n",
    "    x = Conv1D(FILTER_SIZE*4, 5, padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Dropout(DROP_RATE)(x)\n",
    "    \n",
    "    fc = Flatten()(x)\n",
    "    fc = Dense(FC_SIZE*2, activation='relu')(fc)\n",
    "    fc = BatchNormalization()(fc)\n",
    "    fc = Dropout(DROP_RATE)(fc)\n",
    "    \n",
    "    fc = Dense(FC_SIZE, activation='relu')(fc)\n",
    "    fc = BatchNormalization()(fc)\n",
    "    fc = Dropout(DROP_RATE)(fc)\n",
    "    \n",
    "    fc = Dense(class_list.shape[0])(fc)\n",
    "    softmax = Activation('softmax')(fc)\n",
    "    model = Model(inputs=inputs, outputs=softmax)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mynet_squeeze():\n",
    "    inputs = Input(shape=(DATA_LEN,1))\n",
    "    # Due to memory limitation, images will resized on-the-fly.\n",
    "    x = Conv1D(FILTER_SIZE, 5, padding='same', input_shape=(DATA_LEN,1), activation='relu')(inputs)\n",
    "    x = Conv1D(FILTER_SIZE, 5, padding='same',activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Conv1D(int(FILTER_SIZE*2), 5, padding='same',activation='relu')(x)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Dropout(DROP_RATE)(x)\n",
    "    x = Conv1D(int(FILTER_SIZE), 5, padding='same', activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "    \n",
    "    fc = GlobalMaxPooling1D()(x)\n",
    "    fc = Dropout(DROP_RATE)(fc)\n",
    "    fc = Flatten()(fc)\n",
    "    fc = Dense(class_list.shape[0])(fc)\n",
    "    softmax = Activation('softmax')(fc)\n",
    "    model = Model(inputs=inputs, outputs=softmax)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=False):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig('../logs/'+IN_DIR_PATH+'/cm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4096, 1)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 4096, 64)          384       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 4096, 64)          20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 2048, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 2048, 128)         41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1024, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1024, 64)          41024     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 9)                 585       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 103,625\n",
      "Trainable params: 103,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 702 samples, validate on 176 samples\n",
      "Epoch 1/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 402.1723 - acc: 0.1425 - val_loss: 22.2233 - val_acc: 0.1648\n",
      "Epoch 2/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 59.9027 - acc: 0.1154 - val_loss: 6.9035 - val_acc: 0.1477\n",
      "Epoch 3/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 22.8577 - acc: 0.1154 - val_loss: 3.8358 - val_acc: 0.1307\n",
      "Epoch 4/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 12.4254 - acc: 0.1140 - val_loss: 2.5165 - val_acc: 0.1420\n",
      "Epoch 5/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 6.8922 - acc: 0.1524 - val_loss: 2.2876 - val_acc: 0.2102\n",
      "Epoch 6/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 4.6905 - acc: 0.1225 - val_loss: 2.2200 - val_acc: 0.1705\n",
      "Epoch 7/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 3.7288 - acc: 0.1254 - val_loss: 2.1353 - val_acc: 0.1818\n",
      "Epoch 8/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 3.1761 - acc: 0.1410 - val_loss: 2.0974 - val_acc: 0.2102\n",
      "Epoch 9/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 2.7995 - acc: 0.1311 - val_loss: 2.1103 - val_acc: 0.2443\n",
      "Epoch 10/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 2.5913 - acc: 0.1453 - val_loss: 2.0942 - val_acc: 0.2386\n",
      "Epoch 11/3000\n",
      "702/702 [==============================] - 1s 2ms/sample - loss: 2.4851 - acc: 0.1610 - val_loss: 2.0905 - val_acc: 0.2670\n",
      "Epoch 12/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 2.3412 - acc: 0.1510 - val_loss: 2.0674 - val_acc: 0.2614\n",
      "Epoch 13/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 2.3328 - acc: 0.1610 - val_loss: 2.0644 - val_acc: 0.2614\n",
      "Epoch 14/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 2.2186 - acc: 0.2094 - val_loss: 2.0458 - val_acc: 0.2727\n",
      "Epoch 15/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 2.1899 - acc: 0.2137 - val_loss: 2.0078 - val_acc: 0.3125\n",
      "Epoch 16/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 2.1818 - acc: 0.1923 - val_loss: 1.9799 - val_acc: 0.3693\n",
      "Epoch 17/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 2.1138 - acc: 0.2464 - val_loss: 1.9440 - val_acc: 0.3580\n",
      "Epoch 18/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 2.1035 - acc: 0.2365 - val_loss: 1.9186 - val_acc: 0.3807\n",
      "Epoch 19/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 2.0704 - acc: 0.2393 - val_loss: 1.9079 - val_acc: 0.3580\n",
      "Epoch 20/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 2.0050 - acc: 0.2835 - val_loss: 1.8905 - val_acc: 0.3864\n",
      "Epoch 21/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 1.9756 - acc: 0.3134 - val_loss: 1.8428 - val_acc: 0.3977\n",
      "Epoch 22/3000\n",
      "702/702 [==============================] - 0s 559us/sample - loss: 1.9381 - acc: 0.3077 - val_loss: 1.8223 - val_acc: 0.4432\n",
      "Epoch 23/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.8796 - acc: 0.3419 - val_loss: 1.7961 - val_acc: 0.4148\n",
      "Epoch 24/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 1.9047 - acc: 0.3376 - val_loss: 1.7762 - val_acc: 0.4148\n",
      "Epoch 25/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.7816 - acc: 0.3590 - val_loss: 1.7449 - val_acc: 0.4318\n",
      "Epoch 26/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.7705 - acc: 0.3604 - val_loss: 1.7012 - val_acc: 0.4375\n",
      "Epoch 27/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.7919 - acc: 0.3647 - val_loss: 1.6759 - val_acc: 0.4886\n",
      "Epoch 28/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 1.7176 - acc: 0.3875 - val_loss: 1.6667 - val_acc: 0.4886\n",
      "Epoch 29/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.6784 - acc: 0.4031 - val_loss: 1.6082 - val_acc: 0.5000\n",
      "Epoch 30/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 1.6215 - acc: 0.4316 - val_loss: 1.5860 - val_acc: 0.5114\n",
      "Epoch 31/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 1.6604 - acc: 0.4302 - val_loss: 1.5958 - val_acc: 0.4716\n",
      "Epoch 32/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 1.6974 - acc: 0.4046 - val_loss: 1.7026 - val_acc: 0.5057\n",
      "Epoch 33/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.7202 - acc: 0.4174 - val_loss: 1.5570 - val_acc: 0.5114\n",
      "Epoch 34/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 1.6309 - acc: 0.4259 - val_loss: 1.5338 - val_acc: 0.5114\n",
      "Epoch 35/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.5999 - acc: 0.4587 - val_loss: 1.5032 - val_acc: 0.5227\n",
      "Epoch 36/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 1.5046 - acc: 0.4658 - val_loss: 1.5036 - val_acc: 0.5284\n",
      "Epoch 37/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.5150 - acc: 0.4886 - val_loss: 1.4791 - val_acc: 0.5455\n",
      "Epoch 38/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.5313 - acc: 0.4601 - val_loss: 1.4856 - val_acc: 0.5511\n",
      "Epoch 39/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.4951 - acc: 0.4943 - val_loss: 1.4878 - val_acc: 0.5455\n",
      "Epoch 40/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.5288 - acc: 0.4772 - val_loss: 1.4474 - val_acc: 0.5398\n",
      "Epoch 41/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 1.4919 - acc: 0.4758 - val_loss: 1.4580 - val_acc: 0.5227\n",
      "Epoch 42/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 1.4839 - acc: 0.4972 - val_loss: 1.4250 - val_acc: 0.5568\n",
      "Epoch 43/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 1.4316 - acc: 0.5043 - val_loss: 1.4487 - val_acc: 0.5682\n",
      "Epoch 44/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.4482 - acc: 0.4986 - val_loss: 1.4355 - val_acc: 0.5625\n",
      "Epoch 45/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.4905 - acc: 0.4858 - val_loss: 1.4154 - val_acc: 0.5682\n",
      "Epoch 46/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.4522 - acc: 0.5157 - val_loss: 1.4204 - val_acc: 0.5511\n",
      "Epoch 47/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 1.4093 - acc: 0.5370 - val_loss: 1.3895 - val_acc: 0.5568\n",
      "Epoch 48/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 1.3890 - acc: 0.5256 - val_loss: 1.3743 - val_acc: 0.5739\n",
      "Epoch 49/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.4456 - acc: 0.5228 - val_loss: 1.3971 - val_acc: 0.5739\n",
      "Epoch 50/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.4272 - acc: 0.5085 - val_loss: 1.3629 - val_acc: 0.5682\n",
      "Epoch 51/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 1.4151 - acc: 0.5442 - val_loss: 1.3691 - val_acc: 0.5909\n",
      "Epoch 52/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 1.3978 - acc: 0.5271 - val_loss: 1.3689 - val_acc: 0.5625\n",
      "Epoch 53/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.3836 - acc: 0.5271 - val_loss: 1.3307 - val_acc: 0.5739\n",
      "Epoch 54/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.3469 - acc: 0.5484 - val_loss: 1.3258 - val_acc: 0.5739\n",
      "Epoch 55/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.3005 - acc: 0.5484 - val_loss: 1.3286 - val_acc: 0.5909\n",
      "Epoch 56/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.3207 - acc: 0.5484 - val_loss: 1.3198 - val_acc: 0.5909\n",
      "Epoch 57/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 1.3573 - acc: 0.5413 - val_loss: 1.3533 - val_acc: 0.5852\n",
      "Epoch 58/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.3357 - acc: 0.5370 - val_loss: 1.3007 - val_acc: 0.5909\n",
      "Epoch 59/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.3644 - acc: 0.5385 - val_loss: 1.3101 - val_acc: 0.5795\n",
      "Epoch 60/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.3173 - acc: 0.5356 - val_loss: 1.2959 - val_acc: 0.5966\n",
      "Epoch 61/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 1.3198 - acc: 0.5370 - val_loss: 1.2744 - val_acc: 0.5909\n",
      "Epoch 62/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 1.3223 - acc: 0.5484 - val_loss: 1.3101 - val_acc: 0.5739\n",
      "Epoch 63/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.3187 - acc: 0.5556 - val_loss: 1.2922 - val_acc: 0.5966\n",
      "Epoch 64/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.2629 - acc: 0.5598 - val_loss: 1.2710 - val_acc: 0.5795\n",
      "Epoch 65/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.2866 - acc: 0.5427 - val_loss: 1.2621 - val_acc: 0.6023\n",
      "Epoch 66/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 1.2777 - acc: 0.5698 - val_loss: 1.2709 - val_acc: 0.6136\n",
      "Epoch 67/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.3092 - acc: 0.5499 - val_loss: 1.2652 - val_acc: 0.5909\n",
      "Epoch 68/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.3192 - acc: 0.5556 - val_loss: 1.2788 - val_acc: 0.5852\n",
      "Epoch 69/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.2835 - acc: 0.5570 - val_loss: 1.2544 - val_acc: 0.5909\n",
      "Epoch 70/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.2252 - acc: 0.5826 - val_loss: 1.2484 - val_acc: 0.5909\n",
      "Epoch 71/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 1.2399 - acc: 0.5726 - val_loss: 1.2172 - val_acc: 0.6023\n",
      "Epoch 72/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 1.2048 - acc: 0.5812 - val_loss: 1.2644 - val_acc: 0.5795\n",
      "Epoch 73/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 1.2156 - acc: 0.5840 - val_loss: 1.2969 - val_acc: 0.5739\n",
      "Epoch 74/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 1.3060 - acc: 0.5641 - val_loss: 1.2580 - val_acc: 0.5795\n",
      "Epoch 75/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 1.2811 - acc: 0.5456 - val_loss: 1.2661 - val_acc: 0.5966\n",
      "Epoch 76/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 1.2711 - acc: 0.5613 - val_loss: 1.2606 - val_acc: 0.5795\n",
      "Epoch 77/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.2222 - acc: 0.5755 - val_loss: 1.2050 - val_acc: 0.5966\n",
      "Epoch 78/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.2043 - acc: 0.6040 - val_loss: 1.2260 - val_acc: 0.5966\n",
      "Epoch 79/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.2653 - acc: 0.5670 - val_loss: 1.2401 - val_acc: 0.5909\n",
      "Epoch 80/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.2519 - acc: 0.5556 - val_loss: 1.2262 - val_acc: 0.5909\n",
      "Epoch 81/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 1.2571 - acc: 0.5698 - val_loss: 1.2285 - val_acc: 0.5966\n",
      "Epoch 82/3000\n",
      "702/702 [==============================] - 0s 567us/sample - loss: 1.1973 - acc: 0.5855 - val_loss: 1.2085 - val_acc: 0.6136\n",
      "Epoch 83/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.2506 - acc: 0.5484 - val_loss: 1.1523 - val_acc: 0.6023\n",
      "Epoch 84/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.2080 - acc: 0.5812 - val_loss: 1.2086 - val_acc: 0.5966\n",
      "Epoch 85/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 1.1984 - acc: 0.5897 - val_loss: 1.1904 - val_acc: 0.6364\n",
      "Epoch 86/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.1834 - acc: 0.5840 - val_loss: 1.1969 - val_acc: 0.5966\n",
      "Epoch 87/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.1656 - acc: 0.5983 - val_loss: 1.1715 - val_acc: 0.6023\n",
      "Epoch 88/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 1.1796 - acc: 0.6140 - val_loss: 1.1724 - val_acc: 0.6080\n",
      "Epoch 89/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.1104 - acc: 0.6125 - val_loss: 1.1448 - val_acc: 0.6080\n",
      "Epoch 90/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.1550 - acc: 0.6097 - val_loss: 1.2770 - val_acc: 0.5682\n",
      "Epoch 91/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 1.2405 - acc: 0.5741 - val_loss: 1.1765 - val_acc: 0.6023\n",
      "Epoch 92/3000\n",
      "702/702 [==============================] - 0s 569us/sample - loss: 1.1579 - acc: 0.5954 - val_loss: 1.1896 - val_acc: 0.6023\n",
      "Epoch 93/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.1922 - acc: 0.5940 - val_loss: 1.1495 - val_acc: 0.6080\n",
      "Epoch 94/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.1770 - acc: 0.5926 - val_loss: 1.1677 - val_acc: 0.6023\n",
      "Epoch 95/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 1.1425 - acc: 0.5926 - val_loss: 1.1406 - val_acc: 0.6193\n",
      "Epoch 96/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.1292 - acc: 0.6140 - val_loss: 1.1456 - val_acc: 0.6080\n",
      "Epoch 97/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.0984 - acc: 0.6111 - val_loss: 1.1235 - val_acc: 0.6250\n",
      "Epoch 98/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.0981 - acc: 0.6282 - val_loss: 1.1278 - val_acc: 0.6080\n",
      "Epoch 99/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 1.1223 - acc: 0.6154 - val_loss: 1.1142 - val_acc: 0.6136\n",
      "Epoch 100/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 1.1073 - acc: 0.6111 - val_loss: 1.1340 - val_acc: 0.6080\n",
      "Epoch 101/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 1.1184 - acc: 0.6097 - val_loss: 1.1193 - val_acc: 0.6193\n",
      "Epoch 102/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 1.0737 - acc: 0.6425 - val_loss: 1.1006 - val_acc: 0.6080\n",
      "Epoch 103/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.1174 - acc: 0.5983 - val_loss: 1.1110 - val_acc: 0.6136\n",
      "Epoch 104/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.1102 - acc: 0.6410 - val_loss: 1.1027 - val_acc: 0.6136\n",
      "Epoch 105/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 1.0522 - acc: 0.6510 - val_loss: 1.0866 - val_acc: 0.6023\n",
      "Epoch 106/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 1.0499 - acc: 0.6254 - val_loss: 1.0798 - val_acc: 0.6136\n",
      "Epoch 107/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.0493 - acc: 0.6239 - val_loss: 1.0996 - val_acc: 0.6080\n",
      "Epoch 108/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.0383 - acc: 0.6396 - val_loss: 1.1129 - val_acc: 0.6080\n",
      "Epoch 109/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.0934 - acc: 0.6239 - val_loss: 1.1085 - val_acc: 0.6420\n",
      "Epoch 110/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.0352 - acc: 0.6481 - val_loss: 1.0901 - val_acc: 0.6136\n",
      "Epoch 111/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 1.0485 - acc: 0.6325 - val_loss: 1.0918 - val_acc: 0.6250\n",
      "Epoch 112/3000\n",
      "702/702 [==============================] - 0s 577us/sample - loss: 1.0410 - acc: 0.6368 - val_loss: 1.1134 - val_acc: 0.6193\n",
      "Epoch 113/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.0200 - acc: 0.6296 - val_loss: 1.1064 - val_acc: 0.6023\n",
      "Epoch 114/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.0710 - acc: 0.6282 - val_loss: 1.1296 - val_acc: 0.6080\n",
      "Epoch 115/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.0278 - acc: 0.6439 - val_loss: 1.0944 - val_acc: 0.6136\n",
      "Epoch 116/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.0509 - acc: 0.6239 - val_loss: 1.0470 - val_acc: 0.6193\n",
      "Epoch 117/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.0274 - acc: 0.6339 - val_loss: 1.0795 - val_acc: 0.6136\n",
      "Epoch 118/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.0291 - acc: 0.6439 - val_loss: 1.1110 - val_acc: 0.6080\n",
      "Epoch 119/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.0204 - acc: 0.6282 - val_loss: 1.0792 - val_acc: 0.6250\n",
      "Epoch 120/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.0132 - acc: 0.6553 - val_loss: 1.0962 - val_acc: 0.6193\n",
      "Epoch 121/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 1.0377 - acc: 0.6311 - val_loss: 1.1146 - val_acc: 0.6080\n",
      "Epoch 122/3000\n",
      "702/702 [==============================] - 0s 567us/sample - loss: 1.0355 - acc: 0.6481 - val_loss: 1.0641 - val_acc: 0.6364\n",
      "Epoch 123/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 1.0278 - acc: 0.6439 - val_loss: 1.0976 - val_acc: 0.6136\n",
      "Epoch 124/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.9889 - acc: 0.6610 - val_loss: 1.0393 - val_acc: 0.6420\n",
      "Epoch 125/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.9750 - acc: 0.6439 - val_loss: 1.0366 - val_acc: 0.6250\n",
      "Epoch 126/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 1.0402 - acc: 0.6239 - val_loss: 1.0435 - val_acc: 0.6193\n",
      "Epoch 127/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 1.0119 - acc: 0.6382 - val_loss: 1.0499 - val_acc: 0.6250\n",
      "Epoch 128/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.9965 - acc: 0.6538 - val_loss: 1.0927 - val_acc: 0.6136\n",
      "Epoch 129/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.9957 - acc: 0.6481 - val_loss: 1.1229 - val_acc: 0.5909\n",
      "Epoch 130/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.1688 - acc: 0.5798 - val_loss: 1.2410 - val_acc: 0.5398\n",
      "Epoch 131/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 1.2334 - acc: 0.5456 - val_loss: 1.2113 - val_acc: 0.5739\n",
      "Epoch 132/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 1.1268 - acc: 0.6083 - val_loss: 1.1638 - val_acc: 0.6023\n",
      "Epoch 133/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 1.1284 - acc: 0.6026 - val_loss: 1.0687 - val_acc: 0.6250\n",
      "Epoch 134/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 1.1367 - acc: 0.6311 - val_loss: 1.0715 - val_acc: 0.6534\n",
      "Epoch 135/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 1.0789 - acc: 0.6197 - val_loss: 1.1189 - val_acc: 0.6193\n",
      "Epoch 136/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 1.0795 - acc: 0.5983 - val_loss: 1.0866 - val_acc: 0.6080\n",
      "Epoch 137/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 1.0713 - acc: 0.6368 - val_loss: 1.0884 - val_acc: 0.6080\n",
      "Epoch 138/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 1.0241 - acc: 0.6510 - val_loss: 1.0575 - val_acc: 0.6193\n",
      "Epoch 139/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 1.0364 - acc: 0.6368 - val_loss: 1.1158 - val_acc: 0.6136\n",
      "Epoch 140/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 1.0615 - acc: 0.6339 - val_loss: 1.0999 - val_acc: 0.6193\n",
      "Epoch 141/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 1.0145 - acc: 0.6268 - val_loss: 1.0657 - val_acc: 0.6364\n",
      "Epoch 142/3000\n",
      "702/702 [==============================] - 0s 556us/sample - loss: 1.0352 - acc: 0.6325 - val_loss: 1.0822 - val_acc: 0.6364\n",
      "Epoch 143/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 1.0398 - acc: 0.6396 - val_loss: 1.0608 - val_acc: 0.6250\n",
      "Epoch 144/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.9757 - acc: 0.6368 - val_loss: 1.0418 - val_acc: 0.6420\n",
      "Epoch 145/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.9952 - acc: 0.6581 - val_loss: 1.0851 - val_acc: 0.6080\n",
      "Epoch 146/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 1.0321 - acc: 0.6311 - val_loss: 1.0396 - val_acc: 0.6364\n",
      "Epoch 147/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.9862 - acc: 0.6410 - val_loss: 1.0233 - val_acc: 0.6307\n",
      "Epoch 148/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.0152 - acc: 0.6382 - val_loss: 1.0279 - val_acc: 0.6250\n",
      "Epoch 149/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 1.0053 - acc: 0.6311 - val_loss: 1.0375 - val_acc: 0.6307\n",
      "Epoch 150/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.9459 - acc: 0.6567 - val_loss: 1.0453 - val_acc: 0.6307\n",
      "Epoch 151/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.9759 - acc: 0.6610 - val_loss: 1.0387 - val_acc: 0.6193\n",
      "Epoch 152/3000\n",
      "702/702 [==============================] - 0s 573us/sample - loss: 0.9587 - acc: 0.6524 - val_loss: 1.0236 - val_acc: 0.6591\n",
      "Epoch 153/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.9809 - acc: 0.6538 - val_loss: 1.0355 - val_acc: 0.6420\n",
      "Epoch 154/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.9503 - acc: 0.6638 - val_loss: 1.0363 - val_acc: 0.6420\n",
      "Epoch 155/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.9444 - acc: 0.6681 - val_loss: 1.0363 - val_acc: 0.6307\n",
      "Epoch 156/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.9517 - acc: 0.6610 - val_loss: 1.0218 - val_acc: 0.6534\n",
      "Epoch 157/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.9616 - acc: 0.6510 - val_loss: 1.0399 - val_acc: 0.6250\n",
      "Epoch 158/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.9636 - acc: 0.6624 - val_loss: 1.0292 - val_acc: 0.6307\n",
      "Epoch 159/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.9125 - acc: 0.6667 - val_loss: 1.0391 - val_acc: 0.6307\n",
      "Epoch 160/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.9076 - acc: 0.6823 - val_loss: 1.0850 - val_acc: 0.6080\n",
      "Epoch 161/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.9560 - acc: 0.6510 - val_loss: 1.0185 - val_acc: 0.6477\n",
      "Epoch 162/3000\n",
      "702/702 [==============================] - 0s 569us/sample - loss: 0.9759 - acc: 0.6581 - val_loss: 1.0464 - val_acc: 0.6136\n",
      "Epoch 163/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 1.0022 - acc: 0.6581 - val_loss: 1.0116 - val_acc: 0.6477\n",
      "Epoch 164/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.9590 - acc: 0.6553 - val_loss: 1.0631 - val_acc: 0.6364\n",
      "Epoch 165/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.9421 - acc: 0.6667 - val_loss: 1.0279 - val_acc: 0.6477\n",
      "Epoch 166/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.9469 - acc: 0.6752 - val_loss: 1.0663 - val_acc: 0.6023\n",
      "Epoch 167/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.9538 - acc: 0.6652 - val_loss: 1.0243 - val_acc: 0.6250\n",
      "Epoch 168/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.9634 - acc: 0.6538 - val_loss: 1.0482 - val_acc: 0.6193\n",
      "Epoch 169/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.9523 - acc: 0.6695 - val_loss: 0.9949 - val_acc: 0.6420\n",
      "Epoch 170/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.9081 - acc: 0.6752 - val_loss: 0.9951 - val_acc: 0.6648\n",
      "Epoch 171/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.9329 - acc: 0.6838 - val_loss: 1.0002 - val_acc: 0.6364\n",
      "Epoch 172/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.9520 - acc: 0.6766 - val_loss: 1.0398 - val_acc: 0.6250\n",
      "Epoch 173/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.9270 - acc: 0.6752 - val_loss: 1.0133 - val_acc: 0.6420\n",
      "Epoch 174/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.9245 - acc: 0.6709 - val_loss: 1.0147 - val_acc: 0.6420\n",
      "Epoch 175/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.9327 - acc: 0.6766 - val_loss: 1.0053 - val_acc: 0.6364\n",
      "Epoch 176/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.9425 - acc: 0.6681 - val_loss: 0.9838 - val_acc: 0.6705\n",
      "Epoch 177/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.9016 - acc: 0.6809 - val_loss: 0.9878 - val_acc: 0.6534\n",
      "Epoch 178/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.9173 - acc: 0.6681 - val_loss: 1.0251 - val_acc: 0.6534\n",
      "Epoch 179/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.9073 - acc: 0.6866 - val_loss: 1.0130 - val_acc: 0.6193\n",
      "Epoch 180/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.9801 - acc: 0.6524 - val_loss: 1.0079 - val_acc: 0.6420\n",
      "Epoch 181/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.9197 - acc: 0.6809 - val_loss: 1.0398 - val_acc: 0.6193\n",
      "Epoch 182/3000\n",
      "702/702 [==============================] - 0s 567us/sample - loss: 0.9123 - acc: 0.6937 - val_loss: 0.9996 - val_acc: 0.6420\n",
      "Epoch 183/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.9171 - acc: 0.6880 - val_loss: 0.9519 - val_acc: 0.6534\n",
      "Epoch 184/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.9124 - acc: 0.6895 - val_loss: 0.9830 - val_acc: 0.6648\n",
      "Epoch 185/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.9019 - acc: 0.6866 - val_loss: 0.9890 - val_acc: 0.6307\n",
      "Epoch 186/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.8808 - acc: 0.6795 - val_loss: 0.9548 - val_acc: 0.6534\n",
      "Epoch 187/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.8707 - acc: 0.6980 - val_loss: 0.9878 - val_acc: 0.6477\n",
      "Epoch 188/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.8721 - acc: 0.6781 - val_loss: 0.9863 - val_acc: 0.6534\n",
      "Epoch 189/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.8676 - acc: 0.7080 - val_loss: 0.9682 - val_acc: 0.6420\n",
      "Epoch 190/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.8643 - acc: 0.6866 - val_loss: 0.9597 - val_acc: 0.6818\n",
      "Epoch 191/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.9128 - acc: 0.6781 - val_loss: 1.0133 - val_acc: 0.6420\n",
      "Epoch 192/3000\n",
      "702/702 [==============================] - 0s 573us/sample - loss: 0.8695 - acc: 0.6994 - val_loss: 1.0415 - val_acc: 0.6193\n",
      "Epoch 193/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.8452 - acc: 0.7137 - val_loss: 0.9688 - val_acc: 0.6477\n",
      "Epoch 194/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.8186 - acc: 0.7066 - val_loss: 0.9793 - val_acc: 0.6648\n",
      "Epoch 195/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.8576 - acc: 0.6880 - val_loss: 0.9849 - val_acc: 0.6307\n",
      "Epoch 196/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.8724 - acc: 0.7094 - val_loss: 1.0273 - val_acc: 0.5909\n",
      "Epoch 197/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.8329 - acc: 0.6923 - val_loss: 0.9891 - val_acc: 0.6080\n",
      "Epoch 198/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.8568 - acc: 0.6909 - val_loss: 0.9802 - val_acc: 0.6364\n",
      "Epoch 199/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.8543 - acc: 0.7137 - val_loss: 0.9833 - val_acc: 0.6534\n",
      "Epoch 200/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.8643 - acc: 0.6980 - val_loss: 0.9550 - val_acc: 0.6534\n",
      "Epoch 201/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.9253 - acc: 0.6738 - val_loss: 1.0662 - val_acc: 0.6136\n",
      "Epoch 202/3000\n",
      "702/702 [==============================] - 0s 573us/sample - loss: 0.9469 - acc: 0.6781 - val_loss: 1.0059 - val_acc: 0.6364\n",
      "Epoch 203/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.9059 - acc: 0.6709 - val_loss: 1.0042 - val_acc: 0.6477\n",
      "Epoch 204/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.9343 - acc: 0.6766 - val_loss: 0.9829 - val_acc: 0.6534\n",
      "Epoch 205/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.9828 - acc: 0.6595 - val_loss: 1.0889 - val_acc: 0.5966\n",
      "Epoch 206/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 1.0224 - acc: 0.6538 - val_loss: 1.1376 - val_acc: 0.5795\n",
      "Epoch 207/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 1.0331 - acc: 0.6296 - val_loss: 1.1141 - val_acc: 0.6193\n",
      "Epoch 208/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.9718 - acc: 0.6681 - val_loss: 1.0626 - val_acc: 0.6136\n",
      "Epoch 209/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.9455 - acc: 0.6809 - val_loss: 1.0001 - val_acc: 0.6307\n",
      "Epoch 210/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.8948 - acc: 0.6937 - val_loss: 0.9709 - val_acc: 0.6591\n",
      "Epoch 211/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.8502 - acc: 0.7080 - val_loss: 0.9672 - val_acc: 0.6591\n",
      "Epoch 212/3000\n",
      "702/702 [==============================] - 0s 567us/sample - loss: 0.8972 - acc: 0.6994 - val_loss: 0.9658 - val_acc: 0.6648\n",
      "Epoch 213/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.8221 - acc: 0.6966 - val_loss: 0.9837 - val_acc: 0.6477\n",
      "Epoch 214/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.8413 - acc: 0.6838 - val_loss: 0.9495 - val_acc: 0.6705\n",
      "Epoch 215/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.8624 - acc: 0.7108 - val_loss: 1.0283 - val_acc: 0.6136\n",
      "Epoch 216/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.8190 - acc: 0.7165 - val_loss: 0.9965 - val_acc: 0.6307\n",
      "Epoch 217/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.8041 - acc: 0.7137 - val_loss: 0.9665 - val_acc: 0.6420\n",
      "Epoch 218/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.7891 - acc: 0.7165 - val_loss: 0.9550 - val_acc: 0.6420\n",
      "Epoch 219/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.8043 - acc: 0.7179 - val_loss: 0.9548 - val_acc: 0.6534\n",
      "Epoch 220/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.8146 - acc: 0.7165 - val_loss: 0.9946 - val_acc: 0.6307\n",
      "Epoch 221/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.8131 - acc: 0.7165 - val_loss: 0.9807 - val_acc: 0.6307\n",
      "Epoch 222/3000\n",
      "702/702 [==============================] - 0s 569us/sample - loss: 0.7844 - acc: 0.7236 - val_loss: 0.9307 - val_acc: 0.6534\n",
      "Epoch 223/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.8225 - acc: 0.7194 - val_loss: 0.9416 - val_acc: 0.6534\n",
      "Epoch 224/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.7760 - acc: 0.7236 - val_loss: 0.9666 - val_acc: 0.6420\n",
      "Epoch 225/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.7928 - acc: 0.7123 - val_loss: 0.9343 - val_acc: 0.6591\n",
      "Epoch 226/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.7701 - acc: 0.7550 - val_loss: 0.9377 - val_acc: 0.6250\n",
      "Epoch 227/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.8036 - acc: 0.7236 - val_loss: 0.9578 - val_acc: 0.6534\n",
      "Epoch 228/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.7765 - acc: 0.7407 - val_loss: 0.9681 - val_acc: 0.6193\n",
      "Epoch 229/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.8177 - acc: 0.7137 - val_loss: 0.9948 - val_acc: 0.6364\n",
      "Epoch 230/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.7981 - acc: 0.7108 - val_loss: 1.0267 - val_acc: 0.6364\n",
      "Epoch 231/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.8076 - acc: 0.7080 - val_loss: 0.9707 - val_acc: 0.6477\n",
      "Epoch 232/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 0.7700 - acc: 0.7422 - val_loss: 0.9794 - val_acc: 0.6534\n",
      "Epoch 233/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.7900 - acc: 0.7350 - val_loss: 0.9473 - val_acc: 0.6534\n",
      "Epoch 234/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.7821 - acc: 0.7308 - val_loss: 0.9389 - val_acc: 0.6534\n",
      "Epoch 235/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.7505 - acc: 0.7322 - val_loss: 0.9416 - val_acc: 0.6761\n",
      "Epoch 236/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.7382 - acc: 0.7436 - val_loss: 0.9699 - val_acc: 0.6591\n",
      "Epoch 237/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.7595 - acc: 0.7251 - val_loss: 0.9569 - val_acc: 0.6420\n",
      "Epoch 238/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.7464 - acc: 0.7322 - val_loss: 0.9608 - val_acc: 0.6591\n",
      "Epoch 239/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.6916 - acc: 0.7678 - val_loss: 0.9762 - val_acc: 0.6364\n",
      "Epoch 240/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.7130 - acc: 0.7550 - val_loss: 0.9585 - val_acc: 0.6534\n",
      "Epoch 241/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.7086 - acc: 0.7664 - val_loss: 0.9425 - val_acc: 0.6591\n",
      "Epoch 242/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.7341 - acc: 0.7450 - val_loss: 0.9178 - val_acc: 0.6989\n",
      "Epoch 243/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.7115 - acc: 0.7422 - val_loss: 0.9661 - val_acc: 0.6420\n",
      "Epoch 244/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.7422 - acc: 0.7479 - val_loss: 0.9971 - val_acc: 0.6534\n",
      "Epoch 245/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.7366 - acc: 0.7365 - val_loss: 0.9559 - val_acc: 0.6591\n",
      "Epoch 246/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.6869 - acc: 0.7664 - val_loss: 0.9482 - val_acc: 0.6477\n",
      "Epoch 247/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.6862 - acc: 0.7707 - val_loss: 0.9517 - val_acc: 0.6534\n",
      "Epoch 248/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.6988 - acc: 0.7692 - val_loss: 0.9316 - val_acc: 0.6591\n",
      "Epoch 249/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.7372 - acc: 0.7536 - val_loss: 0.9502 - val_acc: 0.6534\n",
      "Epoch 250/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.7100 - acc: 0.7550 - val_loss: 0.9368 - val_acc: 0.6705\n",
      "Epoch 251/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.6682 - acc: 0.7664 - val_loss: 0.9167 - val_acc: 0.6818\n",
      "Epoch 252/3000\n",
      "702/702 [==============================] - 0s 570us/sample - loss: 0.7242 - acc: 0.7692 - val_loss: 0.9727 - val_acc: 0.6364\n",
      "Epoch 253/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.6895 - acc: 0.7536 - val_loss: 0.9552 - val_acc: 0.6420\n",
      "Epoch 254/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.7072 - acc: 0.7721 - val_loss: 0.9443 - val_acc: 0.6534\n",
      "Epoch 255/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.6326 - acc: 0.7863 - val_loss: 0.9525 - val_acc: 0.6534\n",
      "Epoch 256/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.6872 - acc: 0.7963 - val_loss: 0.9547 - val_acc: 0.6705\n",
      "Epoch 257/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.7047 - acc: 0.7578 - val_loss: 0.9803 - val_acc: 0.6307\n",
      "Epoch 258/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.7002 - acc: 0.7806 - val_loss: 0.9439 - val_acc: 0.6420\n",
      "Epoch 259/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.6956 - acc: 0.7607 - val_loss: 0.9509 - val_acc: 0.6534\n",
      "Epoch 260/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.6957 - acc: 0.7635 - val_loss: 0.9623 - val_acc: 0.6648\n",
      "Epoch 261/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.6561 - acc: 0.7721 - val_loss: 0.9572 - val_acc: 0.6591\n",
      "Epoch 262/3000\n",
      "702/702 [==============================] - 0s 570us/sample - loss: 0.6633 - acc: 0.7849 - val_loss: 0.9679 - val_acc: 0.6534\n",
      "Epoch 263/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.6787 - acc: 0.7749 - val_loss: 0.9471 - val_acc: 0.6534\n",
      "Epoch 264/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.6488 - acc: 0.7920 - val_loss: 0.9371 - val_acc: 0.6705\n",
      "Epoch 265/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.6674 - acc: 0.7792 - val_loss: 0.9312 - val_acc: 0.6818\n",
      "Epoch 266/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.6342 - acc: 0.7707 - val_loss: 0.9482 - val_acc: 0.6420\n",
      "Epoch 267/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.6721 - acc: 0.7749 - val_loss: 0.9727 - val_acc: 0.6420\n",
      "Epoch 268/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.6833 - acc: 0.7920 - val_loss: 0.9453 - val_acc: 0.6534\n",
      "Epoch 269/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.6660 - acc: 0.7635 - val_loss: 0.9609 - val_acc: 0.6420\n",
      "Epoch 270/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.6509 - acc: 0.7707 - val_loss: 0.9372 - val_acc: 0.6307\n",
      "Epoch 271/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.6526 - acc: 0.7778 - val_loss: 0.9531 - val_acc: 0.6534\n",
      "Epoch 272/3000\n",
      "702/702 [==============================] - 0s 580us/sample - loss: 0.6740 - acc: 0.7450 - val_loss: 0.9546 - val_acc: 0.6648\n",
      "Epoch 273/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.6223 - acc: 0.8134 - val_loss: 0.9269 - val_acc: 0.6875\n",
      "Epoch 274/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.6440 - acc: 0.8063 - val_loss: 0.9421 - val_acc: 0.6648\n",
      "Epoch 275/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.5790 - acc: 0.8020 - val_loss: 0.9345 - val_acc: 0.6761\n",
      "Epoch 276/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.5988 - acc: 0.8120 - val_loss: 0.9438 - val_acc: 0.6420\n",
      "Epoch 277/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.5707 - acc: 0.8063 - val_loss: 0.8941 - val_acc: 0.6705\n",
      "Epoch 278/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.6155 - acc: 0.7963 - val_loss: 0.9583 - val_acc: 0.6307\n",
      "Epoch 279/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.6011 - acc: 0.7963 - val_loss: 0.9467 - val_acc: 0.6534\n",
      "Epoch 280/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.6620 - acc: 0.7863 - val_loss: 0.9474 - val_acc: 0.6477\n",
      "Epoch 281/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.6484 - acc: 0.7920 - val_loss: 0.9386 - val_acc: 0.6477\n",
      "Epoch 282/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.6662 - acc: 0.7906 - val_loss: 0.9662 - val_acc: 0.6477\n",
      "Epoch 283/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.6813 - acc: 0.7806 - val_loss: 0.9652 - val_acc: 0.6420\n",
      "Epoch 284/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.6402 - acc: 0.7778 - val_loss: 0.9792 - val_acc: 0.6534\n",
      "Epoch 285/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.6216 - acc: 0.8105 - val_loss: 0.9843 - val_acc: 0.6761\n",
      "Epoch 286/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.7091 - acc: 0.7906 - val_loss: 0.9887 - val_acc: 0.6591\n",
      "Epoch 287/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.6851 - acc: 0.7749 - val_loss: 0.9695 - val_acc: 0.6705\n",
      "Epoch 288/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.6564 - acc: 0.7806 - val_loss: 0.9767 - val_acc: 0.6250\n",
      "Epoch 289/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.6313 - acc: 0.7877 - val_loss: 0.9663 - val_acc: 0.6364\n",
      "Epoch 290/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.6163 - acc: 0.7920 - val_loss: 0.9183 - val_acc: 0.6648\n",
      "Epoch 291/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.6451 - acc: 0.7806 - val_loss: 0.9162 - val_acc: 0.6761\n",
      "Epoch 292/3000\n",
      "702/702 [==============================] - 0s 573us/sample - loss: 0.6702 - acc: 0.7721 - val_loss: 0.9614 - val_acc: 0.6591\n",
      "Epoch 293/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.6084 - acc: 0.8063 - val_loss: 0.9509 - val_acc: 0.6761\n",
      "Epoch 294/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.6515 - acc: 0.7906 - val_loss: 0.9509 - val_acc: 0.6534\n",
      "Epoch 295/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.6212 - acc: 0.8091 - val_loss: 0.9840 - val_acc: 0.6477\n",
      "Epoch 296/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.6074 - acc: 0.8148 - val_loss: 0.9646 - val_acc: 0.6591\n",
      "Epoch 297/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.6367 - acc: 0.7934 - val_loss: 0.9452 - val_acc: 0.6534\n",
      "Epoch 298/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.6246 - acc: 0.8077 - val_loss: 0.9291 - val_acc: 0.6705\n",
      "Epoch 299/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.5984 - acc: 0.7920 - val_loss: 0.9575 - val_acc: 0.6648\n",
      "Epoch 300/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.6296 - acc: 0.7934 - val_loss: 0.9302 - val_acc: 0.6818\n",
      "Epoch 301/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.5477 - acc: 0.8219 - val_loss: 0.9257 - val_acc: 0.6818\n",
      "Epoch 302/3000\n",
      "702/702 [==============================] - 0s 556us/sample - loss: 0.5628 - acc: 0.8120 - val_loss: 0.9401 - val_acc: 0.6591\n",
      "Epoch 303/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.5133 - acc: 0.8362 - val_loss: 0.9366 - val_acc: 0.6591\n",
      "Epoch 304/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.5002 - acc: 0.8376 - val_loss: 0.9381 - val_acc: 0.6534\n",
      "Epoch 305/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.5281 - acc: 0.8333 - val_loss: 0.9608 - val_acc: 0.6420\n",
      "Epoch 306/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.5256 - acc: 0.8419 - val_loss: 0.9287 - val_acc: 0.6761\n",
      "Epoch 307/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.5463 - acc: 0.8234 - val_loss: 0.9382 - val_acc: 0.6477\n",
      "Epoch 308/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.5295 - acc: 0.8376 - val_loss: 0.9482 - val_acc: 0.6477\n",
      "Epoch 309/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.5342 - acc: 0.8276 - val_loss: 0.9515 - val_acc: 0.6534\n",
      "Epoch 310/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.5514 - acc: 0.8504 - val_loss: 0.9541 - val_acc: 0.6591\n",
      "Epoch 311/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.5433 - acc: 0.8305 - val_loss: 0.9696 - val_acc: 0.6591\n",
      "Epoch 312/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.5490 - acc: 0.8490 - val_loss: 0.9342 - val_acc: 0.6875\n",
      "Epoch 313/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.5980 - acc: 0.8063 - val_loss: 0.9308 - val_acc: 0.6591\n",
      "Epoch 314/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.5376 - acc: 0.8291 - val_loss: 0.9383 - val_acc: 0.6648\n",
      "Epoch 315/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.5220 - acc: 0.8291 - val_loss: 0.9304 - val_acc: 0.6591\n",
      "Epoch 316/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.5235 - acc: 0.8490 - val_loss: 0.9427 - val_acc: 0.6648\n",
      "Epoch 317/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.5099 - acc: 0.8462 - val_loss: 0.9176 - val_acc: 0.6591\n",
      "Epoch 318/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.5213 - acc: 0.8248 - val_loss: 0.9312 - val_acc: 0.6648\n",
      "Epoch 319/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.5082 - acc: 0.8390 - val_loss: 0.9224 - val_acc: 0.6591\n",
      "Epoch 320/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.5155 - acc: 0.8433 - val_loss: 0.9112 - val_acc: 0.6761\n",
      "Epoch 321/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.5033 - acc: 0.8390 - val_loss: 0.9713 - val_acc: 0.6534\n",
      "Epoch 322/3000\n",
      "702/702 [==============================] - 0s 578us/sample - loss: 0.5492 - acc: 0.8219 - val_loss: 0.9403 - val_acc: 0.6705\n",
      "Epoch 323/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.5145 - acc: 0.8433 - val_loss: 0.9690 - val_acc: 0.6307\n",
      "Epoch 324/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.4947 - acc: 0.8575 - val_loss: 0.9556 - val_acc: 0.6420\n",
      "Epoch 325/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.5492 - acc: 0.8319 - val_loss: 0.9723 - val_acc: 0.6477\n",
      "Epoch 326/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.5329 - acc: 0.8162 - val_loss: 0.9319 - val_acc: 0.6818\n",
      "Epoch 327/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.5596 - acc: 0.8333 - val_loss: 0.9364 - val_acc: 0.6705\n",
      "Epoch 328/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.5243 - acc: 0.8362 - val_loss: 0.8916 - val_acc: 0.6932\n",
      "Epoch 329/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.5416 - acc: 0.8319 - val_loss: 0.9185 - val_acc: 0.6818\n",
      "Epoch 330/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.5648 - acc: 0.8248 - val_loss: 0.9653 - val_acc: 0.6250\n",
      "Epoch 331/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.5673 - acc: 0.8134 - val_loss: 0.9308 - val_acc: 0.6648\n",
      "Epoch 332/3000\n",
      "702/702 [==============================] - 0s 553us/sample - loss: 0.5605 - acc: 0.8248 - val_loss: 0.9367 - val_acc: 0.6591\n",
      "Epoch 333/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.5629 - acc: 0.8291 - val_loss: 0.9541 - val_acc: 0.6648\n",
      "Epoch 334/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.6283 - acc: 0.7892 - val_loss: 0.9490 - val_acc: 0.6534\n",
      "Epoch 335/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.6077 - acc: 0.8063 - val_loss: 0.9842 - val_acc: 0.6818\n",
      "Epoch 336/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.5197 - acc: 0.8319 - val_loss: 0.9228 - val_acc: 0.6989\n",
      "Epoch 337/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.5629 - acc: 0.8305 - val_loss: 0.9681 - val_acc: 0.6705\n",
      "Epoch 338/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.5508 - acc: 0.8205 - val_loss: 0.9395 - val_acc: 0.7045\n",
      "Epoch 339/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.5381 - acc: 0.8319 - val_loss: 0.8995 - val_acc: 0.6818\n",
      "Epoch 340/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.5579 - acc: 0.8276 - val_loss: 0.9534 - val_acc: 0.6818\n",
      "Epoch 341/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.5468 - acc: 0.8177 - val_loss: 0.9638 - val_acc: 0.6591\n",
      "Epoch 342/3000\n",
      "702/702 [==============================] - 0s 566us/sample - loss: 0.5277 - acc: 0.8419 - val_loss: 0.9558 - val_acc: 0.6648\n",
      "Epoch 343/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.5195 - acc: 0.8362 - val_loss: 0.9186 - val_acc: 0.6534\n",
      "Epoch 344/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.5580 - acc: 0.8433 - val_loss: 0.9508 - val_acc: 0.6477\n",
      "Epoch 345/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.4157 - acc: 0.8661 - val_loss: 0.9372 - val_acc: 0.6761\n",
      "Epoch 346/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.4429 - acc: 0.8732 - val_loss: 0.9443 - val_acc: 0.6705\n",
      "Epoch 347/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.5021 - acc: 0.8419 - val_loss: 0.9070 - val_acc: 0.6761\n",
      "Epoch 348/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.4991 - acc: 0.8433 - val_loss: 0.9053 - val_acc: 0.7045\n",
      "Epoch 349/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.4979 - acc: 0.8433 - val_loss: 0.9466 - val_acc: 0.6477\n",
      "Epoch 350/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.4752 - acc: 0.8647 - val_loss: 0.9193 - val_acc: 0.6875\n",
      "Epoch 351/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.4533 - acc: 0.8689 - val_loss: 0.9150 - val_acc: 0.6989\n",
      "Epoch 352/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.4727 - acc: 0.8618 - val_loss: 0.9770 - val_acc: 0.6534\n",
      "Epoch 353/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.5198 - acc: 0.8348 - val_loss: 0.9651 - val_acc: 0.6591\n",
      "Epoch 354/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4576 - acc: 0.8632 - val_loss: 0.9706 - val_acc: 0.6705\n",
      "Epoch 355/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4522 - acc: 0.8575 - val_loss: 0.9297 - val_acc: 0.6875\n",
      "Epoch 356/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.4491 - acc: 0.8590 - val_loss: 0.9115 - val_acc: 0.6818\n",
      "Epoch 357/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.4555 - acc: 0.8704 - val_loss: 0.9402 - val_acc: 0.6648\n",
      "Epoch 358/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.4311 - acc: 0.8761 - val_loss: 0.9128 - val_acc: 0.6591\n",
      "Epoch 359/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.4584 - acc: 0.8632 - val_loss: 0.8966 - val_acc: 0.6705\n",
      "Epoch 360/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.4467 - acc: 0.8561 - val_loss: 0.9397 - val_acc: 0.6591\n",
      "Epoch 361/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.3949 - acc: 0.8832 - val_loss: 0.8959 - val_acc: 0.6989\n",
      "Epoch 362/3000\n",
      "702/702 [==============================] - 0s 577us/sample - loss: 0.4403 - acc: 0.8761 - val_loss: 0.9564 - val_acc: 0.6648\n",
      "Epoch 363/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4658 - acc: 0.8632 - val_loss: 0.9903 - val_acc: 0.6591\n",
      "Epoch 364/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.4790 - acc: 0.8647 - val_loss: 0.9803 - val_acc: 0.6761\n",
      "Epoch 365/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4710 - acc: 0.8462 - val_loss: 0.9866 - val_acc: 0.6761\n",
      "Epoch 366/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.4554 - acc: 0.8590 - val_loss: 0.9299 - val_acc: 0.6818\n",
      "Epoch 367/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.4506 - acc: 0.8704 - val_loss: 0.9248 - val_acc: 0.6818\n",
      "Epoch 368/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.4186 - acc: 0.8718 - val_loss: 0.9103 - val_acc: 0.7102\n",
      "Epoch 369/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.4582 - acc: 0.8561 - val_loss: 0.9227 - val_acc: 0.6989\n",
      "Epoch 370/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.4362 - acc: 0.8803 - val_loss: 0.9170 - val_acc: 0.6818\n",
      "Epoch 371/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.5066 - acc: 0.8575 - val_loss: 0.9894 - val_acc: 0.6591\n",
      "Epoch 372/3000\n",
      "702/702 [==============================] - 0s 556us/sample - loss: 0.4769 - acc: 0.8561 - val_loss: 0.9966 - val_acc: 0.6761\n",
      "Epoch 373/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.4472 - acc: 0.8689 - val_loss: 0.9372 - val_acc: 0.6761\n",
      "Epoch 374/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.4329 - acc: 0.8746 - val_loss: 0.8874 - val_acc: 0.7045\n",
      "Epoch 375/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.4368 - acc: 0.8575 - val_loss: 0.9154 - val_acc: 0.6705\n",
      "Epoch 376/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.4631 - acc: 0.8561 - val_loss: 0.9627 - val_acc: 0.6761\n",
      "Epoch 377/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.4364 - acc: 0.8632 - val_loss: 0.9472 - val_acc: 0.6705\n",
      "Epoch 378/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.4106 - acc: 0.8789 - val_loss: 0.9122 - val_acc: 0.6761\n",
      "Epoch 379/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.4266 - acc: 0.8661 - val_loss: 0.9717 - val_acc: 0.6705\n",
      "Epoch 380/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.4204 - acc: 0.8704 - val_loss: 0.9958 - val_acc: 0.6761\n",
      "Epoch 381/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.4578 - acc: 0.8689 - val_loss: 0.9492 - val_acc: 0.6591\n",
      "Epoch 382/3000\n",
      "702/702 [==============================] - 0s 567us/sample - loss: 0.3915 - acc: 0.8889 - val_loss: 0.9304 - val_acc: 0.6705\n",
      "Epoch 383/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.4235 - acc: 0.8618 - val_loss: 0.9293 - val_acc: 0.6705\n",
      "Epoch 384/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.4164 - acc: 0.8860 - val_loss: 0.9500 - val_acc: 0.6648\n",
      "Epoch 385/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3654 - acc: 0.8932 - val_loss: 0.9059 - val_acc: 0.6932\n",
      "Epoch 386/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.3819 - acc: 0.8846 - val_loss: 0.9500 - val_acc: 0.6818\n",
      "Epoch 387/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.4095 - acc: 0.8889 - val_loss: 0.9195 - val_acc: 0.6932\n",
      "Epoch 388/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.4243 - acc: 0.8789 - val_loss: 0.9698 - val_acc: 0.6591\n",
      "Epoch 389/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.3685 - acc: 0.8889 - val_loss: 0.8868 - val_acc: 0.7159\n",
      "Epoch 390/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.4060 - acc: 0.8775 - val_loss: 0.9223 - val_acc: 0.6648\n",
      "Epoch 391/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.3735 - acc: 0.8917 - val_loss: 0.9649 - val_acc: 0.6705\n",
      "Epoch 392/3000\n",
      "702/702 [==============================] - 0s 578us/sample - loss: 0.3821 - acc: 0.8903 - val_loss: 0.9174 - val_acc: 0.6648\n",
      "Epoch 393/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.3742 - acc: 0.8917 - val_loss: 0.9003 - val_acc: 0.6705\n",
      "Epoch 394/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.3872 - acc: 0.8789 - val_loss: 0.9033 - val_acc: 0.6875\n",
      "Epoch 395/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.3745 - acc: 0.8917 - val_loss: 0.8761 - val_acc: 0.6875\n",
      "Epoch 396/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.4181 - acc: 0.8789 - val_loss: 0.8614 - val_acc: 0.6989\n",
      "Epoch 397/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.3635 - acc: 0.8932 - val_loss: 0.9785 - val_acc: 0.6591\n",
      "Epoch 398/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.4189 - acc: 0.8746 - val_loss: 0.9064 - val_acc: 0.7159\n",
      "Epoch 399/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.4019 - acc: 0.8818 - val_loss: 0.9215 - val_acc: 0.7045\n",
      "Epoch 400/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.4073 - acc: 0.8775 - val_loss: 0.9728 - val_acc: 0.6875\n",
      "Epoch 401/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.4030 - acc: 0.8846 - val_loss: 0.9182 - val_acc: 0.6875\n",
      "Epoch 402/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.4151 - acc: 0.8718 - val_loss: 0.9075 - val_acc: 0.7045\n",
      "Epoch 403/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.4006 - acc: 0.8832 - val_loss: 0.9000 - val_acc: 0.7045\n",
      "Epoch 404/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.4077 - acc: 0.8860 - val_loss: 0.8757 - val_acc: 0.6818\n",
      "Epoch 405/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3830 - acc: 0.8875 - val_loss: 0.8810 - val_acc: 0.7045\n",
      "Epoch 406/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3700 - acc: 0.8889 - val_loss: 0.9618 - val_acc: 0.6591\n",
      "Epoch 407/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.4665 - acc: 0.8575 - val_loss: 0.9391 - val_acc: 0.6818\n",
      "Epoch 408/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.4159 - acc: 0.8832 - val_loss: 0.9136 - val_acc: 0.7216\n",
      "Epoch 409/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.3999 - acc: 0.8889 - val_loss: 0.9796 - val_acc: 0.6818\n",
      "Epoch 410/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.4065 - acc: 0.8746 - val_loss: 0.9326 - val_acc: 0.6989\n",
      "Epoch 411/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.3783 - acc: 0.8889 - val_loss: 0.8827 - val_acc: 0.7159\n",
      "Epoch 412/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 0.3739 - acc: 0.9060 - val_loss: 0.8522 - val_acc: 0.7045\n",
      "Epoch 413/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.3624 - acc: 0.8803 - val_loss: 0.8530 - val_acc: 0.6989\n",
      "Epoch 414/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.3659 - acc: 0.8789 - val_loss: 0.9485 - val_acc: 0.6648\n",
      "Epoch 415/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.3667 - acc: 0.8846 - val_loss: 0.9044 - val_acc: 0.7045\n",
      "Epoch 416/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3407 - acc: 0.9003 - val_loss: 0.9863 - val_acc: 0.6818\n",
      "Epoch 417/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3896 - acc: 0.8889 - val_loss: 0.9135 - val_acc: 0.7159\n",
      "Epoch 418/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.4099 - acc: 0.8903 - val_loss: 0.9546 - val_acc: 0.6932\n",
      "Epoch 419/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.4050 - acc: 0.8818 - val_loss: 0.8882 - val_acc: 0.7216\n",
      "Epoch 420/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.4213 - acc: 0.8704 - val_loss: 0.9289 - val_acc: 0.6818\n",
      "Epoch 421/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.3948 - acc: 0.8775 - val_loss: 0.8599 - val_acc: 0.7386\n",
      "Epoch 422/3000\n",
      "702/702 [==============================] - 0s 582us/sample - loss: 0.3900 - acc: 0.8989 - val_loss: 0.8869 - val_acc: 0.7045\n",
      "Epoch 423/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.3877 - acc: 0.8932 - val_loss: 0.8966 - val_acc: 0.6875\n",
      "Epoch 424/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.3858 - acc: 0.8860 - val_loss: 0.8900 - val_acc: 0.7216\n",
      "Epoch 425/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.3634 - acc: 0.8974 - val_loss: 0.9789 - val_acc: 0.6705\n",
      "Epoch 426/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.4202 - acc: 0.8832 - val_loss: 0.8754 - val_acc: 0.7159\n",
      "Epoch 427/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3823 - acc: 0.8818 - val_loss: 0.9094 - val_acc: 0.7045\n",
      "Epoch 428/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.3800 - acc: 0.8889 - val_loss: 0.9096 - val_acc: 0.6648\n",
      "Epoch 429/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.4243 - acc: 0.8632 - val_loss: 0.9597 - val_acc: 0.6875\n",
      "Epoch 430/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.3957 - acc: 0.8889 - val_loss: 0.9055 - val_acc: 0.7216\n",
      "Epoch 431/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.3862 - acc: 0.8789 - val_loss: 0.9011 - val_acc: 0.6932\n",
      "Epoch 432/3000\n",
      "702/702 [==============================] - 0s 578us/sample - loss: 0.4030 - acc: 0.8846 - val_loss: 0.9397 - val_acc: 0.6705\n",
      "Epoch 433/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.3670 - acc: 0.8860 - val_loss: 0.9240 - val_acc: 0.6761\n",
      "Epoch 434/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.3303 - acc: 0.9003 - val_loss: 0.8893 - val_acc: 0.7159\n",
      "Epoch 435/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.3584 - acc: 0.8775 - val_loss: 0.8839 - val_acc: 0.7102\n",
      "Epoch 436/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3821 - acc: 0.8903 - val_loss: 0.9107 - val_acc: 0.6761\n",
      "Epoch 437/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3747 - acc: 0.8932 - val_loss: 0.9591 - val_acc: 0.6818\n",
      "Epoch 438/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.3778 - acc: 0.8860 - val_loss: 0.9194 - val_acc: 0.7045\n",
      "Epoch 439/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.3571 - acc: 0.9031 - val_loss: 0.9639 - val_acc: 0.6875\n",
      "Epoch 440/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.3565 - acc: 0.8917 - val_loss: 0.8672 - val_acc: 0.7159\n",
      "Epoch 441/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.3682 - acc: 0.8875 - val_loss: 0.9254 - val_acc: 0.6989\n",
      "Epoch 442/3000\n",
      "702/702 [==============================] - 0s 590us/sample - loss: 0.3672 - acc: 0.9060 - val_loss: 0.9032 - val_acc: 0.6989\n",
      "Epoch 443/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3522 - acc: 0.8946 - val_loss: 0.9030 - val_acc: 0.6932\n",
      "Epoch 444/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.3688 - acc: 0.8917 - val_loss: 0.9150 - val_acc: 0.6818\n",
      "Epoch 445/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.3493 - acc: 0.8789 - val_loss: 0.9555 - val_acc: 0.6932\n",
      "Epoch 446/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.3263 - acc: 0.9088 - val_loss: 0.9185 - val_acc: 0.6705\n",
      "Epoch 447/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.3444 - acc: 0.9031 - val_loss: 0.9070 - val_acc: 0.6818\n",
      "Epoch 448/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3459 - acc: 0.8989 - val_loss: 0.9486 - val_acc: 0.6705\n",
      "Epoch 449/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3390 - acc: 0.8960 - val_loss: 0.9193 - val_acc: 0.6989\n",
      "Epoch 450/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3264 - acc: 0.9017 - val_loss: 0.8795 - val_acc: 0.7330\n",
      "Epoch 451/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.3438 - acc: 0.8946 - val_loss: 0.9338 - val_acc: 0.6761\n",
      "Epoch 452/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.3645 - acc: 0.8917 - val_loss: 0.9590 - val_acc: 0.6932\n",
      "Epoch 453/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.3348 - acc: 0.8903 - val_loss: 0.9001 - val_acc: 0.7045\n",
      "Epoch 454/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.3331 - acc: 0.9060 - val_loss: 0.8850 - val_acc: 0.7159\n",
      "Epoch 455/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.3146 - acc: 0.9174 - val_loss: 0.9581 - val_acc: 0.6648\n",
      "Epoch 456/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.3511 - acc: 0.8974 - val_loss: 0.9123 - val_acc: 0.6989\n",
      "Epoch 457/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.2816 - acc: 0.9231 - val_loss: 0.9047 - val_acc: 0.6932\n",
      "Epoch 458/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.2893 - acc: 0.9217 - val_loss: 0.8630 - val_acc: 0.7216\n",
      "Epoch 459/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.3288 - acc: 0.9145 - val_loss: 0.9003 - val_acc: 0.7102\n",
      "Epoch 460/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.3325 - acc: 0.9117 - val_loss: 0.9118 - val_acc: 0.7102\n",
      "Epoch 461/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2913 - acc: 0.9131 - val_loss: 0.9260 - val_acc: 0.7045\n",
      "Epoch 462/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.2794 - acc: 0.9202 - val_loss: 0.8955 - val_acc: 0.6989\n",
      "Epoch 463/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.3258 - acc: 0.9060 - val_loss: 0.9033 - val_acc: 0.7102\n",
      "Epoch 464/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3261 - acc: 0.9031 - val_loss: 0.9265 - val_acc: 0.6932\n",
      "Epoch 465/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.3040 - acc: 0.9074 - val_loss: 0.9038 - val_acc: 0.7045\n",
      "Epoch 466/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.2989 - acc: 0.9288 - val_loss: 0.9206 - val_acc: 0.6875\n",
      "Epoch 467/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.2563 - acc: 0.9288 - val_loss: 0.9087 - val_acc: 0.6932\n",
      "Epoch 468/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.2841 - acc: 0.9202 - val_loss: 0.9218 - val_acc: 0.6989\n",
      "Epoch 469/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.2619 - acc: 0.9274 - val_loss: 0.9295 - val_acc: 0.6875\n",
      "Epoch 470/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.3023 - acc: 0.9074 - val_loss: 0.9124 - val_acc: 0.7102\n",
      "Epoch 471/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2943 - acc: 0.9217 - val_loss: 0.9136 - val_acc: 0.6932\n",
      "Epoch 472/3000\n",
      "702/702 [==============================] - 0s 580us/sample - loss: 0.2624 - acc: 0.9316 - val_loss: 0.9077 - val_acc: 0.6875\n",
      "Epoch 473/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.3791 - acc: 0.8818 - val_loss: 0.9040 - val_acc: 0.7159\n",
      "Epoch 474/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.3100 - acc: 0.9117 - val_loss: 0.9789 - val_acc: 0.6705\n",
      "Epoch 475/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.2940 - acc: 0.9259 - val_loss: 0.8875 - val_acc: 0.7102\n",
      "Epoch 476/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3243 - acc: 0.9103 - val_loss: 0.9293 - val_acc: 0.6875\n",
      "Epoch 477/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.2815 - acc: 0.9103 - val_loss: 0.8987 - val_acc: 0.7159\n",
      "Epoch 478/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.2767 - acc: 0.9188 - val_loss: 0.9536 - val_acc: 0.6761\n",
      "Epoch 479/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.2568 - acc: 0.9117 - val_loss: 0.8537 - val_acc: 0.7443\n",
      "Epoch 480/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.3343 - acc: 0.9031 - val_loss: 0.9536 - val_acc: 0.6818\n",
      "Epoch 481/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2960 - acc: 0.9103 - val_loss: 0.9406 - val_acc: 0.6989\n",
      "Epoch 482/3000\n",
      "702/702 [==============================] - 0s 567us/sample - loss: 0.2917 - acc: 0.9202 - val_loss: 0.9651 - val_acc: 0.6932\n",
      "Epoch 483/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.3048 - acc: 0.9160 - val_loss: 0.9301 - val_acc: 0.7159\n",
      "Epoch 484/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.2818 - acc: 0.9231 - val_loss: 0.9506 - val_acc: 0.6818\n",
      "Epoch 485/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3038 - acc: 0.8974 - val_loss: 0.9005 - val_acc: 0.6989\n",
      "Epoch 486/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.2716 - acc: 0.9274 - val_loss: 0.8522 - val_acc: 0.7159\n",
      "Epoch 487/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.3086 - acc: 0.9088 - val_loss: 0.9109 - val_acc: 0.6818\n",
      "Epoch 488/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.3271 - acc: 0.9103 - val_loss: 0.9551 - val_acc: 0.6875\n",
      "Epoch 489/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3181 - acc: 0.9046 - val_loss: 0.8831 - val_acc: 0.7386\n",
      "Epoch 490/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.2889 - acc: 0.9259 - val_loss: 0.9473 - val_acc: 0.6705\n",
      "Epoch 491/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.3027 - acc: 0.9103 - val_loss: 0.8485 - val_acc: 0.7273\n",
      "Epoch 492/3000\n",
      "702/702 [==============================] - 0s 594us/sample - loss: 0.2651 - acc: 0.9302 - val_loss: 0.8839 - val_acc: 0.6761\n",
      "Epoch 493/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.2962 - acc: 0.9174 - val_loss: 0.8627 - val_acc: 0.7159\n",
      "Epoch 494/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.3154 - acc: 0.9103 - val_loss: 0.9773 - val_acc: 0.6818\n",
      "Epoch 495/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.2641 - acc: 0.9245 - val_loss: 0.9214 - val_acc: 0.6989\n",
      "Epoch 496/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.2874 - acc: 0.9074 - val_loss: 0.9381 - val_acc: 0.6705\n",
      "Epoch 497/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2462 - acc: 0.9288 - val_loss: 0.9092 - val_acc: 0.6875\n",
      "Epoch 498/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.3077 - acc: 0.9003 - val_loss: 0.8500 - val_acc: 0.7216\n",
      "Epoch 499/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.2789 - acc: 0.9160 - val_loss: 0.9099 - val_acc: 0.6875\n",
      "Epoch 500/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.2900 - acc: 0.9202 - val_loss: 0.9321 - val_acc: 0.7102\n",
      "Epoch 501/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2748 - acc: 0.9145 - val_loss: 0.9020 - val_acc: 0.7330\n",
      "Epoch 502/3000\n",
      "702/702 [==============================] - 0s 580us/sample - loss: 0.2685 - acc: 0.9288 - val_loss: 0.9085 - val_acc: 0.7330\n",
      "Epoch 503/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.3019 - acc: 0.9117 - val_loss: 0.9138 - val_acc: 0.6932\n",
      "Epoch 504/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.3365 - acc: 0.9145 - val_loss: 0.9842 - val_acc: 0.6648\n",
      "Epoch 505/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.2837 - acc: 0.9103 - val_loss: 0.8848 - val_acc: 0.6932\n",
      "Epoch 506/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.2749 - acc: 0.9160 - val_loss: 0.8603 - val_acc: 0.7159\n",
      "Epoch 507/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.2900 - acc: 0.9316 - val_loss: 0.9370 - val_acc: 0.6648\n",
      "Epoch 508/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3091 - acc: 0.9217 - val_loss: 0.9395 - val_acc: 0.6875\n",
      "Epoch 509/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.2786 - acc: 0.9202 - val_loss: 0.8432 - val_acc: 0.7045\n",
      "Epoch 510/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.3045 - acc: 0.9117 - val_loss: 0.8849 - val_acc: 0.7045\n",
      "Epoch 511/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2776 - acc: 0.9231 - val_loss: 0.9192 - val_acc: 0.6989\n",
      "Epoch 512/3000\n",
      "702/702 [==============================] - 0s 583us/sample - loss: 0.3009 - acc: 0.9088 - val_loss: 0.8762 - val_acc: 0.7330\n",
      "Epoch 513/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.2840 - acc: 0.9245 - val_loss: 0.8966 - val_acc: 0.7045\n",
      "Epoch 514/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3123 - acc: 0.8960 - val_loss: 0.9561 - val_acc: 0.6989\n",
      "Epoch 515/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3033 - acc: 0.9046 - val_loss: 0.9293 - val_acc: 0.7159\n",
      "Epoch 516/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.2699 - acc: 0.9174 - val_loss: 0.9132 - val_acc: 0.7045\n",
      "Epoch 517/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.2912 - acc: 0.9074 - val_loss: 0.9276 - val_acc: 0.6932\n",
      "Epoch 518/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3809 - acc: 0.8989 - val_loss: 0.9484 - val_acc: 0.6932\n",
      "Epoch 519/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3572 - acc: 0.9145 - val_loss: 0.9342 - val_acc: 0.7216\n",
      "Epoch 520/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.3537 - acc: 0.9088 - val_loss: 1.0099 - val_acc: 0.6761\n",
      "Epoch 521/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.3602 - acc: 0.8903 - val_loss: 0.9166 - val_acc: 0.7045\n",
      "Epoch 522/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.3473 - acc: 0.8903 - val_loss: 0.8797 - val_acc: 0.7102\n",
      "Epoch 523/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.3349 - acc: 0.8974 - val_loss: 0.9543 - val_acc: 0.6932\n",
      "Epoch 524/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.3542 - acc: 0.8932 - val_loss: 0.8707 - val_acc: 0.7159\n",
      "Epoch 525/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3233 - acc: 0.8989 - val_loss: 0.9926 - val_acc: 0.6875\n",
      "Epoch 526/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.3716 - acc: 0.8903 - val_loss: 0.9746 - val_acc: 0.6989\n",
      "Epoch 527/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.4005 - acc: 0.8732 - val_loss: 1.0300 - val_acc: 0.6875\n",
      "Epoch 528/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.4151 - acc: 0.8846 - val_loss: 0.9210 - val_acc: 0.7102\n",
      "Epoch 529/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3692 - acc: 0.8875 - val_loss: 1.0451 - val_acc: 0.6761\n",
      "Epoch 530/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.5044 - acc: 0.8661 - val_loss: 1.0216 - val_acc: 0.6875\n",
      "Epoch 531/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.4350 - acc: 0.8689 - val_loss: 0.9868 - val_acc: 0.6761\n",
      "Epoch 532/3000\n",
      "702/702 [==============================] - 0s 587us/sample - loss: 0.3625 - acc: 0.8860 - val_loss: 0.9580 - val_acc: 0.6818\n",
      "Epoch 533/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3598 - acc: 0.8846 - val_loss: 0.9326 - val_acc: 0.7045\n",
      "Epoch 534/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3111 - acc: 0.9060 - val_loss: 1.0538 - val_acc: 0.6818\n",
      "Epoch 535/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.3448 - acc: 0.8946 - val_loss: 1.0114 - val_acc: 0.7045\n",
      "Epoch 536/3000\n",
      "702/702 [==============================] - 0s 521us/sample - loss: 0.3390 - acc: 0.9031 - val_loss: 0.9709 - val_acc: 0.7216\n",
      "Epoch 537/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.3319 - acc: 0.9060 - val_loss: 0.8927 - val_acc: 0.7045\n",
      "Epoch 538/3000\n",
      "702/702 [==============================] - 0s 524us/sample - loss: 0.2539 - acc: 0.9202 - val_loss: 0.9480 - val_acc: 0.6932\n",
      "Epoch 539/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.3166 - acc: 0.9046 - val_loss: 0.9894 - val_acc: 0.6818\n",
      "Epoch 540/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.2641 - acc: 0.9202 - val_loss: 0.9701 - val_acc: 0.6989\n",
      "Epoch 541/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2912 - acc: 0.9217 - val_loss: 1.0334 - val_acc: 0.6761\n",
      "Epoch 542/3000\n",
      "702/702 [==============================] - 0s 602us/sample - loss: 0.3412 - acc: 0.8932 - val_loss: 1.0102 - val_acc: 0.6818\n",
      "Epoch 543/3000\n",
      "702/702 [==============================] - 0s 519us/sample - loss: 0.3008 - acc: 0.9131 - val_loss: 1.0208 - val_acc: 0.7045\n",
      "Epoch 544/3000\n",
      "702/702 [==============================] - 0s 529us/sample - loss: 0.3319 - acc: 0.9103 - val_loss: 1.0194 - val_acc: 0.6818\n",
      "Epoch 545/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.3395 - acc: 0.8932 - val_loss: 0.8876 - val_acc: 0.7102\n",
      "Epoch 546/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.2910 - acc: 0.9060 - val_loss: 0.9154 - val_acc: 0.6989\n",
      "Epoch 547/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.3047 - acc: 0.9160 - val_loss: 0.9726 - val_acc: 0.6989\n",
      "Epoch 548/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.3023 - acc: 0.9174 - val_loss: 1.0495 - val_acc: 0.6705\n",
      "Epoch 549/3000\n",
      "702/702 [==============================] - 0s 522us/sample - loss: 0.2712 - acc: 0.9188 - val_loss: 1.0311 - val_acc: 0.6705\n",
      "Epoch 550/3000\n",
      "702/702 [==============================] - 0s 528us/sample - loss: 0.2597 - acc: 0.9202 - val_loss: 0.9907 - val_acc: 0.7045\n",
      "Epoch 551/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.3005 - acc: 0.9259 - val_loss: 0.9973 - val_acc: 0.6932\n",
      "Epoch 552/3000\n",
      "702/702 [==============================] - 0s 595us/sample - loss: 0.2616 - acc: 0.9274 - val_loss: 0.9317 - val_acc: 0.7045\n",
      "Epoch 553/3000\n",
      "702/702 [==============================] - 0s 525us/sample - loss: 0.2452 - acc: 0.9174 - val_loss: 0.9719 - val_acc: 0.6705\n",
      "Epoch 554/3000\n",
      "702/702 [==============================] - 0s 526us/sample - loss: 0.2534 - acc: 0.9288 - val_loss: 0.9069 - val_acc: 0.6875\n",
      "Epoch 555/3000\n",
      "702/702 [==============================] - 0s 518us/sample - loss: 0.2412 - acc: 0.9274 - val_loss: 1.0255 - val_acc: 0.6591\n",
      "Epoch 556/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.2714 - acc: 0.9103 - val_loss: 0.9680 - val_acc: 0.6875\n",
      "Epoch 557/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.2420 - acc: 0.9302 - val_loss: 1.0002 - val_acc: 0.6648\n",
      "Epoch 558/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.2442 - acc: 0.9145 - val_loss: 0.9256 - val_acc: 0.7102\n",
      "Epoch 559/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.2668 - acc: 0.9231 - val_loss: 0.9777 - val_acc: 0.6932\n",
      "Epoch 560/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.2612 - acc: 0.9245 - val_loss: 1.0053 - val_acc: 0.6761\n",
      "Epoch 561/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2449 - acc: 0.9274 - val_loss: 0.9730 - val_acc: 0.6875\n",
      "Epoch 562/3000\n",
      "702/702 [==============================] - 0s 580us/sample - loss: 0.3164 - acc: 0.9231 - val_loss: 0.9354 - val_acc: 0.6932\n",
      "Epoch 563/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.2187 - acc: 0.9359 - val_loss: 1.0106 - val_acc: 0.7045\n",
      "Epoch 564/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.2285 - acc: 0.9288 - val_loss: 0.9472 - val_acc: 0.7102\n",
      "Epoch 565/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3026 - acc: 0.9103 - val_loss: 1.0250 - val_acc: 0.6875\n",
      "Epoch 566/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.2761 - acc: 0.9231 - val_loss: 1.0139 - val_acc: 0.6875\n",
      "Epoch 567/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.3154 - acc: 0.9060 - val_loss: 1.0511 - val_acc: 0.6761\n",
      "Epoch 568/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.2981 - acc: 0.9031 - val_loss: 0.9757 - val_acc: 0.6932\n",
      "Epoch 569/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3083 - acc: 0.9131 - val_loss: 0.9964 - val_acc: 0.6705\n",
      "Epoch 570/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.2477 - acc: 0.9274 - val_loss: 0.9494 - val_acc: 0.6989\n",
      "Epoch 571/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2348 - acc: 0.9359 - val_loss: 0.9984 - val_acc: 0.6705\n",
      "Epoch 572/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 0.2706 - acc: 0.9231 - val_loss: 0.9028 - val_acc: 0.7216\n",
      "Epoch 573/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.2258 - acc: 0.9302 - val_loss: 0.9333 - val_acc: 0.6932\n",
      "Epoch 574/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.2762 - acc: 0.9088 - val_loss: 0.8299 - val_acc: 0.7443\n",
      "Epoch 575/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.2396 - acc: 0.9330 - val_loss: 0.9987 - val_acc: 0.6932\n",
      "Epoch 576/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.2561 - acc: 0.9202 - val_loss: 0.9491 - val_acc: 0.7216\n",
      "Epoch 577/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.2565 - acc: 0.9274 - val_loss: 0.9108 - val_acc: 0.7330\n",
      "Epoch 578/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.2185 - acc: 0.9459 - val_loss: 1.0322 - val_acc: 0.6761\n",
      "Epoch 579/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.2455 - acc: 0.9217 - val_loss: 0.9703 - val_acc: 0.7102\n",
      "Epoch 580/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.2634 - acc: 0.9103 - val_loss: 0.9771 - val_acc: 0.7045\n",
      "Epoch 581/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2932 - acc: 0.9117 - val_loss: 0.9467 - val_acc: 0.7102\n",
      "Epoch 582/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.3143 - acc: 0.9074 - val_loss: 0.9814 - val_acc: 0.6705\n",
      "Epoch 583/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3008 - acc: 0.9131 - val_loss: 0.9379 - val_acc: 0.6932\n",
      "Epoch 584/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.2548 - acc: 0.9330 - val_loss: 0.9464 - val_acc: 0.7045\n",
      "Epoch 585/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.2199 - acc: 0.9487 - val_loss: 0.9392 - val_acc: 0.6989\n",
      "Epoch 586/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.2339 - acc: 0.9259 - val_loss: 0.9089 - val_acc: 0.6875\n",
      "Epoch 587/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.2349 - acc: 0.9231 - val_loss: 1.0142 - val_acc: 0.6761\n",
      "Epoch 588/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.2297 - acc: 0.9302 - val_loss: 0.9607 - val_acc: 0.6761\n",
      "Epoch 589/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.2095 - acc: 0.9330 - val_loss: 0.9215 - val_acc: 0.6932\n",
      "Epoch 590/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.2642 - acc: 0.9188 - val_loss: 0.9645 - val_acc: 0.6705\n",
      "Epoch 591/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2439 - acc: 0.9316 - val_loss: 0.9327 - val_acc: 0.7045\n",
      "Epoch 592/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.2494 - acc: 0.9330 - val_loss: 0.9888 - val_acc: 0.6818\n",
      "Epoch 593/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.2438 - acc: 0.9217 - val_loss: 0.9603 - val_acc: 0.6932\n",
      "Epoch 594/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.2327 - acc: 0.9316 - val_loss: 0.9773 - val_acc: 0.6875\n",
      "Epoch 595/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.2026 - acc: 0.9387 - val_loss: 0.9536 - val_acc: 0.7216\n",
      "Epoch 596/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.2160 - acc: 0.9387 - val_loss: 0.9860 - val_acc: 0.6989\n",
      "Epoch 597/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.2262 - acc: 0.9316 - val_loss: 1.0249 - val_acc: 0.7216\n",
      "Epoch 598/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.2458 - acc: 0.9302 - val_loss: 0.9734 - val_acc: 0.6989\n",
      "Epoch 599/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.2591 - acc: 0.9330 - val_loss: 0.9992 - val_acc: 0.6989\n",
      "Epoch 600/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.2361 - acc: 0.9444 - val_loss: 0.8998 - val_acc: 0.7386\n",
      "Epoch 601/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1952 - acc: 0.9444 - val_loss: 1.0478 - val_acc: 0.6932\n",
      "Epoch 602/3000\n",
      "702/702 [==============================] - 0s 573us/sample - loss: 0.2031 - acc: 0.9387 - val_loss: 0.9919 - val_acc: 0.6989\n",
      "Epoch 603/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.1752 - acc: 0.9473 - val_loss: 1.0158 - val_acc: 0.7045\n",
      "Epoch 604/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.1939 - acc: 0.9501 - val_loss: 0.9443 - val_acc: 0.7102\n",
      "Epoch 605/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.1989 - acc: 0.9416 - val_loss: 0.9336 - val_acc: 0.7102\n",
      "Epoch 606/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2323 - acc: 0.9245 - val_loss: 0.9272 - val_acc: 0.7273\n",
      "Epoch 607/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.2236 - acc: 0.9359 - val_loss: 1.0195 - val_acc: 0.6989\n",
      "Epoch 608/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1972 - acc: 0.9416 - val_loss: 0.9538 - val_acc: 0.7216\n",
      "Epoch 609/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.2467 - acc: 0.9302 - val_loss: 0.9365 - val_acc: 0.7216\n",
      "Epoch 610/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.2386 - acc: 0.9288 - val_loss: 0.9766 - val_acc: 0.7330\n",
      "Epoch 611/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2240 - acc: 0.9302 - val_loss: 0.9765 - val_acc: 0.7159\n",
      "Epoch 612/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.2075 - acc: 0.9345 - val_loss: 0.9420 - val_acc: 0.7102\n",
      "Epoch 613/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.2481 - acc: 0.9330 - val_loss: 0.9144 - val_acc: 0.7102\n",
      "Epoch 614/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.2125 - acc: 0.9387 - val_loss: 0.9158 - val_acc: 0.7216\n",
      "Epoch 615/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.2545 - acc: 0.9288 - val_loss: 0.9626 - val_acc: 0.7045\n",
      "Epoch 616/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.2153 - acc: 0.9416 - val_loss: 1.0170 - val_acc: 0.6932\n",
      "Epoch 617/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.4605 - acc: 0.8632 - val_loss: 1.0176 - val_acc: 0.7159\n",
      "Epoch 618/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.4278 - acc: 0.8704 - val_loss: 1.1032 - val_acc: 0.6818\n",
      "Epoch 619/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.3961 - acc: 0.8746 - val_loss: 1.0811 - val_acc: 0.6989\n",
      "Epoch 620/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.4044 - acc: 0.8675 - val_loss: 1.0368 - val_acc: 0.6989\n",
      "Epoch 621/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.4773 - acc: 0.8618 - val_loss: 1.0372 - val_acc: 0.6875\n",
      "Epoch 622/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.3855 - acc: 0.8846 - val_loss: 1.0840 - val_acc: 0.6818\n",
      "Epoch 623/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.4304 - acc: 0.8647 - val_loss: 1.0633 - val_acc: 0.6705\n",
      "Epoch 624/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.3622 - acc: 0.8889 - val_loss: 1.0199 - val_acc: 0.7045\n",
      "Epoch 625/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.3463 - acc: 0.8932 - val_loss: 1.0477 - val_acc: 0.6875\n",
      "Epoch 626/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.3605 - acc: 0.8989 - val_loss: 1.0155 - val_acc: 0.7045\n",
      "Epoch 627/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3059 - acc: 0.9031 - val_loss: 1.0465 - val_acc: 0.6875\n",
      "Epoch 628/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.3041 - acc: 0.9074 - val_loss: 0.9240 - val_acc: 0.7216\n",
      "Epoch 629/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.2693 - acc: 0.9174 - val_loss: 1.0696 - val_acc: 0.6875\n",
      "Epoch 630/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3706 - acc: 0.8989 - val_loss: 1.0958 - val_acc: 0.6705\n",
      "Epoch 631/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.3201 - acc: 0.9131 - val_loss: 1.0513 - val_acc: 0.7102\n",
      "Epoch 632/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 0.3207 - acc: 0.9103 - val_loss: 1.0520 - val_acc: 0.6761\n",
      "Epoch 633/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2473 - acc: 0.9345 - val_loss: 0.9633 - val_acc: 0.6875\n",
      "Epoch 634/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.3668 - acc: 0.8832 - val_loss: 1.0066 - val_acc: 0.6761\n",
      "Epoch 635/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.2972 - acc: 0.9074 - val_loss: 0.9637 - val_acc: 0.6761\n",
      "Epoch 636/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.2516 - acc: 0.9217 - val_loss: 0.9510 - val_acc: 0.7330\n",
      "Epoch 637/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.2913 - acc: 0.9174 - val_loss: 0.9414 - val_acc: 0.7102\n",
      "Epoch 638/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.2424 - acc: 0.9359 - val_loss: 0.9450 - val_acc: 0.7273\n",
      "Epoch 639/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.2600 - acc: 0.9174 - val_loss: 1.0919 - val_acc: 0.6989\n",
      "Epoch 640/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.2662 - acc: 0.9217 - val_loss: 1.1246 - val_acc: 0.6761\n",
      "Epoch 641/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.3573 - acc: 0.9088 - val_loss: 0.9804 - val_acc: 0.7045\n",
      "Epoch 642/3000\n",
      "702/702 [==============================] - 0s 580us/sample - loss: 0.3381 - acc: 0.9017 - val_loss: 1.0018 - val_acc: 0.7102\n",
      "Epoch 643/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3393 - acc: 0.9031 - val_loss: 1.0370 - val_acc: 0.6989\n",
      "Epoch 644/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3451 - acc: 0.8761 - val_loss: 0.9518 - val_acc: 0.6932\n",
      "Epoch 645/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.2342 - acc: 0.9288 - val_loss: 0.9065 - val_acc: 0.6989\n",
      "Epoch 646/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.2426 - acc: 0.9302 - val_loss: 0.8514 - val_acc: 0.7102\n",
      "Epoch 647/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.2635 - acc: 0.9302 - val_loss: 0.9628 - val_acc: 0.6989\n",
      "Epoch 648/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.2290 - acc: 0.9316 - val_loss: 0.9924 - val_acc: 0.6875\n",
      "Epoch 649/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.2597 - acc: 0.9345 - val_loss: 0.9718 - val_acc: 0.7159\n",
      "Epoch 650/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.2352 - acc: 0.9316 - val_loss: 0.9056 - val_acc: 0.7102\n",
      "Epoch 651/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2484 - acc: 0.9316 - val_loss: 0.9502 - val_acc: 0.6875\n",
      "Epoch 652/3000\n",
      "702/702 [==============================] - 0s 578us/sample - loss: 0.2295 - acc: 0.9444 - val_loss: 0.9218 - val_acc: 0.6818\n",
      "Epoch 653/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.2261 - acc: 0.9373 - val_loss: 0.9268 - val_acc: 0.7045\n",
      "Epoch 654/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.2125 - acc: 0.9387 - val_loss: 0.9275 - val_acc: 0.6818\n",
      "Epoch 655/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1903 - acc: 0.9359 - val_loss: 0.9843 - val_acc: 0.6932\n",
      "Epoch 656/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.2497 - acc: 0.9359 - val_loss: 1.0146 - val_acc: 0.7216\n",
      "Epoch 657/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2576 - acc: 0.9316 - val_loss: 1.0001 - val_acc: 0.7045\n",
      "Epoch 658/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.2201 - acc: 0.9316 - val_loss: 0.9281 - val_acc: 0.7386\n",
      "Epoch 659/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.2306 - acc: 0.9430 - val_loss: 0.9744 - val_acc: 0.7159\n",
      "Epoch 660/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.2095 - acc: 0.9444 - val_loss: 0.9710 - val_acc: 0.7216\n",
      "Epoch 661/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2912 - acc: 0.9387 - val_loss: 0.9782 - val_acc: 0.7102\n",
      "Epoch 662/3000\n",
      "702/702 [==============================] - 0s 580us/sample - loss: 0.2273 - acc: 0.9373 - val_loss: 0.9773 - val_acc: 0.7273\n",
      "Epoch 663/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.2053 - acc: 0.9501 - val_loss: 1.1196 - val_acc: 0.6989\n",
      "Epoch 664/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.2032 - acc: 0.9330 - val_loss: 1.0328 - val_acc: 0.7159\n",
      "Epoch 665/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.2037 - acc: 0.9345 - val_loss: 0.9955 - val_acc: 0.7159\n",
      "Epoch 666/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.2224 - acc: 0.9345 - val_loss: 1.0101 - val_acc: 0.6989\n",
      "Epoch 667/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.2285 - acc: 0.9373 - val_loss: 1.0133 - val_acc: 0.6932\n",
      "Epoch 668/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.2332 - acc: 0.9259 - val_loss: 0.9712 - val_acc: 0.6932\n",
      "Epoch 669/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.2840 - acc: 0.9359 - val_loss: 0.8740 - val_acc: 0.7386\n",
      "Epoch 670/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.2654 - acc: 0.9288 - val_loss: 1.0101 - val_acc: 0.6932\n",
      "Epoch 671/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2463 - acc: 0.9330 - val_loss: 0.9648 - val_acc: 0.7045\n",
      "Epoch 672/3000\n",
      "702/702 [==============================] - 0s 582us/sample - loss: 0.2941 - acc: 0.9131 - val_loss: 1.0773 - val_acc: 0.6648\n",
      "Epoch 673/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.2682 - acc: 0.9231 - val_loss: 1.0202 - val_acc: 0.7102\n",
      "Epoch 674/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.2271 - acc: 0.9245 - val_loss: 1.0761 - val_acc: 0.7102\n",
      "Epoch 675/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.2308 - acc: 0.9288 - val_loss: 1.0232 - val_acc: 0.7159\n",
      "Epoch 676/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.2074 - acc: 0.9430 - val_loss: 1.0207 - val_acc: 0.7102\n",
      "Epoch 677/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1959 - acc: 0.9444 - val_loss: 1.0900 - val_acc: 0.6932\n",
      "Epoch 678/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.2094 - acc: 0.9359 - val_loss: 0.9882 - val_acc: 0.6989\n",
      "Epoch 679/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1859 - acc: 0.9444 - val_loss: 0.9467 - val_acc: 0.7045\n",
      "Epoch 680/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.1675 - acc: 0.9402 - val_loss: 1.0402 - val_acc: 0.6989\n",
      "Epoch 681/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1780 - acc: 0.9345 - val_loss: 0.9685 - val_acc: 0.7330\n",
      "Epoch 682/3000\n",
      "702/702 [==============================] - 0s 583us/sample - loss: 0.1469 - acc: 0.9558 - val_loss: 1.0323 - val_acc: 0.6818\n",
      "Epoch 683/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1609 - acc: 0.9530 - val_loss: 1.0182 - val_acc: 0.6761\n",
      "Epoch 684/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1830 - acc: 0.9487 - val_loss: 0.9966 - val_acc: 0.6875\n",
      "Epoch 685/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1617 - acc: 0.9459 - val_loss: 0.9407 - val_acc: 0.7386\n",
      "Epoch 686/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.1966 - acc: 0.9430 - val_loss: 0.8680 - val_acc: 0.7330\n",
      "Epoch 687/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.1666 - acc: 0.9530 - val_loss: 0.9286 - val_acc: 0.7159\n",
      "Epoch 688/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1855 - acc: 0.9530 - val_loss: 0.9516 - val_acc: 0.7045\n",
      "Epoch 689/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.2675 - acc: 0.9217 - val_loss: 1.0465 - val_acc: 0.6932\n",
      "Epoch 690/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3300 - acc: 0.9103 - val_loss: 0.9452 - val_acc: 0.6875\n",
      "Epoch 691/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2805 - acc: 0.9316 - val_loss: 0.9782 - val_acc: 0.7045\n",
      "Epoch 692/3000\n",
      "702/702 [==============================] - 0s 599us/sample - loss: 0.3003 - acc: 0.9103 - val_loss: 0.9718 - val_acc: 0.7045\n",
      "Epoch 693/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.2701 - acc: 0.9103 - val_loss: 0.9884 - val_acc: 0.6989\n",
      "Epoch 694/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.2345 - acc: 0.9174 - val_loss: 0.9201 - val_acc: 0.6875\n",
      "Epoch 695/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.2235 - acc: 0.9316 - val_loss: 0.8873 - val_acc: 0.7102\n",
      "Epoch 696/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.2325 - acc: 0.9259 - val_loss: 0.9609 - val_acc: 0.6932\n",
      "Epoch 697/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.2459 - acc: 0.9274 - val_loss: 0.9864 - val_acc: 0.6761\n",
      "Epoch 698/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.2209 - acc: 0.9402 - val_loss: 0.9155 - val_acc: 0.7273\n",
      "Epoch 699/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1824 - acc: 0.9430 - val_loss: 0.9880 - val_acc: 0.7102\n",
      "Epoch 700/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.2296 - acc: 0.9245 - val_loss: 1.0002 - val_acc: 0.6989\n",
      "Epoch 701/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2207 - acc: 0.9259 - val_loss: 1.0614 - val_acc: 0.6648\n",
      "Epoch 702/3000\n",
      "702/702 [==============================] - 0s 569us/sample - loss: 0.2123 - acc: 0.9316 - val_loss: 0.9111 - val_acc: 0.6932\n",
      "Epoch 703/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.2420 - acc: 0.9302 - val_loss: 0.9429 - val_acc: 0.6989\n",
      "Epoch 704/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1893 - acc: 0.9487 - val_loss: 1.0291 - val_acc: 0.6932\n",
      "Epoch 705/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.1659 - acc: 0.9601 - val_loss: 0.8958 - val_acc: 0.7216\n",
      "Epoch 706/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.2316 - acc: 0.9359 - val_loss: 0.9719 - val_acc: 0.6932\n",
      "Epoch 707/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.2188 - acc: 0.9373 - val_loss: 0.9572 - val_acc: 0.6932\n",
      "Epoch 708/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.2141 - acc: 0.9245 - val_loss: 0.8835 - val_acc: 0.7102\n",
      "Epoch 709/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.2123 - acc: 0.9316 - val_loss: 0.9556 - val_acc: 0.7045\n",
      "Epoch 710/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2177 - acc: 0.9387 - val_loss: 0.9406 - val_acc: 0.7216\n",
      "Epoch 711/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2168 - acc: 0.9302 - val_loss: 0.9580 - val_acc: 0.7216\n",
      "Epoch 712/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.2072 - acc: 0.9330 - val_loss: 0.9559 - val_acc: 0.7159\n",
      "Epoch 713/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1828 - acc: 0.9501 - val_loss: 1.0127 - val_acc: 0.7102\n",
      "Epoch 714/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2132 - acc: 0.9387 - val_loss: 0.9683 - val_acc: 0.7045\n",
      "Epoch 715/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1762 - acc: 0.9501 - val_loss: 0.9486 - val_acc: 0.6989\n",
      "Epoch 716/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.2139 - acc: 0.9459 - val_loss: 0.9896 - val_acc: 0.6932\n",
      "Epoch 717/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1790 - acc: 0.9530 - val_loss: 0.9626 - val_acc: 0.7045\n",
      "Epoch 718/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.1885 - acc: 0.9473 - val_loss: 0.9402 - val_acc: 0.7102\n",
      "Epoch 719/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.1932 - acc: 0.9430 - val_loss: 0.9679 - val_acc: 0.6818\n",
      "Epoch 720/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1999 - acc: 0.9402 - val_loss: 0.9830 - val_acc: 0.7216\n",
      "Epoch 721/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1946 - acc: 0.9359 - val_loss: 0.9964 - val_acc: 0.7330\n",
      "Epoch 722/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.1840 - acc: 0.9459 - val_loss: 0.9696 - val_acc: 0.7443\n",
      "Epoch 723/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.1895 - acc: 0.9516 - val_loss: 1.0288 - val_acc: 0.7045\n",
      "Epoch 724/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.2066 - acc: 0.9316 - val_loss: 1.0261 - val_acc: 0.7159\n",
      "Epoch 725/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1538 - acc: 0.9573 - val_loss: 1.0483 - val_acc: 0.7102\n",
      "Epoch 726/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.1505 - acc: 0.9530 - val_loss: 1.0128 - val_acc: 0.6875\n",
      "Epoch 727/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1356 - acc: 0.9658 - val_loss: 0.9961 - val_acc: 0.6989\n",
      "Epoch 728/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.1857 - acc: 0.9501 - val_loss: 1.0362 - val_acc: 0.6932\n",
      "Epoch 729/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.1734 - acc: 0.9558 - val_loss: 1.0236 - val_acc: 0.7045\n",
      "Epoch 730/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1450 - acc: 0.9544 - val_loss: 1.0022 - val_acc: 0.7159\n",
      "Epoch 731/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1494 - acc: 0.9573 - val_loss: 0.9522 - val_acc: 0.7102\n",
      "Epoch 732/3000\n",
      "702/702 [==============================] - 0s 602us/sample - loss: 0.1980 - acc: 0.9416 - val_loss: 0.9807 - val_acc: 0.6989\n",
      "Epoch 733/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.1810 - acc: 0.9402 - val_loss: 1.1250 - val_acc: 0.6818\n",
      "Epoch 734/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1912 - acc: 0.9387 - val_loss: 1.0703 - val_acc: 0.6875\n",
      "Epoch 735/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.1250 - acc: 0.9658 - val_loss: 1.0607 - val_acc: 0.7102\n",
      "Epoch 736/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.1515 - acc: 0.9501 - val_loss: 1.0250 - val_acc: 0.6989\n",
      "Epoch 737/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1440 - acc: 0.9615 - val_loss: 1.0738 - val_acc: 0.6989\n",
      "Epoch 738/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1739 - acc: 0.9416 - val_loss: 1.0747 - val_acc: 0.7216\n",
      "Epoch 739/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1694 - acc: 0.9630 - val_loss: 0.9637 - val_acc: 0.7330\n",
      "Epoch 740/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1764 - acc: 0.9501 - val_loss: 0.9920 - val_acc: 0.7102\n",
      "Epoch 741/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1638 - acc: 0.9558 - val_loss: 1.0798 - val_acc: 0.6989\n",
      "Epoch 742/3000\n",
      "702/702 [==============================] - 0s 580us/sample - loss: 0.2612 - acc: 0.9416 - val_loss: 1.0016 - val_acc: 0.7216\n",
      "Epoch 743/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.1929 - acc: 0.9516 - val_loss: 1.0635 - val_acc: 0.7045\n",
      "Epoch 744/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.2082 - acc: 0.9430 - val_loss: 1.0124 - val_acc: 0.7159\n",
      "Epoch 745/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.1956 - acc: 0.9459 - val_loss: 0.9762 - val_acc: 0.7330\n",
      "Epoch 746/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.1630 - acc: 0.9501 - val_loss: 1.0410 - val_acc: 0.6875\n",
      "Epoch 747/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.2065 - acc: 0.9345 - val_loss: 1.0067 - val_acc: 0.6989\n",
      "Epoch 748/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1815 - acc: 0.9373 - val_loss: 1.0825 - val_acc: 0.6818\n",
      "Epoch 749/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1614 - acc: 0.9473 - val_loss: 0.9219 - val_acc: 0.7330\n",
      "Epoch 750/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.1509 - acc: 0.9544 - val_loss: 0.9843 - val_acc: 0.6932\n",
      "Epoch 751/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1798 - acc: 0.9516 - val_loss: 1.0376 - val_acc: 0.6932\n",
      "Epoch 752/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.1643 - acc: 0.9516 - val_loss: 0.9911 - val_acc: 0.6989\n",
      "Epoch 753/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.1964 - acc: 0.9288 - val_loss: 0.9895 - val_acc: 0.7159\n",
      "Epoch 754/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1492 - acc: 0.9487 - val_loss: 0.9581 - val_acc: 0.7386\n",
      "Epoch 755/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1256 - acc: 0.9615 - val_loss: 0.9622 - val_acc: 0.7386\n",
      "Epoch 756/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1526 - acc: 0.9644 - val_loss: 1.0006 - val_acc: 0.7045\n",
      "Epoch 757/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1548 - acc: 0.9530 - val_loss: 0.9863 - val_acc: 0.7102\n",
      "Epoch 758/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1321 - acc: 0.9587 - val_loss: 1.0282 - val_acc: 0.7216\n",
      "Epoch 759/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.1177 - acc: 0.9687 - val_loss: 0.9806 - val_acc: 0.7216\n",
      "Epoch 760/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.1538 - acc: 0.9501 - val_loss: 0.9884 - val_acc: 0.7045\n",
      "Epoch 761/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1298 - acc: 0.9630 - val_loss: 1.0710 - val_acc: 0.6648\n",
      "Epoch 762/3000\n",
      "702/702 [==============================] - 0s 580us/sample - loss: 0.1548 - acc: 0.9516 - val_loss: 1.0110 - val_acc: 0.7159\n",
      "Epoch 763/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.1618 - acc: 0.9516 - val_loss: 0.9876 - val_acc: 0.7102\n",
      "Epoch 764/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.1828 - acc: 0.9473 - val_loss: 1.0760 - val_acc: 0.6932\n",
      "Epoch 765/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1588 - acc: 0.9530 - val_loss: 1.0873 - val_acc: 0.6989\n",
      "Epoch 766/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.1573 - acc: 0.9516 - val_loss: 1.0048 - val_acc: 0.6932\n",
      "Epoch 767/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1836 - acc: 0.9487 - val_loss: 1.0517 - val_acc: 0.7045\n",
      "Epoch 768/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1552 - acc: 0.9615 - val_loss: 0.9948 - val_acc: 0.6989\n",
      "Epoch 769/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.1454 - acc: 0.9530 - val_loss: 1.0003 - val_acc: 0.6932\n",
      "Epoch 770/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1212 - acc: 0.9658 - val_loss: 1.0796 - val_acc: 0.6875\n",
      "Epoch 771/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1448 - acc: 0.9587 - val_loss: 0.9504 - val_acc: 0.7159\n",
      "Epoch 772/3000\n",
      "702/702 [==============================] - 0s 580us/sample - loss: 0.1581 - acc: 0.9530 - val_loss: 0.8819 - val_acc: 0.7330\n",
      "Epoch 773/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1890 - acc: 0.9444 - val_loss: 0.9366 - val_acc: 0.7330\n",
      "Epoch 774/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.1555 - acc: 0.9501 - val_loss: 0.9720 - val_acc: 0.7330\n",
      "Epoch 775/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.2288 - acc: 0.9359 - val_loss: 0.9287 - val_acc: 0.7102\n",
      "Epoch 776/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1828 - acc: 0.9402 - val_loss: 0.9734 - val_acc: 0.7159\n",
      "Epoch 777/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.2092 - acc: 0.9373 - val_loss: 0.9917 - val_acc: 0.6989\n",
      "Epoch 778/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.2152 - acc: 0.9302 - val_loss: 1.0500 - val_acc: 0.7216\n",
      "Epoch 779/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.2095 - acc: 0.9501 - val_loss: 1.1118 - val_acc: 0.7216\n",
      "Epoch 780/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.1709 - acc: 0.9530 - val_loss: 0.9787 - val_acc: 0.7102\n",
      "Epoch 781/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1714 - acc: 0.9473 - val_loss: 1.0699 - val_acc: 0.7557\n",
      "Epoch 782/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.1810 - acc: 0.9516 - val_loss: 1.0430 - val_acc: 0.7045\n",
      "Epoch 783/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.1584 - acc: 0.9516 - val_loss: 0.9943 - val_acc: 0.7159\n",
      "Epoch 784/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1416 - acc: 0.9630 - val_loss: 0.9551 - val_acc: 0.7614\n",
      "Epoch 785/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.2044 - acc: 0.9330 - val_loss: 1.0526 - val_acc: 0.7216\n",
      "Epoch 786/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1716 - acc: 0.9416 - val_loss: 0.9990 - val_acc: 0.6932\n",
      "Epoch 787/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.2240 - acc: 0.9330 - val_loss: 0.9217 - val_acc: 0.7159\n",
      "Epoch 788/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1942 - acc: 0.9516 - val_loss: 0.8802 - val_acc: 0.7273\n",
      "Epoch 789/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1794 - acc: 0.9444 - val_loss: 1.0186 - val_acc: 0.7216\n",
      "Epoch 790/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.2062 - acc: 0.9402 - val_loss: 0.9218 - val_acc: 0.7159\n",
      "Epoch 791/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1845 - acc: 0.9516 - val_loss: 0.9631 - val_acc: 0.7273\n",
      "Epoch 792/3000\n",
      "702/702 [==============================] - 0s 580us/sample - loss: 0.1622 - acc: 0.9473 - val_loss: 1.0103 - val_acc: 0.7216\n",
      "Epoch 793/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.1212 - acc: 0.9544 - val_loss: 1.0805 - val_acc: 0.7045\n",
      "Epoch 794/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.1382 - acc: 0.9573 - val_loss: 0.9969 - val_acc: 0.7216\n",
      "Epoch 795/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1908 - acc: 0.9615 - val_loss: 1.0074 - val_acc: 0.7216\n",
      "Epoch 796/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1079 - acc: 0.9772 - val_loss: 0.9811 - val_acc: 0.7159\n",
      "Epoch 797/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1395 - acc: 0.9487 - val_loss: 0.9950 - val_acc: 0.7216\n",
      "Epoch 798/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1596 - acc: 0.9758 - val_loss: 0.9639 - val_acc: 0.7443\n",
      "Epoch 799/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.2260 - acc: 0.9459 - val_loss: 0.9490 - val_acc: 0.6818\n",
      "Epoch 800/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.2165 - acc: 0.9373 - val_loss: 1.0128 - val_acc: 0.6932\n",
      "Epoch 801/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2170 - acc: 0.9274 - val_loss: 1.0148 - val_acc: 0.7159\n",
      "Epoch 802/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.1963 - acc: 0.9402 - val_loss: 1.0098 - val_acc: 0.7386\n",
      "Epoch 803/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1483 - acc: 0.9615 - val_loss: 1.1022 - val_acc: 0.7102\n",
      "Epoch 804/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1622 - acc: 0.9530 - val_loss: 1.0638 - val_acc: 0.7045\n",
      "Epoch 805/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1372 - acc: 0.9544 - val_loss: 1.0244 - val_acc: 0.7159\n",
      "Epoch 806/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1451 - acc: 0.9544 - val_loss: 0.9233 - val_acc: 0.7216\n",
      "Epoch 807/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1452 - acc: 0.9587 - val_loss: 1.0372 - val_acc: 0.7216\n",
      "Epoch 808/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1818 - acc: 0.9373 - val_loss: 1.0292 - val_acc: 0.7045\n",
      "Epoch 809/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.2562 - acc: 0.9359 - val_loss: 0.9539 - val_acc: 0.7330\n",
      "Epoch 810/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3186 - acc: 0.9117 - val_loss: 1.0534 - val_acc: 0.6761\n",
      "Epoch 811/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.5500 - acc: 0.8291 - val_loss: 1.2662 - val_acc: 0.6477\n",
      "Epoch 812/3000\n",
      "702/702 [==============================] - 0s 580us/sample - loss: 0.5406 - acc: 0.8234 - val_loss: 1.2360 - val_acc: 0.6193\n",
      "Epoch 813/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.6926 - acc: 0.8006 - val_loss: 1.1452 - val_acc: 0.6534\n",
      "Epoch 814/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.6687 - acc: 0.7835 - val_loss: 1.2371 - val_acc: 0.6250\n",
      "Epoch 815/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.6880 - acc: 0.7749 - val_loss: 1.0758 - val_acc: 0.6477\n",
      "Epoch 816/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.7336 - acc: 0.7963 - val_loss: 1.1207 - val_acc: 0.6534\n",
      "Epoch 817/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.5020 - acc: 0.8433 - val_loss: 1.1722 - val_acc: 0.6648\n",
      "Epoch 818/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.4717 - acc: 0.8604 - val_loss: 0.9870 - val_acc: 0.6818\n",
      "Epoch 819/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.3823 - acc: 0.8675 - val_loss: 1.0571 - val_acc: 0.6989\n",
      "Epoch 820/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.3295 - acc: 0.8889 - val_loss: 1.0499 - val_acc: 0.6989\n",
      "Epoch 821/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.3608 - acc: 0.8789 - val_loss: 1.1884 - val_acc: 0.7102\n",
      "Epoch 822/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 0.3794 - acc: 0.8932 - val_loss: 1.0420 - val_acc: 0.6875\n",
      "Epoch 823/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2771 - acc: 0.9046 - val_loss: 1.0103 - val_acc: 0.7045\n",
      "Epoch 824/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.2638 - acc: 0.9245 - val_loss: 1.1057 - val_acc: 0.6875\n",
      "Epoch 825/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2681 - acc: 0.9160 - val_loss: 1.1615 - val_acc: 0.6648\n",
      "Epoch 826/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.2591 - acc: 0.9160 - val_loss: 1.0902 - val_acc: 0.7102\n",
      "Epoch 827/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.2551 - acc: 0.9188 - val_loss: 1.0503 - val_acc: 0.6875\n",
      "Epoch 828/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.3551 - acc: 0.8989 - val_loss: 1.0080 - val_acc: 0.7159\n",
      "Epoch 829/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.3239 - acc: 0.8960 - val_loss: 1.0503 - val_acc: 0.6989\n",
      "Epoch 830/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.3369 - acc: 0.9074 - val_loss: 1.0302 - val_acc: 0.6818\n",
      "Epoch 831/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.3589 - acc: 0.9046 - val_loss: 0.9364 - val_acc: 0.7102\n",
      "Epoch 832/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.3843 - acc: 0.9017 - val_loss: 0.9153 - val_acc: 0.7159\n",
      "Epoch 833/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.4626 - acc: 0.8818 - val_loss: 1.0524 - val_acc: 0.7045\n",
      "Epoch 834/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.3678 - acc: 0.8960 - val_loss: 1.3054 - val_acc: 0.6875\n",
      "Epoch 835/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.4021 - acc: 0.8746 - val_loss: 1.1375 - val_acc: 0.6989\n",
      "Epoch 836/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.3889 - acc: 0.8704 - val_loss: 0.9313 - val_acc: 0.6875\n",
      "Epoch 837/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.3623 - acc: 0.8675 - val_loss: 0.9378 - val_acc: 0.7159\n",
      "Epoch 838/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.3478 - acc: 0.8832 - val_loss: 0.9974 - val_acc: 0.7045\n",
      "Epoch 839/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.3594 - acc: 0.9003 - val_loss: 1.0181 - val_acc: 0.7273\n",
      "Epoch 840/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2897 - acc: 0.9017 - val_loss: 1.0243 - val_acc: 0.7273\n",
      "Epoch 841/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2489 - acc: 0.9288 - val_loss: 1.0125 - val_acc: 0.7273\n",
      "Epoch 842/3000\n",
      "702/702 [==============================] - 0s 577us/sample - loss: 0.2540 - acc: 0.9188 - val_loss: 1.0394 - val_acc: 0.7045\n",
      "Epoch 843/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.2085 - acc: 0.9259 - val_loss: 1.0467 - val_acc: 0.7102\n",
      "Epoch 844/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1890 - acc: 0.9430 - val_loss: 1.0888 - val_acc: 0.6989\n",
      "Epoch 845/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1843 - acc: 0.9373 - val_loss: 1.0547 - val_acc: 0.7273\n",
      "Epoch 846/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.2028 - acc: 0.9373 - val_loss: 1.1081 - val_acc: 0.6932\n",
      "Epoch 847/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1760 - acc: 0.9402 - val_loss: 1.0229 - val_acc: 0.7216\n",
      "Epoch 848/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1677 - acc: 0.9473 - val_loss: 1.0696 - val_acc: 0.7045\n",
      "Epoch 849/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1684 - acc: 0.9430 - val_loss: 1.1078 - val_acc: 0.7159\n",
      "Epoch 850/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.2043 - acc: 0.9430 - val_loss: 1.0288 - val_acc: 0.7216\n",
      "Epoch 851/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1910 - acc: 0.9430 - val_loss: 1.0256 - val_acc: 0.7159\n",
      "Epoch 852/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.1866 - acc: 0.9487 - val_loss: 0.9694 - val_acc: 0.7216\n",
      "Epoch 853/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.1860 - acc: 0.9544 - val_loss: 1.0544 - val_acc: 0.6989\n",
      "Epoch 854/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.2913 - acc: 0.9117 - val_loss: 1.1494 - val_acc: 0.7102\n",
      "Epoch 855/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.3109 - acc: 0.9017 - val_loss: 0.9286 - val_acc: 0.6875\n",
      "Epoch 856/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.2778 - acc: 0.8932 - val_loss: 0.9488 - val_acc: 0.7102\n",
      "Epoch 857/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.3166 - acc: 0.9003 - val_loss: 1.0039 - val_acc: 0.7045\n",
      "Epoch 858/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.2558 - acc: 0.9217 - val_loss: 0.9975 - val_acc: 0.7102\n",
      "Epoch 859/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.2399 - acc: 0.9245 - val_loss: 1.0818 - val_acc: 0.7102\n",
      "Epoch 860/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.2145 - acc: 0.9316 - val_loss: 1.0482 - val_acc: 0.6989\n",
      "Epoch 861/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1801 - acc: 0.9416 - val_loss: 1.0441 - val_acc: 0.7159\n",
      "Epoch 862/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.1819 - acc: 0.9444 - val_loss: 1.1279 - val_acc: 0.6989\n",
      "Epoch 863/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.1615 - acc: 0.9487 - val_loss: 1.0550 - val_acc: 0.6989\n",
      "Epoch 864/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1741 - acc: 0.9615 - val_loss: 0.9507 - val_acc: 0.7330\n",
      "Epoch 865/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1750 - acc: 0.9558 - val_loss: 0.9384 - val_acc: 0.7159\n",
      "Epoch 866/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1528 - acc: 0.9558 - val_loss: 1.0206 - val_acc: 0.6989\n",
      "Epoch 867/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.1844 - acc: 0.9430 - val_loss: 1.0508 - val_acc: 0.6989\n",
      "Epoch 868/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1720 - acc: 0.9444 - val_loss: 1.0742 - val_acc: 0.7045\n",
      "Epoch 869/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.1661 - acc: 0.9544 - val_loss: 1.0092 - val_acc: 0.7159\n",
      "Epoch 870/3000\n",
      "702/702 [==============================] - 0s 515us/sample - loss: 0.1599 - acc: 0.9558 - val_loss: 0.9611 - val_acc: 0.7045\n",
      "Epoch 871/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1568 - acc: 0.9444 - val_loss: 0.9799 - val_acc: 0.6989\n",
      "Epoch 872/3000\n",
      "702/702 [==============================] - 0s 580us/sample - loss: 0.1285 - acc: 0.9672 - val_loss: 1.0719 - val_acc: 0.6932\n",
      "Epoch 873/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1201 - acc: 0.9644 - val_loss: 1.0629 - val_acc: 0.6989\n",
      "Epoch 874/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.1479 - acc: 0.9587 - val_loss: 1.0088 - val_acc: 0.7102\n",
      "Epoch 875/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1335 - acc: 0.9630 - val_loss: 1.0643 - val_acc: 0.6989\n",
      "Epoch 876/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.1194 - acc: 0.9672 - val_loss: 0.9726 - val_acc: 0.7159\n",
      "Epoch 877/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1578 - acc: 0.9587 - val_loss: 1.0260 - val_acc: 0.7045\n",
      "Epoch 878/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1930 - acc: 0.9544 - val_loss: 1.0635 - val_acc: 0.7045\n",
      "Epoch 879/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.1565 - acc: 0.9558 - val_loss: 1.0660 - val_acc: 0.6818\n",
      "Epoch 880/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.1620 - acc: 0.9459 - val_loss: 1.0825 - val_acc: 0.6761\n",
      "Epoch 881/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1585 - acc: 0.9530 - val_loss: 0.9794 - val_acc: 0.7216\n",
      "Epoch 882/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.1725 - acc: 0.9473 - val_loss: 1.0480 - val_acc: 0.6989\n",
      "Epoch 883/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1668 - acc: 0.9487 - val_loss: 1.0513 - val_acc: 0.7045\n",
      "Epoch 884/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.1276 - acc: 0.9672 - val_loss: 0.9750 - val_acc: 0.7216\n",
      "Epoch 885/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.1150 - acc: 0.9658 - val_loss: 1.0064 - val_acc: 0.7045\n",
      "Epoch 886/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1547 - acc: 0.9573 - val_loss: 1.0090 - val_acc: 0.6989\n",
      "Epoch 887/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.1130 - acc: 0.9615 - val_loss: 1.0016 - val_acc: 0.7045\n",
      "Epoch 888/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1509 - acc: 0.9601 - val_loss: 0.9482 - val_acc: 0.7102\n",
      "Epoch 889/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1162 - acc: 0.9744 - val_loss: 0.9814 - val_acc: 0.7045\n",
      "Epoch 890/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.1590 - acc: 0.9530 - val_loss: 0.9560 - val_acc: 0.7159\n",
      "Epoch 891/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1256 - acc: 0.9644 - val_loss: 0.9866 - val_acc: 0.7216\n",
      "Epoch 892/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 0.1263 - acc: 0.9630 - val_loss: 1.0407 - val_acc: 0.7216\n",
      "Epoch 893/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1445 - acc: 0.9587 - val_loss: 0.9826 - val_acc: 0.7045\n",
      "Epoch 894/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.1455 - acc: 0.9530 - val_loss: 0.9404 - val_acc: 0.7159\n",
      "Epoch 895/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1138 - acc: 0.9672 - val_loss: 0.9674 - val_acc: 0.7159\n",
      "Epoch 896/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1372 - acc: 0.9544 - val_loss: 1.0151 - val_acc: 0.6932\n",
      "Epoch 897/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.1527 - acc: 0.9587 - val_loss: 1.0383 - val_acc: 0.6875\n",
      "Epoch 898/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1008 - acc: 0.9758 - val_loss: 1.0778 - val_acc: 0.6989\n",
      "Epoch 899/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.1307 - acc: 0.9715 - val_loss: 1.0767 - val_acc: 0.7045\n",
      "Epoch 900/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.1035 - acc: 0.9729 - val_loss: 1.0259 - val_acc: 0.7159\n",
      "Epoch 901/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1280 - acc: 0.9558 - val_loss: 1.0192 - val_acc: 0.6989\n",
      "Epoch 902/3000\n",
      "702/702 [==============================] - 0s 586us/sample - loss: 0.1511 - acc: 0.9615 - val_loss: 0.9968 - val_acc: 0.7045\n",
      "Epoch 903/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1115 - acc: 0.9672 - val_loss: 1.0063 - val_acc: 0.7159\n",
      "Epoch 904/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.1555 - acc: 0.9601 - val_loss: 1.0691 - val_acc: 0.6932\n",
      "Epoch 905/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1402 - acc: 0.9573 - val_loss: 1.1277 - val_acc: 0.6875\n",
      "Epoch 906/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.1245 - acc: 0.9601 - val_loss: 1.0804 - val_acc: 0.6989\n",
      "Epoch 907/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1666 - acc: 0.9501 - val_loss: 1.1922 - val_acc: 0.6875\n",
      "Epoch 908/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1560 - acc: 0.9501 - val_loss: 1.1157 - val_acc: 0.6875\n",
      "Epoch 909/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.1404 - acc: 0.9715 - val_loss: 1.0373 - val_acc: 0.7045\n",
      "Epoch 910/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.1609 - acc: 0.9444 - val_loss: 1.1007 - val_acc: 0.6932\n",
      "Epoch 911/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1587 - acc: 0.9487 - val_loss: 1.0251 - val_acc: 0.7102\n",
      "Epoch 912/3000\n",
      "702/702 [==============================] - 0s 577us/sample - loss: 0.1591 - acc: 0.9658 - val_loss: 1.0367 - val_acc: 0.7102\n",
      "Epoch 913/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.1287 - acc: 0.9687 - val_loss: 1.1250 - val_acc: 0.7045\n",
      "Epoch 914/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.2369 - acc: 0.9544 - val_loss: 1.0946 - val_acc: 0.6989\n",
      "Epoch 915/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1913 - acc: 0.9501 - val_loss: 1.1688 - val_acc: 0.6705\n",
      "Epoch 916/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1563 - acc: 0.9459 - val_loss: 1.0810 - val_acc: 0.6989\n",
      "Epoch 917/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1394 - acc: 0.9558 - val_loss: 1.0769 - val_acc: 0.6989\n",
      "Epoch 918/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1945 - acc: 0.9430 - val_loss: 1.1069 - val_acc: 0.7045\n",
      "Epoch 919/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1812 - acc: 0.9544 - val_loss: 1.0396 - val_acc: 0.7045\n",
      "Epoch 920/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.1330 - acc: 0.9601 - val_loss: 1.0350 - val_acc: 0.6932\n",
      "Epoch 921/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1453 - acc: 0.9644 - val_loss: 1.0654 - val_acc: 0.6875\n",
      "Epoch 922/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 0.1180 - acc: 0.9658 - val_loss: 1.0813 - val_acc: 0.6818\n",
      "Epoch 923/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.1355 - acc: 0.9630 - val_loss: 1.0857 - val_acc: 0.6932\n",
      "Epoch 924/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1397 - acc: 0.9687 - val_loss: 1.0494 - val_acc: 0.6875\n",
      "Epoch 925/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1183 - acc: 0.9687 - val_loss: 0.9923 - val_acc: 0.7159\n",
      "Epoch 926/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1236 - acc: 0.9672 - val_loss: 1.0028 - val_acc: 0.7102\n",
      "Epoch 927/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1413 - acc: 0.9601 - val_loss: 1.0668 - val_acc: 0.7102\n",
      "Epoch 928/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1254 - acc: 0.9672 - val_loss: 1.0393 - val_acc: 0.7216\n",
      "Epoch 929/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1419 - acc: 0.9601 - val_loss: 1.0890 - val_acc: 0.7102\n",
      "Epoch 930/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1159 - acc: 0.9658 - val_loss: 1.1133 - val_acc: 0.7102\n",
      "Epoch 931/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1277 - acc: 0.9687 - val_loss: 1.0726 - val_acc: 0.7159\n",
      "Epoch 932/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.0827 - acc: 0.9729 - val_loss: 1.0041 - val_acc: 0.6932\n",
      "Epoch 933/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.1040 - acc: 0.9658 - val_loss: 0.9741 - val_acc: 0.7102\n",
      "Epoch 934/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0877 - acc: 0.9772 - val_loss: 1.0475 - val_acc: 0.6875\n",
      "Epoch 935/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0988 - acc: 0.9744 - val_loss: 1.0609 - val_acc: 0.7045\n",
      "Epoch 936/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.1030 - acc: 0.9658 - val_loss: 1.0302 - val_acc: 0.7102\n",
      "Epoch 937/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1048 - acc: 0.9729 - val_loss: 1.0847 - val_acc: 0.7159\n",
      "Epoch 938/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0988 - acc: 0.9672 - val_loss: 1.0986 - val_acc: 0.7045\n",
      "Epoch 939/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0887 - acc: 0.9786 - val_loss: 1.0963 - val_acc: 0.7273\n",
      "Epoch 940/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1271 - acc: 0.9687 - val_loss: 1.1267 - val_acc: 0.6932\n",
      "Epoch 941/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1444 - acc: 0.9658 - val_loss: 1.0599 - val_acc: 0.7102\n",
      "Epoch 942/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 0.0871 - acc: 0.9772 - val_loss: 1.0425 - val_acc: 0.7443\n",
      "Epoch 943/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.1013 - acc: 0.9744 - val_loss: 1.0505 - val_acc: 0.7330\n",
      "Epoch 944/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0859 - acc: 0.9744 - val_loss: 1.0535 - val_acc: 0.7045\n",
      "Epoch 945/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.1057 - acc: 0.9744 - val_loss: 1.0170 - val_acc: 0.7102\n",
      "Epoch 946/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1468 - acc: 0.9658 - val_loss: 1.0843 - val_acc: 0.6989\n",
      "Epoch 947/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.1096 - acc: 0.9672 - val_loss: 1.1509 - val_acc: 0.6875\n",
      "Epoch 948/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0983 - acc: 0.9715 - val_loss: 1.0680 - val_acc: 0.7045\n",
      "Epoch 949/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0840 - acc: 0.9772 - val_loss: 1.1095 - val_acc: 0.6932\n",
      "Epoch 950/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1213 - acc: 0.9615 - val_loss: 1.0406 - val_acc: 0.7102\n",
      "Epoch 951/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1031 - acc: 0.9687 - val_loss: 1.0747 - val_acc: 0.7102\n",
      "Epoch 952/3000\n",
      "702/702 [==============================] - 0s 583us/sample - loss: 0.0999 - acc: 0.9644 - val_loss: 1.1582 - val_acc: 0.6818\n",
      "Epoch 953/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0985 - acc: 0.9701 - val_loss: 1.0810 - val_acc: 0.7216\n",
      "Epoch 954/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1137 - acc: 0.9672 - val_loss: 0.9974 - val_acc: 0.7273\n",
      "Epoch 955/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0844 - acc: 0.9758 - val_loss: 1.0170 - val_acc: 0.7159\n",
      "Epoch 956/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1115 - acc: 0.9715 - val_loss: 1.0553 - val_acc: 0.7045\n",
      "Epoch 957/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1033 - acc: 0.9701 - val_loss: 1.0351 - val_acc: 0.7102\n",
      "Epoch 958/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0750 - acc: 0.9815 - val_loss: 1.0636 - val_acc: 0.7159\n",
      "Epoch 959/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0834 - acc: 0.9772 - val_loss: 1.0732 - val_acc: 0.7273\n",
      "Epoch 960/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0936 - acc: 0.9744 - val_loss: 1.0241 - val_acc: 0.7330\n",
      "Epoch 961/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1061 - acc: 0.9729 - val_loss: 1.0773 - val_acc: 0.7330\n",
      "Epoch 962/3000\n",
      "702/702 [==============================] - 0s 577us/sample - loss: 0.0661 - acc: 0.9858 - val_loss: 1.1299 - val_acc: 0.7216\n",
      "Epoch 963/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0728 - acc: 0.9815 - val_loss: 1.0538 - val_acc: 0.7102\n",
      "Epoch 964/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0703 - acc: 0.9886 - val_loss: 1.0212 - val_acc: 0.7386\n",
      "Epoch 965/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0937 - acc: 0.9701 - val_loss: 1.1047 - val_acc: 0.6932\n",
      "Epoch 966/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0683 - acc: 0.9858 - val_loss: 1.1148 - val_acc: 0.6932\n",
      "Epoch 967/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0843 - acc: 0.9701 - val_loss: 1.0798 - val_acc: 0.7102\n",
      "Epoch 968/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0787 - acc: 0.9801 - val_loss: 1.0938 - val_acc: 0.7045\n",
      "Epoch 969/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1365 - acc: 0.9729 - val_loss: 1.0612 - val_acc: 0.6989\n",
      "Epoch 970/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0971 - acc: 0.9758 - val_loss: 1.1158 - val_acc: 0.6989\n",
      "Epoch 971/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1037 - acc: 0.9658 - val_loss: 1.2300 - val_acc: 0.6989\n",
      "Epoch 972/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.1039 - acc: 0.9701 - val_loss: 1.1429 - val_acc: 0.7159\n",
      "Epoch 973/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0859 - acc: 0.9786 - val_loss: 1.1900 - val_acc: 0.6932\n",
      "Epoch 974/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.1058 - acc: 0.9687 - val_loss: 1.1230 - val_acc: 0.7216\n",
      "Epoch 975/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0951 - acc: 0.9744 - val_loss: 1.1001 - val_acc: 0.7273\n",
      "Epoch 976/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0941 - acc: 0.9744 - val_loss: 1.0861 - val_acc: 0.7273\n",
      "Epoch 977/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.1481 - acc: 0.9672 - val_loss: 1.0939 - val_acc: 0.7102\n",
      "Epoch 978/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.1450 - acc: 0.9601 - val_loss: 1.0893 - val_acc: 0.7102\n",
      "Epoch 979/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1518 - acc: 0.9573 - val_loss: 1.0642 - val_acc: 0.7159\n",
      "Epoch 980/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.1490 - acc: 0.9587 - val_loss: 1.1375 - val_acc: 0.6875\n",
      "Epoch 981/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1321 - acc: 0.9558 - val_loss: 1.0749 - val_acc: 0.7273\n",
      "Epoch 982/3000\n",
      "702/702 [==============================] - 0s 578us/sample - loss: 0.1453 - acc: 0.9601 - val_loss: 1.1335 - val_acc: 0.7273\n",
      "Epoch 983/3000\n",
      "702/702 [==============================] - 0s 514us/sample - loss: 0.1219 - acc: 0.9601 - val_loss: 1.1106 - val_acc: 0.7045\n",
      "Epoch 984/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.1366 - acc: 0.9587 - val_loss: 1.0965 - val_acc: 0.7102\n",
      "Epoch 985/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0933 - acc: 0.9701 - val_loss: 1.0622 - val_acc: 0.7159\n",
      "Epoch 986/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0885 - acc: 0.9786 - val_loss: 1.0560 - val_acc: 0.7330\n",
      "Epoch 987/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0967 - acc: 0.9687 - val_loss: 1.1857 - val_acc: 0.6932\n",
      "Epoch 988/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.1182 - acc: 0.9658 - val_loss: 1.2570 - val_acc: 0.6875\n",
      "Epoch 989/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1170 - acc: 0.9672 - val_loss: 1.1202 - val_acc: 0.7045\n",
      "Epoch 990/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0846 - acc: 0.9744 - val_loss: 1.0734 - val_acc: 0.7216\n",
      "Epoch 991/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1049 - acc: 0.9687 - val_loss: 1.1509 - val_acc: 0.7045\n",
      "Epoch 992/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.0828 - acc: 0.9801 - val_loss: 1.1614 - val_acc: 0.7273\n",
      "Epoch 993/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0992 - acc: 0.9729 - val_loss: 1.0787 - val_acc: 0.7330\n",
      "Epoch 994/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0773 - acc: 0.9786 - val_loss: 1.0678 - val_acc: 0.7045\n",
      "Epoch 995/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0865 - acc: 0.9729 - val_loss: 1.0319 - val_acc: 0.7045\n",
      "Epoch 996/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0848 - acc: 0.9786 - val_loss: 1.0132 - val_acc: 0.7386\n",
      "Epoch 997/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0975 - acc: 0.9687 - val_loss: 1.0895 - val_acc: 0.7273\n",
      "Epoch 998/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0958 - acc: 0.9715 - val_loss: 1.1284 - val_acc: 0.7102\n",
      "Epoch 999/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0756 - acc: 0.9815 - val_loss: 1.1220 - val_acc: 0.7216\n",
      "Epoch 1000/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0770 - acc: 0.9786 - val_loss: 1.1494 - val_acc: 0.7216\n",
      "Epoch 1001/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0929 - acc: 0.9772 - val_loss: 1.0586 - val_acc: 0.7159\n",
      "Epoch 1002/3000\n",
      "702/702 [==============================] - 0s 583us/sample - loss: 0.1022 - acc: 0.9601 - val_loss: 1.0297 - val_acc: 0.7386\n",
      "Epoch 1003/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.1129 - acc: 0.9644 - val_loss: 0.9998 - val_acc: 0.7443\n",
      "Epoch 1004/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.1089 - acc: 0.9744 - val_loss: 1.0756 - val_acc: 0.7330\n",
      "Epoch 1005/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1076 - acc: 0.9701 - val_loss: 1.0502 - val_acc: 0.7386\n",
      "Epoch 1006/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1360 - acc: 0.9573 - val_loss: 1.0379 - val_acc: 0.7216\n",
      "Epoch 1007/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0976 - acc: 0.9715 - val_loss: 1.1696 - val_acc: 0.6818\n",
      "Epoch 1008/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1053 - acc: 0.9715 - val_loss: 1.0758 - val_acc: 0.7443\n",
      "Epoch 1009/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0799 - acc: 0.9815 - val_loss: 1.1282 - val_acc: 0.7102\n",
      "Epoch 1010/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1312 - acc: 0.9544 - val_loss: 1.1895 - val_acc: 0.7045\n",
      "Epoch 1011/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1241 - acc: 0.9558 - val_loss: 1.0663 - val_acc: 0.7159\n",
      "Epoch 1012/3000\n",
      "702/702 [==============================] - 0s 577us/sample - loss: 0.1055 - acc: 0.9658 - val_loss: 1.0864 - val_acc: 0.7159\n",
      "Epoch 1013/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.1852 - acc: 0.9701 - val_loss: 1.1989 - val_acc: 0.7159\n",
      "Epoch 1014/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.1266 - acc: 0.9630 - val_loss: 1.1583 - val_acc: 0.7045\n",
      "Epoch 1015/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1222 - acc: 0.9530 - val_loss: 1.1450 - val_acc: 0.6932\n",
      "Epoch 1016/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1446 - acc: 0.9516 - val_loss: 1.1039 - val_acc: 0.7045\n",
      "Epoch 1017/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1312 - acc: 0.9587 - val_loss: 1.2611 - val_acc: 0.6875\n",
      "Epoch 1018/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1288 - acc: 0.9573 - val_loss: 1.2076 - val_acc: 0.6875\n",
      "Epoch 1019/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1074 - acc: 0.9744 - val_loss: 1.1253 - val_acc: 0.6932\n",
      "Epoch 1020/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.1119 - acc: 0.9658 - val_loss: 1.0827 - val_acc: 0.7216\n",
      "Epoch 1021/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1105 - acc: 0.9744 - val_loss: 1.1218 - val_acc: 0.7386\n",
      "Epoch 1022/3000\n",
      "702/702 [==============================] - 0s 584us/sample - loss: 0.1089 - acc: 0.9744 - val_loss: 1.1734 - val_acc: 0.6932\n",
      "Epoch 1023/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1000 - acc: 0.9701 - val_loss: 1.0939 - val_acc: 0.7045\n",
      "Epoch 1024/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1109 - acc: 0.9630 - val_loss: 1.0868 - val_acc: 0.7330\n",
      "Epoch 1025/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.1140 - acc: 0.9630 - val_loss: 1.1914 - val_acc: 0.7045\n",
      "Epoch 1026/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1056 - acc: 0.9729 - val_loss: 1.2417 - val_acc: 0.6875\n",
      "Epoch 1027/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1034 - acc: 0.9701 - val_loss: 1.1585 - val_acc: 0.7273\n",
      "Epoch 1028/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1114 - acc: 0.9644 - val_loss: 1.0648 - val_acc: 0.7330\n",
      "Epoch 1029/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0904 - acc: 0.9658 - val_loss: 1.1861 - val_acc: 0.7045\n",
      "Epoch 1030/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0984 - acc: 0.9715 - val_loss: 1.1089 - val_acc: 0.6818\n",
      "Epoch 1031/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1049 - acc: 0.9729 - val_loss: 1.0628 - val_acc: 0.7102\n",
      "Epoch 1032/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 0.1078 - acc: 0.9672 - val_loss: 1.0905 - val_acc: 0.7273\n",
      "Epoch 1033/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0917 - acc: 0.9786 - val_loss: 1.0893 - val_acc: 0.7386\n",
      "Epoch 1034/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0920 - acc: 0.9701 - val_loss: 1.0035 - val_acc: 0.7386\n",
      "Epoch 1035/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1107 - acc: 0.9729 - val_loss: 1.1187 - val_acc: 0.7216\n",
      "Epoch 1036/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0918 - acc: 0.9729 - val_loss: 1.1161 - val_acc: 0.7330\n",
      "Epoch 1037/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0974 - acc: 0.9687 - val_loss: 0.9976 - val_acc: 0.7557\n",
      "Epoch 1038/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.1156 - acc: 0.9672 - val_loss: 1.0808 - val_acc: 0.7330\n",
      "Epoch 1039/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1049 - acc: 0.9729 - val_loss: 1.0537 - val_acc: 0.7273\n",
      "Epoch 1040/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1170 - acc: 0.9658 - val_loss: 1.0334 - val_acc: 0.7216\n",
      "Epoch 1041/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1193 - acc: 0.9615 - val_loss: 1.1444 - val_acc: 0.7159\n",
      "Epoch 1042/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.1200 - acc: 0.9630 - val_loss: 1.0981 - val_acc: 0.7102\n",
      "Epoch 1043/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0978 - acc: 0.9729 - val_loss: 1.1138 - val_acc: 0.6989\n",
      "Epoch 1044/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1080 - acc: 0.9672 - val_loss: 1.1870 - val_acc: 0.7102\n",
      "Epoch 1045/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0963 - acc: 0.9672 - val_loss: 1.1936 - val_acc: 0.7159\n",
      "Epoch 1046/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0954 - acc: 0.9715 - val_loss: 1.1679 - val_acc: 0.7216\n",
      "Epoch 1047/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.1048 - acc: 0.9744 - val_loss: 1.0999 - val_acc: 0.7045\n",
      "Epoch 1048/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1131 - acc: 0.9687 - val_loss: 1.1376 - val_acc: 0.6989\n",
      "Epoch 1049/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0870 - acc: 0.9729 - val_loss: 1.2194 - val_acc: 0.7102\n",
      "Epoch 1050/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1071 - acc: 0.9701 - val_loss: 1.2243 - val_acc: 0.6875\n",
      "Epoch 1051/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1396 - acc: 0.9558 - val_loss: 1.1601 - val_acc: 0.7045\n",
      "Epoch 1052/3000\n",
      "702/702 [==============================] - 0s 584us/sample - loss: 0.1557 - acc: 0.9558 - val_loss: 1.0684 - val_acc: 0.6875\n",
      "Epoch 1053/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1760 - acc: 0.9430 - val_loss: 1.1689 - val_acc: 0.6875\n",
      "Epoch 1054/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1470 - acc: 0.9573 - val_loss: 1.2976 - val_acc: 0.6932\n",
      "Epoch 1055/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.1735 - acc: 0.9630 - val_loss: 1.1690 - val_acc: 0.7045\n",
      "Epoch 1056/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.1296 - acc: 0.9630 - val_loss: 1.0872 - val_acc: 0.7045\n",
      "Epoch 1057/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1678 - acc: 0.9573 - val_loss: 1.0261 - val_acc: 0.7273\n",
      "Epoch 1058/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1621 - acc: 0.9459 - val_loss: 1.1011 - val_acc: 0.6989\n",
      "Epoch 1059/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.1029 - acc: 0.9630 - val_loss: 1.1973 - val_acc: 0.7045\n",
      "Epoch 1060/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0953 - acc: 0.9672 - val_loss: 1.2069 - val_acc: 0.7045\n",
      "Epoch 1061/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2105 - acc: 0.9615 - val_loss: 1.0836 - val_acc: 0.7159\n",
      "Epoch 1062/3000\n",
      "702/702 [==============================] - 0s 581us/sample - loss: 0.1408 - acc: 0.9587 - val_loss: 1.1727 - val_acc: 0.7159\n",
      "Epoch 1063/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1651 - acc: 0.9544 - val_loss: 1.2453 - val_acc: 0.6875\n",
      "Epoch 1064/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2934 - acc: 0.9245 - val_loss: 1.0708 - val_acc: 0.7330\n",
      "Epoch 1065/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.2554 - acc: 0.9288 - val_loss: 1.1387 - val_acc: 0.7102\n",
      "Epoch 1066/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1647 - acc: 0.9473 - val_loss: 1.1945 - val_acc: 0.7102\n",
      "Epoch 1067/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1788 - acc: 0.9530 - val_loss: 1.0927 - val_acc: 0.6818\n",
      "Epoch 1068/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.2125 - acc: 0.9430 - val_loss: 1.0416 - val_acc: 0.7102\n",
      "Epoch 1069/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.2949 - acc: 0.9131 - val_loss: 1.1988 - val_acc: 0.6875\n",
      "Epoch 1070/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3347 - acc: 0.9031 - val_loss: 1.1544 - val_acc: 0.7102\n",
      "Epoch 1071/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2724 - acc: 0.9131 - val_loss: 1.0899 - val_acc: 0.7102\n",
      "Epoch 1072/3000\n",
      "702/702 [==============================] - 0s 570us/sample - loss: 0.3139 - acc: 0.9031 - val_loss: 1.1135 - val_acc: 0.6989\n",
      "Epoch 1073/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.3777 - acc: 0.8974 - val_loss: 1.2288 - val_acc: 0.7045\n",
      "Epoch 1074/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2613 - acc: 0.9288 - val_loss: 1.2134 - val_acc: 0.6761\n",
      "Epoch 1075/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.2461 - acc: 0.9245 - val_loss: 1.1332 - val_acc: 0.6875\n",
      "Epoch 1076/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.2690 - acc: 0.9373 - val_loss: 1.0203 - val_acc: 0.6989\n",
      "Epoch 1077/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.2729 - acc: 0.9231 - val_loss: 1.0981 - val_acc: 0.7102\n",
      "Epoch 1078/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.1955 - acc: 0.9330 - val_loss: 1.1413 - val_acc: 0.6989\n",
      "Epoch 1079/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1683 - acc: 0.9544 - val_loss: 1.0209 - val_acc: 0.7216\n",
      "Epoch 1080/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.1802 - acc: 0.9473 - val_loss: 0.9924 - val_acc: 0.7045\n",
      "Epoch 1081/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2190 - acc: 0.9516 - val_loss: 1.0304 - val_acc: 0.7216\n",
      "Epoch 1082/3000\n",
      "702/702 [==============================] - 0s 581us/sample - loss: 0.2812 - acc: 0.9288 - val_loss: 1.2062 - val_acc: 0.6989\n",
      "Epoch 1083/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2162 - acc: 0.9259 - val_loss: 1.0293 - val_acc: 0.7045\n",
      "Epoch 1084/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.2908 - acc: 0.9046 - val_loss: 1.0197 - val_acc: 0.7273\n",
      "Epoch 1085/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.2893 - acc: 0.9217 - val_loss: 1.0483 - val_acc: 0.7159\n",
      "Epoch 1086/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.2211 - acc: 0.9259 - val_loss: 1.0430 - val_acc: 0.7330\n",
      "Epoch 1087/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.2321 - acc: 0.9274 - val_loss: 1.1345 - val_acc: 0.6989\n",
      "Epoch 1088/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1939 - acc: 0.9345 - val_loss: 1.0990 - val_acc: 0.7330\n",
      "Epoch 1089/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1870 - acc: 0.9459 - val_loss: 1.1060 - val_acc: 0.7045\n",
      "Epoch 1090/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.2783 - acc: 0.9202 - val_loss: 1.1233 - val_acc: 0.7159\n",
      "Epoch 1091/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1220 - acc: 0.9530 - val_loss: 1.1059 - val_acc: 0.7045\n",
      "Epoch 1092/3000\n",
      "702/702 [==============================] - 0s 578us/sample - loss: 0.1615 - acc: 0.9530 - val_loss: 1.0422 - val_acc: 0.7216\n",
      "Epoch 1093/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.1599 - acc: 0.9587 - val_loss: 1.0258 - val_acc: 0.7216\n",
      "Epoch 1094/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1219 - acc: 0.9587 - val_loss: 1.1068 - val_acc: 0.7045\n",
      "Epoch 1095/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1368 - acc: 0.9601 - val_loss: 1.0505 - val_acc: 0.7159\n",
      "Epoch 1096/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1680 - acc: 0.9487 - val_loss: 1.1419 - val_acc: 0.7159\n",
      "Epoch 1097/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1654 - acc: 0.9516 - val_loss: 1.0570 - val_acc: 0.7273\n",
      "Epoch 1098/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.1609 - acc: 0.9459 - val_loss: 1.0473 - val_acc: 0.7386\n",
      "Epoch 1099/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.1314 - acc: 0.9658 - val_loss: 1.1318 - val_acc: 0.7102\n",
      "Epoch 1100/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.1502 - acc: 0.9573 - val_loss: 0.9908 - val_acc: 0.7216\n",
      "Epoch 1101/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1135 - acc: 0.9658 - val_loss: 1.0283 - val_acc: 0.7273\n",
      "Epoch 1102/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.1890 - acc: 0.9459 - val_loss: 1.0955 - val_acc: 0.7443\n",
      "Epoch 1103/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1677 - acc: 0.9473 - val_loss: 1.0549 - val_acc: 0.6989\n",
      "Epoch 1104/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.2074 - acc: 0.9402 - val_loss: 1.0937 - val_acc: 0.7045\n",
      "Epoch 1105/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1971 - acc: 0.9444 - val_loss: 1.1579 - val_acc: 0.6875\n",
      "Epoch 1106/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1989 - acc: 0.9416 - val_loss: 1.1462 - val_acc: 0.6932\n",
      "Epoch 1107/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1739 - acc: 0.9558 - val_loss: 1.0042 - val_acc: 0.7102\n",
      "Epoch 1108/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1509 - acc: 0.9587 - val_loss: 1.0318 - val_acc: 0.6932\n",
      "Epoch 1109/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.1589 - acc: 0.9501 - val_loss: 1.0288 - val_acc: 0.7273\n",
      "Epoch 1110/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1547 - acc: 0.9501 - val_loss: 1.0895 - val_acc: 0.7045\n",
      "Epoch 1111/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1427 - acc: 0.9544 - val_loss: 1.1226 - val_acc: 0.7159\n",
      "Epoch 1112/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.1225 - acc: 0.9658 - val_loss: 1.1881 - val_acc: 0.6761\n",
      "Epoch 1113/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.1215 - acc: 0.9601 - val_loss: 1.2538 - val_acc: 0.6875\n",
      "Epoch 1114/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1200 - acc: 0.9615 - val_loss: 1.2439 - val_acc: 0.6875\n",
      "Epoch 1115/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.1217 - acc: 0.9615 - val_loss: 1.1843 - val_acc: 0.7273\n",
      "Epoch 1116/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1361 - acc: 0.9644 - val_loss: 1.2074 - val_acc: 0.7273\n",
      "Epoch 1117/3000\n",
      "702/702 [==============================] - 0s 516us/sample - loss: 0.1265 - acc: 0.9715 - val_loss: 1.1461 - val_acc: 0.7330\n",
      "Epoch 1118/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1343 - acc: 0.9587 - val_loss: 1.1027 - val_acc: 0.7159\n",
      "Epoch 1119/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1080 - acc: 0.9729 - val_loss: 1.1828 - val_acc: 0.7102\n",
      "Epoch 1120/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0956 - acc: 0.9687 - val_loss: 1.1750 - val_acc: 0.7045\n",
      "Epoch 1121/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1013 - acc: 0.9715 - val_loss: 1.1786 - val_acc: 0.7045\n",
      "Epoch 1122/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 0.0914 - acc: 0.9672 - val_loss: 1.1574 - val_acc: 0.6989\n",
      "Epoch 1123/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0956 - acc: 0.9801 - val_loss: 1.2031 - val_acc: 0.7045\n",
      "Epoch 1124/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0910 - acc: 0.9729 - val_loss: 1.1715 - val_acc: 0.6989\n",
      "Epoch 1125/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.1268 - acc: 0.9658 - val_loss: 1.1525 - val_acc: 0.7102\n",
      "Epoch 1126/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.1253 - acc: 0.9573 - val_loss: 1.2315 - val_acc: 0.6989\n",
      "Epoch 1127/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0965 - acc: 0.9729 - val_loss: 1.1944 - val_acc: 0.7045\n",
      "Epoch 1128/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0841 - acc: 0.9786 - val_loss: 1.1969 - val_acc: 0.7102\n",
      "Epoch 1129/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.1070 - acc: 0.9615 - val_loss: 1.1550 - val_acc: 0.7045\n",
      "Epoch 1130/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0799 - acc: 0.9772 - val_loss: 1.1629 - val_acc: 0.7216\n",
      "Epoch 1131/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0866 - acc: 0.9801 - val_loss: 1.1755 - val_acc: 0.7216\n",
      "Epoch 1132/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 0.1083 - acc: 0.9772 - val_loss: 1.1787 - val_acc: 0.7216\n",
      "Epoch 1133/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.1220 - acc: 0.9687 - val_loss: 1.2046 - val_acc: 0.7045\n",
      "Epoch 1134/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.2628 - acc: 0.9601 - val_loss: 1.1674 - val_acc: 0.7045\n",
      "Epoch 1135/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1322 - acc: 0.9615 - val_loss: 1.1990 - val_acc: 0.7102\n",
      "Epoch 1136/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1430 - acc: 0.9558 - val_loss: 1.2812 - val_acc: 0.7102\n",
      "Epoch 1137/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.1436 - acc: 0.9658 - val_loss: 1.2624 - val_acc: 0.7159\n",
      "Epoch 1138/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1200 - acc: 0.9615 - val_loss: 1.2250 - val_acc: 0.7159\n",
      "Epoch 1139/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1335 - acc: 0.9601 - val_loss: 1.1632 - val_acc: 0.7159\n",
      "Epoch 1140/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.1514 - acc: 0.9587 - val_loss: 1.1944 - val_acc: 0.7045\n",
      "Epoch 1141/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1583 - acc: 0.9459 - val_loss: 1.3420 - val_acc: 0.6932\n",
      "Epoch 1142/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.1067 - acc: 0.9658 - val_loss: 1.3056 - val_acc: 0.7102\n",
      "Epoch 1143/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1247 - acc: 0.9615 - val_loss: 1.2656 - val_acc: 0.7045\n",
      "Epoch 1144/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1275 - acc: 0.9601 - val_loss: 1.2054 - val_acc: 0.6989\n",
      "Epoch 1145/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0960 - acc: 0.9672 - val_loss: 1.2168 - val_acc: 0.6932\n",
      "Epoch 1146/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0728 - acc: 0.9815 - val_loss: 1.2029 - val_acc: 0.7102\n",
      "Epoch 1147/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1070 - acc: 0.9615 - val_loss: 1.1729 - val_acc: 0.7159\n",
      "Epoch 1148/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0979 - acc: 0.9687 - val_loss: 1.1414 - val_acc: 0.7159\n",
      "Epoch 1149/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1180 - acc: 0.9658 - val_loss: 1.1753 - val_acc: 0.6875\n",
      "Epoch 1150/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1033 - acc: 0.9701 - val_loss: 1.1312 - val_acc: 0.6989\n",
      "Epoch 1151/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1038 - acc: 0.9658 - val_loss: 1.1905 - val_acc: 0.6989\n",
      "Epoch 1152/3000\n",
      "702/702 [==============================] - 0s 596us/sample - loss: 0.1047 - acc: 0.9658 - val_loss: 1.3582 - val_acc: 0.6818\n",
      "Epoch 1153/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.1063 - acc: 0.9672 - val_loss: 1.2149 - val_acc: 0.6761\n",
      "Epoch 1154/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0825 - acc: 0.9772 - val_loss: 1.1289 - val_acc: 0.7045\n",
      "Epoch 1155/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0889 - acc: 0.9786 - val_loss: 1.2565 - val_acc: 0.7045\n",
      "Epoch 1156/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0557 - acc: 0.9872 - val_loss: 1.2211 - val_acc: 0.7045\n",
      "Epoch 1157/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0676 - acc: 0.9815 - val_loss: 1.0924 - val_acc: 0.6989\n",
      "Epoch 1158/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0890 - acc: 0.9758 - val_loss: 1.0583 - val_acc: 0.7102\n",
      "Epoch 1159/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0966 - acc: 0.9744 - val_loss: 1.0509 - val_acc: 0.7273\n",
      "Epoch 1160/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0862 - acc: 0.9744 - val_loss: 1.1242 - val_acc: 0.6932\n",
      "Epoch 1161/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0654 - acc: 0.9815 - val_loss: 1.1795 - val_acc: 0.7102\n",
      "Epoch 1162/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.1162 - acc: 0.9658 - val_loss: 1.1753 - val_acc: 0.7045\n",
      "Epoch 1163/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0926 - acc: 0.9729 - val_loss: 1.0793 - val_acc: 0.7159\n",
      "Epoch 1164/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0796 - acc: 0.9801 - val_loss: 1.1486 - val_acc: 0.7273\n",
      "Epoch 1165/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0734 - acc: 0.9772 - val_loss: 1.2643 - val_acc: 0.7102\n",
      "Epoch 1166/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0931 - acc: 0.9772 - val_loss: 1.1730 - val_acc: 0.7159\n",
      "Epoch 1167/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0843 - acc: 0.9786 - val_loss: 1.1142 - val_acc: 0.7102\n",
      "Epoch 1168/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1044 - acc: 0.9687 - val_loss: 1.1279 - val_acc: 0.7102\n",
      "Epoch 1169/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0818 - acc: 0.9729 - val_loss: 1.1967 - val_acc: 0.6989\n",
      "Epoch 1170/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.1030 - acc: 0.9744 - val_loss: 1.2152 - val_acc: 0.7102\n",
      "Epoch 1171/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0935 - acc: 0.9729 - val_loss: 1.1958 - val_acc: 0.6989\n",
      "Epoch 1172/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.0932 - acc: 0.9772 - val_loss: 1.1397 - val_acc: 0.7045\n",
      "Epoch 1173/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0961 - acc: 0.9744 - val_loss: 1.1602 - val_acc: 0.7102\n",
      "Epoch 1174/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1197 - acc: 0.9715 - val_loss: 1.2254 - val_acc: 0.6989\n",
      "Epoch 1175/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0896 - acc: 0.9758 - val_loss: 1.2713 - val_acc: 0.6932\n",
      "Epoch 1176/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0826 - acc: 0.9801 - val_loss: 1.3102 - val_acc: 0.6818\n",
      "Epoch 1177/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.1153 - acc: 0.9630 - val_loss: 1.2986 - val_acc: 0.6705\n",
      "Epoch 1178/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0969 - acc: 0.9715 - val_loss: 1.1391 - val_acc: 0.7159\n",
      "Epoch 1179/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0794 - acc: 0.9729 - val_loss: 1.2310 - val_acc: 0.6818\n",
      "Epoch 1180/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1116 - acc: 0.9744 - val_loss: 1.2135 - val_acc: 0.6875\n",
      "Epoch 1181/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0785 - acc: 0.9786 - val_loss: 1.1859 - val_acc: 0.6932\n",
      "Epoch 1182/3000\n",
      "702/702 [==============================] - 0s 564us/sample - loss: 0.1102 - acc: 0.9858 - val_loss: 1.2172 - val_acc: 0.6932\n",
      "Epoch 1183/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.1112 - acc: 0.9644 - val_loss: 1.1680 - val_acc: 0.7102\n",
      "Epoch 1184/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0807 - acc: 0.9729 - val_loss: 1.1405 - val_acc: 0.7273\n",
      "Epoch 1185/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0888 - acc: 0.9772 - val_loss: 1.2215 - val_acc: 0.6989\n",
      "Epoch 1186/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0826 - acc: 0.9843 - val_loss: 1.2364 - val_acc: 0.6932\n",
      "Epoch 1187/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.1128 - acc: 0.9772 - val_loss: 1.2159 - val_acc: 0.6932\n",
      "Epoch 1188/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0670 - acc: 0.9758 - val_loss: 1.2584 - val_acc: 0.6875\n",
      "Epoch 1189/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0746 - acc: 0.9729 - val_loss: 1.1564 - val_acc: 0.7159\n",
      "Epoch 1190/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0848 - acc: 0.9786 - val_loss: 1.1933 - val_acc: 0.7216\n",
      "Epoch 1191/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0576 - acc: 0.9815 - val_loss: 1.2294 - val_acc: 0.7159\n",
      "Epoch 1192/3000\n",
      "702/702 [==============================] - 0s 581us/sample - loss: 0.0606 - acc: 0.9858 - val_loss: 1.2383 - val_acc: 0.7216\n",
      "Epoch 1193/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.0634 - acc: 0.9801 - val_loss: 1.2943 - val_acc: 0.7102\n",
      "Epoch 1194/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0722 - acc: 0.9772 - val_loss: 1.2092 - val_acc: 0.7159\n",
      "Epoch 1195/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0512 - acc: 0.9858 - val_loss: 1.2619 - val_acc: 0.7102\n",
      "Epoch 1196/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0915 - acc: 0.9786 - val_loss: 1.2434 - val_acc: 0.7216\n",
      "Epoch 1197/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0432 - acc: 0.9915 - val_loss: 1.1615 - val_acc: 0.6989\n",
      "Epoch 1198/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.1087 - acc: 0.9744 - val_loss: 1.1355 - val_acc: 0.7045\n",
      "Epoch 1199/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0871 - acc: 0.9758 - val_loss: 1.1108 - val_acc: 0.7102\n",
      "Epoch 1200/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0899 - acc: 0.9687 - val_loss: 1.1094 - val_acc: 0.7216\n",
      "Epoch 1201/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0969 - acc: 0.9701 - val_loss: 1.0641 - val_acc: 0.7273\n",
      "Epoch 1202/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.1149 - acc: 0.9658 - val_loss: 1.2501 - val_acc: 0.6648\n",
      "Epoch 1203/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0833 - acc: 0.9786 - val_loss: 1.2012 - val_acc: 0.6932\n",
      "Epoch 1204/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0611 - acc: 0.9858 - val_loss: 1.1447 - val_acc: 0.6932\n",
      "Epoch 1205/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0967 - acc: 0.9772 - val_loss: 1.1875 - val_acc: 0.6932\n",
      "Epoch 1206/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0627 - acc: 0.9815 - val_loss: 1.2033 - val_acc: 0.7159\n",
      "Epoch 1207/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0838 - acc: 0.9801 - val_loss: 1.2301 - val_acc: 0.6989\n",
      "Epoch 1208/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0587 - acc: 0.9843 - val_loss: 1.2420 - val_acc: 0.7045\n",
      "Epoch 1209/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.1029 - acc: 0.9729 - val_loss: 1.2559 - val_acc: 0.7045\n",
      "Epoch 1210/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0723 - acc: 0.9758 - val_loss: 1.2817 - val_acc: 0.7045\n",
      "Epoch 1211/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0764 - acc: 0.9758 - val_loss: 1.2827 - val_acc: 0.7102\n",
      "Epoch 1212/3000\n",
      "702/702 [==============================] - 0s 573us/sample - loss: 0.0690 - acc: 0.9772 - val_loss: 1.2705 - val_acc: 0.7159\n",
      "Epoch 1213/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1057 - acc: 0.9715 - val_loss: 1.1669 - val_acc: 0.7045\n",
      "Epoch 1214/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0506 - acc: 0.9872 - val_loss: 1.1834 - val_acc: 0.7045\n",
      "Epoch 1215/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0994 - acc: 0.9658 - val_loss: 1.1540 - val_acc: 0.7273\n",
      "Epoch 1216/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1035 - acc: 0.9758 - val_loss: 1.1622 - val_acc: 0.7273\n",
      "Epoch 1217/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0750 - acc: 0.9843 - val_loss: 1.1462 - val_acc: 0.7330\n",
      "Epoch 1218/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0719 - acc: 0.9772 - val_loss: 1.1905 - val_acc: 0.7216\n",
      "Epoch 1219/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0629 - acc: 0.9801 - val_loss: 1.3175 - val_acc: 0.6989\n",
      "Epoch 1220/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0673 - acc: 0.9801 - val_loss: 1.1984 - val_acc: 0.7045\n",
      "Epoch 1221/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0751 - acc: 0.9758 - val_loss: 1.1511 - val_acc: 0.7273\n",
      "Epoch 1222/3000\n",
      "702/702 [==============================] - 0s 570us/sample - loss: 0.0572 - acc: 0.9858 - val_loss: 1.2525 - val_acc: 0.7102\n",
      "Epoch 1223/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0844 - acc: 0.9786 - val_loss: 1.3398 - val_acc: 0.6989\n",
      "Epoch 1224/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0628 - acc: 0.9815 - val_loss: 1.2610 - val_acc: 0.7216\n",
      "Epoch 1225/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0599 - acc: 0.9815 - val_loss: 1.1731 - val_acc: 0.7159\n",
      "Epoch 1226/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0691 - acc: 0.9786 - val_loss: 1.1124 - val_acc: 0.7330\n",
      "Epoch 1227/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0738 - acc: 0.9758 - val_loss: 1.0922 - val_acc: 0.7273\n",
      "Epoch 1228/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0967 - acc: 0.9758 - val_loss: 1.1703 - val_acc: 0.7159\n",
      "Epoch 1229/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0610 - acc: 0.9815 - val_loss: 1.1280 - val_acc: 0.7216\n",
      "Epoch 1230/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0671 - acc: 0.9815 - val_loss: 1.0991 - val_acc: 0.7216\n",
      "Epoch 1231/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0565 - acc: 0.9758 - val_loss: 1.1263 - val_acc: 0.7330\n",
      "Epoch 1232/3000\n",
      "702/702 [==============================] - 0s 577us/sample - loss: 0.0776 - acc: 0.9786 - val_loss: 1.1768 - val_acc: 0.7216\n",
      "Epoch 1233/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0844 - acc: 0.9744 - val_loss: 1.1171 - val_acc: 0.7102\n",
      "Epoch 1234/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0844 - acc: 0.9758 - val_loss: 1.1861 - val_acc: 0.6989\n",
      "Epoch 1235/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0632 - acc: 0.9829 - val_loss: 1.4190 - val_acc: 0.6818\n",
      "Epoch 1236/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1026 - acc: 0.9801 - val_loss: 1.1830 - val_acc: 0.6989\n",
      "Epoch 1237/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1178 - acc: 0.9615 - val_loss: 1.2070 - val_acc: 0.6818\n",
      "Epoch 1238/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0877 - acc: 0.9801 - val_loss: 1.2197 - val_acc: 0.6932\n",
      "Epoch 1239/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.1253 - acc: 0.9658 - val_loss: 1.1069 - val_acc: 0.7330\n",
      "Epoch 1240/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0932 - acc: 0.9729 - val_loss: 1.1275 - val_acc: 0.7273\n",
      "Epoch 1241/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0796 - acc: 0.9744 - val_loss: 1.2086 - val_acc: 0.7102\n",
      "Epoch 1242/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 0.0555 - acc: 0.9829 - val_loss: 1.2928 - val_acc: 0.7102\n",
      "Epoch 1243/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0802 - acc: 0.9758 - val_loss: 1.1878 - val_acc: 0.6989\n",
      "Epoch 1244/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0933 - acc: 0.9701 - val_loss: 1.1646 - val_acc: 0.7045\n",
      "Epoch 1245/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0669 - acc: 0.9786 - val_loss: 1.2028 - val_acc: 0.7045\n",
      "Epoch 1246/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0802 - acc: 0.9801 - val_loss: 1.2637 - val_acc: 0.6989\n",
      "Epoch 1247/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0473 - acc: 0.9858 - val_loss: 1.2136 - val_acc: 0.7045\n",
      "Epoch 1248/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0549 - acc: 0.9829 - val_loss: 1.2556 - val_acc: 0.7045\n",
      "Epoch 1249/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0661 - acc: 0.9843 - val_loss: 1.1984 - val_acc: 0.7102\n",
      "Epoch 1250/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0641 - acc: 0.9786 - val_loss: 1.1809 - val_acc: 0.7102\n",
      "Epoch 1251/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0667 - acc: 0.9786 - val_loss: 1.2120 - val_acc: 0.6932\n",
      "Epoch 1252/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.0528 - acc: 0.9858 - val_loss: 1.1828 - val_acc: 0.6989\n",
      "Epoch 1253/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0648 - acc: 0.9829 - val_loss: 1.1358 - val_acc: 0.7273\n",
      "Epoch 1254/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0581 - acc: 0.9815 - val_loss: 1.2016 - val_acc: 0.7273\n",
      "Epoch 1255/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0554 - acc: 0.9829 - val_loss: 1.1990 - val_acc: 0.7273\n",
      "Epoch 1256/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0526 - acc: 0.9829 - val_loss: 1.2593 - val_acc: 0.7102\n",
      "Epoch 1257/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0549 - acc: 0.9858 - val_loss: 1.1999 - val_acc: 0.7045\n",
      "Epoch 1258/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0437 - acc: 0.9858 - val_loss: 1.2378 - val_acc: 0.6989\n",
      "Epoch 1259/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1254 - acc: 0.9687 - val_loss: 1.1636 - val_acc: 0.7216\n",
      "Epoch 1260/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1141 - acc: 0.9786 - val_loss: 1.1996 - val_acc: 0.7102\n",
      "Epoch 1261/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0700 - acc: 0.9801 - val_loss: 1.1851 - val_acc: 0.7159\n",
      "Epoch 1262/3000\n",
      "702/702 [==============================] - 0s 578us/sample - loss: 0.0873 - acc: 0.9744 - val_loss: 1.1790 - val_acc: 0.6932\n",
      "Epoch 1263/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0800 - acc: 0.9715 - val_loss: 1.2147 - val_acc: 0.7102\n",
      "Epoch 1264/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0769 - acc: 0.9744 - val_loss: 1.1446 - val_acc: 0.7216\n",
      "Epoch 1265/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0724 - acc: 0.9758 - val_loss: 1.1828 - val_acc: 0.7159\n",
      "Epoch 1266/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0897 - acc: 0.9815 - val_loss: 1.2396 - val_acc: 0.7159\n",
      "Epoch 1267/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.1028 - acc: 0.9744 - val_loss: 1.2756 - val_acc: 0.7045\n",
      "Epoch 1268/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0798 - acc: 0.9744 - val_loss: 1.2417 - val_acc: 0.7102\n",
      "Epoch 1269/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0552 - acc: 0.9858 - val_loss: 1.3228 - val_acc: 0.7216\n",
      "Epoch 1270/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0715 - acc: 0.9815 - val_loss: 1.2205 - val_acc: 0.7216\n",
      "Epoch 1271/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0688 - acc: 0.9843 - val_loss: 1.1305 - val_acc: 0.7273\n",
      "Epoch 1272/3000\n",
      "702/702 [==============================] - 0s 568us/sample - loss: 0.0723 - acc: 0.9772 - val_loss: 1.1748 - val_acc: 0.7159\n",
      "Epoch 1273/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0581 - acc: 0.9786 - val_loss: 1.1950 - val_acc: 0.7045\n",
      "Epoch 1274/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0568 - acc: 0.9858 - val_loss: 1.2470 - val_acc: 0.7216\n",
      "Epoch 1275/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0578 - acc: 0.9858 - val_loss: 1.3156 - val_acc: 0.7045\n",
      "Epoch 1276/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0753 - acc: 0.9786 - val_loss: 1.2358 - val_acc: 0.7159\n",
      "Epoch 1277/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1203 - acc: 0.9801 - val_loss: 1.1620 - val_acc: 0.7159\n",
      "Epoch 1278/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.1273 - acc: 0.9573 - val_loss: 1.0941 - val_acc: 0.7216\n",
      "Epoch 1279/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1401 - acc: 0.9587 - val_loss: 1.1043 - val_acc: 0.6932\n",
      "Epoch 1280/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.1864 - acc: 0.9430 - val_loss: 1.1837 - val_acc: 0.7273\n",
      "Epoch 1281/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1848 - acc: 0.9501 - val_loss: 1.2605 - val_acc: 0.7330\n",
      "Epoch 1282/3000\n",
      "702/702 [==============================] - 0s 557us/sample - loss: 0.1280 - acc: 0.9672 - val_loss: 1.2484 - val_acc: 0.7330\n",
      "Epoch 1283/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1509 - acc: 0.9573 - val_loss: 1.2108 - val_acc: 0.7159\n",
      "Epoch 1284/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1637 - acc: 0.9473 - val_loss: 1.2385 - val_acc: 0.7102\n",
      "Epoch 1285/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1369 - acc: 0.9630 - val_loss: 1.2693 - val_acc: 0.7273\n",
      "Epoch 1286/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1094 - acc: 0.9630 - val_loss: 1.4489 - val_acc: 0.7159\n",
      "Epoch 1287/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1271 - acc: 0.9601 - val_loss: 1.2184 - val_acc: 0.7386\n",
      "Epoch 1288/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.1392 - acc: 0.9601 - val_loss: 1.1974 - val_acc: 0.7102\n",
      "Epoch 1289/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0942 - acc: 0.9715 - val_loss: 1.2866 - val_acc: 0.7045\n",
      "Epoch 1290/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1190 - acc: 0.9658 - val_loss: 1.3216 - val_acc: 0.7159\n",
      "Epoch 1291/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1079 - acc: 0.9758 - val_loss: 1.2125 - val_acc: 0.7045\n",
      "Epoch 1292/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 0.0858 - acc: 0.9715 - val_loss: 1.3206 - val_acc: 0.7102\n",
      "Epoch 1293/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.0844 - acc: 0.9701 - val_loss: 1.2619 - val_acc: 0.7045\n",
      "Epoch 1294/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0763 - acc: 0.9715 - val_loss: 1.2388 - val_acc: 0.7045\n",
      "Epoch 1295/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0792 - acc: 0.9744 - val_loss: 1.2654 - val_acc: 0.7216\n",
      "Epoch 1296/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.1013 - acc: 0.9729 - val_loss: 1.2674 - val_acc: 0.7159\n",
      "Epoch 1297/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2397 - acc: 0.9715 - val_loss: 1.1484 - val_acc: 0.7216\n",
      "Epoch 1298/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.1150 - acc: 0.9644 - val_loss: 1.1973 - val_acc: 0.7045\n",
      "Epoch 1299/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.1417 - acc: 0.9601 - val_loss: 1.1532 - val_acc: 0.7102\n",
      "Epoch 1300/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1049 - acc: 0.9701 - val_loss: 1.4268 - val_acc: 0.7045\n",
      "Epoch 1301/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1190 - acc: 0.9715 - val_loss: 1.2136 - val_acc: 0.7159\n",
      "Epoch 1302/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.1648 - acc: 0.9630 - val_loss: 1.2689 - val_acc: 0.6875\n",
      "Epoch 1303/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1339 - acc: 0.9630 - val_loss: 1.1807 - val_acc: 0.6875\n",
      "Epoch 1304/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.1254 - acc: 0.9687 - val_loss: 1.1062 - val_acc: 0.7102\n",
      "Epoch 1305/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1424 - acc: 0.9687 - val_loss: 1.1285 - val_acc: 0.6989\n",
      "Epoch 1306/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0989 - acc: 0.9729 - val_loss: 1.1560 - val_acc: 0.7102\n",
      "Epoch 1307/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0997 - acc: 0.9786 - val_loss: 1.1440 - val_acc: 0.7159\n",
      "Epoch 1308/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0638 - acc: 0.9829 - val_loss: 1.1397 - val_acc: 0.7273\n",
      "Epoch 1309/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1243 - acc: 0.9658 - val_loss: 1.1654 - val_acc: 0.7159\n",
      "Epoch 1310/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0591 - acc: 0.9843 - val_loss: 1.2069 - val_acc: 0.7045\n",
      "Epoch 1311/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0794 - acc: 0.9801 - val_loss: 1.2242 - val_acc: 0.6989\n",
      "Epoch 1312/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.0854 - acc: 0.9701 - val_loss: 1.2126 - val_acc: 0.7159\n",
      "Epoch 1313/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0812 - acc: 0.9744 - val_loss: 1.1339 - val_acc: 0.7330\n",
      "Epoch 1314/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0579 - acc: 0.9858 - val_loss: 1.1889 - val_acc: 0.7102\n",
      "Epoch 1315/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0506 - acc: 0.9872 - val_loss: 1.2129 - val_acc: 0.7102\n",
      "Epoch 1316/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0449 - acc: 0.9858 - val_loss: 1.1778 - val_acc: 0.7159\n",
      "Epoch 1317/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0509 - acc: 0.9929 - val_loss: 1.1882 - val_acc: 0.7216\n",
      "Epoch 1318/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0558 - acc: 0.9801 - val_loss: 1.2098 - val_acc: 0.6989\n",
      "Epoch 1319/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0649 - acc: 0.9772 - val_loss: 1.2142 - val_acc: 0.7045\n",
      "Epoch 1320/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0569 - acc: 0.9829 - val_loss: 1.1981 - val_acc: 0.7102\n",
      "Epoch 1321/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0504 - acc: 0.9872 - val_loss: 1.2412 - val_acc: 0.6989\n",
      "Epoch 1322/3000\n",
      "702/702 [==============================] - 0s 581us/sample - loss: 0.0839 - acc: 0.9772 - val_loss: 1.2749 - val_acc: 0.6932\n",
      "Epoch 1323/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0721 - acc: 0.9829 - val_loss: 1.1453 - val_acc: 0.7159\n",
      "Epoch 1324/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0624 - acc: 0.9829 - val_loss: 1.1920 - val_acc: 0.7216\n",
      "Epoch 1325/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.1105 - acc: 0.9744 - val_loss: 1.1360 - val_acc: 0.7102\n",
      "Epoch 1326/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0652 - acc: 0.9829 - val_loss: 1.1145 - val_acc: 0.7330\n",
      "Epoch 1327/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0838 - acc: 0.9815 - val_loss: 1.1991 - val_acc: 0.7273\n",
      "Epoch 1328/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0854 - acc: 0.9758 - val_loss: 1.2632 - val_acc: 0.7045\n",
      "Epoch 1329/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0505 - acc: 0.9858 - val_loss: 1.2227 - val_acc: 0.7159\n",
      "Epoch 1330/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0601 - acc: 0.9801 - val_loss: 1.2727 - val_acc: 0.6989\n",
      "Epoch 1331/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0679 - acc: 0.9744 - val_loss: 1.3615 - val_acc: 0.6989\n",
      "Epoch 1332/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.0647 - acc: 0.9829 - val_loss: 1.1780 - val_acc: 0.7102\n",
      "Epoch 1333/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0631 - acc: 0.9858 - val_loss: 1.1329 - val_acc: 0.7273\n",
      "Epoch 1334/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0752 - acc: 0.9786 - val_loss: 1.2605 - val_acc: 0.7102\n",
      "Epoch 1335/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0657 - acc: 0.9829 - val_loss: 1.2384 - val_acc: 0.7045\n",
      "Epoch 1336/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0769 - acc: 0.9772 - val_loss: 1.3138 - val_acc: 0.6989\n",
      "Epoch 1337/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0921 - acc: 0.9801 - val_loss: 1.3847 - val_acc: 0.6989\n",
      "Epoch 1338/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0932 - acc: 0.9744 - val_loss: 1.3409 - val_acc: 0.6932\n",
      "Epoch 1339/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0800 - acc: 0.9701 - val_loss: 1.2753 - val_acc: 0.6989\n",
      "Epoch 1340/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0984 - acc: 0.9744 - val_loss: 1.2589 - val_acc: 0.6932\n",
      "Epoch 1341/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0797 - acc: 0.9786 - val_loss: 1.2341 - val_acc: 0.7159\n",
      "Epoch 1342/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.1217 - acc: 0.9658 - val_loss: 1.2436 - val_acc: 0.7102\n",
      "Epoch 1343/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0734 - acc: 0.9829 - val_loss: 1.4561 - val_acc: 0.6818\n",
      "Epoch 1344/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0712 - acc: 0.9672 - val_loss: 1.2996 - val_acc: 0.7045\n",
      "Epoch 1345/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0686 - acc: 0.9744 - val_loss: 1.3028 - val_acc: 0.6989\n",
      "Epoch 1346/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0693 - acc: 0.9772 - val_loss: 1.2834 - val_acc: 0.7102\n",
      "Epoch 1347/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0604 - acc: 0.9772 - val_loss: 1.3007 - val_acc: 0.7159\n",
      "Epoch 1348/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0713 - acc: 0.9829 - val_loss: 1.3588 - val_acc: 0.7159\n",
      "Epoch 1349/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0719 - acc: 0.9801 - val_loss: 1.4534 - val_acc: 0.6989\n",
      "Epoch 1350/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0768 - acc: 0.9801 - val_loss: 1.3703 - val_acc: 0.7045\n",
      "Epoch 1351/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0578 - acc: 0.9801 - val_loss: 1.3319 - val_acc: 0.7159\n",
      "Epoch 1352/3000\n",
      "702/702 [==============================] - 0s 578us/sample - loss: 0.0580 - acc: 0.9843 - val_loss: 1.3803 - val_acc: 0.7330\n",
      "Epoch 1353/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0592 - acc: 0.9843 - val_loss: 1.4513 - val_acc: 0.7159\n",
      "Epoch 1354/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0767 - acc: 0.9843 - val_loss: 1.4234 - val_acc: 0.7045\n",
      "Epoch 1355/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1191 - acc: 0.9701 - val_loss: 1.4294 - val_acc: 0.7159\n",
      "Epoch 1356/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0803 - acc: 0.9772 - val_loss: 1.5106 - val_acc: 0.6875\n",
      "Epoch 1357/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0733 - acc: 0.9772 - val_loss: 1.3968 - val_acc: 0.7159\n",
      "Epoch 1358/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0571 - acc: 0.9801 - val_loss: 1.2717 - val_acc: 0.7386\n",
      "Epoch 1359/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0921 - acc: 0.9701 - val_loss: 1.2629 - val_acc: 0.7273\n",
      "Epoch 1360/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0804 - acc: 0.9772 - val_loss: 1.3017 - val_acc: 0.6932\n",
      "Epoch 1361/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0804 - acc: 0.9744 - val_loss: 1.3327 - val_acc: 0.7102\n",
      "Epoch 1362/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.0535 - acc: 0.9858 - val_loss: 1.4223 - val_acc: 0.7045\n",
      "Epoch 1363/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0527 - acc: 0.9872 - val_loss: 1.5408 - val_acc: 0.7102\n",
      "Epoch 1364/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0739 - acc: 0.9843 - val_loss: 1.4009 - val_acc: 0.7102\n",
      "Epoch 1365/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0432 - acc: 0.9872 - val_loss: 1.3743 - val_acc: 0.7102\n",
      "Epoch 1366/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0515 - acc: 0.9801 - val_loss: 1.3314 - val_acc: 0.7159\n",
      "Epoch 1367/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0440 - acc: 0.9886 - val_loss: 1.3024 - val_acc: 0.7102\n",
      "Epoch 1368/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0619 - acc: 0.9729 - val_loss: 1.4448 - val_acc: 0.7045\n",
      "Epoch 1369/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0826 - acc: 0.9729 - val_loss: 1.3812 - val_acc: 0.7159\n",
      "Epoch 1370/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0814 - acc: 0.9829 - val_loss: 1.2077 - val_acc: 0.7330\n",
      "Epoch 1371/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0827 - acc: 0.9815 - val_loss: 1.3238 - val_acc: 0.7273\n",
      "Epoch 1372/3000\n",
      "702/702 [==============================] - 0s 582us/sample - loss: 0.1756 - acc: 0.9658 - val_loss: 1.2810 - val_acc: 0.7159\n",
      "Epoch 1373/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0961 - acc: 0.9687 - val_loss: 1.2671 - val_acc: 0.7159\n",
      "Epoch 1374/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.1022 - acc: 0.9658 - val_loss: 1.4094 - val_acc: 0.7159\n",
      "Epoch 1375/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1105 - acc: 0.9630 - val_loss: 1.4064 - val_acc: 0.6932\n",
      "Epoch 1376/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0771 - acc: 0.9786 - val_loss: 1.1516 - val_acc: 0.7045\n",
      "Epoch 1377/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1581 - acc: 0.9501 - val_loss: 1.1964 - val_acc: 0.6932\n",
      "Epoch 1378/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1054 - acc: 0.9729 - val_loss: 1.3018 - val_acc: 0.7102\n",
      "Epoch 1379/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.1513 - acc: 0.9701 - val_loss: 1.3753 - val_acc: 0.6818\n",
      "Epoch 1380/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0917 - acc: 0.9658 - val_loss: 1.4429 - val_acc: 0.6875\n",
      "Epoch 1381/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0966 - acc: 0.9729 - val_loss: 1.3978 - val_acc: 0.6818\n",
      "Epoch 1382/3000\n",
      "702/702 [==============================] - 0s 577us/sample - loss: 0.0901 - acc: 0.9758 - val_loss: 1.3096 - val_acc: 0.6932\n",
      "Epoch 1383/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0704 - acc: 0.9829 - val_loss: 1.2539 - val_acc: 0.7102\n",
      "Epoch 1384/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0863 - acc: 0.9701 - val_loss: 1.2988 - val_acc: 0.6989\n",
      "Epoch 1385/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0702 - acc: 0.9758 - val_loss: 1.2923 - val_acc: 0.6875\n",
      "Epoch 1386/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.1010 - acc: 0.9601 - val_loss: 1.1928 - val_acc: 0.6989\n",
      "Epoch 1387/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0740 - acc: 0.9772 - val_loss: 1.2487 - val_acc: 0.7216\n",
      "Epoch 1388/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0984 - acc: 0.9672 - val_loss: 1.3834 - val_acc: 0.6818\n",
      "Epoch 1389/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1228 - acc: 0.9772 - val_loss: 1.3023 - val_acc: 0.6989\n",
      "Epoch 1390/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0832 - acc: 0.9744 - val_loss: 1.1981 - val_acc: 0.7216\n",
      "Epoch 1391/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0945 - acc: 0.9701 - val_loss: 1.3652 - val_acc: 0.7102\n",
      "Epoch 1392/3000\n",
      "702/702 [==============================] - 0s 578us/sample - loss: 0.0720 - acc: 0.9758 - val_loss: 1.2062 - val_acc: 0.7273\n",
      "Epoch 1393/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0843 - acc: 0.9729 - val_loss: 1.3412 - val_acc: 0.6989\n",
      "Epoch 1394/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1047 - acc: 0.9644 - val_loss: 1.5141 - val_acc: 0.6932\n",
      "Epoch 1395/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0850 - acc: 0.9744 - val_loss: 1.3483 - val_acc: 0.7102\n",
      "Epoch 1396/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0803 - acc: 0.9744 - val_loss: 1.2441 - val_acc: 0.7159\n",
      "Epoch 1397/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0912 - acc: 0.9701 - val_loss: 1.3224 - val_acc: 0.7159\n",
      "Epoch 1398/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0973 - acc: 0.9744 - val_loss: 1.2794 - val_acc: 0.7045\n",
      "Epoch 1399/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0755 - acc: 0.9772 - val_loss: 1.1859 - val_acc: 0.6989\n",
      "Epoch 1400/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0986 - acc: 0.9758 - val_loss: 1.1695 - val_acc: 0.7273\n",
      "Epoch 1401/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0772 - acc: 0.9758 - val_loss: 1.2667 - val_acc: 0.7045\n",
      "Epoch 1402/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 0.0763 - acc: 0.9815 - val_loss: 1.2851 - val_acc: 0.7045\n",
      "Epoch 1403/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0807 - acc: 0.9744 - val_loss: 1.2437 - val_acc: 0.7159\n",
      "Epoch 1404/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0667 - acc: 0.9786 - val_loss: 1.2275 - val_acc: 0.7273\n",
      "Epoch 1405/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0741 - acc: 0.9843 - val_loss: 1.2377 - val_acc: 0.7216\n",
      "Epoch 1406/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0714 - acc: 0.9786 - val_loss: 1.3332 - val_acc: 0.6989\n",
      "Epoch 1407/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0598 - acc: 0.9815 - val_loss: 1.3357 - val_acc: 0.7159\n",
      "Epoch 1408/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0598 - acc: 0.9858 - val_loss: 1.2905 - val_acc: 0.7216\n",
      "Epoch 1409/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0553 - acc: 0.9872 - val_loss: 1.3189 - val_acc: 0.7216\n",
      "Epoch 1410/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0483 - acc: 0.9843 - val_loss: 1.3765 - val_acc: 0.6989\n",
      "Epoch 1411/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0481 - acc: 0.9886 - val_loss: 1.3653 - val_acc: 0.7216\n",
      "Epoch 1412/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 0.0684 - acc: 0.9815 - val_loss: 1.2400 - val_acc: 0.7273\n",
      "Epoch 1413/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0925 - acc: 0.9829 - val_loss: 1.2355 - val_acc: 0.7045\n",
      "Epoch 1414/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.0482 - acc: 0.9886 - val_loss: 1.3724 - val_acc: 0.7045\n",
      "Epoch 1415/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0735 - acc: 0.9829 - val_loss: 1.2965 - val_acc: 0.6932\n",
      "Epoch 1416/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.1675 - acc: 0.9729 - val_loss: 1.1745 - val_acc: 0.7045\n",
      "Epoch 1417/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.1338 - acc: 0.9601 - val_loss: 1.2982 - val_acc: 0.6932\n",
      "Epoch 1418/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.2416 - acc: 0.9573 - val_loss: 1.2761 - val_acc: 0.7045\n",
      "Epoch 1419/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1042 - acc: 0.9672 - val_loss: 1.2916 - val_acc: 0.7273\n",
      "Epoch 1420/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0754 - acc: 0.9772 - val_loss: 1.3723 - val_acc: 0.7159\n",
      "Epoch 1421/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1094 - acc: 0.9758 - val_loss: 1.4330 - val_acc: 0.6989\n",
      "Epoch 1422/3000\n",
      "702/702 [==============================] - 0s 582us/sample - loss: 0.0790 - acc: 0.9729 - val_loss: 1.2348 - val_acc: 0.7102\n",
      "Epoch 1423/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0684 - acc: 0.9858 - val_loss: 1.0949 - val_acc: 0.7216\n",
      "Epoch 1424/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1169 - acc: 0.9658 - val_loss: 1.1043 - val_acc: 0.7216\n",
      "Epoch 1425/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1028 - acc: 0.9672 - val_loss: 1.2330 - val_acc: 0.7102\n",
      "Epoch 1426/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0509 - acc: 0.9872 - val_loss: 1.2158 - val_acc: 0.7102\n",
      "Epoch 1427/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0523 - acc: 0.9886 - val_loss: 1.2009 - val_acc: 0.6989\n",
      "Epoch 1428/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0670 - acc: 0.9758 - val_loss: 1.2024 - val_acc: 0.6989\n",
      "Epoch 1429/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0557 - acc: 0.9858 - val_loss: 1.1517 - val_acc: 0.7159\n",
      "Epoch 1430/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0909 - acc: 0.9729 - val_loss: 1.1397 - val_acc: 0.7159\n",
      "Epoch 1431/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0769 - acc: 0.9801 - val_loss: 1.1918 - val_acc: 0.6932\n",
      "Epoch 1432/3000\n",
      "702/702 [==============================] - 0s 597us/sample - loss: 0.0735 - acc: 0.9772 - val_loss: 1.2468 - val_acc: 0.7045\n",
      "Epoch 1433/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0544 - acc: 0.9801 - val_loss: 1.2637 - val_acc: 0.7045\n",
      "Epoch 1434/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0757 - acc: 0.9886 - val_loss: 1.2394 - val_acc: 0.7159\n",
      "Epoch 1435/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0595 - acc: 0.9858 - val_loss: 1.2818 - val_acc: 0.7159\n",
      "Epoch 1436/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0616 - acc: 0.9872 - val_loss: 1.3432 - val_acc: 0.7273\n",
      "Epoch 1437/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0625 - acc: 0.9772 - val_loss: 1.3316 - val_acc: 0.7159\n",
      "Epoch 1438/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0408 - acc: 0.9886 - val_loss: 1.3022 - val_acc: 0.7216\n",
      "Epoch 1439/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0452 - acc: 0.9858 - val_loss: 1.2496 - val_acc: 0.7216\n",
      "Epoch 1440/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0545 - acc: 0.9900 - val_loss: 1.2276 - val_acc: 0.7216\n",
      "Epoch 1441/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0443 - acc: 0.9872 - val_loss: 1.1969 - val_acc: 0.7273\n",
      "Epoch 1442/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.0373 - acc: 0.9915 - val_loss: 1.2677 - val_acc: 0.7216\n",
      "Epoch 1443/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0276 - acc: 0.9957 - val_loss: 1.2738 - val_acc: 0.7159\n",
      "Epoch 1444/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0435 - acc: 0.9815 - val_loss: 1.2795 - val_acc: 0.7102\n",
      "Epoch 1445/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0196 - acc: 0.9972 - val_loss: 1.2775 - val_acc: 0.7045\n",
      "Epoch 1446/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0450 - acc: 0.9886 - val_loss: 1.3242 - val_acc: 0.6932\n",
      "Epoch 1447/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0419 - acc: 0.9872 - val_loss: 1.3742 - val_acc: 0.6761\n",
      "Epoch 1448/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0477 - acc: 0.9843 - val_loss: 1.3413 - val_acc: 0.6761\n",
      "Epoch 1449/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0371 - acc: 0.9900 - val_loss: 1.3403 - val_acc: 0.6875\n",
      "Epoch 1450/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0686 - acc: 0.9829 - val_loss: 1.2899 - val_acc: 0.6875\n",
      "Epoch 1451/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0424 - acc: 0.9929 - val_loss: 1.2429 - val_acc: 0.7045\n",
      "Epoch 1452/3000\n",
      "702/702 [==============================] - 0s 586us/sample - loss: 0.0333 - acc: 0.9943 - val_loss: 1.2515 - val_acc: 0.6989\n",
      "Epoch 1453/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0566 - acc: 0.9772 - val_loss: 1.3216 - val_acc: 0.6989\n",
      "Epoch 1454/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0491 - acc: 0.9872 - val_loss: 1.3527 - val_acc: 0.7045\n",
      "Epoch 1455/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0398 - acc: 0.9858 - val_loss: 1.3412 - val_acc: 0.6875\n",
      "Epoch 1456/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0488 - acc: 0.9843 - val_loss: 1.2935 - val_acc: 0.6761\n",
      "Epoch 1457/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0303 - acc: 0.9915 - val_loss: 1.2482 - val_acc: 0.6932\n",
      "Epoch 1458/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0492 - acc: 0.9872 - val_loss: 1.2996 - val_acc: 0.6932\n",
      "Epoch 1459/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0598 - acc: 0.9843 - val_loss: 1.2777 - val_acc: 0.6989\n",
      "Epoch 1460/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0572 - acc: 0.9843 - val_loss: 1.2912 - val_acc: 0.7159\n",
      "Epoch 1461/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0403 - acc: 0.9929 - val_loss: 1.3278 - val_acc: 0.7102\n",
      "Epoch 1462/3000\n",
      "702/702 [==============================] - 0s 582us/sample - loss: 0.0539 - acc: 0.9886 - val_loss: 1.3478 - val_acc: 0.7045\n",
      "Epoch 1463/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0402 - acc: 0.9900 - val_loss: 1.4368 - val_acc: 0.6932\n",
      "Epoch 1464/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0619 - acc: 0.9758 - val_loss: 1.3782 - val_acc: 0.6875\n",
      "Epoch 1465/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0445 - acc: 0.9872 - val_loss: 1.3138 - val_acc: 0.7045\n",
      "Epoch 1466/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0343 - acc: 0.9943 - val_loss: 1.2851 - val_acc: 0.6932\n",
      "Epoch 1467/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0498 - acc: 0.9858 - val_loss: 1.3318 - val_acc: 0.6932\n",
      "Epoch 1468/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0355 - acc: 0.9886 - val_loss: 1.2519 - val_acc: 0.6818\n",
      "Epoch 1469/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0409 - acc: 0.9872 - val_loss: 1.1932 - val_acc: 0.6932\n",
      "Epoch 1470/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0392 - acc: 0.9872 - val_loss: 1.2307 - val_acc: 0.6932\n",
      "Epoch 1471/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0410 - acc: 0.9900 - val_loss: 1.4237 - val_acc: 0.6818\n",
      "Epoch 1472/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.0425 - acc: 0.9886 - val_loss: 1.4308 - val_acc: 0.6761\n",
      "Epoch 1473/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0357 - acc: 0.9858 - val_loss: 1.2634 - val_acc: 0.6875\n",
      "Epoch 1474/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0627 - acc: 0.9829 - val_loss: 1.2465 - val_acc: 0.6875\n",
      "Epoch 1475/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0476 - acc: 0.9858 - val_loss: 1.2327 - val_acc: 0.7102\n",
      "Epoch 1476/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0841 - acc: 0.9815 - val_loss: 1.3030 - val_acc: 0.7045\n",
      "Epoch 1477/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0549 - acc: 0.9858 - val_loss: 1.2312 - val_acc: 0.6989\n",
      "Epoch 1478/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0972 - acc: 0.9758 - val_loss: 1.2325 - val_acc: 0.7216\n",
      "Epoch 1479/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.1399 - acc: 0.9658 - val_loss: 1.1810 - val_acc: 0.7273\n",
      "Epoch 1480/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0923 - acc: 0.9758 - val_loss: 1.2065 - val_acc: 0.7102\n",
      "Epoch 1481/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1379 - acc: 0.9644 - val_loss: 1.2797 - val_acc: 0.7216\n",
      "Epoch 1482/3000\n",
      "702/702 [==============================] - 0s 586us/sample - loss: 0.1293 - acc: 0.9601 - val_loss: 1.1233 - val_acc: 0.7102\n",
      "Epoch 1483/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1416 - acc: 0.9601 - val_loss: 1.1599 - val_acc: 0.7102\n",
      "Epoch 1484/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.0829 - acc: 0.9786 - val_loss: 1.1139 - val_acc: 0.6875\n",
      "Epoch 1485/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.1651 - acc: 0.9573 - val_loss: 1.1599 - val_acc: 0.6989\n",
      "Epoch 1486/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1469 - acc: 0.9601 - val_loss: 1.3205 - val_acc: 0.6705\n",
      "Epoch 1487/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1153 - acc: 0.9587 - val_loss: 1.3092 - val_acc: 0.7159\n",
      "Epoch 1488/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.1779 - acc: 0.9601 - val_loss: 1.3119 - val_acc: 0.6932\n",
      "Epoch 1489/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1126 - acc: 0.9630 - val_loss: 1.3264 - val_acc: 0.6818\n",
      "Epoch 1490/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.1422 - acc: 0.9601 - val_loss: 1.1747 - val_acc: 0.6989\n",
      "Epoch 1491/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1065 - acc: 0.9644 - val_loss: 1.2756 - val_acc: 0.7045\n",
      "Epoch 1492/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.1237 - acc: 0.9601 - val_loss: 1.3755 - val_acc: 0.6818\n",
      "Epoch 1493/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.1335 - acc: 0.9601 - val_loss: 1.2714 - val_acc: 0.7045\n",
      "Epoch 1494/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0868 - acc: 0.9801 - val_loss: 1.1465 - val_acc: 0.7386\n",
      "Epoch 1495/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0900 - acc: 0.9744 - val_loss: 1.3409 - val_acc: 0.7216\n",
      "Epoch 1496/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0523 - acc: 0.9886 - val_loss: 1.4337 - val_acc: 0.7102\n",
      "Epoch 1497/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0695 - acc: 0.9801 - val_loss: 1.3612 - val_acc: 0.7216\n",
      "Epoch 1498/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0942 - acc: 0.9744 - val_loss: 1.3548 - val_acc: 0.7216\n",
      "Epoch 1499/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0736 - acc: 0.9786 - val_loss: 1.3706 - val_acc: 0.7159\n",
      "Epoch 1500/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0590 - acc: 0.9786 - val_loss: 1.3137 - val_acc: 0.7102\n",
      "Epoch 1501/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0457 - acc: 0.9843 - val_loss: 1.2657 - val_acc: 0.7273\n",
      "Epoch 1502/3000\n",
      "702/702 [==============================] - 0s 585us/sample - loss: 0.0719 - acc: 0.9786 - val_loss: 1.3451 - val_acc: 0.7216\n",
      "Epoch 1503/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0707 - acc: 0.9772 - val_loss: 1.4619 - val_acc: 0.6875\n",
      "Epoch 1504/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0776 - acc: 0.9815 - val_loss: 1.2108 - val_acc: 0.7273\n",
      "Epoch 1505/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.0488 - acc: 0.9858 - val_loss: 1.2099 - val_acc: 0.7330\n",
      "Epoch 1506/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0633 - acc: 0.9815 - val_loss: 1.4606 - val_acc: 0.7045\n",
      "Epoch 1507/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0721 - acc: 0.9815 - val_loss: 1.4360 - val_acc: 0.7216\n",
      "Epoch 1508/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0490 - acc: 0.9872 - val_loss: 1.4665 - val_acc: 0.7159\n",
      "Epoch 1509/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.1171 - acc: 0.9701 - val_loss: 1.3386 - val_acc: 0.7443\n",
      "Epoch 1510/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0671 - acc: 0.9744 - val_loss: 1.3120 - val_acc: 0.7159\n",
      "Epoch 1511/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0827 - acc: 0.9729 - val_loss: 1.2752 - val_acc: 0.7273\n",
      "Epoch 1512/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 0.1455 - acc: 0.9573 - val_loss: 1.3042 - val_acc: 0.7330\n",
      "Epoch 1513/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.1004 - acc: 0.9658 - val_loss: 1.2903 - val_acc: 0.7216\n",
      "Epoch 1514/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1273 - acc: 0.9701 - val_loss: 1.3905 - val_acc: 0.7386\n",
      "Epoch 1515/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0815 - acc: 0.9729 - val_loss: 1.4543 - val_acc: 0.7216\n",
      "Epoch 1516/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0878 - acc: 0.9687 - val_loss: 1.3935 - val_acc: 0.7159\n",
      "Epoch 1517/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1140 - acc: 0.9687 - val_loss: 1.3350 - val_acc: 0.7216\n",
      "Epoch 1518/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1137 - acc: 0.9601 - val_loss: 1.2921 - val_acc: 0.7500\n",
      "Epoch 1519/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1090 - acc: 0.9729 - val_loss: 1.2332 - val_acc: 0.7216\n",
      "Epoch 1520/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0846 - acc: 0.9758 - val_loss: 1.2691 - val_acc: 0.7102\n",
      "Epoch 1521/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0693 - acc: 0.9772 - val_loss: 1.2371 - val_acc: 0.7216\n",
      "Epoch 1522/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.0876 - acc: 0.9715 - val_loss: 1.2410 - val_acc: 0.7216\n",
      "Epoch 1523/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0608 - acc: 0.9815 - val_loss: 1.2301 - val_acc: 0.7273\n",
      "Epoch 1524/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0716 - acc: 0.9786 - val_loss: 1.1959 - val_acc: 0.7216\n",
      "Epoch 1525/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0340 - acc: 0.9915 - val_loss: 1.3545 - val_acc: 0.7102\n",
      "Epoch 1526/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0809 - acc: 0.9758 - val_loss: 1.2818 - val_acc: 0.7273\n",
      "Epoch 1527/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0710 - acc: 0.9858 - val_loss: 1.3415 - val_acc: 0.7273\n",
      "Epoch 1528/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0538 - acc: 0.9801 - val_loss: 1.2956 - val_acc: 0.7330\n",
      "Epoch 1529/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0788 - acc: 0.9744 - val_loss: 1.1393 - val_acc: 0.7330\n",
      "Epoch 1530/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0496 - acc: 0.9843 - val_loss: 1.2778 - val_acc: 0.7159\n",
      "Epoch 1531/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0471 - acc: 0.9900 - val_loss: 1.3572 - val_acc: 0.7102\n",
      "Epoch 1532/3000\n",
      "702/702 [==============================] - 0s 588us/sample - loss: 0.0520 - acc: 0.9815 - val_loss: 1.3107 - val_acc: 0.7102\n",
      "Epoch 1533/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0506 - acc: 0.9815 - val_loss: 1.2301 - val_acc: 0.7045\n",
      "Epoch 1534/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0659 - acc: 0.9843 - val_loss: 1.2622 - val_acc: 0.7216\n",
      "Epoch 1535/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0365 - acc: 0.9872 - val_loss: 1.2580 - val_acc: 0.7216\n",
      "Epoch 1536/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0502 - acc: 0.9843 - val_loss: 1.2692 - val_acc: 0.7102\n",
      "Epoch 1537/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0369 - acc: 0.9900 - val_loss: 1.3011 - val_acc: 0.7216\n",
      "Epoch 1538/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0453 - acc: 0.9915 - val_loss: 1.3834 - val_acc: 0.7045\n",
      "Epoch 1539/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0320 - acc: 0.9915 - val_loss: 1.3956 - val_acc: 0.6989\n",
      "Epoch 1540/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0487 - acc: 0.9872 - val_loss: 1.3267 - val_acc: 0.6932\n",
      "Epoch 1541/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0806 - acc: 0.9801 - val_loss: 1.3814 - val_acc: 0.6989\n",
      "Epoch 1542/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.0339 - acc: 0.9915 - val_loss: 1.4310 - val_acc: 0.7045\n",
      "Epoch 1543/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0575 - acc: 0.9772 - val_loss: 1.3501 - val_acc: 0.6932\n",
      "Epoch 1544/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0941 - acc: 0.9872 - val_loss: 1.2235 - val_acc: 0.7102\n",
      "Epoch 1545/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0684 - acc: 0.9801 - val_loss: 1.2575 - val_acc: 0.7159\n",
      "Epoch 1546/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0372 - acc: 0.9858 - val_loss: 1.4386 - val_acc: 0.6932\n",
      "Epoch 1547/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0545 - acc: 0.9872 - val_loss: 1.3970 - val_acc: 0.7045\n",
      "Epoch 1548/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0499 - acc: 0.9858 - val_loss: 1.3152 - val_acc: 0.7159\n",
      "Epoch 1549/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0481 - acc: 0.9815 - val_loss: 1.3035 - val_acc: 0.7273\n",
      "Epoch 1550/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0559 - acc: 0.9815 - val_loss: 1.3147 - val_acc: 0.7273\n",
      "Epoch 1551/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0598 - acc: 0.9843 - val_loss: 1.2439 - val_acc: 0.7386\n",
      "Epoch 1552/3000\n",
      "702/702 [==============================] - 0s 568us/sample - loss: 0.0409 - acc: 0.9886 - val_loss: 1.4142 - val_acc: 0.6875\n",
      "Epoch 1553/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0369 - acc: 0.9900 - val_loss: 1.3991 - val_acc: 0.6818\n",
      "Epoch 1554/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0541 - acc: 0.9801 - val_loss: 1.3213 - val_acc: 0.7273\n",
      "Epoch 1555/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0453 - acc: 0.9872 - val_loss: 1.3211 - val_acc: 0.7273\n",
      "Epoch 1556/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0560 - acc: 0.9858 - val_loss: 1.3314 - val_acc: 0.7273\n",
      "Epoch 1557/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0409 - acc: 0.9858 - val_loss: 1.3531 - val_acc: 0.6989\n",
      "Epoch 1558/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.0441 - acc: 0.9886 - val_loss: 1.3262 - val_acc: 0.7216\n",
      "Epoch 1559/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0454 - acc: 0.9829 - val_loss: 1.2328 - val_acc: 0.7159\n",
      "Epoch 1560/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0416 - acc: 0.9915 - val_loss: 1.2872 - val_acc: 0.7045\n",
      "Epoch 1561/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0628 - acc: 0.9801 - val_loss: 1.2553 - val_acc: 0.6932\n",
      "Epoch 1562/3000\n",
      "702/702 [==============================] - 0s 577us/sample - loss: 0.0501 - acc: 0.9872 - val_loss: 1.1782 - val_acc: 0.7045\n",
      "Epoch 1563/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0529 - acc: 0.9786 - val_loss: 1.2478 - val_acc: 0.7045\n",
      "Epoch 1564/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0522 - acc: 0.9843 - val_loss: 1.2705 - val_acc: 0.7273\n",
      "Epoch 1565/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0508 - acc: 0.9858 - val_loss: 1.2836 - val_acc: 0.7102\n",
      "Epoch 1566/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0750 - acc: 0.9772 - val_loss: 1.3166 - val_acc: 0.7102\n",
      "Epoch 1567/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0276 - acc: 0.9929 - val_loss: 1.3858 - val_acc: 0.6875\n",
      "Epoch 1568/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0463 - acc: 0.9858 - val_loss: 1.3275 - val_acc: 0.7102\n",
      "Epoch 1569/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0321 - acc: 0.9886 - val_loss: 1.2007 - val_acc: 0.7386\n",
      "Epoch 1570/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0634 - acc: 0.9786 - val_loss: 1.1859 - val_acc: 0.7273\n",
      "Epoch 1571/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0480 - acc: 0.9858 - val_loss: 1.1975 - val_acc: 0.7330\n",
      "Epoch 1572/3000\n",
      "702/702 [==============================] - 0s 583us/sample - loss: 0.0408 - acc: 0.9929 - val_loss: 1.2308 - val_acc: 0.7159\n",
      "Epoch 1573/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0439 - acc: 0.9929 - val_loss: 1.3135 - val_acc: 0.6932\n",
      "Epoch 1574/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0474 - acc: 0.9843 - val_loss: 1.3717 - val_acc: 0.7159\n",
      "Epoch 1575/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0481 - acc: 0.9815 - val_loss: 1.3806 - val_acc: 0.7102\n",
      "Epoch 1576/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0391 - acc: 0.9886 - val_loss: 1.4049 - val_acc: 0.7045\n",
      "Epoch 1577/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0449 - acc: 0.9872 - val_loss: 1.2943 - val_acc: 0.7102\n",
      "Epoch 1578/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0573 - acc: 0.9858 - val_loss: 1.2607 - val_acc: 0.7273\n",
      "Epoch 1579/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0287 - acc: 0.9943 - val_loss: 1.2813 - val_acc: 0.7386\n",
      "Epoch 1580/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0463 - acc: 0.9872 - val_loss: 1.3768 - val_acc: 0.7159\n",
      "Epoch 1581/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0404 - acc: 0.9858 - val_loss: 1.4590 - val_acc: 0.6989\n",
      "Epoch 1582/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.0449 - acc: 0.9900 - val_loss: 1.3619 - val_acc: 0.7159\n",
      "Epoch 1583/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0431 - acc: 0.9915 - val_loss: 1.3922 - val_acc: 0.7045\n",
      "Epoch 1584/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0468 - acc: 0.9886 - val_loss: 1.4040 - val_acc: 0.7159\n",
      "Epoch 1585/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0429 - acc: 0.9886 - val_loss: 1.2862 - val_acc: 0.7159\n",
      "Epoch 1586/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0413 - acc: 0.9886 - val_loss: 1.2479 - val_acc: 0.7216\n",
      "Epoch 1587/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0627 - acc: 0.9772 - val_loss: 1.2768 - val_acc: 0.7273\n",
      "Epoch 1588/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0443 - acc: 0.9886 - val_loss: 1.2840 - val_acc: 0.7102\n",
      "Epoch 1589/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0397 - acc: 0.9900 - val_loss: 1.3266 - val_acc: 0.7273\n",
      "Epoch 1590/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0433 - acc: 0.9829 - val_loss: 1.3804 - val_acc: 0.7102\n",
      "Epoch 1591/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0512 - acc: 0.9829 - val_loss: 1.3304 - val_acc: 0.7159\n",
      "Epoch 1592/3000\n",
      "702/702 [==============================] - 0s 604us/sample - loss: 0.0422 - acc: 0.9915 - val_loss: 1.3286 - val_acc: 0.7216\n",
      "Epoch 1593/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0633 - acc: 0.9815 - val_loss: 1.2560 - val_acc: 0.7273\n",
      "Epoch 1594/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0680 - acc: 0.9843 - val_loss: 1.2099 - val_acc: 0.7159\n",
      "Epoch 1595/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0942 - acc: 0.9829 - val_loss: 1.4536 - val_acc: 0.7273\n",
      "Epoch 1596/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0888 - acc: 0.9687 - val_loss: 1.4708 - val_acc: 0.6932\n",
      "Epoch 1597/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1102 - acc: 0.9729 - val_loss: 1.2949 - val_acc: 0.7159\n",
      "Epoch 1598/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.1460 - acc: 0.9615 - val_loss: 1.3153 - val_acc: 0.7102\n",
      "Epoch 1599/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1331 - acc: 0.9601 - val_loss: 1.2790 - val_acc: 0.7216\n",
      "Epoch 1600/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1000 - acc: 0.9744 - val_loss: 1.2843 - val_acc: 0.7614\n",
      "Epoch 1601/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0783 - acc: 0.9758 - val_loss: 1.3736 - val_acc: 0.7216\n",
      "Epoch 1602/3000\n",
      "702/702 [==============================] - 0s 580us/sample - loss: 0.1475 - acc: 0.9587 - val_loss: 1.2602 - val_acc: 0.7045\n",
      "Epoch 1603/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1326 - acc: 0.9573 - val_loss: 1.2908 - val_acc: 0.7273\n",
      "Epoch 1604/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1385 - acc: 0.9630 - val_loss: 1.3516 - val_acc: 0.7216\n",
      "Epoch 1605/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.1727 - acc: 0.9516 - val_loss: 1.3061 - val_acc: 0.7216\n",
      "Epoch 1606/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1187 - acc: 0.9601 - val_loss: 1.2615 - val_acc: 0.7159\n",
      "Epoch 1607/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0917 - acc: 0.9729 - val_loss: 1.3828 - val_acc: 0.7216\n",
      "Epoch 1608/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0978 - acc: 0.9672 - val_loss: 1.5349 - val_acc: 0.7102\n",
      "Epoch 1609/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1209 - acc: 0.9701 - val_loss: 1.5516 - val_acc: 0.6989\n",
      "Epoch 1610/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0813 - acc: 0.9815 - val_loss: 1.5743 - val_acc: 0.6932\n",
      "Epoch 1611/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1984 - acc: 0.9630 - val_loss: 1.4686 - val_acc: 0.6875\n",
      "Epoch 1612/3000\n",
      "702/702 [==============================] - 0s 582us/sample - loss: 0.2774 - acc: 0.9430 - val_loss: 1.5641 - val_acc: 0.6648\n",
      "Epoch 1613/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.3392 - acc: 0.9117 - val_loss: 1.9217 - val_acc: 0.6705\n",
      "Epoch 1614/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.3328 - acc: 0.9359 - val_loss: 1.3395 - val_acc: 0.7102\n",
      "Epoch 1615/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.3980 - acc: 0.9103 - val_loss: 1.2394 - val_acc: 0.6989\n",
      "Epoch 1616/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2895 - acc: 0.9088 - val_loss: 1.2574 - val_acc: 0.6989\n",
      "Epoch 1617/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2442 - acc: 0.9145 - val_loss: 1.2617 - val_acc: 0.6932\n",
      "Epoch 1618/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.3215 - acc: 0.9174 - val_loss: 1.3782 - val_acc: 0.6875\n",
      "Epoch 1619/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.2300 - acc: 0.9202 - val_loss: 1.4937 - val_acc: 0.6932\n",
      "Epoch 1620/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.1764 - acc: 0.9459 - val_loss: 1.4933 - val_acc: 0.6989\n",
      "Epoch 1621/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1326 - acc: 0.9630 - val_loss: 1.4310 - val_acc: 0.7045\n",
      "Epoch 1622/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.0980 - acc: 0.9644 - val_loss: 1.4455 - val_acc: 0.7045\n",
      "Epoch 1623/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1387 - acc: 0.9587 - val_loss: 1.4125 - val_acc: 0.7159\n",
      "Epoch 1624/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0999 - acc: 0.9687 - val_loss: 1.3919 - val_acc: 0.7216\n",
      "Epoch 1625/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0826 - acc: 0.9772 - val_loss: 1.3582 - val_acc: 0.7045\n",
      "Epoch 1626/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0828 - acc: 0.9744 - val_loss: 1.3378 - val_acc: 0.7045\n",
      "Epoch 1627/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1021 - acc: 0.9687 - val_loss: 1.2483 - val_acc: 0.7216\n",
      "Epoch 1628/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0858 - acc: 0.9672 - val_loss: 1.2729 - val_acc: 0.7045\n",
      "Epoch 1629/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0957 - acc: 0.9687 - val_loss: 1.2840 - val_acc: 0.7216\n",
      "Epoch 1630/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0740 - acc: 0.9701 - val_loss: 1.3692 - val_acc: 0.7102\n",
      "Epoch 1631/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0843 - acc: 0.9815 - val_loss: 1.3935 - val_acc: 0.7102\n",
      "Epoch 1632/3000\n",
      "702/702 [==============================] - 0s 578us/sample - loss: 0.0747 - acc: 0.9772 - val_loss: 1.3540 - val_acc: 0.7045\n",
      "Epoch 1633/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0654 - acc: 0.9815 - val_loss: 1.3721 - val_acc: 0.7273\n",
      "Epoch 1634/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0977 - acc: 0.9715 - val_loss: 1.3916 - val_acc: 0.7216\n",
      "Epoch 1635/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0777 - acc: 0.9701 - val_loss: 1.3818 - val_acc: 0.7273\n",
      "Epoch 1636/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.1069 - acc: 0.9701 - val_loss: 1.5054 - val_acc: 0.7159\n",
      "Epoch 1637/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0806 - acc: 0.9786 - val_loss: 1.5357 - val_acc: 0.6989\n",
      "Epoch 1638/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0620 - acc: 0.9801 - val_loss: 1.4992 - val_acc: 0.7216\n",
      "Epoch 1639/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0890 - acc: 0.9772 - val_loss: 1.3785 - val_acc: 0.7045\n",
      "Epoch 1640/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0959 - acc: 0.9729 - val_loss: 1.3762 - val_acc: 0.7159\n",
      "Epoch 1641/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0943 - acc: 0.9744 - val_loss: 1.3983 - val_acc: 0.7216\n",
      "Epoch 1642/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.0752 - acc: 0.9786 - val_loss: 1.3054 - val_acc: 0.7102\n",
      "Epoch 1643/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0599 - acc: 0.9829 - val_loss: 1.3236 - val_acc: 0.6875\n",
      "Epoch 1644/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0681 - acc: 0.9801 - val_loss: 1.2297 - val_acc: 0.7102\n",
      "Epoch 1645/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.1229 - acc: 0.9744 - val_loss: 1.2526 - val_acc: 0.7216\n",
      "Epoch 1646/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0649 - acc: 0.9772 - val_loss: 1.2800 - val_acc: 0.7102\n",
      "Epoch 1647/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0561 - acc: 0.9801 - val_loss: 1.3602 - val_acc: 0.7159\n",
      "Epoch 1648/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0463 - acc: 0.9858 - val_loss: 1.4120 - val_acc: 0.7273\n",
      "Epoch 1649/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0704 - acc: 0.9801 - val_loss: 1.4535 - val_acc: 0.7102\n",
      "Epoch 1650/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0569 - acc: 0.9858 - val_loss: 1.4180 - val_acc: 0.7102\n",
      "Epoch 1651/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0886 - acc: 0.9772 - val_loss: 1.3605 - val_acc: 0.7045\n",
      "Epoch 1652/3000\n",
      "702/702 [==============================] - 0s 578us/sample - loss: 0.0688 - acc: 0.9815 - val_loss: 1.3050 - val_acc: 0.7159\n",
      "Epoch 1653/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0730 - acc: 0.9786 - val_loss: 1.3015 - val_acc: 0.7216\n",
      "Epoch 1654/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0507 - acc: 0.9801 - val_loss: 1.4901 - val_acc: 0.6875\n",
      "Epoch 1655/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1144 - acc: 0.9715 - val_loss: 1.3425 - val_acc: 0.7159\n",
      "Epoch 1656/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0752 - acc: 0.9829 - val_loss: 1.4213 - val_acc: 0.7045\n",
      "Epoch 1657/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0833 - acc: 0.9672 - val_loss: 1.4477 - val_acc: 0.7273\n",
      "Epoch 1658/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0872 - acc: 0.9786 - val_loss: 1.4701 - val_acc: 0.7159\n",
      "Epoch 1659/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.1262 - acc: 0.9672 - val_loss: 1.2867 - val_acc: 0.7102\n",
      "Epoch 1660/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1485 - acc: 0.9601 - val_loss: 1.2987 - val_acc: 0.7045\n",
      "Epoch 1661/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1195 - acc: 0.9715 - val_loss: 1.2617 - val_acc: 0.7045\n",
      "Epoch 1662/3000\n",
      "702/702 [==============================] - 0s 568us/sample - loss: 0.0719 - acc: 0.9772 - val_loss: 1.3372 - val_acc: 0.6989\n",
      "Epoch 1663/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1114 - acc: 0.9672 - val_loss: 1.3866 - val_acc: 0.7159\n",
      "Epoch 1664/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.1253 - acc: 0.9601 - val_loss: 1.1732 - val_acc: 0.6989\n",
      "Epoch 1665/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0968 - acc: 0.9758 - val_loss: 1.1121 - val_acc: 0.7159\n",
      "Epoch 1666/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0561 - acc: 0.9872 - val_loss: 1.2884 - val_acc: 0.6989\n",
      "Epoch 1667/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0855 - acc: 0.9715 - val_loss: 1.4578 - val_acc: 0.6875\n",
      "Epoch 1668/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0577 - acc: 0.9858 - val_loss: 1.5144 - val_acc: 0.6875\n",
      "Epoch 1669/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1082 - acc: 0.9658 - val_loss: 1.4719 - val_acc: 0.7102\n",
      "Epoch 1670/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0693 - acc: 0.9815 - val_loss: 1.3553 - val_acc: 0.7273\n",
      "Epoch 1671/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0939 - acc: 0.9786 - val_loss: 1.4423 - val_acc: 0.7159\n",
      "Epoch 1672/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.0686 - acc: 0.9801 - val_loss: 1.4390 - val_acc: 0.7045\n",
      "Epoch 1673/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0901 - acc: 0.9744 - val_loss: 1.4136 - val_acc: 0.7159\n",
      "Epoch 1674/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0671 - acc: 0.9801 - val_loss: 1.3956 - val_acc: 0.7216\n",
      "Epoch 1675/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0394 - acc: 0.9858 - val_loss: 1.3621 - val_acc: 0.7216\n",
      "Epoch 1676/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.0677 - acc: 0.9772 - val_loss: 1.3825 - val_acc: 0.7159\n",
      "Epoch 1677/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0382 - acc: 0.9872 - val_loss: 1.3269 - val_acc: 0.7216\n",
      "Epoch 1678/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0614 - acc: 0.9872 - val_loss: 1.2206 - val_acc: 0.7330\n",
      "Epoch 1679/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0752 - acc: 0.9829 - val_loss: 1.3016 - val_acc: 0.7273\n",
      "Epoch 1680/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0481 - acc: 0.9815 - val_loss: 1.4845 - val_acc: 0.7045\n",
      "Epoch 1681/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0562 - acc: 0.9786 - val_loss: 1.4961 - val_acc: 0.7102\n",
      "Epoch 1682/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 0.0612 - acc: 0.9872 - val_loss: 1.4192 - val_acc: 0.7102\n",
      "Epoch 1683/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0536 - acc: 0.9872 - val_loss: 1.3359 - val_acc: 0.7330\n",
      "Epoch 1684/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0481 - acc: 0.9915 - val_loss: 1.4002 - val_acc: 0.7330\n",
      "Epoch 1685/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0353 - acc: 0.9900 - val_loss: 1.4385 - val_acc: 0.7102\n",
      "Epoch 1686/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0564 - acc: 0.9829 - val_loss: 1.3747 - val_acc: 0.7102\n",
      "Epoch 1687/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0453 - acc: 0.9886 - val_loss: 1.2591 - val_acc: 0.7330\n",
      "Epoch 1688/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0443 - acc: 0.9886 - val_loss: 1.3092 - val_acc: 0.7216\n",
      "Epoch 1689/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0437 - acc: 0.9900 - val_loss: 1.3576 - val_acc: 0.7102\n",
      "Epoch 1690/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0643 - acc: 0.9858 - val_loss: 1.4170 - val_acc: 0.7216\n",
      "Epoch 1691/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0555 - acc: 0.9801 - val_loss: 1.3160 - val_acc: 0.7273\n",
      "Epoch 1692/3000\n",
      "702/702 [==============================] - 0s 585us/sample - loss: 0.1470 - acc: 0.9758 - val_loss: 1.2658 - val_acc: 0.6989\n",
      "Epoch 1693/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1400 - acc: 0.9516 - val_loss: 1.4444 - val_acc: 0.6875\n",
      "Epoch 1694/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1354 - acc: 0.9544 - val_loss: 1.3243 - val_acc: 0.7159\n",
      "Epoch 1695/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1137 - acc: 0.9672 - val_loss: 1.4160 - val_acc: 0.7273\n",
      "Epoch 1696/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1119 - acc: 0.9715 - val_loss: 1.4984 - val_acc: 0.7216\n",
      "Epoch 1697/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0858 - acc: 0.9744 - val_loss: 1.3525 - val_acc: 0.7159\n",
      "Epoch 1698/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0775 - acc: 0.9815 - val_loss: 1.5161 - val_acc: 0.7045\n",
      "Epoch 1699/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0881 - acc: 0.9729 - val_loss: 1.6177 - val_acc: 0.7216\n",
      "Epoch 1700/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0628 - acc: 0.9772 - val_loss: 1.3695 - val_acc: 0.7386\n",
      "Epoch 1701/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0725 - acc: 0.9786 - val_loss: 1.4233 - val_acc: 0.7159\n",
      "Epoch 1702/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 0.1143 - acc: 0.9672 - val_loss: 1.2613 - val_acc: 0.7273\n",
      "Epoch 1703/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1166 - acc: 0.9744 - val_loss: 1.1944 - val_acc: 0.7386\n",
      "Epoch 1704/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.1069 - acc: 0.9672 - val_loss: 1.2912 - val_acc: 0.7216\n",
      "Epoch 1705/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0762 - acc: 0.9744 - val_loss: 1.5071 - val_acc: 0.7159\n",
      "Epoch 1706/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.1315 - acc: 0.9687 - val_loss: 1.4116 - val_acc: 0.7159\n",
      "Epoch 1707/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1348 - acc: 0.9601 - val_loss: 1.2382 - val_acc: 0.7216\n",
      "Epoch 1708/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1871 - acc: 0.9387 - val_loss: 1.2701 - val_acc: 0.6534\n",
      "Epoch 1709/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3305 - acc: 0.9003 - val_loss: 1.2110 - val_acc: 0.6761\n",
      "Epoch 1710/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.6020 - acc: 0.8704 - val_loss: 1.3782 - val_acc: 0.6364\n",
      "Epoch 1711/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.7559 - acc: 0.8390 - val_loss: 1.3964 - val_acc: 0.6534\n",
      "Epoch 1712/3000\n",
      "702/702 [==============================] - 0s 584us/sample - loss: 0.7422 - acc: 0.7835 - val_loss: 1.2792 - val_acc: 0.6534\n",
      "Epoch 1713/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.7214 - acc: 0.8020 - val_loss: 1.2710 - val_acc: 0.6705\n",
      "Epoch 1714/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.6605 - acc: 0.8362 - val_loss: 1.4927 - val_acc: 0.6420\n",
      "Epoch 1715/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.6077 - acc: 0.8547 - val_loss: 1.2857 - val_acc: 0.6875\n",
      "Epoch 1716/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.5927 - acc: 0.8604 - val_loss: 1.2750 - val_acc: 0.6761\n",
      "Epoch 1717/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.3914 - acc: 0.8946 - val_loss: 1.1599 - val_acc: 0.7216\n",
      "Epoch 1718/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.2918 - acc: 0.9231 - val_loss: 1.1097 - val_acc: 0.7159\n",
      "Epoch 1719/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.2798 - acc: 0.9217 - val_loss: 1.1704 - val_acc: 0.7102\n",
      "Epoch 1720/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.2456 - acc: 0.9274 - val_loss: 1.2974 - val_acc: 0.6932\n",
      "Epoch 1721/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2042 - acc: 0.9330 - val_loss: 1.2612 - val_acc: 0.7159\n",
      "Epoch 1722/3000\n",
      "702/702 [==============================] - 0s 580us/sample - loss: 0.1800 - acc: 0.9444 - val_loss: 1.2923 - val_acc: 0.7045\n",
      "Epoch 1723/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1085 - acc: 0.9687 - val_loss: 1.4125 - val_acc: 0.7045\n",
      "Epoch 1724/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1649 - acc: 0.9516 - val_loss: 1.3275 - val_acc: 0.7045\n",
      "Epoch 1725/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1472 - acc: 0.9530 - val_loss: 1.2280 - val_acc: 0.7159\n",
      "Epoch 1726/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1235 - acc: 0.9587 - val_loss: 1.3643 - val_acc: 0.6989\n",
      "Epoch 1727/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.1025 - acc: 0.9772 - val_loss: 1.5463 - val_acc: 0.6761\n",
      "Epoch 1728/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.0884 - acc: 0.9687 - val_loss: 1.3490 - val_acc: 0.6875\n",
      "Epoch 1729/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0753 - acc: 0.9772 - val_loss: 1.2670 - val_acc: 0.6932\n",
      "Epoch 1730/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0930 - acc: 0.9715 - val_loss: 1.2749 - val_acc: 0.7159\n",
      "Epoch 1731/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1017 - acc: 0.9772 - val_loss: 1.2636 - val_acc: 0.7273\n",
      "Epoch 1732/3000\n",
      "702/702 [==============================] - 0s 578us/sample - loss: 0.0899 - acc: 0.9758 - val_loss: 1.2364 - val_acc: 0.7216\n",
      "Epoch 1733/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.1130 - acc: 0.9729 - val_loss: 1.2292 - val_acc: 0.6989\n",
      "Epoch 1734/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0827 - acc: 0.9758 - val_loss: 1.2994 - val_acc: 0.6989\n",
      "Epoch 1735/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0815 - acc: 0.9758 - val_loss: 1.2830 - val_acc: 0.6989\n",
      "Epoch 1736/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0712 - acc: 0.9758 - val_loss: 1.3305 - val_acc: 0.6989\n",
      "Epoch 1737/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0628 - acc: 0.9786 - val_loss: 1.3885 - val_acc: 0.6989\n",
      "Epoch 1738/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0661 - acc: 0.9772 - val_loss: 1.2780 - val_acc: 0.7102\n",
      "Epoch 1739/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0504 - acc: 0.9843 - val_loss: 1.3511 - val_acc: 0.6932\n",
      "Epoch 1740/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0552 - acc: 0.9815 - val_loss: 1.3529 - val_acc: 0.7045\n",
      "Epoch 1741/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0698 - acc: 0.9815 - val_loss: 1.3439 - val_acc: 0.7159\n",
      "Epoch 1742/3000\n",
      "702/702 [==============================] - 0s 582us/sample - loss: 0.1144 - acc: 0.9715 - val_loss: 1.2511 - val_acc: 0.7273\n",
      "Epoch 1743/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0823 - acc: 0.9801 - val_loss: 1.2782 - val_acc: 0.7159\n",
      "Epoch 1744/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0644 - acc: 0.9801 - val_loss: 1.3656 - val_acc: 0.7159\n",
      "Epoch 1745/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0613 - acc: 0.9758 - val_loss: 1.4317 - val_acc: 0.7102\n",
      "Epoch 1746/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0800 - acc: 0.9758 - val_loss: 1.4278 - val_acc: 0.6989\n",
      "Epoch 1747/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.1010 - acc: 0.9729 - val_loss: 1.3235 - val_acc: 0.7159\n",
      "Epoch 1748/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0629 - acc: 0.9843 - val_loss: 1.3534 - val_acc: 0.7045\n",
      "Epoch 1749/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0703 - acc: 0.9815 - val_loss: 1.4495 - val_acc: 0.6875\n",
      "Epoch 1750/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0840 - acc: 0.9786 - val_loss: 1.3344 - val_acc: 0.7216\n",
      "Epoch 1751/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0538 - acc: 0.9843 - val_loss: 1.1857 - val_acc: 0.7330\n",
      "Epoch 1752/3000\n",
      "702/702 [==============================] - 0s 585us/sample - loss: 0.0830 - acc: 0.9829 - val_loss: 1.1885 - val_acc: 0.7216\n",
      "Epoch 1753/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1135 - acc: 0.9687 - val_loss: 1.1521 - val_acc: 0.7273\n",
      "Epoch 1754/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0825 - acc: 0.9729 - val_loss: 1.1831 - val_acc: 0.7216\n",
      "Epoch 1755/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0404 - acc: 0.9929 - val_loss: 1.2354 - val_acc: 0.7159\n",
      "Epoch 1756/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0627 - acc: 0.9858 - val_loss: 1.3292 - val_acc: 0.7102\n",
      "Epoch 1757/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0425 - acc: 0.9915 - val_loss: 1.3806 - val_acc: 0.7102\n",
      "Epoch 1758/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0555 - acc: 0.9843 - val_loss: 1.3356 - val_acc: 0.6989\n",
      "Epoch 1759/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0620 - acc: 0.9786 - val_loss: 1.2913 - val_acc: 0.6989\n",
      "Epoch 1760/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0542 - acc: 0.9815 - val_loss: 1.2952 - val_acc: 0.7216\n",
      "Epoch 1761/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1040 - acc: 0.9843 - val_loss: 1.2611 - val_acc: 0.7216\n",
      "Epoch 1762/3000\n",
      "702/702 [==============================] - 0s 578us/sample - loss: 0.0553 - acc: 0.9829 - val_loss: 1.2517 - val_acc: 0.7159\n",
      "Epoch 1763/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0696 - acc: 0.9772 - val_loss: 1.2598 - val_acc: 0.7216\n",
      "Epoch 1764/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0701 - acc: 0.9815 - val_loss: 1.2269 - val_acc: 0.7216\n",
      "Epoch 1765/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0686 - acc: 0.9786 - val_loss: 1.2564 - val_acc: 0.7159\n",
      "Epoch 1766/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0594 - acc: 0.9843 - val_loss: 1.3678 - val_acc: 0.6875\n",
      "Epoch 1767/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0847 - acc: 0.9786 - val_loss: 1.3920 - val_acc: 0.6875\n",
      "Epoch 1768/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0433 - acc: 0.9886 - val_loss: 1.3447 - val_acc: 0.6989\n",
      "Epoch 1769/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0427 - acc: 0.9872 - val_loss: 1.3344 - val_acc: 0.7159\n",
      "Epoch 1770/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0520 - acc: 0.9858 - val_loss: 1.3411 - val_acc: 0.6932\n",
      "Epoch 1771/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0605 - acc: 0.9815 - val_loss: 1.3513 - val_acc: 0.7045\n",
      "Epoch 1772/3000\n",
      "702/702 [==============================] - 0s 582us/sample - loss: 0.0668 - acc: 0.9815 - val_loss: 1.3121 - val_acc: 0.6989\n",
      "Epoch 1773/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0668 - acc: 0.9815 - val_loss: 1.3785 - val_acc: 0.6932\n",
      "Epoch 1774/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0321 - acc: 0.9900 - val_loss: 1.4467 - val_acc: 0.6761\n",
      "Epoch 1775/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0597 - acc: 0.9858 - val_loss: 1.3207 - val_acc: 0.7216\n",
      "Epoch 1776/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0809 - acc: 0.9744 - val_loss: 1.2178 - val_acc: 0.7500\n",
      "Epoch 1777/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0901 - acc: 0.9729 - val_loss: 1.3448 - val_acc: 0.7216\n",
      "Epoch 1778/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0915 - acc: 0.9744 - val_loss: 1.3830 - val_acc: 0.7045\n",
      "Epoch 1779/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0585 - acc: 0.9829 - val_loss: 1.2959 - val_acc: 0.6989\n",
      "Epoch 1780/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0573 - acc: 0.9829 - val_loss: 1.2284 - val_acc: 0.7216\n",
      "Epoch 1781/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0302 - acc: 0.9915 - val_loss: 1.2537 - val_acc: 0.7386\n",
      "Epoch 1782/3000\n",
      "702/702 [==============================] - 0s 573us/sample - loss: 0.0742 - acc: 0.9815 - val_loss: 1.3054 - val_acc: 0.7330\n",
      "Epoch 1783/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0471 - acc: 0.9815 - val_loss: 1.3694 - val_acc: 0.6989\n",
      "Epoch 1784/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0846 - acc: 0.9786 - val_loss: 1.3478 - val_acc: 0.7273\n",
      "Epoch 1785/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0859 - acc: 0.9815 - val_loss: 1.2043 - val_acc: 0.7500\n",
      "Epoch 1786/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0464 - acc: 0.9815 - val_loss: 1.1846 - val_acc: 0.7386\n",
      "Epoch 1787/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0588 - acc: 0.9858 - val_loss: 1.2648 - val_acc: 0.7159\n",
      "Epoch 1788/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0595 - acc: 0.9858 - val_loss: 1.3938 - val_acc: 0.7102\n",
      "Epoch 1789/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0511 - acc: 0.9801 - val_loss: 1.4923 - val_acc: 0.7045\n",
      "Epoch 1790/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0353 - acc: 0.9929 - val_loss: 1.5122 - val_acc: 0.7159\n",
      "Epoch 1791/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0754 - acc: 0.9858 - val_loss: 1.4020 - val_acc: 0.7159\n",
      "Epoch 1792/3000\n",
      "702/702 [==============================] - 0s 584us/sample - loss: 0.0391 - acc: 0.9929 - val_loss: 1.2782 - val_acc: 0.7159\n",
      "Epoch 1793/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0326 - acc: 0.9858 - val_loss: 1.2238 - val_acc: 0.7273\n",
      "Epoch 1794/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0285 - acc: 0.9957 - val_loss: 1.2720 - val_acc: 0.7386\n",
      "Epoch 1795/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0316 - acc: 0.9957 - val_loss: 1.3823 - val_acc: 0.7386\n",
      "Epoch 1796/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0493 - acc: 0.9858 - val_loss: 1.3834 - val_acc: 0.7273\n",
      "Epoch 1797/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1125 - acc: 0.9858 - val_loss: 1.4007 - val_acc: 0.7216\n",
      "Epoch 1798/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.0400 - acc: 0.9900 - val_loss: 1.4640 - val_acc: 0.7216\n",
      "Epoch 1799/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.0357 - acc: 0.9872 - val_loss: 1.4126 - val_acc: 0.7330\n",
      "Epoch 1800/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0678 - acc: 0.9872 - val_loss: 1.4033 - val_acc: 0.7216\n",
      "Epoch 1801/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0757 - acc: 0.9815 - val_loss: 1.3454 - val_acc: 0.7159\n",
      "Epoch 1802/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.0461 - acc: 0.9843 - val_loss: 1.2868 - val_acc: 0.7102\n",
      "Epoch 1803/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0370 - acc: 0.9900 - val_loss: 1.3813 - val_acc: 0.7102\n",
      "Epoch 1804/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0543 - acc: 0.9815 - val_loss: 1.3965 - val_acc: 0.7102\n",
      "Epoch 1805/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0368 - acc: 0.9872 - val_loss: 1.3735 - val_acc: 0.7045\n",
      "Epoch 1806/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0700 - acc: 0.9829 - val_loss: 1.3467 - val_acc: 0.7159\n",
      "Epoch 1807/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0382 - acc: 0.9886 - val_loss: 1.3745 - val_acc: 0.7102\n",
      "Epoch 1808/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0441 - acc: 0.9829 - val_loss: 1.3247 - val_acc: 0.7045\n",
      "Epoch 1809/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0328 - acc: 0.9957 - val_loss: 1.2774 - val_acc: 0.7102\n",
      "Epoch 1810/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0349 - acc: 0.9886 - val_loss: 1.2583 - val_acc: 0.7159\n",
      "Epoch 1811/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0418 - acc: 0.9886 - val_loss: 1.2386 - val_acc: 0.7330\n",
      "Epoch 1812/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 0.0398 - acc: 0.9929 - val_loss: 1.2443 - val_acc: 0.7500\n",
      "Epoch 1813/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0174 - acc: 0.9957 - val_loss: 1.3018 - val_acc: 0.7500\n",
      "Epoch 1814/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0278 - acc: 0.9929 - val_loss: 1.3366 - val_acc: 0.7443\n",
      "Epoch 1815/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0479 - acc: 0.9843 - val_loss: 1.2662 - val_acc: 0.7386\n",
      "Epoch 1816/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0307 - acc: 0.9915 - val_loss: 1.2880 - val_acc: 0.7443\n",
      "Epoch 1817/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0419 - acc: 0.9915 - val_loss: 1.2779 - val_acc: 0.7500\n",
      "Epoch 1818/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0338 - acc: 0.9929 - val_loss: 1.3052 - val_acc: 0.7500\n",
      "Epoch 1819/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0316 - acc: 0.9929 - val_loss: 1.3326 - val_acc: 0.7443\n",
      "Epoch 1820/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0486 - acc: 0.9858 - val_loss: 1.2624 - val_acc: 0.7500\n",
      "Epoch 1821/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0266 - acc: 0.9943 - val_loss: 1.2376 - val_acc: 0.7386\n",
      "Epoch 1822/3000\n",
      "702/702 [==============================] - 0s 585us/sample - loss: 0.0337 - acc: 0.9900 - val_loss: 1.2246 - val_acc: 0.7386\n",
      "Epoch 1823/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0293 - acc: 0.9900 - val_loss: 1.2595 - val_acc: 0.7330\n",
      "Epoch 1824/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0283 - acc: 0.9915 - val_loss: 1.3260 - val_acc: 0.7216\n",
      "Epoch 1825/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0150 - acc: 0.9972 - val_loss: 1.4721 - val_acc: 0.7102\n",
      "Epoch 1826/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0481 - acc: 0.9872 - val_loss: 1.4448 - val_acc: 0.7159\n",
      "Epoch 1827/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0269 - acc: 0.9900 - val_loss: 1.3439 - val_acc: 0.7216\n",
      "Epoch 1828/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0616 - acc: 0.9886 - val_loss: 1.2638 - val_acc: 0.7330\n",
      "Epoch 1829/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0343 - acc: 0.9886 - val_loss: 1.3275 - val_acc: 0.7216\n",
      "Epoch 1830/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0279 - acc: 0.9886 - val_loss: 1.3830 - val_acc: 0.7273\n",
      "Epoch 1831/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0344 - acc: 0.9900 - val_loss: 1.4111 - val_acc: 0.7216\n",
      "Epoch 1832/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.0312 - acc: 0.9929 - val_loss: 1.4121 - val_acc: 0.7330\n",
      "Epoch 1833/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0160 - acc: 0.9943 - val_loss: 1.4535 - val_acc: 0.7159\n",
      "Epoch 1834/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0213 - acc: 0.9929 - val_loss: 1.4875 - val_acc: 0.7273\n",
      "Epoch 1835/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0319 - acc: 0.9900 - val_loss: 1.4406 - val_acc: 0.7386\n",
      "Epoch 1836/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0277 - acc: 0.9886 - val_loss: 1.4075 - val_acc: 0.7386\n",
      "Epoch 1837/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0241 - acc: 0.9957 - val_loss: 1.4005 - val_acc: 0.7500\n",
      "Epoch 1838/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0132 - acc: 0.9957 - val_loss: 1.3560 - val_acc: 0.7216\n",
      "Epoch 1839/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0183 - acc: 0.9972 - val_loss: 1.3583 - val_acc: 0.7330\n",
      "Epoch 1840/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.2111 - acc: 0.9900 - val_loss: 1.8940 - val_acc: 0.6818\n",
      "Epoch 1841/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1219 - acc: 0.9744 - val_loss: 1.6186 - val_acc: 0.6989\n",
      "Epoch 1842/3000\n",
      "702/702 [==============================] - 0s 573us/sample - loss: 0.0842 - acc: 0.9715 - val_loss: 1.3838 - val_acc: 0.7330\n",
      "Epoch 1843/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.1244 - acc: 0.9687 - val_loss: 1.3297 - val_acc: 0.7273\n",
      "Epoch 1844/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0649 - acc: 0.9786 - val_loss: 1.3869 - val_acc: 0.6989\n",
      "Epoch 1845/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0759 - acc: 0.9815 - val_loss: 1.4508 - val_acc: 0.7216\n",
      "Epoch 1846/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0451 - acc: 0.9886 - val_loss: 1.4731 - val_acc: 0.7102\n",
      "Epoch 1847/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0528 - acc: 0.9872 - val_loss: 1.3988 - val_acc: 0.7386\n",
      "Epoch 1848/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0809 - acc: 0.9786 - val_loss: 1.3817 - val_acc: 0.7273\n",
      "Epoch 1849/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0443 - acc: 0.9858 - val_loss: 1.4194 - val_acc: 0.7330\n",
      "Epoch 1850/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0400 - acc: 0.9886 - val_loss: 1.3603 - val_acc: 0.7330\n",
      "Epoch 1851/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0504 - acc: 0.9843 - val_loss: 1.3229 - val_acc: 0.7330\n",
      "Epoch 1852/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.0497 - acc: 0.9872 - val_loss: 1.3423 - val_acc: 0.7216\n",
      "Epoch 1853/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0619 - acc: 0.9815 - val_loss: 1.4008 - val_acc: 0.7102\n",
      "Epoch 1854/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0476 - acc: 0.9858 - val_loss: 1.4726 - val_acc: 0.7045\n",
      "Epoch 1855/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0362 - acc: 0.9915 - val_loss: 1.4791 - val_acc: 0.6875\n",
      "Epoch 1856/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0312 - acc: 0.9886 - val_loss: 1.4815 - val_acc: 0.6989\n",
      "Epoch 1857/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0509 - acc: 0.9843 - val_loss: 1.4779 - val_acc: 0.6989\n",
      "Epoch 1858/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0431 - acc: 0.9872 - val_loss: 1.4610 - val_acc: 0.7102\n",
      "Epoch 1859/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0370 - acc: 0.9915 - val_loss: 1.5128 - val_acc: 0.7045\n",
      "Epoch 1860/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0435 - acc: 0.9872 - val_loss: 1.4206 - val_acc: 0.7102\n",
      "Epoch 1861/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0338 - acc: 0.9843 - val_loss: 1.3980 - val_acc: 0.7045\n",
      "Epoch 1862/3000\n",
      "702/702 [==============================] - 0s 570us/sample - loss: 0.0233 - acc: 0.9957 - val_loss: 1.3442 - val_acc: 0.7216\n",
      "Epoch 1863/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0330 - acc: 0.9929 - val_loss: 1.3834 - val_acc: 0.7045\n",
      "Epoch 1864/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0489 - acc: 0.9843 - val_loss: 1.4545 - val_acc: 0.7102\n",
      "Epoch 1865/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0389 - acc: 0.9900 - val_loss: 1.4535 - val_acc: 0.7102\n",
      "Epoch 1866/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0540 - acc: 0.9801 - val_loss: 1.4949 - val_acc: 0.6875\n",
      "Epoch 1867/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0228 - acc: 0.9915 - val_loss: 1.5220 - val_acc: 0.6932\n",
      "Epoch 1868/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0531 - acc: 0.9858 - val_loss: 1.4625 - val_acc: 0.7045\n",
      "Epoch 1869/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0337 - acc: 0.9886 - val_loss: 1.4457 - val_acc: 0.7216\n",
      "Epoch 1870/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0437 - acc: 0.9815 - val_loss: 1.4622 - val_acc: 0.7216\n",
      "Epoch 1871/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0333 - acc: 0.9915 - val_loss: 1.4672 - val_acc: 0.6989\n",
      "Epoch 1872/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.0283 - acc: 0.9915 - val_loss: 1.4448 - val_acc: 0.6989\n",
      "Epoch 1873/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0609 - acc: 0.9843 - val_loss: 1.4885 - val_acc: 0.6989\n",
      "Epoch 1874/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0328 - acc: 0.9872 - val_loss: 1.5003 - val_acc: 0.7159\n",
      "Epoch 1875/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0379 - acc: 0.9915 - val_loss: 1.4200 - val_acc: 0.7159\n",
      "Epoch 1876/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0218 - acc: 0.9929 - val_loss: 1.3878 - val_acc: 0.7216\n",
      "Epoch 1877/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0361 - acc: 0.9872 - val_loss: 1.4336 - val_acc: 0.7102\n",
      "Epoch 1878/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0680 - acc: 0.9858 - val_loss: 1.3986 - val_acc: 0.7102\n",
      "Epoch 1879/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0590 - acc: 0.9886 - val_loss: 1.2883 - val_acc: 0.7273\n",
      "Epoch 1880/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0638 - acc: 0.9829 - val_loss: 1.3200 - val_acc: 0.7102\n",
      "Epoch 1881/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0519 - acc: 0.9858 - val_loss: 1.4604 - val_acc: 0.7102\n",
      "Epoch 1882/3000\n",
      "702/702 [==============================] - 0s 573us/sample - loss: 0.0383 - acc: 0.9872 - val_loss: 1.5077 - val_acc: 0.7045\n",
      "Epoch 1883/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0600 - acc: 0.9858 - val_loss: 1.3726 - val_acc: 0.7216\n",
      "Epoch 1884/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0638 - acc: 0.9801 - val_loss: 1.3264 - val_acc: 0.7216\n",
      "Epoch 1885/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0453 - acc: 0.9858 - val_loss: 1.4598 - val_acc: 0.7102\n",
      "Epoch 1886/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0366 - acc: 0.9900 - val_loss: 1.4634 - val_acc: 0.6989\n",
      "Epoch 1887/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0378 - acc: 0.9915 - val_loss: 1.5239 - val_acc: 0.6875\n",
      "Epoch 1888/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0517 - acc: 0.9872 - val_loss: 1.6682 - val_acc: 0.6761\n",
      "Epoch 1889/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0558 - acc: 0.9858 - val_loss: 1.5594 - val_acc: 0.6818\n",
      "Epoch 1890/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0305 - acc: 0.9943 - val_loss: 1.4675 - val_acc: 0.6648\n",
      "Epoch 1891/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0373 - acc: 0.9915 - val_loss: 1.5161 - val_acc: 0.6705\n",
      "Epoch 1892/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 0.0310 - acc: 0.9886 - val_loss: 1.6119 - val_acc: 0.6648\n",
      "Epoch 1893/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0370 - acc: 0.9872 - val_loss: 1.5877 - val_acc: 0.6875\n",
      "Epoch 1894/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0340 - acc: 0.9900 - val_loss: 1.4911 - val_acc: 0.6989\n",
      "Epoch 1895/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0366 - acc: 0.9886 - val_loss: 1.3412 - val_acc: 0.7216\n",
      "Epoch 1896/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0311 - acc: 0.9900 - val_loss: 1.2912 - val_acc: 0.7330\n",
      "Epoch 1897/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0371 - acc: 0.9900 - val_loss: 1.2886 - val_acc: 0.7330\n",
      "Epoch 1898/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0441 - acc: 0.9886 - val_loss: 1.3596 - val_acc: 0.7216\n",
      "Epoch 1899/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0206 - acc: 0.9943 - val_loss: 1.4693 - val_acc: 0.7102\n",
      "Epoch 1900/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0192 - acc: 0.9972 - val_loss: 1.4697 - val_acc: 0.7102\n",
      "Epoch 1901/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0149 - acc: 0.9972 - val_loss: 1.4980 - val_acc: 0.7159\n",
      "Epoch 1902/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.0298 - acc: 0.9915 - val_loss: 1.5305 - val_acc: 0.7216\n",
      "Epoch 1903/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0232 - acc: 0.9943 - val_loss: 1.4127 - val_acc: 0.7273\n",
      "Epoch 1904/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0375 - acc: 0.9886 - val_loss: 1.3405 - val_acc: 0.7330\n",
      "Epoch 1905/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0589 - acc: 0.9829 - val_loss: 1.4180 - val_acc: 0.7330\n",
      "Epoch 1906/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0380 - acc: 0.9886 - val_loss: 1.4336 - val_acc: 0.7045\n",
      "Epoch 1907/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0449 - acc: 0.9858 - val_loss: 1.3953 - val_acc: 0.7273\n",
      "Epoch 1908/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0413 - acc: 0.9872 - val_loss: 1.3827 - val_acc: 0.7159\n",
      "Epoch 1909/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0340 - acc: 0.9872 - val_loss: 1.4248 - val_acc: 0.7159\n",
      "Epoch 1910/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0310 - acc: 0.9929 - val_loss: 1.4639 - val_acc: 0.6989\n",
      "Epoch 1911/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0221 - acc: 0.9943 - val_loss: 1.4979 - val_acc: 0.6818\n",
      "Epoch 1912/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 0.0151 - acc: 0.9972 - val_loss: 1.5567 - val_acc: 0.6932\n",
      "Epoch 1913/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0414 - acc: 0.9858 - val_loss: 1.5150 - val_acc: 0.7045\n",
      "Epoch 1914/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0212 - acc: 0.9943 - val_loss: 1.5259 - val_acc: 0.7045\n",
      "Epoch 1915/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0249 - acc: 0.9915 - val_loss: 1.5351 - val_acc: 0.6932\n",
      "Epoch 1916/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0277 - acc: 0.9915 - val_loss: 1.5949 - val_acc: 0.7045\n",
      "Epoch 1917/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0304 - acc: 0.9900 - val_loss: 1.5880 - val_acc: 0.7159\n",
      "Epoch 1918/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.2657 - acc: 0.9900 - val_loss: 1.4877 - val_acc: 0.7159\n",
      "Epoch 1919/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0707 - acc: 0.9801 - val_loss: 1.5003 - val_acc: 0.7216\n",
      "Epoch 1920/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0660 - acc: 0.9772 - val_loss: 1.4925 - val_acc: 0.7045\n",
      "Epoch 1921/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1045 - acc: 0.9786 - val_loss: 1.5483 - val_acc: 0.6932\n",
      "Epoch 1922/3000\n",
      "702/702 [==============================] - 0s 566us/sample - loss: 0.0753 - acc: 0.9772 - val_loss: 1.3695 - val_acc: 0.7216\n",
      "Epoch 1923/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.1096 - acc: 0.9715 - val_loss: 1.2569 - val_acc: 0.7216\n",
      "Epoch 1924/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0958 - acc: 0.9715 - val_loss: 1.4915 - val_acc: 0.7216\n",
      "Epoch 1925/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0649 - acc: 0.9772 - val_loss: 1.4761 - val_acc: 0.7159\n",
      "Epoch 1926/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0504 - acc: 0.9843 - val_loss: 1.4170 - val_acc: 0.7273\n",
      "Epoch 1927/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0961 - acc: 0.9687 - val_loss: 1.3213 - val_acc: 0.7273\n",
      "Epoch 1928/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0456 - acc: 0.9872 - val_loss: 1.4136 - val_acc: 0.7102\n",
      "Epoch 1929/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0628 - acc: 0.9815 - val_loss: 1.4206 - val_acc: 0.6932\n",
      "Epoch 1930/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0548 - acc: 0.9815 - val_loss: 1.4507 - val_acc: 0.6989\n",
      "Epoch 1931/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0585 - acc: 0.9815 - val_loss: 1.5855 - val_acc: 0.6989\n",
      "Epoch 1932/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.0890 - acc: 0.9729 - val_loss: 1.8942 - val_acc: 0.6932\n",
      "Epoch 1933/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1408 - acc: 0.9729 - val_loss: 1.4015 - val_acc: 0.7273\n",
      "Epoch 1934/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1643 - acc: 0.9615 - val_loss: 1.4681 - val_acc: 0.7045\n",
      "Epoch 1935/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.1187 - acc: 0.9644 - val_loss: 1.3451 - val_acc: 0.6989\n",
      "Epoch 1936/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1073 - acc: 0.9729 - val_loss: 1.3229 - val_acc: 0.7045\n",
      "Epoch 1937/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0553 - acc: 0.9886 - val_loss: 1.3706 - val_acc: 0.7159\n",
      "Epoch 1938/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.1330 - acc: 0.9658 - val_loss: 1.2977 - val_acc: 0.7216\n",
      "Epoch 1939/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0584 - acc: 0.9858 - val_loss: 1.2929 - val_acc: 0.7273\n",
      "Epoch 1940/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0705 - acc: 0.9829 - val_loss: 1.3784 - val_acc: 0.7216\n",
      "Epoch 1941/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0501 - acc: 0.9872 - val_loss: 1.4078 - val_acc: 0.7159\n",
      "Epoch 1942/3000\n",
      "702/702 [==============================] - 0s 569us/sample - loss: 0.0553 - acc: 0.9801 - val_loss: 1.3984 - val_acc: 0.7330\n",
      "Epoch 1943/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0418 - acc: 0.9915 - val_loss: 1.3774 - val_acc: 0.7386\n",
      "Epoch 1944/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0647 - acc: 0.9815 - val_loss: 1.4361 - val_acc: 0.7045\n",
      "Epoch 1945/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0970 - acc: 0.9687 - val_loss: 1.4251 - val_acc: 0.7045\n",
      "Epoch 1946/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0822 - acc: 0.9758 - val_loss: 1.3888 - val_acc: 0.7045\n",
      "Epoch 1947/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0842 - acc: 0.9729 - val_loss: 1.3411 - val_acc: 0.7216\n",
      "Epoch 1948/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.0427 - acc: 0.9886 - val_loss: 1.3778 - val_acc: 0.7273\n",
      "Epoch 1949/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0600 - acc: 0.9872 - val_loss: 1.3996 - val_acc: 0.7216\n",
      "Epoch 1950/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0606 - acc: 0.9815 - val_loss: 1.4070 - val_acc: 0.7159\n",
      "Epoch 1951/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0605 - acc: 0.9801 - val_loss: 1.2198 - val_acc: 0.7330\n",
      "Epoch 1952/3000\n",
      "702/702 [==============================] - 0s 573us/sample - loss: 0.0989 - acc: 0.9772 - val_loss: 1.2260 - val_acc: 0.7330\n",
      "Epoch 1953/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0516 - acc: 0.9829 - val_loss: 1.3285 - val_acc: 0.7216\n",
      "Epoch 1954/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0943 - acc: 0.9758 - val_loss: 1.4330 - val_acc: 0.7273\n",
      "Epoch 1955/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.1077 - acc: 0.9744 - val_loss: 1.3094 - val_acc: 0.7273\n",
      "Epoch 1956/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0388 - acc: 0.9886 - val_loss: 1.1624 - val_acc: 0.7330\n",
      "Epoch 1957/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0727 - acc: 0.9758 - val_loss: 1.2919 - val_acc: 0.7216\n",
      "Epoch 1958/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0456 - acc: 0.9858 - val_loss: 1.3739 - val_acc: 0.7045\n",
      "Epoch 1959/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0694 - acc: 0.9801 - val_loss: 1.2846 - val_acc: 0.7102\n",
      "Epoch 1960/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0480 - acc: 0.9900 - val_loss: 1.3621 - val_acc: 0.7159\n",
      "Epoch 1961/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0553 - acc: 0.9843 - val_loss: 1.3620 - val_acc: 0.7045\n",
      "Epoch 1962/3000\n",
      "702/702 [==============================] - 0s 573us/sample - loss: 0.0306 - acc: 0.9915 - val_loss: 1.2479 - val_acc: 0.7216\n",
      "Epoch 1963/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0608 - acc: 0.9829 - val_loss: 1.3417 - val_acc: 0.7045\n",
      "Epoch 1964/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0184 - acc: 0.9943 - val_loss: 1.4189 - val_acc: 0.7045\n",
      "Epoch 1965/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0580 - acc: 0.9801 - val_loss: 1.3978 - val_acc: 0.7102\n",
      "Epoch 1966/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0392 - acc: 0.9872 - val_loss: 1.3817 - val_acc: 0.7330\n",
      "Epoch 1967/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0492 - acc: 0.9872 - val_loss: 1.4837 - val_acc: 0.7102\n",
      "Epoch 1968/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0196 - acc: 0.9929 - val_loss: 1.5182 - val_acc: 0.7159\n",
      "Epoch 1969/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0550 - acc: 0.9900 - val_loss: 1.3803 - val_acc: 0.7216\n",
      "Epoch 1970/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0811 - acc: 0.9758 - val_loss: 1.2894 - val_acc: 0.7273\n",
      "Epoch 1971/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0495 - acc: 0.9872 - val_loss: 1.2354 - val_acc: 0.7273\n",
      "Epoch 1972/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.0393 - acc: 0.9886 - val_loss: 1.2726 - val_acc: 0.7159\n",
      "Epoch 1973/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0422 - acc: 0.9843 - val_loss: 1.2674 - val_acc: 0.7330\n",
      "Epoch 1974/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0372 - acc: 0.9886 - val_loss: 1.3093 - val_acc: 0.7273\n",
      "Epoch 1975/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0457 - acc: 0.9872 - val_loss: 1.3237 - val_acc: 0.7386\n",
      "Epoch 1976/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0290 - acc: 0.9900 - val_loss: 1.3427 - val_acc: 0.7386\n",
      "Epoch 1977/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0389 - acc: 0.9886 - val_loss: 1.4470 - val_acc: 0.7216\n",
      "Epoch 1978/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.1702 - acc: 0.9929 - val_loss: 1.4034 - val_acc: 0.7273\n",
      "Epoch 1979/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.1355 - acc: 0.9630 - val_loss: 1.3865 - val_acc: 0.6818\n",
      "Epoch 1980/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.5906 - acc: 0.8889 - val_loss: 1.8719 - val_acc: 0.6250\n",
      "Epoch 1981/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.4500 - acc: 0.8761 - val_loss: 1.4007 - val_acc: 0.6477\n",
      "Epoch 1982/3000\n",
      "702/702 [==============================] - 0s 564us/sample - loss: 0.5567 - acc: 0.8519 - val_loss: 1.2638 - val_acc: 0.6307\n",
      "Epoch 1983/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.5780 - acc: 0.8575 - val_loss: 1.5124 - val_acc: 0.6420\n",
      "Epoch 1984/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.3948 - acc: 0.8960 - val_loss: 1.3874 - val_acc: 0.6648\n",
      "Epoch 1985/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.4751 - acc: 0.8875 - val_loss: 1.6601 - val_acc: 0.6420\n",
      "Epoch 1986/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3652 - acc: 0.9031 - val_loss: 1.6751 - val_acc: 0.6477\n",
      "Epoch 1987/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.3901 - acc: 0.9103 - val_loss: 1.7478 - val_acc: 0.6818\n",
      "Epoch 1988/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.2960 - acc: 0.9217 - val_loss: 1.8311 - val_acc: 0.6818\n",
      "Epoch 1989/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.1959 - acc: 0.9402 - val_loss: 1.6649 - val_acc: 0.6648\n",
      "Epoch 1990/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.2266 - acc: 0.9359 - val_loss: 1.5591 - val_acc: 0.6875\n",
      "Epoch 1991/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2057 - acc: 0.9459 - val_loss: 1.4529 - val_acc: 0.6989\n",
      "Epoch 1992/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 0.2566 - acc: 0.9416 - val_loss: 1.3538 - val_acc: 0.7102\n",
      "Epoch 1993/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.3158 - acc: 0.9245 - val_loss: 1.3850 - val_acc: 0.6875\n",
      "Epoch 1994/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.2379 - acc: 0.9444 - val_loss: 1.4681 - val_acc: 0.6648\n",
      "Epoch 1995/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.2063 - acc: 0.9473 - val_loss: 1.6809 - val_acc: 0.6932\n",
      "Epoch 1996/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2233 - acc: 0.9416 - val_loss: 1.6020 - val_acc: 0.6875\n",
      "Epoch 1997/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1338 - acc: 0.9672 - val_loss: 1.6672 - val_acc: 0.6761\n",
      "Epoch 1998/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1528 - acc: 0.9530 - val_loss: 1.6394 - val_acc: 0.6818\n",
      "Epoch 1999/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1499 - acc: 0.9601 - val_loss: 1.4957 - val_acc: 0.7045\n",
      "Epoch 2000/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0744 - acc: 0.9758 - val_loss: 1.5369 - val_acc: 0.6818\n",
      "Epoch 2001/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0822 - acc: 0.9715 - val_loss: 1.4499 - val_acc: 0.7045\n",
      "Epoch 2002/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.0967 - acc: 0.9729 - val_loss: 1.5485 - val_acc: 0.7045\n",
      "Epoch 2003/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0784 - acc: 0.9815 - val_loss: 1.5691 - val_acc: 0.6989\n",
      "Epoch 2004/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1047 - acc: 0.9672 - val_loss: 1.3602 - val_acc: 0.7273\n",
      "Epoch 2005/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0871 - acc: 0.9701 - val_loss: 1.3772 - val_acc: 0.7102\n",
      "Epoch 2006/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1161 - acc: 0.9687 - val_loss: 1.4760 - val_acc: 0.7159\n",
      "Epoch 2007/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0579 - acc: 0.9829 - val_loss: 1.6059 - val_acc: 0.6705\n",
      "Epoch 2008/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0664 - acc: 0.9829 - val_loss: 1.6408 - val_acc: 0.7159\n",
      "Epoch 2009/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0634 - acc: 0.9801 - val_loss: 1.6339 - val_acc: 0.7045\n",
      "Epoch 2010/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0666 - acc: 0.9858 - val_loss: 1.6113 - val_acc: 0.7102\n",
      "Epoch 2011/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0472 - acc: 0.9858 - val_loss: 1.6311 - val_acc: 0.7045\n",
      "Epoch 2012/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.0643 - acc: 0.9829 - val_loss: 1.5899 - val_acc: 0.7045\n",
      "Epoch 2013/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0600 - acc: 0.9872 - val_loss: 1.5495 - val_acc: 0.6989\n",
      "Epoch 2014/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0573 - acc: 0.9815 - val_loss: 1.4869 - val_acc: 0.7216\n",
      "Epoch 2015/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0413 - acc: 0.9872 - val_loss: 1.4704 - val_acc: 0.7330\n",
      "Epoch 2016/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0542 - acc: 0.9801 - val_loss: 1.4993 - val_acc: 0.7386\n",
      "Epoch 2017/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0376 - acc: 0.9900 - val_loss: 1.5475 - val_acc: 0.6932\n",
      "Epoch 2018/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0430 - acc: 0.9900 - val_loss: 1.5751 - val_acc: 0.6818\n",
      "Epoch 2019/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0374 - acc: 0.9858 - val_loss: 1.4878 - val_acc: 0.7159\n",
      "Epoch 2020/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0594 - acc: 0.9815 - val_loss: 1.3710 - val_acc: 0.7330\n",
      "Epoch 2021/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0572 - acc: 0.9843 - val_loss: 1.3546 - val_acc: 0.7273\n",
      "Epoch 2022/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.0479 - acc: 0.9872 - val_loss: 1.4214 - val_acc: 0.7273\n",
      "Epoch 2023/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0919 - acc: 0.9858 - val_loss: 1.5670 - val_acc: 0.6989\n",
      "Epoch 2024/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0379 - acc: 0.9886 - val_loss: 1.6140 - val_acc: 0.7159\n",
      "Epoch 2025/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0588 - acc: 0.9843 - val_loss: 1.5223 - val_acc: 0.7216\n",
      "Epoch 2026/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0385 - acc: 0.9829 - val_loss: 1.4942 - val_acc: 0.7102\n",
      "Epoch 2027/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0254 - acc: 0.9943 - val_loss: 1.5334 - val_acc: 0.7159\n",
      "Epoch 2028/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0178 - acc: 0.9972 - val_loss: 1.5777 - val_acc: 0.7045\n",
      "Epoch 2029/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0433 - acc: 0.9929 - val_loss: 1.5480 - val_acc: 0.7216\n",
      "Epoch 2030/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0169 - acc: 0.9972 - val_loss: 1.5246 - val_acc: 0.7102\n",
      "Epoch 2031/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0388 - acc: 0.9872 - val_loss: 1.5349 - val_acc: 0.7159\n",
      "Epoch 2032/3000\n",
      "702/702 [==============================] - 0s 566us/sample - loss: 0.0270 - acc: 0.9929 - val_loss: 1.5379 - val_acc: 0.7273\n",
      "Epoch 2033/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0524 - acc: 0.9900 - val_loss: 1.5133 - val_acc: 0.6989\n",
      "Epoch 2034/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0313 - acc: 0.9929 - val_loss: 1.5133 - val_acc: 0.6932\n",
      "Epoch 2035/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0376 - acc: 0.9900 - val_loss: 1.5258 - val_acc: 0.7102\n",
      "Epoch 2036/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.0267 - acc: 0.9943 - val_loss: 1.5057 - val_acc: 0.7102\n",
      "Epoch 2037/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0494 - acc: 0.9872 - val_loss: 1.4619 - val_acc: 0.7159\n",
      "Epoch 2038/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0467 - acc: 0.9900 - val_loss: 1.5136 - val_acc: 0.7216\n",
      "Epoch 2039/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0170 - acc: 0.9943 - val_loss: 1.5243 - val_acc: 0.7045\n",
      "Epoch 2040/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0303 - acc: 0.9900 - val_loss: 1.5606 - val_acc: 0.6989\n",
      "Epoch 2041/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0447 - acc: 0.9872 - val_loss: 1.5387 - val_acc: 0.7216\n",
      "Epoch 2042/3000\n",
      "702/702 [==============================] - 0s 592us/sample - loss: 0.0259 - acc: 0.9900 - val_loss: 1.5403 - val_acc: 0.7273\n",
      "Epoch 2043/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0305 - acc: 0.9915 - val_loss: 1.4965 - val_acc: 0.7273\n",
      "Epoch 2044/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0483 - acc: 0.9886 - val_loss: 1.4736 - val_acc: 0.7159\n",
      "Epoch 2045/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0637 - acc: 0.9815 - val_loss: 1.3366 - val_acc: 0.7330\n",
      "Epoch 2046/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0805 - acc: 0.9801 - val_loss: 1.4468 - val_acc: 0.7273\n",
      "Epoch 2047/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0409 - acc: 0.9915 - val_loss: 1.4638 - val_acc: 0.7330\n",
      "Epoch 2048/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0463 - acc: 0.9900 - val_loss: 1.4356 - val_acc: 0.7443\n",
      "Epoch 2049/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.1302 - acc: 0.9758 - val_loss: 1.3852 - val_acc: 0.7330\n",
      "Epoch 2050/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0716 - acc: 0.9815 - val_loss: 1.4063 - val_acc: 0.7386\n",
      "Epoch 2051/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0491 - acc: 0.9815 - val_loss: 1.5298 - val_acc: 0.7216\n",
      "Epoch 2052/3000\n",
      "702/702 [==============================] - 0s 596us/sample - loss: 0.0551 - acc: 0.9843 - val_loss: 1.5142 - val_acc: 0.7330\n",
      "Epoch 2053/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0730 - acc: 0.9843 - val_loss: 1.4888 - val_acc: 0.7159\n",
      "Epoch 2054/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0768 - acc: 0.9858 - val_loss: 1.5580 - val_acc: 0.6989\n",
      "Epoch 2055/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0947 - acc: 0.9758 - val_loss: 1.5259 - val_acc: 0.6989\n",
      "Epoch 2056/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0820 - acc: 0.9843 - val_loss: 1.4421 - val_acc: 0.6932\n",
      "Epoch 2057/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0679 - acc: 0.9829 - val_loss: 1.4831 - val_acc: 0.6875\n",
      "Epoch 2058/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0341 - acc: 0.9858 - val_loss: 1.4936 - val_acc: 0.6932\n",
      "Epoch 2059/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0553 - acc: 0.9815 - val_loss: 1.4916 - val_acc: 0.6932\n",
      "Epoch 2060/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0441 - acc: 0.9886 - val_loss: 1.4566 - val_acc: 0.7102\n",
      "Epoch 2061/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0370 - acc: 0.9915 - val_loss: 1.4144 - val_acc: 0.7159\n",
      "Epoch 2062/3000\n",
      "702/702 [==============================] - 0s 577us/sample - loss: 0.0634 - acc: 0.9843 - val_loss: 1.4021 - val_acc: 0.7216\n",
      "Epoch 2063/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0496 - acc: 0.9872 - val_loss: 1.3688 - val_acc: 0.7330\n",
      "Epoch 2064/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0364 - acc: 0.9872 - val_loss: 1.3742 - val_acc: 0.7500\n",
      "Epoch 2065/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0231 - acc: 0.9915 - val_loss: 1.3858 - val_acc: 0.7500\n",
      "Epoch 2066/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0469 - acc: 0.9886 - val_loss: 1.3759 - val_acc: 0.7443\n",
      "Epoch 2067/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0331 - acc: 0.9858 - val_loss: 1.3979 - val_acc: 0.7273\n",
      "Epoch 2068/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0416 - acc: 0.9872 - val_loss: 1.3807 - val_acc: 0.7216\n",
      "Epoch 2069/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0475 - acc: 0.9801 - val_loss: 1.4526 - val_acc: 0.7216\n",
      "Epoch 2070/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0350 - acc: 0.9858 - val_loss: 1.5908 - val_acc: 0.7102\n",
      "Epoch 2071/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0420 - acc: 0.9886 - val_loss: 1.5432 - val_acc: 0.7273\n",
      "Epoch 2072/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 0.0275 - acc: 0.9915 - val_loss: 1.4857 - val_acc: 0.7386\n",
      "Epoch 2073/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0241 - acc: 0.9943 - val_loss: 1.4809 - val_acc: 0.7216\n",
      "Epoch 2074/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0355 - acc: 0.9886 - val_loss: 1.5472 - val_acc: 0.7159\n",
      "Epoch 2075/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0301 - acc: 0.9886 - val_loss: 1.5895 - val_acc: 0.7159\n",
      "Epoch 2076/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0419 - acc: 0.9872 - val_loss: 1.5372 - val_acc: 0.6989\n",
      "Epoch 2077/3000\n",
      "702/702 [==============================] - 0s 513us/sample - loss: 0.0572 - acc: 0.9829 - val_loss: 1.4760 - val_acc: 0.7159\n",
      "Epoch 2078/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0444 - acc: 0.9886 - val_loss: 1.5173 - val_acc: 0.7102\n",
      "Epoch 2079/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0873 - acc: 0.9772 - val_loss: 1.5058 - val_acc: 0.7216\n",
      "Epoch 2080/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0447 - acc: 0.9872 - val_loss: 1.4888 - val_acc: 0.7102\n",
      "Epoch 2081/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0468 - acc: 0.9915 - val_loss: 1.5077 - val_acc: 0.7216\n",
      "Epoch 2082/3000\n",
      "702/702 [==============================] - 0s 570us/sample - loss: 0.0482 - acc: 0.9900 - val_loss: 1.6672 - val_acc: 0.6989\n",
      "Epoch 2083/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1343 - acc: 0.9786 - val_loss: 1.3578 - val_acc: 0.7330\n",
      "Epoch 2084/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1412 - acc: 0.9815 - val_loss: 1.2858 - val_acc: 0.7159\n",
      "Epoch 2085/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0812 - acc: 0.9715 - val_loss: 1.3044 - val_acc: 0.7045\n",
      "Epoch 2086/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0846 - acc: 0.9744 - val_loss: 1.3940 - val_acc: 0.7159\n",
      "Epoch 2087/3000\n",
      "702/702 [==============================] - 0s 480us/sample - loss: 0.0576 - acc: 0.9801 - val_loss: 1.2934 - val_acc: 0.7216\n",
      "Epoch 2088/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1360 - acc: 0.9758 - val_loss: 1.2336 - val_acc: 0.7330\n",
      "Epoch 2089/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0407 - acc: 0.9858 - val_loss: 1.3253 - val_acc: 0.7045\n",
      "Epoch 2090/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0451 - acc: 0.9900 - val_loss: 1.3405 - val_acc: 0.7216\n",
      "Epoch 2091/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1410 - acc: 0.9729 - val_loss: 1.2531 - val_acc: 0.7386\n",
      "Epoch 2092/3000\n",
      "702/702 [==============================] - 0s 570us/sample - loss: 0.1568 - acc: 0.9672 - val_loss: 1.1856 - val_acc: 0.7386\n",
      "Epoch 2093/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0818 - acc: 0.9729 - val_loss: 1.3730 - val_acc: 0.7386\n",
      "Epoch 2094/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0764 - acc: 0.9772 - val_loss: 1.3769 - val_acc: 0.7045\n",
      "Epoch 2095/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0549 - acc: 0.9829 - val_loss: 1.2692 - val_acc: 0.7102\n",
      "Epoch 2096/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1455 - acc: 0.9658 - val_loss: 1.3109 - val_acc: 0.7102\n",
      "Epoch 2097/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1077 - acc: 0.9715 - val_loss: 1.3748 - val_acc: 0.7102\n",
      "Epoch 2098/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0652 - acc: 0.9815 - val_loss: 1.4076 - val_acc: 0.7159\n",
      "Epoch 2099/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0563 - acc: 0.9829 - val_loss: 1.4068 - val_acc: 0.7216\n",
      "Epoch 2100/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0663 - acc: 0.9815 - val_loss: 1.4507 - val_acc: 0.7159\n",
      "Epoch 2101/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1363 - acc: 0.9615 - val_loss: 1.4769 - val_acc: 0.7273\n",
      "Epoch 2102/3000\n",
      "702/702 [==============================] - 0s 583us/sample - loss: 0.1105 - acc: 0.9786 - val_loss: 1.4787 - val_acc: 0.7045\n",
      "Epoch 2103/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0558 - acc: 0.9858 - val_loss: 1.4888 - val_acc: 0.6932\n",
      "Epoch 2104/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0750 - acc: 0.9786 - val_loss: 1.4787 - val_acc: 0.6932\n",
      "Epoch 2105/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0427 - acc: 0.9872 - val_loss: 1.5491 - val_acc: 0.6989\n",
      "Epoch 2106/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0563 - acc: 0.9801 - val_loss: 1.5085 - val_acc: 0.7102\n",
      "Epoch 2107/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.1288 - acc: 0.9815 - val_loss: 1.4373 - val_acc: 0.7159\n",
      "Epoch 2108/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0681 - acc: 0.9829 - val_loss: 1.4931 - val_acc: 0.7159\n",
      "Epoch 2109/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0727 - acc: 0.9829 - val_loss: 1.6011 - val_acc: 0.7159\n",
      "Epoch 2110/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0430 - acc: 0.9829 - val_loss: 1.6227 - val_acc: 0.6932\n",
      "Epoch 2111/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0526 - acc: 0.9815 - val_loss: 1.5198 - val_acc: 0.7102\n",
      "Epoch 2112/3000\n",
      "702/702 [==============================] - 0s 581us/sample - loss: 0.0736 - acc: 0.9815 - val_loss: 1.5343 - val_acc: 0.7045\n",
      "Epoch 2113/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0476 - acc: 0.9886 - val_loss: 1.5559 - val_acc: 0.6989\n",
      "Epoch 2114/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1194 - acc: 0.9815 - val_loss: 1.5020 - val_acc: 0.6932\n",
      "Epoch 2115/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.0721 - acc: 0.9786 - val_loss: 1.3676 - val_acc: 0.6989\n",
      "Epoch 2116/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0591 - acc: 0.9872 - val_loss: 1.3280 - val_acc: 0.7216\n",
      "Epoch 2117/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0402 - acc: 0.9872 - val_loss: 1.4786 - val_acc: 0.7102\n",
      "Epoch 2118/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0443 - acc: 0.9886 - val_loss: 1.6557 - val_acc: 0.7045\n",
      "Epoch 2119/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0353 - acc: 0.9886 - val_loss: 1.5765 - val_acc: 0.7045\n",
      "Epoch 2120/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0284 - acc: 0.9915 - val_loss: 1.4355 - val_acc: 0.7216\n",
      "Epoch 2121/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0250 - acc: 0.9929 - val_loss: 1.4099 - val_acc: 0.7386\n",
      "Epoch 2122/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.0387 - acc: 0.9915 - val_loss: 1.4641 - val_acc: 0.7273\n",
      "Epoch 2123/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0192 - acc: 0.9957 - val_loss: 1.5165 - val_acc: 0.7159\n",
      "Epoch 2124/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0617 - acc: 0.9829 - val_loss: 1.5244 - val_acc: 0.7102\n",
      "Epoch 2125/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0346 - acc: 0.9886 - val_loss: 1.4804 - val_acc: 0.7273\n",
      "Epoch 2126/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0291 - acc: 0.9915 - val_loss: 1.4907 - val_acc: 0.7443\n",
      "Epoch 2127/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0248 - acc: 0.9943 - val_loss: 1.4772 - val_acc: 0.7330\n",
      "Epoch 2128/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0243 - acc: 0.9886 - val_loss: 1.4896 - val_acc: 0.7159\n",
      "Epoch 2129/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0494 - acc: 0.9886 - val_loss: 1.3988 - val_acc: 0.7159\n",
      "Epoch 2130/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0434 - acc: 0.9872 - val_loss: 1.3847 - val_acc: 0.7216\n",
      "Epoch 2131/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0259 - acc: 0.9943 - val_loss: 1.4086 - val_acc: 0.7045\n",
      "Epoch 2132/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 0.0242 - acc: 0.9943 - val_loss: 1.4873 - val_acc: 0.6932\n",
      "Epoch 2133/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0153 - acc: 0.9986 - val_loss: 1.5567 - val_acc: 0.6989\n",
      "Epoch 2134/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0339 - acc: 0.9886 - val_loss: 1.4632 - val_acc: 0.6989\n",
      "Epoch 2135/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0361 - acc: 0.9858 - val_loss: 1.4009 - val_acc: 0.7045\n",
      "Epoch 2136/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0669 - acc: 0.9801 - val_loss: 1.3911 - val_acc: 0.7216\n",
      "Epoch 2137/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0702 - acc: 0.9801 - val_loss: 1.4151 - val_acc: 0.7159\n",
      "Epoch 2138/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0331 - acc: 0.9900 - val_loss: 1.4725 - val_acc: 0.7102\n",
      "Epoch 2139/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0409 - acc: 0.9872 - val_loss: 1.5387 - val_acc: 0.7045\n",
      "Epoch 2140/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0429 - acc: 0.9900 - val_loss: 1.5246 - val_acc: 0.7216\n",
      "Epoch 2141/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0201 - acc: 0.9943 - val_loss: 1.4667 - val_acc: 0.7216\n",
      "Epoch 2142/3000\n",
      "702/702 [==============================] - 0s 584us/sample - loss: 0.0336 - acc: 0.9915 - val_loss: 1.4131 - val_acc: 0.7216\n",
      "Epoch 2143/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0390 - acc: 0.9915 - val_loss: 1.4135 - val_acc: 0.7330\n",
      "Epoch 2144/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0178 - acc: 0.9957 - val_loss: 1.4389 - val_acc: 0.7273\n",
      "Epoch 2145/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0434 - acc: 0.9858 - val_loss: 1.5433 - val_acc: 0.6989\n",
      "Epoch 2146/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0368 - acc: 0.9915 - val_loss: 1.5194 - val_acc: 0.6989\n",
      "Epoch 2147/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0389 - acc: 0.9929 - val_loss: 1.4507 - val_acc: 0.7045\n",
      "Epoch 2148/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0235 - acc: 0.9943 - val_loss: 1.4126 - val_acc: 0.7102\n",
      "Epoch 2149/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0322 - acc: 0.9929 - val_loss: 1.4286 - val_acc: 0.7102\n",
      "Epoch 2150/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0414 - acc: 0.9843 - val_loss: 1.3948 - val_acc: 0.7216\n",
      "Epoch 2151/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0348 - acc: 0.9829 - val_loss: 1.3756 - val_acc: 0.7330\n",
      "Epoch 2152/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.0448 - acc: 0.9858 - val_loss: 1.4437 - val_acc: 0.7159\n",
      "Epoch 2153/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0555 - acc: 0.9872 - val_loss: 1.5125 - val_acc: 0.6989\n",
      "Epoch 2154/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0372 - acc: 0.9886 - val_loss: 1.3967 - val_acc: 0.7216\n",
      "Epoch 2155/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0200 - acc: 0.9972 - val_loss: 1.4049 - val_acc: 0.7216\n",
      "Epoch 2156/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0207 - acc: 0.9943 - val_loss: 1.4269 - val_acc: 0.7159\n",
      "Epoch 2157/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0571 - acc: 0.9815 - val_loss: 1.4683 - val_acc: 0.7159\n",
      "Epoch 2158/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0388 - acc: 0.9886 - val_loss: 1.3887 - val_acc: 0.7273\n",
      "Epoch 2159/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0395 - acc: 0.9843 - val_loss: 1.2957 - val_acc: 0.7386\n",
      "Epoch 2160/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0362 - acc: 0.9900 - val_loss: 1.3953 - val_acc: 0.7216\n",
      "Epoch 2161/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0436 - acc: 0.9886 - val_loss: 1.4434 - val_acc: 0.7216\n",
      "Epoch 2162/3000\n",
      "702/702 [==============================] - 0s 599us/sample - loss: 0.0334 - acc: 0.9915 - val_loss: 1.2893 - val_acc: 0.7273\n",
      "Epoch 2163/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0331 - acc: 0.9872 - val_loss: 1.2367 - val_acc: 0.7386\n",
      "Epoch 2164/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0213 - acc: 0.9943 - val_loss: 1.2305 - val_acc: 0.7330\n",
      "Epoch 2165/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.0557 - acc: 0.9886 - val_loss: 1.2071 - val_acc: 0.7386\n",
      "Epoch 2166/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0204 - acc: 0.9957 - val_loss: 1.2158 - val_acc: 0.7557\n",
      "Epoch 2167/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0308 - acc: 0.9915 - val_loss: 1.2546 - val_acc: 0.7670\n",
      "Epoch 2168/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0301 - acc: 0.9943 - val_loss: 1.3703 - val_acc: 0.7443\n",
      "Epoch 2169/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1311 - acc: 0.9843 - val_loss: 1.3953 - val_acc: 0.7273\n",
      "Epoch 2170/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0719 - acc: 0.9758 - val_loss: 1.5394 - val_acc: 0.7045\n",
      "Epoch 2171/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1269 - acc: 0.9601 - val_loss: 1.5534 - val_acc: 0.6875\n",
      "Epoch 2172/3000\n",
      "702/702 [==============================] - 0s 570us/sample - loss: 0.0543 - acc: 0.9772 - val_loss: 1.4669 - val_acc: 0.7102\n",
      "Epoch 2173/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0374 - acc: 0.9872 - val_loss: 1.4858 - val_acc: 0.7216\n",
      "Epoch 2174/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0908 - acc: 0.9729 - val_loss: 1.4971 - val_acc: 0.7216\n",
      "Epoch 2175/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0840 - acc: 0.9758 - val_loss: 1.4557 - val_acc: 0.7443\n",
      "Epoch 2176/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0990 - acc: 0.9744 - val_loss: 1.5014 - val_acc: 0.7216\n",
      "Epoch 2177/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0460 - acc: 0.9843 - val_loss: 1.5107 - val_acc: 0.7045\n",
      "Epoch 2178/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0890 - acc: 0.9715 - val_loss: 1.6073 - val_acc: 0.7045\n",
      "Epoch 2179/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.0400 - acc: 0.9858 - val_loss: 1.6175 - val_acc: 0.7045\n",
      "Epoch 2180/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0381 - acc: 0.9886 - val_loss: 1.4668 - val_acc: 0.7273\n",
      "Epoch 2181/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0440 - acc: 0.9829 - val_loss: 1.5286 - val_acc: 0.7159\n",
      "Epoch 2182/3000\n",
      "702/702 [==============================] - 0s 577us/sample - loss: 0.0446 - acc: 0.9886 - val_loss: 1.4877 - val_acc: 0.7273\n",
      "Epoch 2183/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0656 - acc: 0.9858 - val_loss: 1.5211 - val_acc: 0.7159\n",
      "Epoch 2184/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0493 - acc: 0.9858 - val_loss: 1.6416 - val_acc: 0.6932\n",
      "Epoch 2185/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0485 - acc: 0.9843 - val_loss: 1.6097 - val_acc: 0.7216\n",
      "Epoch 2186/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0402 - acc: 0.9943 - val_loss: 1.5863 - val_acc: 0.7159\n",
      "Epoch 2187/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0525 - acc: 0.9858 - val_loss: 1.6136 - val_acc: 0.7045\n",
      "Epoch 2188/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0332 - acc: 0.9929 - val_loss: 1.6305 - val_acc: 0.7273\n",
      "Epoch 2189/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.0279 - acc: 0.9943 - val_loss: 1.5516 - val_acc: 0.7159\n",
      "Epoch 2190/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0332 - acc: 0.9900 - val_loss: 1.5528 - val_acc: 0.7045\n",
      "Epoch 2191/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0340 - acc: 0.9886 - val_loss: 1.5502 - val_acc: 0.7159\n",
      "Epoch 2192/3000\n",
      "702/702 [==============================] - 0s 568us/sample - loss: 0.0295 - acc: 0.9900 - val_loss: 1.6559 - val_acc: 0.7045\n",
      "Epoch 2193/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0348 - acc: 0.9915 - val_loss: 1.5589 - val_acc: 0.6875\n",
      "Epoch 2194/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0398 - acc: 0.9858 - val_loss: 1.5556 - val_acc: 0.6932\n",
      "Epoch 2195/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0381 - acc: 0.9929 - val_loss: 1.6146 - val_acc: 0.6875\n",
      "Epoch 2196/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0250 - acc: 0.9943 - val_loss: 1.5239 - val_acc: 0.7102\n",
      "Epoch 2197/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0352 - acc: 0.9943 - val_loss: 1.4340 - val_acc: 0.7216\n",
      "Epoch 2198/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0227 - acc: 0.9943 - val_loss: 1.3591 - val_acc: 0.7159\n",
      "Epoch 2199/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0328 - acc: 0.9915 - val_loss: 1.3608 - val_acc: 0.7216\n",
      "Epoch 2200/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0285 - acc: 0.9915 - val_loss: 1.4468 - val_acc: 0.7273\n",
      "Epoch 2201/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0294 - acc: 0.9929 - val_loss: 1.5899 - val_acc: 0.7273\n",
      "Epoch 2202/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.0257 - acc: 0.9929 - val_loss: 1.5157 - val_acc: 0.7273\n",
      "Epoch 2203/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0126 - acc: 0.9972 - val_loss: 1.4521 - val_acc: 0.7443\n",
      "Epoch 2204/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0285 - acc: 0.9915 - val_loss: 1.4277 - val_acc: 0.7330\n",
      "Epoch 2205/3000\n",
      "702/702 [==============================] - 0s 483us/sample - loss: 0.0125 - acc: 0.9972 - val_loss: 1.4536 - val_acc: 0.7330\n",
      "Epoch 2206/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0227 - acc: 0.9929 - val_loss: 1.4708 - val_acc: 0.7273\n",
      "Epoch 2207/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0202 - acc: 0.9943 - val_loss: 1.5229 - val_acc: 0.7273\n",
      "Epoch 2208/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0122 - acc: 0.9972 - val_loss: 1.6554 - val_acc: 0.7273\n",
      "Epoch 2209/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0747 - acc: 0.9900 - val_loss: 1.6084 - val_acc: 0.7330\n",
      "Epoch 2210/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0153 - acc: 0.9972 - val_loss: 1.6083 - val_acc: 0.7386\n",
      "Epoch 2211/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0204 - acc: 0.9943 - val_loss: 1.5614 - val_acc: 0.7330\n",
      "Epoch 2212/3000\n",
      "702/702 [==============================] - 0s 559us/sample - loss: 0.0479 - acc: 0.9900 - val_loss: 1.4779 - val_acc: 0.7159\n",
      "Epoch 2213/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.1256 - acc: 0.9858 - val_loss: 1.5786 - val_acc: 0.7273\n",
      "Epoch 2214/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0398 - acc: 0.9872 - val_loss: 1.6590 - val_acc: 0.7386\n",
      "Epoch 2215/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0759 - acc: 0.9772 - val_loss: 1.5217 - val_acc: 0.7273\n",
      "Epoch 2216/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0934 - acc: 0.9786 - val_loss: 1.5078 - val_acc: 0.7273\n",
      "Epoch 2217/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.1453 - acc: 0.9658 - val_loss: 1.4396 - val_acc: 0.7216\n",
      "Epoch 2218/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0862 - acc: 0.9744 - val_loss: 1.7319 - val_acc: 0.7045\n",
      "Epoch 2219/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0830 - acc: 0.9744 - val_loss: 1.8380 - val_acc: 0.6932\n",
      "Epoch 2220/3000\n",
      "702/702 [==============================] - 0s 512us/sample - loss: 0.1328 - acc: 0.9815 - val_loss: 1.7782 - val_acc: 0.7159\n",
      "Epoch 2221/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1109 - acc: 0.9687 - val_loss: 1.5828 - val_acc: 0.7102\n",
      "Epoch 2222/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.0759 - acc: 0.9729 - val_loss: 1.4691 - val_acc: 0.7159\n",
      "Epoch 2223/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0551 - acc: 0.9829 - val_loss: 1.4797 - val_acc: 0.7443\n",
      "Epoch 2224/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0320 - acc: 0.9872 - val_loss: 1.5259 - val_acc: 0.7273\n",
      "Epoch 2225/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0791 - acc: 0.9801 - val_loss: 1.4204 - val_acc: 0.7273\n",
      "Epoch 2226/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0635 - acc: 0.9872 - val_loss: 1.3979 - val_acc: 0.7159\n",
      "Epoch 2227/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0523 - acc: 0.9801 - val_loss: 1.5158 - val_acc: 0.7159\n",
      "Epoch 2228/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0958 - acc: 0.9758 - val_loss: 1.5730 - val_acc: 0.7273\n",
      "Epoch 2229/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0328 - acc: 0.9900 - val_loss: 1.7015 - val_acc: 0.7045\n",
      "Epoch 2230/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1085 - acc: 0.9786 - val_loss: 1.4734 - val_acc: 0.7330\n",
      "Epoch 2231/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0624 - acc: 0.9872 - val_loss: 1.3238 - val_acc: 0.7216\n",
      "Epoch 2232/3000\n",
      "702/702 [==============================] - 0s 564us/sample - loss: 0.0346 - acc: 0.9886 - val_loss: 1.3760 - val_acc: 0.7216\n",
      "Epoch 2233/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0516 - acc: 0.9786 - val_loss: 1.3532 - val_acc: 0.7159\n",
      "Epoch 2234/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0608 - acc: 0.9872 - val_loss: 1.3968 - val_acc: 0.7557\n",
      "Epoch 2235/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0362 - acc: 0.9886 - val_loss: 1.3923 - val_acc: 0.7443\n",
      "Epoch 2236/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0396 - acc: 0.9915 - val_loss: 1.4328 - val_acc: 0.7330\n",
      "Epoch 2237/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0497 - acc: 0.9843 - val_loss: 1.4580 - val_acc: 0.7386\n",
      "Epoch 2238/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0366 - acc: 0.9858 - val_loss: 1.5679 - val_acc: 0.7330\n",
      "Epoch 2239/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0737 - acc: 0.9801 - val_loss: 1.6105 - val_acc: 0.7102\n",
      "Epoch 2240/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0209 - acc: 0.9943 - val_loss: 1.6031 - val_acc: 0.7159\n",
      "Epoch 2241/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0398 - acc: 0.9858 - val_loss: 1.4734 - val_acc: 0.7386\n",
      "Epoch 2242/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 0.0504 - acc: 0.9886 - val_loss: 1.5037 - val_acc: 0.7500\n",
      "Epoch 2243/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0334 - acc: 0.9872 - val_loss: 1.5184 - val_acc: 0.7330\n",
      "Epoch 2244/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0399 - acc: 0.9872 - val_loss: 1.5798 - val_acc: 0.7330\n",
      "Epoch 2245/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0352 - acc: 0.9915 - val_loss: 1.6034 - val_acc: 0.7330\n",
      "Epoch 2246/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0444 - acc: 0.9872 - val_loss: 1.5286 - val_acc: 0.7330\n",
      "Epoch 2247/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0193 - acc: 0.9957 - val_loss: 1.4959 - val_acc: 0.7273\n",
      "Epoch 2248/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0702 - acc: 0.9801 - val_loss: 1.3744 - val_acc: 0.7216\n",
      "Epoch 2249/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0568 - acc: 0.9872 - val_loss: 1.4704 - val_acc: 0.7102\n",
      "Epoch 2250/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0712 - acc: 0.9815 - val_loss: 1.5209 - val_acc: 0.7216\n",
      "Epoch 2251/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0820 - acc: 0.9858 - val_loss: 1.5522 - val_acc: 0.7273\n",
      "Epoch 2252/3000\n",
      "702/702 [==============================] - 0s 582us/sample - loss: 0.0609 - acc: 0.9786 - val_loss: 1.6007 - val_acc: 0.7159\n",
      "Epoch 2253/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.0678 - acc: 0.9815 - val_loss: 1.4505 - val_acc: 0.7102\n",
      "Epoch 2254/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0708 - acc: 0.9772 - val_loss: 1.4377 - val_acc: 0.6932\n",
      "Epoch 2255/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0472 - acc: 0.9858 - val_loss: 1.4186 - val_acc: 0.7102\n",
      "Epoch 2256/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.1905 - acc: 0.9858 - val_loss: 1.5320 - val_acc: 0.6818\n",
      "Epoch 2257/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0971 - acc: 0.9801 - val_loss: 1.5610 - val_acc: 0.6761\n",
      "Epoch 2258/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0629 - acc: 0.9815 - val_loss: 1.6595 - val_acc: 0.6875\n",
      "Epoch 2259/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0551 - acc: 0.9815 - val_loss: 1.8659 - val_acc: 0.6989\n",
      "Epoch 2260/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0802 - acc: 0.9786 - val_loss: 1.9553 - val_acc: 0.6818\n",
      "Epoch 2261/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0637 - acc: 0.9815 - val_loss: 1.6992 - val_acc: 0.6932\n",
      "Epoch 2262/3000\n",
      "702/702 [==============================] - 0s 570us/sample - loss: 0.0502 - acc: 0.9843 - val_loss: 1.4511 - val_acc: 0.6989\n",
      "Epoch 2263/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0493 - acc: 0.9843 - val_loss: 1.4049 - val_acc: 0.6932\n",
      "Epoch 2264/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0445 - acc: 0.9843 - val_loss: 1.4574 - val_acc: 0.6989\n",
      "Epoch 2265/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0346 - acc: 0.9858 - val_loss: 1.5833 - val_acc: 0.6875\n",
      "Epoch 2266/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0553 - acc: 0.9858 - val_loss: 1.5933 - val_acc: 0.6875\n",
      "Epoch 2267/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0678 - acc: 0.9843 - val_loss: 1.4303 - val_acc: 0.6761\n",
      "Epoch 2268/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0761 - acc: 0.9772 - val_loss: 1.5870 - val_acc: 0.6648\n",
      "Epoch 2269/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0725 - acc: 0.9829 - val_loss: 1.7450 - val_acc: 0.6761\n",
      "Epoch 2270/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0446 - acc: 0.9829 - val_loss: 1.6840 - val_acc: 0.6875\n",
      "Epoch 2271/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1010 - acc: 0.9815 - val_loss: 1.6911 - val_acc: 0.6875\n",
      "Epoch 2272/3000\n",
      "702/702 [==============================] - 0s 573us/sample - loss: 0.0653 - acc: 0.9801 - val_loss: 1.7080 - val_acc: 0.6761\n",
      "Epoch 2273/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0430 - acc: 0.9872 - val_loss: 1.7405 - val_acc: 0.6932\n",
      "Epoch 2274/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0416 - acc: 0.9872 - val_loss: 1.7515 - val_acc: 0.7102\n",
      "Epoch 2275/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.0420 - acc: 0.9858 - val_loss: 1.6738 - val_acc: 0.7159\n",
      "Epoch 2276/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0191 - acc: 0.9929 - val_loss: 1.7104 - val_acc: 0.6989\n",
      "Epoch 2277/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0237 - acc: 0.9915 - val_loss: 1.7133 - val_acc: 0.6875\n",
      "Epoch 2278/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0306 - acc: 0.9886 - val_loss: 1.6706 - val_acc: 0.6989\n",
      "Epoch 2279/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0291 - acc: 0.9900 - val_loss: 1.5906 - val_acc: 0.7273\n",
      "Epoch 2280/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0295 - acc: 0.9915 - val_loss: 1.5803 - val_acc: 0.7273\n",
      "Epoch 2281/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0413 - acc: 0.9915 - val_loss: 1.5324 - val_acc: 0.7386\n",
      "Epoch 2282/3000\n",
      "702/702 [==============================] - 0s 570us/sample - loss: 0.0297 - acc: 0.9943 - val_loss: 1.5189 - val_acc: 0.7443\n",
      "Epoch 2283/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0260 - acc: 0.9900 - val_loss: 1.5417 - val_acc: 0.7216\n",
      "Epoch 2284/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0208 - acc: 0.9915 - val_loss: 1.5328 - val_acc: 0.7045\n",
      "Epoch 2285/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0473 - acc: 0.9886 - val_loss: 1.5061 - val_acc: 0.7159\n",
      "Epoch 2286/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0377 - acc: 0.9886 - val_loss: 1.5313 - val_acc: 0.6989\n",
      "Epoch 2287/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0567 - acc: 0.9929 - val_loss: 1.4791 - val_acc: 0.7216\n",
      "Epoch 2288/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0911 - acc: 0.9801 - val_loss: 1.3981 - val_acc: 0.7216\n",
      "Epoch 2289/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0801 - acc: 0.9744 - val_loss: 1.3917 - val_acc: 0.7216\n",
      "Epoch 2290/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0807 - acc: 0.9786 - val_loss: 1.4144 - val_acc: 0.7216\n",
      "Epoch 2291/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0735 - acc: 0.9772 - val_loss: 1.5492 - val_acc: 0.7045\n",
      "Epoch 2292/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.0991 - acc: 0.9615 - val_loss: 1.6811 - val_acc: 0.6932\n",
      "Epoch 2293/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.1132 - acc: 0.9772 - val_loss: 1.7137 - val_acc: 0.6932\n",
      "Epoch 2294/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0667 - acc: 0.9801 - val_loss: 1.5259 - val_acc: 0.6818\n",
      "Epoch 2295/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1186 - acc: 0.9829 - val_loss: 1.4863 - val_acc: 0.6932\n",
      "Epoch 2296/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0721 - acc: 0.9758 - val_loss: 1.7269 - val_acc: 0.6818\n",
      "Epoch 2297/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0517 - acc: 0.9801 - val_loss: 1.8346 - val_acc: 0.6818\n",
      "Epoch 2298/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.0747 - acc: 0.9772 - val_loss: 1.9288 - val_acc: 0.6989\n",
      "Epoch 2299/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0736 - acc: 0.9801 - val_loss: 1.6405 - val_acc: 0.7386\n",
      "Epoch 2300/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0701 - acc: 0.9815 - val_loss: 1.5663 - val_acc: 0.7102\n",
      "Epoch 2301/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0600 - acc: 0.9872 - val_loss: 1.6044 - val_acc: 0.7102\n",
      "Epoch 2302/3000\n",
      "702/702 [==============================] - 0s 570us/sample - loss: 0.0348 - acc: 0.9900 - val_loss: 1.5119 - val_acc: 0.7216\n",
      "Epoch 2303/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.1478 - acc: 0.9701 - val_loss: 1.2825 - val_acc: 0.7273\n",
      "Epoch 2304/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1983 - acc: 0.9416 - val_loss: 1.3055 - val_acc: 0.7159\n",
      "Epoch 2305/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1853 - acc: 0.9430 - val_loss: 1.5165 - val_acc: 0.7045\n",
      "Epoch 2306/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1967 - acc: 0.9473 - val_loss: 1.5456 - val_acc: 0.6761\n",
      "Epoch 2307/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.1770 - acc: 0.9459 - val_loss: 1.5476 - val_acc: 0.6932\n",
      "Epoch 2308/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.1516 - acc: 0.9615 - val_loss: 1.5017 - val_acc: 0.7045\n",
      "Epoch 2309/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1688 - acc: 0.9501 - val_loss: 1.5045 - val_acc: 0.7159\n",
      "Epoch 2310/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1177 - acc: 0.9601 - val_loss: 1.4419 - val_acc: 0.7159\n",
      "Epoch 2311/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1111 - acc: 0.9644 - val_loss: 1.6214 - val_acc: 0.7273\n",
      "Epoch 2312/3000\n",
      "702/702 [==============================] - 0s 594us/sample - loss: 0.0843 - acc: 0.9744 - val_loss: 1.5725 - val_acc: 0.7330\n",
      "Epoch 2313/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1048 - acc: 0.9701 - val_loss: 1.3707 - val_acc: 0.7273\n",
      "Epoch 2314/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0768 - acc: 0.9772 - val_loss: 1.3172 - val_acc: 0.7159\n",
      "Epoch 2315/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0777 - acc: 0.9801 - val_loss: 1.4519 - val_acc: 0.6989\n",
      "Epoch 2316/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0587 - acc: 0.9843 - val_loss: 1.5588 - val_acc: 0.7159\n",
      "Epoch 2317/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0780 - acc: 0.9729 - val_loss: 1.6310 - val_acc: 0.6989\n",
      "Epoch 2318/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0738 - acc: 0.9758 - val_loss: 1.5812 - val_acc: 0.6989\n",
      "Epoch 2319/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1048 - acc: 0.9687 - val_loss: 1.5436 - val_acc: 0.7045\n",
      "Epoch 2320/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1295 - acc: 0.9630 - val_loss: 1.5438 - val_acc: 0.6989\n",
      "Epoch 2321/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0511 - acc: 0.9858 - val_loss: 1.7930 - val_acc: 0.6818\n",
      "Epoch 2322/3000\n",
      "702/702 [==============================] - 0s 582us/sample - loss: 0.0910 - acc: 0.9772 - val_loss: 1.5295 - val_acc: 0.7045\n",
      "Epoch 2323/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0824 - acc: 0.9786 - val_loss: 1.5769 - val_acc: 0.6989\n",
      "Epoch 2324/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0570 - acc: 0.9843 - val_loss: 1.6570 - val_acc: 0.6932\n",
      "Epoch 2325/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0717 - acc: 0.9815 - val_loss: 1.6937 - val_acc: 0.6761\n",
      "Epoch 2326/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0800 - acc: 0.9858 - val_loss: 1.5397 - val_acc: 0.6932\n",
      "Epoch 2327/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0590 - acc: 0.9815 - val_loss: 1.4661 - val_acc: 0.7102\n",
      "Epoch 2328/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0537 - acc: 0.9858 - val_loss: 1.5042 - val_acc: 0.7330\n",
      "Epoch 2329/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0580 - acc: 0.9801 - val_loss: 1.6070 - val_acc: 0.7330\n",
      "Epoch 2330/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0796 - acc: 0.9758 - val_loss: 1.5598 - val_acc: 0.7216\n",
      "Epoch 2331/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0350 - acc: 0.9886 - val_loss: 1.6114 - val_acc: 0.7159\n",
      "Epoch 2332/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 0.0339 - acc: 0.9872 - val_loss: 1.6889 - val_acc: 0.7102\n",
      "Epoch 2333/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0697 - acc: 0.9843 - val_loss: 1.7406 - val_acc: 0.7045\n",
      "Epoch 2334/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0465 - acc: 0.9858 - val_loss: 1.8196 - val_acc: 0.6989\n",
      "Epoch 2335/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0792 - acc: 0.9843 - val_loss: 1.6358 - val_acc: 0.7216\n",
      "Epoch 2336/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0804 - acc: 0.9758 - val_loss: 1.6244 - val_acc: 0.6705\n",
      "Epoch 2337/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1408 - acc: 0.9615 - val_loss: 1.5310 - val_acc: 0.6705\n",
      "Epoch 2338/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0875 - acc: 0.9758 - val_loss: 1.6695 - val_acc: 0.6989\n",
      "Epoch 2339/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1056 - acc: 0.9715 - val_loss: 1.4567 - val_acc: 0.6932\n",
      "Epoch 2340/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1450 - acc: 0.9601 - val_loss: 1.4230 - val_acc: 0.7045\n",
      "Epoch 2341/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1022 - acc: 0.9687 - val_loss: 1.4208 - val_acc: 0.7045\n",
      "Epoch 2342/3000\n",
      "702/702 [==============================] - 0s 602us/sample - loss: 0.1292 - acc: 0.9701 - val_loss: 1.4383 - val_acc: 0.7159\n",
      "Epoch 2343/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.1737 - acc: 0.9601 - val_loss: 1.3060 - val_acc: 0.7159\n",
      "Epoch 2344/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.1435 - acc: 0.9587 - val_loss: 1.2706 - val_acc: 0.6875\n",
      "Epoch 2345/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.1059 - acc: 0.9687 - val_loss: 1.4317 - val_acc: 0.7216\n",
      "Epoch 2346/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0804 - acc: 0.9786 - val_loss: 1.4813 - val_acc: 0.7273\n",
      "Epoch 2347/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0675 - acc: 0.9801 - val_loss: 1.3999 - val_acc: 0.7216\n",
      "Epoch 2348/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.1196 - acc: 0.9772 - val_loss: 1.3774 - val_acc: 0.7273\n",
      "Epoch 2349/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0793 - acc: 0.9715 - val_loss: 1.3298 - val_acc: 0.7330\n",
      "Epoch 2350/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0876 - acc: 0.9715 - val_loss: 1.5288 - val_acc: 0.7273\n",
      "Epoch 2351/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0912 - acc: 0.9687 - val_loss: 1.4774 - val_acc: 0.7216\n",
      "Epoch 2352/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 0.0674 - acc: 0.9843 - val_loss: 1.4678 - val_acc: 0.7216\n",
      "Epoch 2353/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1200 - acc: 0.9829 - val_loss: 1.5322 - val_acc: 0.7045\n",
      "Epoch 2354/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.1332 - acc: 0.9758 - val_loss: 1.3602 - val_acc: 0.6818\n",
      "Epoch 2355/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0808 - acc: 0.9772 - val_loss: 1.3418 - val_acc: 0.6875\n",
      "Epoch 2356/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1226 - acc: 0.9587 - val_loss: 1.3738 - val_acc: 0.7273\n",
      "Epoch 2357/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0906 - acc: 0.9758 - val_loss: 1.4849 - val_acc: 0.7216\n",
      "Epoch 2358/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0624 - acc: 0.9786 - val_loss: 1.4385 - val_acc: 0.7216\n",
      "Epoch 2359/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0766 - acc: 0.9715 - val_loss: 1.4326 - val_acc: 0.7386\n",
      "Epoch 2360/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0423 - acc: 0.9858 - val_loss: 1.4083 - val_acc: 0.7386\n",
      "Epoch 2361/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0771 - acc: 0.9758 - val_loss: 1.5193 - val_acc: 0.7330\n",
      "Epoch 2362/3000\n",
      "702/702 [==============================] - 0s 570us/sample - loss: 0.0762 - acc: 0.9744 - val_loss: 1.9945 - val_acc: 0.6818\n",
      "Epoch 2363/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0683 - acc: 0.9772 - val_loss: 1.7605 - val_acc: 0.7102\n",
      "Epoch 2364/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1013 - acc: 0.9729 - val_loss: 1.4692 - val_acc: 0.7102\n",
      "Epoch 2365/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0772 - acc: 0.9772 - val_loss: 1.3926 - val_acc: 0.7159\n",
      "Epoch 2366/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.0591 - acc: 0.9843 - val_loss: 1.4127 - val_acc: 0.7500\n",
      "Epoch 2367/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0860 - acc: 0.9744 - val_loss: 1.3390 - val_acc: 0.7614\n",
      "Epoch 2368/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0572 - acc: 0.9815 - val_loss: 1.3702 - val_acc: 0.7443\n",
      "Epoch 2369/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1043 - acc: 0.9687 - val_loss: 1.3596 - val_acc: 0.7386\n",
      "Epoch 2370/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0685 - acc: 0.9801 - val_loss: 1.3361 - val_acc: 0.7216\n",
      "Epoch 2371/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0405 - acc: 0.9886 - val_loss: 1.4224 - val_acc: 0.7330\n",
      "Epoch 2372/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 0.1022 - acc: 0.9815 - val_loss: 1.4472 - val_acc: 0.7159\n",
      "Epoch 2373/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0545 - acc: 0.9858 - val_loss: 1.5102 - val_acc: 0.7045\n",
      "Epoch 2374/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0478 - acc: 0.9843 - val_loss: 1.6079 - val_acc: 0.6989\n",
      "Epoch 2375/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0525 - acc: 0.9872 - val_loss: 1.6128 - val_acc: 0.6875\n",
      "Epoch 2376/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0829 - acc: 0.9772 - val_loss: 1.5791 - val_acc: 0.7045\n",
      "Epoch 2377/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0407 - acc: 0.9915 - val_loss: 1.5851 - val_acc: 0.7386\n",
      "Epoch 2378/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0220 - acc: 0.9943 - val_loss: 1.6543 - val_acc: 0.7500\n",
      "Epoch 2379/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0471 - acc: 0.9886 - val_loss: 1.6449 - val_acc: 0.7386\n",
      "Epoch 2380/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0230 - acc: 0.9929 - val_loss: 1.6452 - val_acc: 0.7330\n",
      "Epoch 2381/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0565 - acc: 0.9858 - val_loss: 1.7106 - val_acc: 0.7386\n",
      "Epoch 2382/3000\n",
      "702/702 [==============================] - 0s 581us/sample - loss: 0.0378 - acc: 0.9900 - val_loss: 1.6787 - val_acc: 0.7330\n",
      "Epoch 2383/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0301 - acc: 0.9943 - val_loss: 1.6606 - val_acc: 0.7273\n",
      "Epoch 2384/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0426 - acc: 0.9886 - val_loss: 1.6057 - val_acc: 0.7273\n",
      "Epoch 2385/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0445 - acc: 0.9858 - val_loss: 1.5566 - val_acc: 0.7216\n",
      "Epoch 2386/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0366 - acc: 0.9929 - val_loss: 1.5696 - val_acc: 0.7330\n",
      "Epoch 2387/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0525 - acc: 0.9858 - val_loss: 1.5534 - val_acc: 0.7273\n",
      "Epoch 2388/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0352 - acc: 0.9915 - val_loss: 1.5619 - val_acc: 0.7273\n",
      "Epoch 2389/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0280 - acc: 0.9957 - val_loss: 1.5791 - val_acc: 0.7216\n",
      "Epoch 2390/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0532 - acc: 0.9815 - val_loss: 1.6048 - val_acc: 0.7159\n",
      "Epoch 2391/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0471 - acc: 0.9915 - val_loss: 1.6778 - val_acc: 0.7102\n",
      "Epoch 2392/3000\n",
      "702/702 [==============================] - 0s 578us/sample - loss: 0.0259 - acc: 0.9929 - val_loss: 1.7030 - val_acc: 0.7102\n",
      "Epoch 2393/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0318 - acc: 0.9900 - val_loss: 1.7238 - val_acc: 0.7159\n",
      "Epoch 2394/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0583 - acc: 0.9843 - val_loss: 1.5854 - val_acc: 0.7330\n",
      "Epoch 2395/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0319 - acc: 0.9886 - val_loss: 1.5926 - val_acc: 0.7330\n",
      "Epoch 2396/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0583 - acc: 0.9872 - val_loss: 1.6238 - val_acc: 0.7216\n",
      "Epoch 2397/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0296 - acc: 0.9929 - val_loss: 1.7524 - val_acc: 0.7102\n",
      "Epoch 2398/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 1.4628 - acc: 0.9516 - val_loss: 1.5415 - val_acc: 0.6705\n",
      "Epoch 2399/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.2964 - acc: 0.9288 - val_loss: 1.5536 - val_acc: 0.5966\n",
      "Epoch 2400/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.7209 - acc: 0.8048 - val_loss: 1.8114 - val_acc: 0.5795\n",
      "Epoch 2401/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.8500 - acc: 0.7849 - val_loss: 1.6675 - val_acc: 0.6307\n",
      "Epoch 2402/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.8662 - acc: 0.8063 - val_loss: 1.6361 - val_acc: 0.6477\n",
      "Epoch 2403/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.5317 - acc: 0.8661 - val_loss: 1.6594 - val_acc: 0.6364\n",
      "Epoch 2404/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.4545 - acc: 0.8746 - val_loss: 1.7868 - val_acc: 0.6080\n",
      "Epoch 2405/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.4138 - acc: 0.8775 - val_loss: 1.5605 - val_acc: 0.6534\n",
      "Epoch 2406/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.3774 - acc: 0.8803 - val_loss: 1.5884 - val_acc: 0.6477\n",
      "Epoch 2407/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.3299 - acc: 0.9074 - val_loss: 1.4745 - val_acc: 0.6648\n",
      "Epoch 2408/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.3032 - acc: 0.9202 - val_loss: 1.4367 - val_acc: 0.6818\n",
      "Epoch 2409/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.2759 - acc: 0.9145 - val_loss: 1.4068 - val_acc: 0.6761\n",
      "Epoch 2410/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.3101 - acc: 0.9231 - val_loss: 1.3502 - val_acc: 0.7102\n",
      "Epoch 2411/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.3187 - acc: 0.9459 - val_loss: 1.5584 - val_acc: 0.6875\n",
      "Epoch 2412/3000\n",
      "702/702 [==============================] - 0s 594us/sample - loss: 0.2291 - acc: 0.9459 - val_loss: 1.4329 - val_acc: 0.7159\n",
      "Epoch 2413/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1990 - acc: 0.9473 - val_loss: 1.4574 - val_acc: 0.7273\n",
      "Epoch 2414/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.2350 - acc: 0.9430 - val_loss: 1.5598 - val_acc: 0.6705\n",
      "Epoch 2415/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1477 - acc: 0.9630 - val_loss: 1.5030 - val_acc: 0.6818\n",
      "Epoch 2416/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.2881 - acc: 0.9330 - val_loss: 1.4638 - val_acc: 0.6989\n",
      "Epoch 2417/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1499 - acc: 0.9558 - val_loss: 1.4814 - val_acc: 0.6818\n",
      "Epoch 2418/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.1849 - acc: 0.9416 - val_loss: 1.3958 - val_acc: 0.7102\n",
      "Epoch 2419/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1253 - acc: 0.9672 - val_loss: 1.2909 - val_acc: 0.7045\n",
      "Epoch 2420/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1306 - acc: 0.9558 - val_loss: 1.3368 - val_acc: 0.7045\n",
      "Epoch 2421/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1084 - acc: 0.9658 - val_loss: 1.4046 - val_acc: 0.7045\n",
      "Epoch 2422/3000\n",
      "702/702 [==============================] - 0s 585us/sample - loss: 0.0847 - acc: 0.9772 - val_loss: 1.2865 - val_acc: 0.7216\n",
      "Epoch 2423/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.1231 - acc: 0.9644 - val_loss: 1.1634 - val_acc: 0.7273\n",
      "Epoch 2424/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0857 - acc: 0.9687 - val_loss: 1.2385 - val_acc: 0.7443\n",
      "Epoch 2425/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1163 - acc: 0.9644 - val_loss: 1.4179 - val_acc: 0.7102\n",
      "Epoch 2426/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.2251 - acc: 0.9544 - val_loss: 1.6328 - val_acc: 0.6761\n",
      "Epoch 2427/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1378 - acc: 0.9601 - val_loss: 1.4462 - val_acc: 0.6932\n",
      "Epoch 2428/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.1255 - acc: 0.9530 - val_loss: 1.5089 - val_acc: 0.6875\n",
      "Epoch 2429/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.1960 - acc: 0.9444 - val_loss: 1.3734 - val_acc: 0.7216\n",
      "Epoch 2430/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0921 - acc: 0.9644 - val_loss: 1.3360 - val_acc: 0.7159\n",
      "Epoch 2431/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1395 - acc: 0.9558 - val_loss: 1.4282 - val_acc: 0.6932\n",
      "Epoch 2432/3000\n",
      "702/702 [==============================] - 0s 560us/sample - loss: 0.1058 - acc: 0.9744 - val_loss: 1.5130 - val_acc: 0.6761\n",
      "Epoch 2433/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.1112 - acc: 0.9715 - val_loss: 1.5091 - val_acc: 0.6932\n",
      "Epoch 2434/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0987 - acc: 0.9715 - val_loss: 1.4835 - val_acc: 0.6989\n",
      "Epoch 2435/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0904 - acc: 0.9729 - val_loss: 1.4113 - val_acc: 0.7159\n",
      "Epoch 2436/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0967 - acc: 0.9672 - val_loss: 1.4279 - val_acc: 0.7330\n",
      "Epoch 2437/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0822 - acc: 0.9744 - val_loss: 1.3663 - val_acc: 0.7216\n",
      "Epoch 2438/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0840 - acc: 0.9715 - val_loss: 1.3943 - val_acc: 0.6932\n",
      "Epoch 2439/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.0700 - acc: 0.9772 - val_loss: 1.4541 - val_acc: 0.6932\n",
      "Epoch 2440/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0542 - acc: 0.9786 - val_loss: 1.5102 - val_acc: 0.6989\n",
      "Epoch 2441/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0962 - acc: 0.9701 - val_loss: 1.4013 - val_acc: 0.7159\n",
      "Epoch 2442/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.0794 - acc: 0.9801 - val_loss: 1.3108 - val_acc: 0.7330\n",
      "Epoch 2443/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0695 - acc: 0.9829 - val_loss: 1.3231 - val_acc: 0.7273\n",
      "Epoch 2444/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0897 - acc: 0.9744 - val_loss: 1.3557 - val_acc: 0.7273\n",
      "Epoch 2445/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0664 - acc: 0.9772 - val_loss: 1.4820 - val_acc: 0.6989\n",
      "Epoch 2446/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0703 - acc: 0.9843 - val_loss: 1.3933 - val_acc: 0.6875\n",
      "Epoch 2447/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0549 - acc: 0.9829 - val_loss: 1.4244 - val_acc: 0.6875\n",
      "Epoch 2448/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0541 - acc: 0.9786 - val_loss: 1.4628 - val_acc: 0.6761\n",
      "Epoch 2449/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0744 - acc: 0.9772 - val_loss: 1.4343 - val_acc: 0.6932\n",
      "Epoch 2450/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0764 - acc: 0.9801 - val_loss: 1.4230 - val_acc: 0.6761\n",
      "Epoch 2451/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0717 - acc: 0.9772 - val_loss: 1.4462 - val_acc: 0.6761\n",
      "Epoch 2452/3000\n",
      "702/702 [==============================] - 0s 564us/sample - loss: 0.0457 - acc: 0.9829 - val_loss: 1.4904 - val_acc: 0.6932\n",
      "Epoch 2453/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0569 - acc: 0.9815 - val_loss: 1.4974 - val_acc: 0.7159\n",
      "Epoch 2454/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0807 - acc: 0.9843 - val_loss: 1.4432 - val_acc: 0.7159\n",
      "Epoch 2455/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0551 - acc: 0.9872 - val_loss: 1.4034 - val_acc: 0.7273\n",
      "Epoch 2456/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0432 - acc: 0.9829 - val_loss: 1.4360 - val_acc: 0.7273\n",
      "Epoch 2457/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0446 - acc: 0.9829 - val_loss: 1.5029 - val_acc: 0.7045\n",
      "Epoch 2458/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0512 - acc: 0.9886 - val_loss: 1.5026 - val_acc: 0.7159\n",
      "Epoch 2459/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0313 - acc: 0.9915 - val_loss: 1.4769 - val_acc: 0.7216\n",
      "Epoch 2460/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0638 - acc: 0.9786 - val_loss: 1.4680 - val_acc: 0.7102\n",
      "Epoch 2461/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0244 - acc: 0.9929 - val_loss: 1.4805 - val_acc: 0.7102\n",
      "Epoch 2462/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.0545 - acc: 0.9843 - val_loss: 1.5242 - val_acc: 0.7102\n",
      "Epoch 2463/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0421 - acc: 0.9886 - val_loss: 1.5456 - val_acc: 0.6989\n",
      "Epoch 2464/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0183 - acc: 0.9972 - val_loss: 1.5730 - val_acc: 0.7045\n",
      "Epoch 2465/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0443 - acc: 0.9815 - val_loss: 1.4661 - val_acc: 0.6989\n",
      "Epoch 2466/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0242 - acc: 0.9957 - val_loss: 1.4336 - val_acc: 0.7045\n",
      "Epoch 2467/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0420 - acc: 0.9886 - val_loss: 1.3912 - val_acc: 0.7159\n",
      "Epoch 2468/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0342 - acc: 0.9886 - val_loss: 1.3784 - val_acc: 0.7273\n",
      "Epoch 2469/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0264 - acc: 0.9886 - val_loss: 1.3952 - val_acc: 0.7273\n",
      "Epoch 2470/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0336 - acc: 0.9929 - val_loss: 1.3944 - val_acc: 0.7443\n",
      "Epoch 2471/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1099 - acc: 0.9886 - val_loss: 1.4143 - val_acc: 0.7500\n",
      "Epoch 2472/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 0.0375 - acc: 0.9872 - val_loss: 1.4614 - val_acc: 0.7386\n",
      "Epoch 2473/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0236 - acc: 0.9972 - val_loss: 1.4623 - val_acc: 0.7443\n",
      "Epoch 2474/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0399 - acc: 0.9900 - val_loss: 1.4865 - val_acc: 0.7443\n",
      "Epoch 2475/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0546 - acc: 0.9829 - val_loss: 1.4530 - val_acc: 0.7500\n",
      "Epoch 2476/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0504 - acc: 0.9843 - val_loss: 1.4558 - val_acc: 0.7500\n",
      "Epoch 2477/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0475 - acc: 0.9829 - val_loss: 1.4892 - val_acc: 0.7443\n",
      "Epoch 2478/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0216 - acc: 0.9929 - val_loss: 1.5576 - val_acc: 0.7273\n",
      "Epoch 2479/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0683 - acc: 0.9915 - val_loss: 1.4492 - val_acc: 0.7330\n",
      "Epoch 2480/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0296 - acc: 0.9943 - val_loss: 1.3471 - val_acc: 0.7386\n",
      "Epoch 2481/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0589 - acc: 0.9843 - val_loss: 1.3903 - val_acc: 0.7330\n",
      "Epoch 2482/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 0.0777 - acc: 0.9786 - val_loss: 1.5480 - val_acc: 0.7045\n",
      "Epoch 2483/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0500 - acc: 0.9829 - val_loss: 1.6468 - val_acc: 0.6875\n",
      "Epoch 2484/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0818 - acc: 0.9786 - val_loss: 1.4696 - val_acc: 0.6989\n",
      "Epoch 2485/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0356 - acc: 0.9886 - val_loss: 1.4238 - val_acc: 0.7216\n",
      "Epoch 2486/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0448 - acc: 0.9872 - val_loss: 1.5161 - val_acc: 0.7216\n",
      "Epoch 2487/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0489 - acc: 0.9929 - val_loss: 1.6083 - val_acc: 0.7102\n",
      "Epoch 2488/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0770 - acc: 0.9801 - val_loss: 1.5275 - val_acc: 0.7330\n",
      "Epoch 2489/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0393 - acc: 0.9843 - val_loss: 1.5078 - val_acc: 0.7386\n",
      "Epoch 2490/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0208 - acc: 0.9929 - val_loss: 1.5753 - val_acc: 0.7330\n",
      "Epoch 2491/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0543 - acc: 0.9843 - val_loss: 1.6040 - val_acc: 0.7102\n",
      "Epoch 2492/3000\n",
      "702/702 [==============================] - 0s 570us/sample - loss: 0.0899 - acc: 0.9858 - val_loss: 1.5132 - val_acc: 0.7159\n",
      "Epoch 2493/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0228 - acc: 0.9957 - val_loss: 1.4688 - val_acc: 0.7330\n",
      "Epoch 2494/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0279 - acc: 0.9900 - val_loss: 1.5156 - val_acc: 0.7216\n",
      "Epoch 2495/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0562 - acc: 0.9843 - val_loss: 1.5880 - val_acc: 0.7386\n",
      "Epoch 2496/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0468 - acc: 0.9886 - val_loss: 1.6163 - val_acc: 0.7330\n",
      "Epoch 2497/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0277 - acc: 0.9915 - val_loss: 1.6154 - val_acc: 0.7159\n",
      "Epoch 2498/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0685 - acc: 0.9843 - val_loss: 1.5027 - val_acc: 0.7273\n",
      "Epoch 2499/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0313 - acc: 0.9943 - val_loss: 1.3948 - val_acc: 0.7330\n",
      "Epoch 2500/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0405 - acc: 0.9886 - val_loss: 1.3959 - val_acc: 0.7386\n",
      "Epoch 2501/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0484 - acc: 0.9900 - val_loss: 1.4975 - val_acc: 0.7330\n",
      "Epoch 2502/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 0.0329 - acc: 0.9915 - val_loss: 1.5795 - val_acc: 0.7216\n",
      "Epoch 2503/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0347 - acc: 0.9872 - val_loss: 1.6077 - val_acc: 0.7216\n",
      "Epoch 2504/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0248 - acc: 0.9929 - val_loss: 1.6490 - val_acc: 0.7273\n",
      "Epoch 2505/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0499 - acc: 0.9872 - val_loss: 1.7238 - val_acc: 0.7102\n",
      "Epoch 2506/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0419 - acc: 0.9858 - val_loss: 1.7756 - val_acc: 0.7045\n",
      "Epoch 2507/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.0408 - acc: 0.9900 - val_loss: 1.7814 - val_acc: 0.7045\n",
      "Epoch 2508/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0604 - acc: 0.9744 - val_loss: 1.6428 - val_acc: 0.6875\n",
      "Epoch 2509/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0676 - acc: 0.9886 - val_loss: 1.4701 - val_acc: 0.6989\n",
      "Epoch 2510/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0538 - acc: 0.9815 - val_loss: 1.4278 - val_acc: 0.7102\n",
      "Epoch 2511/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0556 - acc: 0.9815 - val_loss: 1.4382 - val_acc: 0.6932\n",
      "Epoch 2512/3000\n",
      "702/702 [==============================] - 0s 567us/sample - loss: 0.0398 - acc: 0.9886 - val_loss: 1.5919 - val_acc: 0.7102\n",
      "Epoch 2513/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.0418 - acc: 0.9872 - val_loss: 1.6679 - val_acc: 0.6818\n",
      "Epoch 2514/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0410 - acc: 0.9872 - val_loss: 1.6946 - val_acc: 0.6761\n",
      "Epoch 2515/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0295 - acc: 0.9886 - val_loss: 1.6209 - val_acc: 0.6875\n",
      "Epoch 2516/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0510 - acc: 0.9858 - val_loss: 1.4971 - val_acc: 0.7216\n",
      "Epoch 2517/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0296 - acc: 0.9915 - val_loss: 1.4686 - val_acc: 0.7159\n",
      "Epoch 2518/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0329 - acc: 0.9929 - val_loss: 1.4987 - val_acc: 0.7102\n",
      "Epoch 2519/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0368 - acc: 0.9915 - val_loss: 1.5729 - val_acc: 0.6761\n",
      "Epoch 2520/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0387 - acc: 0.9900 - val_loss: 1.5602 - val_acc: 0.6875\n",
      "Epoch 2521/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0431 - acc: 0.9858 - val_loss: 1.5852 - val_acc: 0.6818\n",
      "Epoch 2522/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.0373 - acc: 0.9886 - val_loss: 1.6552 - val_acc: 0.6761\n",
      "Epoch 2523/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0201 - acc: 0.9957 - val_loss: 1.7091 - val_acc: 0.6761\n",
      "Epoch 2524/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0278 - acc: 0.9900 - val_loss: 1.8041 - val_acc: 0.6705\n",
      "Epoch 2525/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.0335 - acc: 0.9872 - val_loss: 1.8037 - val_acc: 0.6932\n",
      "Epoch 2526/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0363 - acc: 0.9929 - val_loss: 1.7339 - val_acc: 0.6875\n",
      "Epoch 2527/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0284 - acc: 0.9900 - val_loss: 1.6861 - val_acc: 0.6875\n",
      "Epoch 2528/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0524 - acc: 0.9858 - val_loss: 1.5750 - val_acc: 0.6989\n",
      "Epoch 2529/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0378 - acc: 0.9843 - val_loss: 1.5632 - val_acc: 0.7045\n",
      "Epoch 2530/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0138 - acc: 0.9986 - val_loss: 1.6354 - val_acc: 0.7102\n",
      "Epoch 2531/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0514 - acc: 0.9858 - val_loss: 1.7088 - val_acc: 0.7102\n",
      "Epoch 2532/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 0.0361 - acc: 0.9900 - val_loss: 1.6905 - val_acc: 0.6989\n",
      "Epoch 2533/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0471 - acc: 0.9858 - val_loss: 1.6011 - val_acc: 0.6932\n",
      "Epoch 2534/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0237 - acc: 0.9929 - val_loss: 1.6139 - val_acc: 0.7102\n",
      "Epoch 2535/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0207 - acc: 0.9929 - val_loss: 1.6794 - val_acc: 0.7159\n",
      "Epoch 2536/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0413 - acc: 0.9886 - val_loss: 1.7421 - val_acc: 0.6818\n",
      "Epoch 2537/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0496 - acc: 0.9872 - val_loss: 1.6988 - val_acc: 0.6875\n",
      "Epoch 2538/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0214 - acc: 0.9929 - val_loss: 1.6056 - val_acc: 0.7102\n",
      "Epoch 2539/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0444 - acc: 0.9872 - val_loss: 1.5785 - val_acc: 0.7216\n",
      "Epoch 2540/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0319 - acc: 0.9872 - val_loss: 1.5854 - val_acc: 0.7102\n",
      "Epoch 2541/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0149 - acc: 0.9972 - val_loss: 1.6665 - val_acc: 0.6932\n",
      "Epoch 2542/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 0.0322 - acc: 0.9915 - val_loss: 1.6346 - val_acc: 0.6818\n",
      "Epoch 2543/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0417 - acc: 0.9900 - val_loss: 1.5880 - val_acc: 0.7102\n",
      "Epoch 2544/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0221 - acc: 0.9929 - val_loss: 1.5443 - val_acc: 0.7159\n",
      "Epoch 2545/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0262 - acc: 0.9943 - val_loss: 1.5516 - val_acc: 0.7102\n",
      "Epoch 2546/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0532 - acc: 0.9886 - val_loss: 1.5739 - val_acc: 0.7216\n",
      "Epoch 2547/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0178 - acc: 0.9943 - val_loss: 1.6192 - val_acc: 0.7216\n",
      "Epoch 2548/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0291 - acc: 0.9900 - val_loss: 1.7101 - val_acc: 0.7216\n",
      "Epoch 2549/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0207 - acc: 0.9915 - val_loss: 1.6771 - val_acc: 0.7045\n",
      "Epoch 2550/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0411 - acc: 0.9886 - val_loss: 1.6222 - val_acc: 0.7045\n",
      "Epoch 2551/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0286 - acc: 0.9915 - val_loss: 1.6281 - val_acc: 0.7045\n",
      "Epoch 2552/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.0273 - acc: 0.9929 - val_loss: 1.6459 - val_acc: 0.6875\n",
      "Epoch 2553/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0261 - acc: 0.9915 - val_loss: 1.6310 - val_acc: 0.6989\n",
      "Epoch 2554/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0168 - acc: 0.9957 - val_loss: 1.6066 - val_acc: 0.6989\n",
      "Epoch 2555/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0238 - acc: 0.9929 - val_loss: 1.6461 - val_acc: 0.6875\n",
      "Epoch 2556/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0389 - acc: 0.9872 - val_loss: 1.6507 - val_acc: 0.7045\n",
      "Epoch 2557/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0418 - acc: 0.9886 - val_loss: 1.7342 - val_acc: 0.6761\n",
      "Epoch 2558/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0355 - acc: 0.9886 - val_loss: 1.7352 - val_acc: 0.6705\n",
      "Epoch 2559/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0524 - acc: 0.9915 - val_loss: 1.7361 - val_acc: 0.6875\n",
      "Epoch 2560/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0416 - acc: 0.9858 - val_loss: 1.7793 - val_acc: 0.6932\n",
      "Epoch 2561/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0176 - acc: 0.9943 - val_loss: 1.8865 - val_acc: 0.6875\n",
      "Epoch 2562/3000\n",
      "702/702 [==============================] - 0s 569us/sample - loss: 0.0303 - acc: 0.9915 - val_loss: 1.8694 - val_acc: 0.6932\n",
      "Epoch 2563/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0401 - acc: 0.9900 - val_loss: 1.7851 - val_acc: 0.6989\n",
      "Epoch 2564/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0308 - acc: 0.9900 - val_loss: 1.7917 - val_acc: 0.7102\n",
      "Epoch 2565/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0256 - acc: 0.9943 - val_loss: 1.9004 - val_acc: 0.6875\n",
      "Epoch 2566/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0548 - acc: 0.9900 - val_loss: 1.8032 - val_acc: 0.6875\n",
      "Epoch 2567/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0272 - acc: 0.9929 - val_loss: 1.6949 - val_acc: 0.6989\n",
      "Epoch 2568/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0948 - acc: 0.9872 - val_loss: 1.5351 - val_acc: 0.7159\n",
      "Epoch 2569/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0288 - acc: 0.9915 - val_loss: 1.4578 - val_acc: 0.7159\n",
      "Epoch 2570/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.0360 - acc: 0.9886 - val_loss: 1.4838 - val_acc: 0.7386\n",
      "Epoch 2571/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0296 - acc: 0.9915 - val_loss: 1.5491 - val_acc: 0.7159\n",
      "Epoch 2572/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 0.0282 - acc: 0.9915 - val_loss: 1.5798 - val_acc: 0.6989\n",
      "Epoch 2573/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0158 - acc: 0.9957 - val_loss: 1.5730 - val_acc: 0.7216\n",
      "Epoch 2574/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0750 - acc: 0.9843 - val_loss: 1.6206 - val_acc: 0.7330\n",
      "Epoch 2575/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0352 - acc: 0.9957 - val_loss: 1.6164 - val_acc: 0.7159\n",
      "Epoch 2576/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0342 - acc: 0.9943 - val_loss: 1.5759 - val_acc: 0.7102\n",
      "Epoch 2577/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0291 - acc: 0.9929 - val_loss: 1.5599 - val_acc: 0.7102\n",
      "Epoch 2578/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0310 - acc: 0.9886 - val_loss: 1.5688 - val_acc: 0.7159\n",
      "Epoch 2579/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0298 - acc: 0.9886 - val_loss: 1.5461 - val_acc: 0.7045\n",
      "Epoch 2580/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0344 - acc: 0.9886 - val_loss: 1.5862 - val_acc: 0.7216\n",
      "Epoch 2581/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0310 - acc: 0.9915 - val_loss: 1.6607 - val_acc: 0.7216\n",
      "Epoch 2582/3000\n",
      "702/702 [==============================] - 0s 560us/sample - loss: 0.0308 - acc: 0.9929 - val_loss: 1.6268 - val_acc: 0.7273\n",
      "Epoch 2583/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0331 - acc: 0.9900 - val_loss: 1.5806 - val_acc: 0.7330\n",
      "Epoch 2584/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0290 - acc: 0.9900 - val_loss: 1.5991 - val_acc: 0.7159\n",
      "Epoch 2585/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0261 - acc: 0.9943 - val_loss: 1.5724 - val_acc: 0.7159\n",
      "Epoch 2586/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0180 - acc: 0.9957 - val_loss: 1.4818 - val_acc: 0.7273\n",
      "Epoch 2587/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.0339 - acc: 0.9915 - val_loss: 1.4840 - val_acc: 0.7330\n",
      "Epoch 2588/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.0286 - acc: 0.9943 - val_loss: 1.5471 - val_acc: 0.7443\n",
      "Epoch 2589/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0193 - acc: 0.9929 - val_loss: 1.5762 - val_acc: 0.7273\n",
      "Epoch 2590/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0274 - acc: 0.9957 - val_loss: 1.6009 - val_acc: 0.7159\n",
      "Epoch 2591/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0402 - acc: 0.9886 - val_loss: 1.6655 - val_acc: 0.7102\n",
      "Epoch 2592/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 0.0253 - acc: 0.9929 - val_loss: 1.6548 - val_acc: 0.7045\n",
      "Epoch 2593/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0289 - acc: 0.9915 - val_loss: 1.6162 - val_acc: 0.7159\n",
      "Epoch 2594/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0391 - acc: 0.9886 - val_loss: 1.6609 - val_acc: 0.7216\n",
      "Epoch 2595/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0164 - acc: 0.9943 - val_loss: 1.6830 - val_acc: 0.6875\n",
      "Epoch 2596/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0212 - acc: 0.9915 - val_loss: 1.6366 - val_acc: 0.7216\n",
      "Epoch 2597/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0294 - acc: 0.9900 - val_loss: 1.5562 - val_acc: 0.7273\n",
      "Epoch 2598/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0185 - acc: 0.9957 - val_loss: 1.5700 - val_acc: 0.7330\n",
      "Epoch 2599/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0306 - acc: 0.9915 - val_loss: 1.6751 - val_acc: 0.6989\n",
      "Epoch 2600/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0287 - acc: 0.9915 - val_loss: 1.6244 - val_acc: 0.7216\n",
      "Epoch 2601/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0200 - acc: 0.9929 - val_loss: 1.5443 - val_acc: 0.7443\n",
      "Epoch 2602/3000\n",
      "702/702 [==============================] - 0s 579us/sample - loss: 0.0236 - acc: 0.9957 - val_loss: 1.5721 - val_acc: 0.7386\n",
      "Epoch 2603/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0228 - acc: 0.9900 - val_loss: 1.6216 - val_acc: 0.7330\n",
      "Epoch 2604/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0380 - acc: 0.9886 - val_loss: 1.6734 - val_acc: 0.7273\n",
      "Epoch 2605/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0236 - acc: 0.9943 - val_loss: 1.6964 - val_acc: 0.7330\n",
      "Epoch 2606/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.0196 - acc: 0.9943 - val_loss: 1.6665 - val_acc: 0.7273\n",
      "Epoch 2607/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0266 - acc: 0.9915 - val_loss: 1.7074 - val_acc: 0.6818\n",
      "Epoch 2608/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0334 - acc: 0.9900 - val_loss: 1.7449 - val_acc: 0.6705\n",
      "Epoch 2609/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.0346 - acc: 0.9929 - val_loss: 1.6825 - val_acc: 0.6761\n",
      "Epoch 2610/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0318 - acc: 0.9915 - val_loss: 1.6113 - val_acc: 0.6932\n",
      "Epoch 2611/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0104 - acc: 0.9957 - val_loss: 1.5969 - val_acc: 0.7045\n",
      "Epoch 2612/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.0289 - acc: 0.9929 - val_loss: 1.6012 - val_acc: 0.7273\n",
      "Epoch 2613/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0130 - acc: 0.9943 - val_loss: 1.6113 - val_acc: 0.7159\n",
      "Epoch 2614/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0347 - acc: 0.9886 - val_loss: 1.6941 - val_acc: 0.7273\n",
      "Epoch 2615/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0411 - acc: 0.9957 - val_loss: 1.8463 - val_acc: 0.6818\n",
      "Epoch 2616/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0419 - acc: 0.9900 - val_loss: 1.7895 - val_acc: 0.6875\n",
      "Epoch 2617/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0384 - acc: 0.9872 - val_loss: 1.7050 - val_acc: 0.6875\n",
      "Epoch 2618/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0367 - acc: 0.9929 - val_loss: 1.7425 - val_acc: 0.6875\n",
      "Epoch 2619/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0307 - acc: 0.9929 - val_loss: 1.8244 - val_acc: 0.6761\n",
      "Epoch 2620/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0407 - acc: 0.9843 - val_loss: 1.8446 - val_acc: 0.6932\n",
      "Epoch 2621/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0269 - acc: 0.9929 - val_loss: 1.8151 - val_acc: 0.6932\n",
      "Epoch 2622/3000\n",
      "702/702 [==============================] - 0s 570us/sample - loss: 0.0260 - acc: 0.9900 - val_loss: 1.7742 - val_acc: 0.7159\n",
      "Epoch 2623/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.0270 - acc: 0.9886 - val_loss: 1.8288 - val_acc: 0.6989\n",
      "Epoch 2624/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0268 - acc: 0.9900 - val_loss: 1.8760 - val_acc: 0.6875\n",
      "Epoch 2625/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0267 - acc: 0.9929 - val_loss: 1.7767 - val_acc: 0.6875\n",
      "Epoch 2626/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0240 - acc: 0.9957 - val_loss: 1.7462 - val_acc: 0.7102\n",
      "Epoch 2627/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0202 - acc: 0.9929 - val_loss: 1.7428 - val_acc: 0.7045\n",
      "Epoch 2628/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0269 - acc: 0.9929 - val_loss: 1.7676 - val_acc: 0.7045\n",
      "Epoch 2629/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0229 - acc: 0.9929 - val_loss: 1.8906 - val_acc: 0.6818\n",
      "Epoch 2630/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0490 - acc: 0.9872 - val_loss: 1.9636 - val_acc: 0.6818\n",
      "Epoch 2631/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0630 - acc: 0.9915 - val_loss: 1.6512 - val_acc: 0.6989\n",
      "Epoch 2632/3000\n",
      "702/702 [==============================] - 0s 561us/sample - loss: 0.0482 - acc: 0.9843 - val_loss: 1.5525 - val_acc: 0.7159\n",
      "Epoch 2633/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0563 - acc: 0.9858 - val_loss: 1.5287 - val_acc: 0.6989\n",
      "Epoch 2634/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0470 - acc: 0.9886 - val_loss: 1.6099 - val_acc: 0.6818\n",
      "Epoch 2635/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0305 - acc: 0.9900 - val_loss: 1.6422 - val_acc: 0.7045\n",
      "Epoch 2636/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0453 - acc: 0.9886 - val_loss: 1.6484 - val_acc: 0.6989\n",
      "Epoch 2637/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0449 - acc: 0.9872 - val_loss: 1.6229 - val_acc: 0.6875\n",
      "Epoch 2638/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0327 - acc: 0.9915 - val_loss: 1.6499 - val_acc: 0.7102\n",
      "Epoch 2639/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0567 - acc: 0.9858 - val_loss: 1.6750 - val_acc: 0.7045\n",
      "Epoch 2640/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0497 - acc: 0.9886 - val_loss: 1.6831 - val_acc: 0.7159\n",
      "Epoch 2641/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.1503 - acc: 0.9858 - val_loss: 1.5799 - val_acc: 0.7102\n",
      "Epoch 2642/3000\n",
      "702/702 [==============================] - 0s 569us/sample - loss: 0.0527 - acc: 0.9843 - val_loss: 1.5634 - val_acc: 0.6989\n",
      "Epoch 2643/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0847 - acc: 0.9744 - val_loss: 1.5537 - val_acc: 0.7159\n",
      "Epoch 2644/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0349 - acc: 0.9900 - val_loss: 1.8043 - val_acc: 0.7045\n",
      "Epoch 2645/3000\n",
      "702/702 [==============================] - 0s 506us/sample - loss: 0.0857 - acc: 0.9801 - val_loss: 1.7539 - val_acc: 0.6875\n",
      "Epoch 2646/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0314 - acc: 0.9929 - val_loss: 1.6744 - val_acc: 0.7045\n",
      "Epoch 2647/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0408 - acc: 0.9815 - val_loss: 1.6856 - val_acc: 0.7102\n",
      "Epoch 2648/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0364 - acc: 0.9843 - val_loss: 1.8027 - val_acc: 0.6989\n",
      "Epoch 2649/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0351 - acc: 0.9843 - val_loss: 1.8029 - val_acc: 0.7045\n",
      "Epoch 2650/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0408 - acc: 0.9900 - val_loss: 1.7185 - val_acc: 0.7102\n",
      "Epoch 2651/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0267 - acc: 0.9929 - val_loss: 1.6913 - val_acc: 0.7102\n",
      "Epoch 2652/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 0.0319 - acc: 0.9900 - val_loss: 1.6976 - val_acc: 0.7159\n",
      "Epoch 2653/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0308 - acc: 0.9900 - val_loss: 1.6811 - val_acc: 0.7102\n",
      "Epoch 2654/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0257 - acc: 0.9915 - val_loss: 1.7199 - val_acc: 0.6989\n",
      "Epoch 2655/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0183 - acc: 0.9957 - val_loss: 1.7924 - val_acc: 0.6989\n",
      "Epoch 2656/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0318 - acc: 0.9872 - val_loss: 1.7592 - val_acc: 0.7102\n",
      "Epoch 2657/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0398 - acc: 0.9858 - val_loss: 1.6228 - val_acc: 0.7159\n",
      "Epoch 2658/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0230 - acc: 0.9929 - val_loss: 1.5464 - val_acc: 0.7045\n",
      "Epoch 2659/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.0178 - acc: 0.9957 - val_loss: 1.5740 - val_acc: 0.7045\n",
      "Epoch 2660/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0424 - acc: 0.9886 - val_loss: 1.6106 - val_acc: 0.6932\n",
      "Epoch 2661/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0304 - acc: 0.9900 - val_loss: 1.6495 - val_acc: 0.6932\n",
      "Epoch 2662/3000\n",
      "702/702 [==============================] - 0s 577us/sample - loss: 0.0561 - acc: 0.9858 - val_loss: 1.8226 - val_acc: 0.6875\n",
      "Epoch 2663/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.0256 - acc: 0.9900 - val_loss: 1.8149 - val_acc: 0.6761\n",
      "Epoch 2664/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0290 - acc: 0.9915 - val_loss: 1.6553 - val_acc: 0.6875\n",
      "Epoch 2665/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0272 - acc: 0.9929 - val_loss: 1.6978 - val_acc: 0.7045\n",
      "Epoch 2666/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0267 - acc: 0.9929 - val_loss: 1.7750 - val_acc: 0.6989\n",
      "Epoch 2667/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0490 - acc: 0.9886 - val_loss: 1.6902 - val_acc: 0.6875\n",
      "Epoch 2668/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0311 - acc: 0.9900 - val_loss: 1.7019 - val_acc: 0.6761\n",
      "Epoch 2669/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0750 - acc: 0.9900 - val_loss: 1.7028 - val_acc: 0.6932\n",
      "Epoch 2670/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0287 - acc: 0.9929 - val_loss: 1.8035 - val_acc: 0.7045\n",
      "Epoch 2671/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0315 - acc: 0.9900 - val_loss: 1.9131 - val_acc: 0.6989\n",
      "Epoch 2672/3000\n",
      "702/702 [==============================] - 0s 576us/sample - loss: 0.0577 - acc: 0.9858 - val_loss: 1.7655 - val_acc: 0.7386\n",
      "Epoch 2673/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0729 - acc: 0.9886 - val_loss: 1.6178 - val_acc: 0.7045\n",
      "Epoch 2674/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.0317 - acc: 0.9886 - val_loss: 1.6188 - val_acc: 0.7045\n",
      "Epoch 2675/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0207 - acc: 0.9957 - val_loss: 1.6618 - val_acc: 0.6932\n",
      "Epoch 2676/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0248 - acc: 0.9929 - val_loss: 1.7156 - val_acc: 0.6989\n",
      "Epoch 2677/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0335 - acc: 0.9929 - val_loss: 1.7327 - val_acc: 0.6932\n",
      "Epoch 2678/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0165 - acc: 0.9986 - val_loss: 1.7613 - val_acc: 0.6875\n",
      "Epoch 2679/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0472 - acc: 0.9872 - val_loss: 1.8054 - val_acc: 0.6875\n",
      "Epoch 2680/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0271 - acc: 0.9929 - val_loss: 1.8711 - val_acc: 0.6932\n",
      "Epoch 2681/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0349 - acc: 0.9886 - val_loss: 1.8428 - val_acc: 0.6875\n",
      "Epoch 2682/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 0.0475 - acc: 0.9886 - val_loss: 1.7798 - val_acc: 0.6932\n",
      "Epoch 2683/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0291 - acc: 0.9929 - val_loss: 1.6977 - val_acc: 0.6932\n",
      "Epoch 2684/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0298 - acc: 0.9915 - val_loss: 1.6611 - val_acc: 0.7159\n",
      "Epoch 2685/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0486 - acc: 0.9957 - val_loss: 1.6812 - val_acc: 0.7159\n",
      "Epoch 2686/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0183 - acc: 0.9943 - val_loss: 1.7782 - val_acc: 0.6989\n",
      "Epoch 2687/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0758 - acc: 0.9829 - val_loss: 1.9346 - val_acc: 0.6818\n",
      "Epoch 2688/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0274 - acc: 0.9915 - val_loss: 1.9497 - val_acc: 0.6761\n",
      "Epoch 2689/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.0339 - acc: 0.9858 - val_loss: 1.7522 - val_acc: 0.6875\n",
      "Epoch 2690/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0301 - acc: 0.9915 - val_loss: 1.6886 - val_acc: 0.6818\n",
      "Epoch 2691/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0443 - acc: 0.9900 - val_loss: 1.7287 - val_acc: 0.6761\n",
      "Epoch 2692/3000\n",
      "702/702 [==============================] - 0s 588us/sample - loss: 0.0458 - acc: 0.9900 - val_loss: 1.8064 - val_acc: 0.6818\n",
      "Epoch 2693/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0457 - acc: 0.9886 - val_loss: 1.7841 - val_acc: 0.6818\n",
      "Epoch 2694/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0663 - acc: 0.9915 - val_loss: 1.8074 - val_acc: 0.6932\n",
      "Epoch 2695/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0281 - acc: 0.9929 - val_loss: 1.9144 - val_acc: 0.6591\n",
      "Epoch 2696/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0448 - acc: 0.9900 - val_loss: 1.9817 - val_acc: 0.6534\n",
      "Epoch 2697/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0608 - acc: 0.9843 - val_loss: 1.9923 - val_acc: 0.6534\n",
      "Epoch 2698/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0598 - acc: 0.9858 - val_loss: 2.1549 - val_acc: 0.6591\n",
      "Epoch 2699/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.0515 - acc: 0.9886 - val_loss: 2.2857 - val_acc: 0.6420\n",
      "Epoch 2700/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0506 - acc: 0.9872 - val_loss: 2.1619 - val_acc: 0.6477\n",
      "Epoch 2701/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0327 - acc: 0.9886 - val_loss: 2.1034 - val_acc: 0.6705\n",
      "Epoch 2702/3000\n",
      "702/702 [==============================] - 0s 582us/sample - loss: 0.0566 - acc: 0.9858 - val_loss: 1.9472 - val_acc: 0.6761\n",
      "Epoch 2703/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0500 - acc: 0.9858 - val_loss: 1.9074 - val_acc: 0.7045\n",
      "Epoch 2704/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0281 - acc: 0.9886 - val_loss: 1.9617 - val_acc: 0.6989\n",
      "Epoch 2705/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0340 - acc: 0.9872 - val_loss: 1.8860 - val_acc: 0.7102\n",
      "Epoch 2706/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0313 - acc: 0.9943 - val_loss: 1.7770 - val_acc: 0.7045\n",
      "Epoch 2707/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0294 - acc: 0.9915 - val_loss: 1.8221 - val_acc: 0.7045\n",
      "Epoch 2708/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0254 - acc: 0.9900 - val_loss: 1.7793 - val_acc: 0.6932\n",
      "Epoch 2709/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0445 - acc: 0.9900 - val_loss: 1.6518 - val_acc: 0.6932\n",
      "Epoch 2710/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0406 - acc: 0.9886 - val_loss: 1.6271 - val_acc: 0.6818\n",
      "Epoch 2711/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0377 - acc: 0.9872 - val_loss: 1.7631 - val_acc: 0.6818\n",
      "Epoch 2712/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.0367 - acc: 0.9886 - val_loss: 1.8063 - val_acc: 0.6761\n",
      "Epoch 2713/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0218 - acc: 0.9986 - val_loss: 1.8666 - val_acc: 0.6818\n",
      "Epoch 2714/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0499 - acc: 0.9872 - val_loss: 1.8712 - val_acc: 0.6818\n",
      "Epoch 2715/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0291 - acc: 0.9900 - val_loss: 1.8277 - val_acc: 0.6875\n",
      "Epoch 2716/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0343 - acc: 0.9886 - val_loss: 1.8686 - val_acc: 0.6818\n",
      "Epoch 2717/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0412 - acc: 0.9915 - val_loss: 1.9739 - val_acc: 0.6818\n",
      "Epoch 2718/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0185 - acc: 0.9943 - val_loss: 2.0708 - val_acc: 0.6818\n",
      "Epoch 2719/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0252 - acc: 0.9957 - val_loss: 1.9936 - val_acc: 0.6875\n",
      "Epoch 2720/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0282 - acc: 0.9915 - val_loss: 1.8300 - val_acc: 0.7159\n",
      "Epoch 2721/3000\n",
      "702/702 [==============================] - 2s 3ms/sample - loss: 0.0196 - acc: 0.9943 - val_loss: 1.7432 - val_acc: 0.7273\n",
      "Epoch 2722/3000\n",
      "702/702 [==============================] - 0s 567us/sample - loss: 0.0390 - acc: 0.9900 - val_loss: 1.6654 - val_acc: 0.7216\n",
      "Epoch 2723/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0273 - acc: 0.9943 - val_loss: 1.7048 - val_acc: 0.7045\n",
      "Epoch 2724/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0242 - acc: 0.9943 - val_loss: 1.8156 - val_acc: 0.6875\n",
      "Epoch 2725/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0431 - acc: 0.9900 - val_loss: 1.5897 - val_acc: 0.6648\n",
      "Epoch 2726/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0229 - acc: 0.9972 - val_loss: 1.5519 - val_acc: 0.6875\n",
      "Epoch 2727/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0518 - acc: 0.9900 - val_loss: 1.7076 - val_acc: 0.6818\n",
      "Epoch 2728/3000\n",
      "702/702 [==============================] - 0s 517us/sample - loss: 0.0268 - acc: 0.9915 - val_loss: 1.7728 - val_acc: 0.6761\n",
      "Epoch 2729/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0336 - acc: 0.9900 - val_loss: 1.7615 - val_acc: 0.6818\n",
      "Epoch 2730/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0343 - acc: 0.9886 - val_loss: 1.6898 - val_acc: 0.6875\n",
      "Epoch 2731/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0537 - acc: 0.9843 - val_loss: 1.7383 - val_acc: 0.6818\n",
      "Epoch 2732/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.0464 - acc: 0.9872 - val_loss: 1.6886 - val_acc: 0.6875\n",
      "Epoch 2733/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0649 - acc: 0.9815 - val_loss: 1.6182 - val_acc: 0.6989\n",
      "Epoch 2734/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0327 - acc: 0.9886 - val_loss: 1.6964 - val_acc: 0.6932\n",
      "Epoch 2735/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0262 - acc: 0.9886 - val_loss: 1.7512 - val_acc: 0.7045\n",
      "Epoch 2736/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0579 - acc: 0.9858 - val_loss: 1.6453 - val_acc: 0.7102\n",
      "Epoch 2737/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.3395 - acc: 0.9701 - val_loss: 1.4789 - val_acc: 0.6932\n",
      "Epoch 2738/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.2233 - acc: 0.9644 - val_loss: 1.4898 - val_acc: 0.6761\n",
      "Epoch 2739/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.2646 - acc: 0.9402 - val_loss: 1.6147 - val_acc: 0.6364\n",
      "Epoch 2740/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.2417 - acc: 0.9359 - val_loss: 1.6283 - val_acc: 0.6591\n",
      "Epoch 2741/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.2150 - acc: 0.9402 - val_loss: 1.6481 - val_acc: 0.6761\n",
      "Epoch 2742/3000\n",
      "702/702 [==============================] - 0s 577us/sample - loss: 0.2123 - acc: 0.9558 - val_loss: 1.5203 - val_acc: 0.6989\n",
      "Epoch 2743/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0605 - acc: 0.9843 - val_loss: 1.6210 - val_acc: 0.6989\n",
      "Epoch 2744/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.1020 - acc: 0.9729 - val_loss: 1.4768 - val_acc: 0.6818\n",
      "Epoch 2745/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0922 - acc: 0.9801 - val_loss: 1.4525 - val_acc: 0.7330\n",
      "Epoch 2746/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.1476 - acc: 0.9701 - val_loss: 1.5851 - val_acc: 0.6932\n",
      "Epoch 2747/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0606 - acc: 0.9786 - val_loss: 1.7246 - val_acc: 0.6875\n",
      "Epoch 2748/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1599 - acc: 0.9758 - val_loss: 1.4894 - val_acc: 0.6932\n",
      "Epoch 2749/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.1309 - acc: 0.9630 - val_loss: 1.4087 - val_acc: 0.7045\n",
      "Epoch 2750/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0589 - acc: 0.9801 - val_loss: 1.5221 - val_acc: 0.7159\n",
      "Epoch 2751/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0985 - acc: 0.9843 - val_loss: 1.5439 - val_acc: 0.7159\n",
      "Epoch 2752/3000\n",
      "702/702 [==============================] - 0s 573us/sample - loss: 0.0733 - acc: 0.9758 - val_loss: 1.4604 - val_acc: 0.6989\n",
      "Epoch 2753/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0620 - acc: 0.9801 - val_loss: 1.4114 - val_acc: 0.6989\n",
      "Epoch 2754/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0453 - acc: 0.9858 - val_loss: 1.5093 - val_acc: 0.7216\n",
      "Epoch 2755/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0508 - acc: 0.9858 - val_loss: 1.6231 - val_acc: 0.7159\n",
      "Epoch 2756/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0524 - acc: 0.9843 - val_loss: 1.6286 - val_acc: 0.7330\n",
      "Epoch 2757/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0807 - acc: 0.9858 - val_loss: 1.6454 - val_acc: 0.7273\n",
      "Epoch 2758/3000\n",
      "702/702 [==============================] - 0s 510us/sample - loss: 0.0799 - acc: 0.9829 - val_loss: 1.6333 - val_acc: 0.7216\n",
      "Epoch 2759/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0543 - acc: 0.9829 - val_loss: 1.5724 - val_acc: 0.7159\n",
      "Epoch 2760/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0273 - acc: 0.9900 - val_loss: 1.5308 - val_acc: 0.7273\n",
      "Epoch 2761/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0265 - acc: 0.9929 - val_loss: 1.5286 - val_acc: 0.7159\n",
      "Epoch 2762/3000\n",
      "702/702 [==============================] - 0s 569us/sample - loss: 0.0358 - acc: 0.9900 - val_loss: 1.6002 - val_acc: 0.7216\n",
      "Epoch 2763/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0488 - acc: 0.9915 - val_loss: 1.6106 - val_acc: 0.7273\n",
      "Epoch 2764/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0372 - acc: 0.9900 - val_loss: 1.4974 - val_acc: 0.7273\n",
      "Epoch 2765/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0404 - acc: 0.9915 - val_loss: 1.4680 - val_acc: 0.7330\n",
      "Epoch 2766/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0321 - acc: 0.9929 - val_loss: 1.4386 - val_acc: 0.7443\n",
      "Epoch 2767/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0590 - acc: 0.9872 - val_loss: 1.3984 - val_acc: 0.7330\n",
      "Epoch 2768/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0380 - acc: 0.9900 - val_loss: 1.4503 - val_acc: 0.7102\n",
      "Epoch 2769/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0346 - acc: 0.9886 - val_loss: 1.5230 - val_acc: 0.7102\n",
      "Epoch 2770/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0287 - acc: 0.9872 - val_loss: 1.5454 - val_acc: 0.7102\n",
      "Epoch 2771/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0257 - acc: 0.9929 - val_loss: 1.5474 - val_acc: 0.7159\n",
      "Epoch 2772/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.0413 - acc: 0.9843 - val_loss: 1.4683 - val_acc: 0.7216\n",
      "Epoch 2773/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0362 - acc: 0.9872 - val_loss: 1.4021 - val_acc: 0.7102\n",
      "Epoch 2774/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0310 - acc: 0.9900 - val_loss: 1.4122 - val_acc: 0.7159\n",
      "Epoch 2775/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0324 - acc: 0.9872 - val_loss: 1.5338 - val_acc: 0.7216\n",
      "Epoch 2776/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0116 - acc: 0.9972 - val_loss: 1.6082 - val_acc: 0.7159\n",
      "Epoch 2777/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0530 - acc: 0.9801 - val_loss: 1.5711 - val_acc: 0.7159\n",
      "Epoch 2778/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0243 - acc: 0.9915 - val_loss: 1.5452 - val_acc: 0.7102\n",
      "Epoch 2779/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0144 - acc: 0.9972 - val_loss: 1.5597 - val_acc: 0.7102\n",
      "Epoch 2780/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0349 - acc: 0.9900 - val_loss: 1.5236 - val_acc: 0.7045\n",
      "Epoch 2781/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0450 - acc: 0.9900 - val_loss: 1.4896 - val_acc: 0.7159\n",
      "Epoch 2782/3000\n",
      "702/702 [==============================] - 0s 573us/sample - loss: 0.0295 - acc: 0.9972 - val_loss: 1.4780 - val_acc: 0.7330\n",
      "Epoch 2783/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0406 - acc: 0.9843 - val_loss: 1.5031 - val_acc: 0.7330\n",
      "Epoch 2784/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0431 - acc: 0.9858 - val_loss: 1.4709 - val_acc: 0.7273\n",
      "Epoch 2785/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0280 - acc: 0.9915 - val_loss: 1.4044 - val_acc: 0.7443\n",
      "Epoch 2786/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0281 - acc: 0.9929 - val_loss: 1.3812 - val_acc: 0.7443\n",
      "Epoch 2787/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0386 - acc: 0.9915 - val_loss: 1.4427 - val_acc: 0.7330\n",
      "Epoch 2788/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0323 - acc: 0.9929 - val_loss: 1.5515 - val_acc: 0.7273\n",
      "Epoch 2789/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0243 - acc: 0.9929 - val_loss: 1.6680 - val_acc: 0.7102\n",
      "Epoch 2790/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0167 - acc: 0.9957 - val_loss: 1.7607 - val_acc: 0.7102\n",
      "Epoch 2791/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0170 - acc: 0.9929 - val_loss: 1.8331 - val_acc: 0.6989\n",
      "Epoch 2792/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 0.0473 - acc: 0.9858 - val_loss: 1.8286 - val_acc: 0.7102\n",
      "Epoch 2793/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0137 - acc: 0.9957 - val_loss: 1.8014 - val_acc: 0.7045\n",
      "Epoch 2794/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0319 - acc: 0.9915 - val_loss: 1.7765 - val_acc: 0.7102\n",
      "Epoch 2795/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0166 - acc: 0.9986 - val_loss: 1.7554 - val_acc: 0.7102\n",
      "Epoch 2796/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0252 - acc: 0.9943 - val_loss: 1.6976 - val_acc: 0.7102\n",
      "Epoch 2797/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0132 - acc: 0.9943 - val_loss: 1.6802 - val_acc: 0.7102\n",
      "Epoch 2798/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0379 - acc: 0.9872 - val_loss: 1.7410 - val_acc: 0.7159\n",
      "Epoch 2799/3000\n",
      "702/702 [==============================] - 0s 507us/sample - loss: 0.0463 - acc: 0.9929 - val_loss: 1.7978 - val_acc: 0.7102\n",
      "Epoch 2800/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0371 - acc: 0.9886 - val_loss: 1.7578 - val_acc: 0.7159\n",
      "Epoch 2801/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0279 - acc: 0.9915 - val_loss: 1.7189 - val_acc: 0.7216\n",
      "Epoch 2802/3000\n",
      "702/702 [==============================] - 0s 573us/sample - loss: 0.0243 - acc: 0.9943 - val_loss: 1.6561 - val_acc: 0.7273\n",
      "Epoch 2803/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0330 - acc: 0.9900 - val_loss: 1.6145 - val_acc: 0.7443\n",
      "Epoch 2804/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0538 - acc: 0.9929 - val_loss: 1.8874 - val_acc: 0.7386\n",
      "Epoch 2805/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1241 - acc: 0.9715 - val_loss: 1.5592 - val_acc: 0.7216\n",
      "Epoch 2806/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0362 - acc: 0.9886 - val_loss: 1.4722 - val_acc: 0.7159\n",
      "Epoch 2807/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0580 - acc: 0.9872 - val_loss: 1.4551 - val_acc: 0.7045\n",
      "Epoch 2808/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0910 - acc: 0.9701 - val_loss: 1.4080 - val_acc: 0.6932\n",
      "Epoch 2809/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0759 - acc: 0.9772 - val_loss: 1.4217 - val_acc: 0.7216\n",
      "Epoch 2810/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0436 - acc: 0.9900 - val_loss: 1.3821 - val_acc: 0.7159\n",
      "Epoch 2811/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0731 - acc: 0.9758 - val_loss: 1.4249 - val_acc: 0.7216\n",
      "Epoch 2812/3000\n",
      "702/702 [==============================] - 0s 585us/sample - loss: 0.0455 - acc: 0.9858 - val_loss: 1.5096 - val_acc: 0.7273\n",
      "Epoch 2813/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.1629 - acc: 0.9815 - val_loss: 1.6245 - val_acc: 0.7443\n",
      "Epoch 2814/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0758 - acc: 0.9829 - val_loss: 1.9564 - val_acc: 0.7330\n",
      "Epoch 2815/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.1154 - acc: 0.9715 - val_loss: 1.9404 - val_acc: 0.7216\n",
      "Epoch 2816/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0685 - acc: 0.9786 - val_loss: 1.8373 - val_acc: 0.7159\n",
      "Epoch 2817/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.1294 - acc: 0.9772 - val_loss: 1.6480 - val_acc: 0.7216\n",
      "Epoch 2818/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0826 - acc: 0.9758 - val_loss: 1.4092 - val_acc: 0.7159\n",
      "Epoch 2819/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.0508 - acc: 0.9815 - val_loss: 1.4413 - val_acc: 0.7273\n",
      "Epoch 2820/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0498 - acc: 0.9872 - val_loss: 1.4481 - val_acc: 0.7216\n",
      "Epoch 2821/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0557 - acc: 0.9829 - val_loss: 1.4182 - val_acc: 0.7330\n",
      "Epoch 2822/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.0509 - acc: 0.9786 - val_loss: 1.5005 - val_acc: 0.7159\n",
      "Epoch 2823/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0422 - acc: 0.9872 - val_loss: 1.6526 - val_acc: 0.7045\n",
      "Epoch 2824/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0358 - acc: 0.9872 - val_loss: 1.5904 - val_acc: 0.7045\n",
      "Epoch 2825/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0389 - acc: 0.9872 - val_loss: 1.5044 - val_acc: 0.7273\n",
      "Epoch 2826/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1284 - acc: 0.9858 - val_loss: 1.5436 - val_acc: 0.7102\n",
      "Epoch 2827/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.1409 - acc: 0.9858 - val_loss: 1.8343 - val_acc: 0.6989\n",
      "Epoch 2828/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.1063 - acc: 0.9843 - val_loss: 1.8542 - val_acc: 0.7045\n",
      "Epoch 2829/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.1244 - acc: 0.9715 - val_loss: 1.5979 - val_acc: 0.7159\n",
      "Epoch 2830/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0682 - acc: 0.9772 - val_loss: 1.5351 - val_acc: 0.7159\n",
      "Epoch 2831/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0879 - acc: 0.9715 - val_loss: 1.6594 - val_acc: 0.7045\n",
      "Epoch 2832/3000\n",
      "702/702 [==============================] - 0s 563us/sample - loss: 0.1011 - acc: 0.9758 - val_loss: 1.6145 - val_acc: 0.7159\n",
      "Epoch 2833/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.0705 - acc: 0.9786 - val_loss: 1.5031 - val_acc: 0.7273\n",
      "Epoch 2834/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0676 - acc: 0.9815 - val_loss: 1.4585 - val_acc: 0.7102\n",
      "Epoch 2835/3000\n",
      "702/702 [==============================] - 0s 509us/sample - loss: 0.1074 - acc: 0.9772 - val_loss: 1.5749 - val_acc: 0.6989\n",
      "Epoch 2836/3000\n",
      "702/702 [==============================] - 0s 508us/sample - loss: 0.0675 - acc: 0.9772 - val_loss: 1.6468 - val_acc: 0.6989\n",
      "Epoch 2837/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0563 - acc: 0.9829 - val_loss: 1.8026 - val_acc: 0.6818\n",
      "Epoch 2838/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.0559 - acc: 0.9843 - val_loss: 1.7235 - val_acc: 0.6989\n",
      "Epoch 2839/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0618 - acc: 0.9815 - val_loss: 1.6742 - val_acc: 0.7159\n",
      "Epoch 2840/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0647 - acc: 0.9872 - val_loss: 1.6348 - val_acc: 0.7216\n",
      "Epoch 2841/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0359 - acc: 0.9900 - val_loss: 1.6032 - val_acc: 0.7102\n",
      "Epoch 2842/3000\n",
      "702/702 [==============================] - 0s 566us/sample - loss: 0.0322 - acc: 0.9843 - val_loss: 1.6360 - val_acc: 0.7045\n",
      "Epoch 2843/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0294 - acc: 0.9943 - val_loss: 1.6285 - val_acc: 0.7102\n",
      "Epoch 2844/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0455 - acc: 0.9858 - val_loss: 1.5904 - val_acc: 0.7330\n",
      "Epoch 2845/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0562 - acc: 0.9815 - val_loss: 1.4859 - val_acc: 0.7386\n",
      "Epoch 2846/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0594 - acc: 0.9843 - val_loss: 1.4954 - val_acc: 0.7443\n",
      "Epoch 2847/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0492 - acc: 0.9858 - val_loss: 1.5817 - val_acc: 0.7386\n",
      "Epoch 2848/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0828 - acc: 0.9786 - val_loss: 1.6305 - val_acc: 0.7386\n",
      "Epoch 2849/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0775 - acc: 0.9829 - val_loss: 1.6893 - val_acc: 0.7216\n",
      "Epoch 2850/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0780 - acc: 0.9843 - val_loss: 1.9281 - val_acc: 0.7273\n",
      "Epoch 2851/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0622 - acc: 0.9801 - val_loss: 2.0064 - val_acc: 0.7216\n",
      "Epoch 2852/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.0559 - acc: 0.9843 - val_loss: 1.9943 - val_acc: 0.7159\n",
      "Epoch 2853/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.0633 - acc: 0.9815 - val_loss: 1.8666 - val_acc: 0.7159\n",
      "Epoch 2854/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0411 - acc: 0.9829 - val_loss: 1.8049 - val_acc: 0.7216\n",
      "Epoch 2855/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0172 - acc: 0.9957 - val_loss: 1.7894 - val_acc: 0.7216\n",
      "Epoch 2856/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0403 - acc: 0.9858 - val_loss: 1.8611 - val_acc: 0.7216\n",
      "Epoch 2857/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0466 - acc: 0.9872 - val_loss: 1.7431 - val_acc: 0.7330\n",
      "Epoch 2858/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0423 - acc: 0.9829 - val_loss: 1.7541 - val_acc: 0.7386\n",
      "Epoch 2859/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0455 - acc: 0.9843 - val_loss: 1.8284 - val_acc: 0.7386\n",
      "Epoch 2860/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0700 - acc: 0.9786 - val_loss: 1.8718 - val_acc: 0.7330\n",
      "Epoch 2861/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0329 - acc: 0.9943 - val_loss: 1.8904 - val_acc: 0.7045\n",
      "Epoch 2862/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 0.0198 - acc: 0.9972 - val_loss: 1.9042 - val_acc: 0.7045\n",
      "Epoch 2863/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0198 - acc: 0.9943 - val_loss: 1.9969 - val_acc: 0.7102\n",
      "Epoch 2864/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0352 - acc: 0.9900 - val_loss: 2.0728 - val_acc: 0.6989\n",
      "Epoch 2865/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0206 - acc: 0.9929 - val_loss: 2.0545 - val_acc: 0.7045\n",
      "Epoch 2866/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0286 - acc: 0.9929 - val_loss: 2.0570 - val_acc: 0.6989\n",
      "Epoch 2867/3000\n",
      "702/702 [==============================] - 0s 484us/sample - loss: 0.0457 - acc: 0.9843 - val_loss: 1.9493 - val_acc: 0.7045\n",
      "Epoch 2868/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0367 - acc: 0.9915 - val_loss: 1.9414 - val_acc: 0.7045\n",
      "Epoch 2869/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0148 - acc: 0.9986 - val_loss: 1.9650 - val_acc: 0.7045\n",
      "Epoch 2870/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0166 - acc: 0.9943 - val_loss: 1.9891 - val_acc: 0.6989\n",
      "Epoch 2871/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0397 - acc: 0.9900 - val_loss: 1.9510 - val_acc: 0.7045\n",
      "Epoch 2872/3000\n",
      "702/702 [==============================] - 0s 565us/sample - loss: 0.0254 - acc: 0.9929 - val_loss: 1.9699 - val_acc: 0.6989\n",
      "Epoch 2873/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0325 - acc: 0.9915 - val_loss: 2.0179 - val_acc: 0.6875\n",
      "Epoch 2874/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0338 - acc: 0.9872 - val_loss: 1.9273 - val_acc: 0.6875\n",
      "Epoch 2875/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0315 - acc: 0.9900 - val_loss: 1.8854 - val_acc: 0.6989\n",
      "Epoch 2876/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0198 - acc: 0.9943 - val_loss: 1.8299 - val_acc: 0.7102\n",
      "Epoch 2877/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0291 - acc: 0.9872 - val_loss: 1.7927 - val_acc: 0.7045\n",
      "Epoch 2878/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0245 - acc: 0.9957 - val_loss: 1.7790 - val_acc: 0.7159\n",
      "Epoch 2879/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0166 - acc: 0.9957 - val_loss: 1.8351 - val_acc: 0.7159\n",
      "Epoch 2880/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0221 - acc: 0.9943 - val_loss: 1.8811 - val_acc: 0.7045\n",
      "Epoch 2881/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0198 - acc: 0.9986 - val_loss: 1.8625 - val_acc: 0.7159\n",
      "Epoch 2882/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.0294 - acc: 0.9886 - val_loss: 1.7957 - val_acc: 0.7159\n",
      "Epoch 2883/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0146 - acc: 0.9943 - val_loss: 1.7007 - val_acc: 0.7159\n",
      "Epoch 2884/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0290 - acc: 0.9886 - val_loss: 1.6530 - val_acc: 0.7159\n",
      "Epoch 2885/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0331 - acc: 0.9872 - val_loss: 1.6434 - val_acc: 0.7045\n",
      "Epoch 2886/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.0263 - acc: 0.9943 - val_loss: 1.7179 - val_acc: 0.6818\n",
      "Epoch 2887/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0242 - acc: 0.9929 - val_loss: 1.7522 - val_acc: 0.6761\n",
      "Epoch 2888/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0566 - acc: 0.9900 - val_loss: 1.7030 - val_acc: 0.6932\n",
      "Epoch 2889/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0190 - acc: 0.9929 - val_loss: 1.6538 - val_acc: 0.7159\n",
      "Epoch 2890/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0220 - acc: 0.9929 - val_loss: 1.6546 - val_acc: 0.7102\n",
      "Epoch 2891/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0540 - acc: 0.9915 - val_loss: 1.6632 - val_acc: 0.6932\n",
      "Epoch 2892/3000\n",
      "702/702 [==============================] - 0s 572us/sample - loss: 0.0369 - acc: 0.9843 - val_loss: 1.7356 - val_acc: 0.6818\n",
      "Epoch 2893/3000\n",
      "702/702 [==============================] - 0s 486us/sample - loss: 0.0299 - acc: 0.9929 - val_loss: 1.6653 - val_acc: 0.6932\n",
      "Epoch 2894/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0153 - acc: 0.9957 - val_loss: 1.6162 - val_acc: 0.7102\n",
      "Epoch 2895/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0204 - acc: 0.9957 - val_loss: 1.6044 - val_acc: 0.6932\n",
      "Epoch 2896/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0321 - acc: 0.9886 - val_loss: 1.6712 - val_acc: 0.6875\n",
      "Epoch 2897/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0309 - acc: 0.9943 - val_loss: 1.6918 - val_acc: 0.7159\n",
      "Epoch 2898/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0246 - acc: 0.9915 - val_loss: 1.7291 - val_acc: 0.7045\n",
      "Epoch 2899/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0175 - acc: 0.9943 - val_loss: 1.7793 - val_acc: 0.7045\n",
      "Epoch 2900/3000\n",
      "702/702 [==============================] - 0s 485us/sample - loss: 0.0204 - acc: 0.9900 - val_loss: 1.8187 - val_acc: 0.7102\n",
      "Epoch 2901/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0141 - acc: 0.9943 - val_loss: 1.8018 - val_acc: 0.7159\n",
      "Epoch 2902/3000\n",
      "702/702 [==============================] - 0s 562us/sample - loss: 0.0110 - acc: 0.9943 - val_loss: 1.8155 - val_acc: 0.7216\n",
      "Epoch 2903/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0362 - acc: 0.9915 - val_loss: 1.8712 - val_acc: 0.7102\n",
      "Epoch 2904/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0276 - acc: 0.9929 - val_loss: 1.8126 - val_acc: 0.7273\n",
      "Epoch 2905/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0207 - acc: 0.9915 - val_loss: 1.7712 - val_acc: 0.7273\n",
      "Epoch 2906/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0553 - acc: 0.9843 - val_loss: 1.7615 - val_acc: 0.7273\n",
      "Epoch 2907/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0359 - acc: 0.9886 - val_loss: 1.8098 - val_acc: 0.7045\n",
      "Epoch 2908/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0437 - acc: 0.9886 - val_loss: 1.8302 - val_acc: 0.6932\n",
      "Epoch 2909/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0250 - acc: 0.9929 - val_loss: 1.8794 - val_acc: 0.6932\n",
      "Epoch 2910/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0237 - acc: 0.9915 - val_loss: 1.8899 - val_acc: 0.6875\n",
      "Epoch 2911/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0160 - acc: 0.9943 - val_loss: 1.8456 - val_acc: 0.6761\n",
      "Epoch 2912/3000\n",
      "702/702 [==============================] - 0s 589us/sample - loss: 0.0254 - acc: 0.9929 - val_loss: 1.8456 - val_acc: 0.6875\n",
      "Epoch 2913/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.0340 - acc: 0.9900 - val_loss: 1.8766 - val_acc: 0.6818\n",
      "Epoch 2914/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0288 - acc: 0.9915 - val_loss: 1.8552 - val_acc: 0.6989\n",
      "Epoch 2915/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0427 - acc: 0.9929 - val_loss: 1.7416 - val_acc: 0.6932\n",
      "Epoch 2916/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0194 - acc: 0.9972 - val_loss: 1.7105 - val_acc: 0.6989\n",
      "Epoch 2917/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0256 - acc: 0.9943 - val_loss: 1.7154 - val_acc: 0.7045\n",
      "Epoch 2918/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0186 - acc: 0.9929 - val_loss: 1.7226 - val_acc: 0.7045\n",
      "Epoch 2919/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0094 - acc: 0.9972 - val_loss: 1.7327 - val_acc: 0.7102\n",
      "Epoch 2920/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0382 - acc: 0.9900 - val_loss: 1.7693 - val_acc: 0.7045\n",
      "Epoch 2921/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0320 - acc: 0.9915 - val_loss: 1.7811 - val_acc: 0.6989\n",
      "Epoch 2922/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 0.0163 - acc: 0.9943 - val_loss: 1.7344 - val_acc: 0.6932\n",
      "Epoch 2923/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0279 - acc: 0.9929 - val_loss: 1.6638 - val_acc: 0.6989\n",
      "Epoch 2924/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0663 - acc: 0.9929 - val_loss: 1.5833 - val_acc: 0.7102\n",
      "Epoch 2925/3000\n",
      "702/702 [==============================] - 0s 491us/sample - loss: 0.0186 - acc: 0.9943 - val_loss: 1.4848 - val_acc: 0.7102\n",
      "Epoch 2926/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0209 - acc: 0.9943 - val_loss: 1.5201 - val_acc: 0.6989\n",
      "Epoch 2927/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0323 - acc: 0.9872 - val_loss: 1.5431 - val_acc: 0.6989\n",
      "Epoch 2928/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0417 - acc: 0.9858 - val_loss: 1.5440 - val_acc: 0.6932\n",
      "Epoch 2929/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0256 - acc: 0.9886 - val_loss: 1.5868 - val_acc: 0.6989\n",
      "Epoch 2930/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0329 - acc: 0.9886 - val_loss: 1.6551 - val_acc: 0.7045\n",
      "Epoch 2931/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0349 - acc: 0.9929 - val_loss: 1.6654 - val_acc: 0.7159\n",
      "Epoch 2932/3000\n",
      "702/702 [==============================] - 0s 575us/sample - loss: 0.0442 - acc: 0.9858 - val_loss: 1.6998 - val_acc: 0.6932\n",
      "Epoch 2933/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0253 - acc: 0.9915 - val_loss: 1.8264 - val_acc: 0.7045\n",
      "Epoch 2934/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0400 - acc: 0.9886 - val_loss: 1.6667 - val_acc: 0.7159\n",
      "Epoch 2935/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0430 - acc: 0.9829 - val_loss: 1.4813 - val_acc: 0.6989\n",
      "Epoch 2936/3000\n",
      "702/702 [==============================] - 0s 489us/sample - loss: 0.0196 - acc: 0.9943 - val_loss: 1.5406 - val_acc: 0.7102\n",
      "Epoch 2937/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0342 - acc: 0.9886 - val_loss: 1.6288 - val_acc: 0.7102\n",
      "Epoch 2938/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0317 - acc: 0.9900 - val_loss: 1.6446 - val_acc: 0.7159\n",
      "Epoch 2939/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0247 - acc: 0.9943 - val_loss: 1.5683 - val_acc: 0.7216\n",
      "Epoch 2940/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0592 - acc: 0.9915 - val_loss: 1.5536 - val_acc: 0.7386\n",
      "Epoch 2941/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0397 - acc: 0.9872 - val_loss: 1.6266 - val_acc: 0.7216\n",
      "Epoch 2942/3000\n",
      "702/702 [==============================] - 0s 577us/sample - loss: 0.0604 - acc: 0.9815 - val_loss: 1.7287 - val_acc: 0.7159\n",
      "Epoch 2943/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0531 - acc: 0.9872 - val_loss: 1.7413 - val_acc: 0.7159\n",
      "Epoch 2944/3000\n",
      "702/702 [==============================] - 0s 511us/sample - loss: 0.0225 - acc: 0.9957 - val_loss: 1.8270 - val_acc: 0.7045\n",
      "Epoch 2945/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0318 - acc: 0.9929 - val_loss: 1.8744 - val_acc: 0.7159\n",
      "Epoch 2946/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0245 - acc: 0.9915 - val_loss: 1.7848 - val_acc: 0.6932\n",
      "Epoch 2947/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.0149 - acc: 0.9943 - val_loss: 1.7746 - val_acc: 0.6989\n",
      "Epoch 2948/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0215 - acc: 0.9943 - val_loss: 1.7969 - val_acc: 0.6989\n",
      "Epoch 2949/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0381 - acc: 0.9900 - val_loss: 1.7233 - val_acc: 0.6989\n",
      "Epoch 2950/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.0304 - acc: 0.9886 - val_loss: 1.5948 - val_acc: 0.6989\n",
      "Epoch 2951/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0249 - acc: 0.9915 - val_loss: 1.6181 - val_acc: 0.7102\n",
      "Epoch 2952/3000\n",
      "702/702 [==============================] - 0s 574us/sample - loss: 0.0427 - acc: 0.9886 - val_loss: 1.7802 - val_acc: 0.6989\n",
      "Epoch 2953/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.0602 - acc: 0.9801 - val_loss: 1.8114 - val_acc: 0.6875\n",
      "Epoch 2954/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0558 - acc: 0.9815 - val_loss: 1.6104 - val_acc: 0.6648\n",
      "Epoch 2955/3000\n",
      "702/702 [==============================] - 0s 503us/sample - loss: 0.0636 - acc: 0.9900 - val_loss: 1.4209 - val_acc: 0.7045\n",
      "Epoch 2956/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0558 - acc: 0.9815 - val_loss: 1.4628 - val_acc: 0.7045\n",
      "Epoch 2957/3000\n",
      "702/702 [==============================] - 0s 493us/sample - loss: 0.0341 - acc: 0.9900 - val_loss: 1.5292 - val_acc: 0.7216\n",
      "Epoch 2958/3000\n",
      "702/702 [==============================] - 0s 488us/sample - loss: 0.0320 - acc: 0.9915 - val_loss: 1.6374 - val_acc: 0.6989\n",
      "Epoch 2959/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0535 - acc: 0.9872 - val_loss: 1.7770 - val_acc: 0.6932\n",
      "Epoch 2960/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0444 - acc: 0.9886 - val_loss: 1.8328 - val_acc: 0.7102\n",
      "Epoch 2961/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0619 - acc: 0.9886 - val_loss: 1.6339 - val_acc: 0.7045\n",
      "Epoch 2962/3000\n",
      "702/702 [==============================] - 0s 569us/sample - loss: 0.0502 - acc: 0.9829 - val_loss: 1.5188 - val_acc: 0.7045\n",
      "Epoch 2963/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0340 - acc: 0.9900 - val_loss: 1.4685 - val_acc: 0.6989\n",
      "Epoch 2964/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0396 - acc: 0.9858 - val_loss: 1.6503 - val_acc: 0.6875\n",
      "Epoch 2965/3000\n",
      "702/702 [==============================] - 0s 482us/sample - loss: 0.0361 - acc: 0.9900 - val_loss: 1.9232 - val_acc: 0.6818\n",
      "Epoch 2966/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0205 - acc: 0.9915 - val_loss: 1.9438 - val_acc: 0.6761\n",
      "Epoch 2967/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0200 - acc: 0.9957 - val_loss: 1.8452 - val_acc: 0.6761\n",
      "Epoch 2968/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0149 - acc: 0.9943 - val_loss: 1.7383 - val_acc: 0.6818\n",
      "Epoch 2969/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0339 - acc: 0.9872 - val_loss: 1.7871 - val_acc: 0.6989\n",
      "Epoch 2970/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0527 - acc: 0.9915 - val_loss: 1.7978 - val_acc: 0.6932\n",
      "Epoch 2971/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0455 - acc: 0.9872 - val_loss: 1.7893 - val_acc: 0.6875\n",
      "Epoch 2972/3000\n",
      "702/702 [==============================] - 0s 583us/sample - loss: 0.0332 - acc: 0.9929 - val_loss: 1.8050 - val_acc: 0.6761\n",
      "Epoch 2973/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.0391 - acc: 0.9900 - val_loss: 1.9013 - val_acc: 0.7045\n",
      "Epoch 2974/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0571 - acc: 0.9829 - val_loss: 1.6852 - val_acc: 0.6875\n",
      "Epoch 2975/3000\n",
      "702/702 [==============================] - 0s 502us/sample - loss: 0.0315 - acc: 0.9886 - val_loss: 1.7519 - val_acc: 0.6648\n",
      "Epoch 2976/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0767 - acc: 0.9829 - val_loss: 1.8373 - val_acc: 0.6534\n",
      "Epoch 2977/3000\n",
      "702/702 [==============================] - 0s 497us/sample - loss: 0.0747 - acc: 0.9858 - val_loss: 1.7557 - val_acc: 0.6818\n",
      "Epoch 2978/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0368 - acc: 0.9886 - val_loss: 1.6827 - val_acc: 0.7045\n",
      "Epoch 2979/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0288 - acc: 0.9929 - val_loss: 1.6076 - val_acc: 0.7102\n",
      "Epoch 2980/3000\n",
      "702/702 [==============================] - 0s 501us/sample - loss: 0.0425 - acc: 0.9843 - val_loss: 1.5902 - val_acc: 0.6989\n",
      "Epoch 2981/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0408 - acc: 0.9886 - val_loss: 1.6662 - val_acc: 0.6989\n",
      "Epoch 2982/3000\n",
      "702/702 [==============================] - 0s 578us/sample - loss: 0.0379 - acc: 0.9957 - val_loss: 1.6971 - val_acc: 0.6818\n",
      "Epoch 2983/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0378 - acc: 0.9886 - val_loss: 1.5607 - val_acc: 0.6875\n",
      "Epoch 2984/3000\n",
      "702/702 [==============================] - 0s 504us/sample - loss: 0.0436 - acc: 0.9900 - val_loss: 1.5012 - val_acc: 0.6875\n",
      "Epoch 2985/3000\n",
      "702/702 [==============================] - 0s 496us/sample - loss: 0.0308 - acc: 0.9943 - val_loss: 1.5192 - val_acc: 0.6875\n",
      "Epoch 2986/3000\n",
      "702/702 [==============================] - 0s 505us/sample - loss: 0.0498 - acc: 0.9886 - val_loss: 1.7026 - val_acc: 0.6875\n",
      "Epoch 2987/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0322 - acc: 0.9886 - val_loss: 1.9068 - val_acc: 0.6875\n",
      "Epoch 2988/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0322 - acc: 0.9900 - val_loss: 1.8794 - val_acc: 0.7045\n",
      "Epoch 2989/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0265 - acc: 0.9900 - val_loss: 1.6618 - val_acc: 0.7045\n",
      "Epoch 2990/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0395 - acc: 0.9915 - val_loss: 1.7346 - val_acc: 0.7102\n",
      "Epoch 2991/3000\n",
      "702/702 [==============================] - 2s 2ms/sample - loss: 0.0414 - acc: 0.9886 - val_loss: 1.8081 - val_acc: 0.7159\n",
      "Epoch 2992/3000\n",
      "702/702 [==============================] - 0s 571us/sample - loss: 0.0194 - acc: 0.9943 - val_loss: 1.8156 - val_acc: 0.7216\n",
      "Epoch 2993/3000\n",
      "702/702 [==============================] - 0s 500us/sample - loss: 0.0240 - acc: 0.9900 - val_loss: 1.9803 - val_acc: 0.6932\n",
      "Epoch 2994/3000\n",
      "702/702 [==============================] - 0s 498us/sample - loss: 0.0343 - acc: 0.9915 - val_loss: 1.9672 - val_acc: 0.6989\n",
      "Epoch 2995/3000\n",
      "702/702 [==============================] - 0s 487us/sample - loss: 0.0135 - acc: 0.9957 - val_loss: 1.7980 - val_acc: 0.6875\n",
      "Epoch 2996/3000\n",
      "702/702 [==============================] - 0s 499us/sample - loss: 0.0229 - acc: 0.9929 - val_loss: 1.6690 - val_acc: 0.7045\n",
      "Epoch 2997/3000\n",
      "702/702 [==============================] - 0s 492us/sample - loss: 0.0269 - acc: 0.9972 - val_loss: 1.6365 - val_acc: 0.7159\n",
      "Epoch 2998/3000\n",
      "702/702 [==============================] - 0s 494us/sample - loss: 0.0186 - acc: 0.9915 - val_loss: 1.7106 - val_acc: 0.7102\n",
      "Epoch 2999/3000\n",
      "702/702 [==============================] - 0s 490us/sample - loss: 0.1047 - acc: 0.9858 - val_loss: 1.8160 - val_acc: 0.7045\n",
      "Epoch 3000/3000\n",
      "702/702 [==============================] - 0s 495us/sample - loss: 0.0792 - acc: 0.9801 - val_loss: 2.2109 - val_acc: 0.6591\n",
      "training_time :  1458.651964187622\n"
     ]
    }
   ],
   "source": [
    "model = Mynet_squeeze()\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\n",
    "             metrics=['accuracy'])\n",
    "start_time = time.time()\n",
    "history = model.fit(trainX,trainY, \n",
    "                    epochs=3000, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    verbose=1, \n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[tf.keras.callbacks.TensorBoard(log_dir='../logs/'+IN_DIR_PATH+'/events', histogram_freq=10)#,\n",
    "#                                tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=700,verbose=0,mode='auto')\n",
    "                              ]\n",
    "                   )\n",
    "end_time = time.time()-start_time\n",
    "print('training_time : ',end_time)\n",
    "score = model.evaluate(valX, valY, batch_size=BATCH_SIZE, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmIAAAEWCAYAAADFDfusAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOydZ5gUxdaA37OzCdgl5wwSJYclKBIUQQEVREVEFAOI6X5eBdGrIoJeuOaACRNgQBAVAyCKgIAgEgQlKCIsShDJednd2fp+9OTpiTuzMzvU+zzzTE91ddXpCd1nTp0gSik0Go1Go9FoNEVPUqwF0Gg0Go1Gozlb0YqYRqPRaDQaTYzQiphGo9FoNBpNjNCKmEaj0Wg0Gk2M0IqYRqPRaDQaTYzQiphGo9FoNBpNjNCKmCZuEJGpIvJ4kH2zRaRnYcfRaDQaO5G6Bmk0oaAVMY1Go9FoNJoYoRUxjUaj0WgSCBFJjrUMmuDRipgmJGzm+NEi8rOInBSRt0SkiojMF5HjIrJQRMq59L9cRDaJyBERWSIiTV32tRGRdbbjZgLpHnP1E5H1tmNXiEjLMGUeLiLbROSQiHwuItVt7SIiz4nIPyJy1HZOzW37+ojIZptsu0VkVFhvmEajiSjF4RokIn1F5CcROSYif4nIOI/9XWzjHbHtH2ZrLyEiz4jITts1abmtrbuI7DJ5H3ratseJyGwReU9EjgHDRKSDiKy0zbFXRCaLSKrL8c1E5BvbdXGfiPxHRKqKyCkRqeDSr52I7BeRlGDOXRM6WhHThMNA4GKgEXAZMB/4D1AR4zv1LwARaQTMAO4BKgHzgC9EJNV2QZgDvAuUBz6yjYvt2LbA28BtQAXgdeBzEUkLRVARuRCYCFwDVAN2Ah/advcCutrOoywwCDho2/cWcJtSKhNoDiwKZV6NRhNV4v0adBK4AeO60he4XUT628atbZP3JZtMrYH1tuOeBtoB59lkuh8oCPI9uQKYbZvzfcAK/Nv2nnQGLgLusMmQCSwEvgKqAw2Ab5VSfwNLMK6Xdq4HPlRK5QUphyZEtCKmCYeXlFL7lFK7gWXAKqXUT0qpM8CnQBtbv0HAXKXUN7Yf8dNACYyLTCcgBXheKZWnlJoNrHaZYzjwulJqlVLKqpSaBpyxHRcKQ4C3lVLrbPI9CHQWkbpAHpAJNAFEKbVFKbXXdlwecK6IlFZKHVZKrQtxXo1GEz3i+hqklFqilPpFKVWglPoZQxnsZts9BFiolJphm/egUmq9iCQBNwP/p5TabZtzhe2cgmGlUmqObc7TSqm1SqkflFL5SqlsDEXSLkM/4G+l1DNKqRyl1HGl1CrbvmkYyhciYgEGYyirmiihFTFNOOxz2T5t8jrDtl0dwwIFgFKqAPgLqGHbt1u5V53f6bJdB7jPZlY/IiJHgFq240LBU4YTGFavGkqpRcBk4GVgn4hMEZHStq4DgT7AThH5TkQ6hzivRqOJHnF9DRKRjiKy2LakdxQYiWGZwjbGHyaHVcRYGjXbFwx/ecjQSES+FJG/bcuV/w1CBoDPMP6E1sewOh5VSv0YpkyaINCKmCaa7MG4mAGGTxbGBWA3sBeoYWuzU9tl+y/gCaVUWZdHSaXUjELKUApjmWE3gFLqRaVUO6AZxjLHaFv7aqXUFUBljOWLWSHOq9FoYk+srkEfAJ8DtZRSZYDXAPs8fwHnmBxzAMjxse8kUNLlPCwYy5quKI/XrwK/Ag2VUqUxlm4DyYBSKgfjejcEGIq2hkUdrYhposksoK+IXGRz9LwPw7S/AlgJ5AP/EpFkEbkS6OBy7BvASNs/SxGRUjYH2MwQZfgAuElEWtt8O/6LsYyRLSJZtvFTMC50OYDV5j8yRETK2JYzjmH4W2g0muJFrK5BmcAhpVSOiHQArnPZ9z7QU0Susc1bQURa26x1bwPPikh1EbGISGfbdWsrkG6bPwV4GAjkq5aJce06ISJNgNtd9n0JVBWRe0QkTUQyRaSjy/7pwDDgcuC9IM5XUwi0IqaJGkqp3zB8DV7C+Ld3GXCZUipXKZULXInxYz+M4cvxicuxazB8NCbb9m+z9Q1Vhm+BR4CPMf4BnwNca9tdGuNiexhjSeIghg8JGP8Es20m/ZG289BoNMWIGF6D7gDGi8hxYCwuFnWl1J8Ybg/3AYcwHPVb2XaPAn7B8FU7BPwPSFJKHbWN+SaGNe8k4BZFacIoDAXwOMZ1bqaLDMcxlh0vA/4Gfgd6uOz/HiNIYJ3Nv0wTRcR9eVyj0Wg0Gs3ZjogsAj5QSr0Za1kSHa2IaTQajUajcSAiWcA3GD5ux2MtT6ITtaVJEUkXkR9FZIMYyfQeM+mTJiIzxUi2ucqWUkCj0Wg0Gk0MEJFpGDnG7tFKWNEQNYuYLRKllFLqhM25cDlGfpQfXPrcAbRUSo0UkWuBAUqpQVERSKPRaDQajSbOiJpFTBmcsL1MsT08tb4rMJLHgZER+CKPUGKNRqPRaDSahCWqhUFtuU7WYpRPeNklc6+dGtiS0Cml8m2J7ypgRLe4jjMCGAFQqlSpdk2aNAlq/vx9v3LKKpSq2ghLktbvNJriytq1aw8opTzzJhVLKlasqOrWrRtrMTQaTRER6PoVVUVMKWUFWotIWeBTEWmulNro0sVMO/JaK1VKTQGmALRv316tWbMmqPkPPNOJ9UfSafvA15QvlRr4AI1GE5eIyM7AvYoHdevWJdhrmEajKf4Eun4VSR4xpdQRjEKil3js2oWR5RgRSQbKYOROiQhie+jIUI1Go9FoNPFINKMmK9ksYYhICaAnRrkFVz4HbrRtXwUsUhHUmpTN3UyrYRqNRqPRaOKRaC5NVgOm2fzEkoBZSqkvRWQ8sEYp9TnwFvCuiGzDsIRd63u40DEsYgptENNoNBqNRhOPRE0RU0r9DLQxaR/rsp0DXB01GbBbxLQmVlzJy8tj165d5OTkxFoUTRGQnp5OzZo1SUlJibUoGo1GUyRE1Vk/1ug4yeLPrl27yMzMpG7duujMJomNUoqDBw+ya9cu6tWrF2txNBqNpkhI+KLfgtJOYsWYnJwcKlSooJWwswARoUKFCnFj/RSRt0XkHxHZ6GO/iMiLtsogP4tI26KWUaPRFH8SWhHTzvqJgVbCzh7i7LOeinektyuXAg1tjxHAq0Ugk0ajSTASWhFzpq+ItSQajaa4oZRaiv90OlcA021VRH4AyopItaKRThPPnMm3cuJMfpHOWVCgOHIql4Mnzri15+RZ2XPkNKdzrX6PP3wyl0Mnc33u/3nXEdb/dYQjp3KxFhg3VaUU3287ENFz/edYDsdz8hyvCwoUs1b/xT/Hg7OUr9p+kK37jqOU4pN1u9h95LRbCquNu4/y3db9bN3nXkYzJ8/KtBXZTFuRTZ61gDxrATsPnjSdw9/7FA4J7SOmnfU1kSAjI4MTJ04E7hgi+/fvp1+/fuTm5vLiiy9ywQUXhDzG1KlT6dWrF9WrVw/puNdee42SJUtyww03+OyzZs0apk+fzosvvhiyXGcJjsogNnbZ2vZ6dnStDlK7du0iEU4TefKsBRw+mUvl0ul++zV++CsAbutWn2Hn1aVamRKm/dbuPMyqHQe5vds5IVmDj57Kw2IRMtKSGTF9DV9v3ue2f1SvRqz78wijezfm0heWOdqzJ/UFYHX2If745wSXNK9K2ZJGsvM2E74BYME9XaleNp1Pf9rN0E51EBFmrf6L+z/+2W2O27ufw6tL/nC8XnZ/Dy54cjEvDW5Dx/rlmbxoG/df0oSMNG8149mvf+PFRduY0L85QzvVcbR/sOpP/vPpL47XvZtVYcGmfV7HezKqVyNW/HGQFX8cdLRd1a4ms9fuAuCxy5txXcfa3DNzPXN/dv48tz1xKZYk4dlvtvLSom2O9kc/3+Q1x78uasiL3/7ueP35XefTsmbZgLIFQ9SKfkeLUDLrH3yuCxsPCQ3v+5rqZc1/CJr4ZsuWLTRt2jSmMkRLEfvwww+ZP38+06ZNC9zZhtVqxWKxOF53796dp59+mvbt2wfsW1ww+8xFZK1Syvsko4yI1AW+VEo1N9k3F5iolFpue/0tcL9Saq2/MUO5hmkCczwnj5NnrFQt4185CsRjX2zi4IlcXhzsFezv4L5ZG/h43S62Pn4pqclJHMvJ43SulSo2xWzxb/9QOTONvi8udzvup0cuxmqzHv3fh+tNx173yMVeFWB2HznN+ZMW8eLgNlzeyvlnq+4Dc8M6x+cGteLfMze4tWVP6suWvcfcFDZXVjxwIedNWhTWfADv3dKRdnXKceDEGWqVL0lufgGNHp7v2L/psd6Usilr4Z5XMDSsnMHv/3hfx9+6sT23TAv99/jUVS25un2toPoGun4l9NIk8eVvoinmKKUYPXo0zZs3p0WLFsycOROAvXv30rVrV1q3bk3z5s1ZtmwZVquVYcOGOfo+99xzbmOtX7+e+++/n3nz5tG6dWtOnz7NjBkzaNGiBc2bN2fMmDGOvhkZGYwdO5aOHTuycuVKR/vs2bNZs2YNQ4YMcYxRt25dxo8fT5cuXfjoo4944403yMrKolWrVgwcOJBTp04BMG7cOJ5++mnAUObGjBlDhw4daNSoEcuWGRfkJUuW0K9fP0f/m2++me7du1O/fn03K9mECRNo0qQJF198MYMHD3aMexbgqAxioyawJ0aynLW0GPc1nSZ+G9axV7+2gse/3My7P+zkne+z+XzDHuo+MJcvf97Dg5/8Qt0H5lL3gblM/X4HAB+vMywsjR6ez44DJ2k57ms6/teY+/o3V3HTO6u9lDAwrE3tH1/oUwkD+H6bs8TypS8so+4DcznfpgD9a8ZPjn1b9h4L61wBLyUMjKU/X0oYUCglDOD6t1bRdOxXXPDkYlZsO+CmhIFhZQQ44LGkGmnMlDAgLCUMoEyJyKXYSeilSTvFy+an8cVjX2xi857wL0JmnFu9NI9e1iyovp988gnr169nw4YNHDhwgKysLLp27coHH3xA7969eeihh7BarZw6dYr169eze/duNm40Au6OHDniNlbr1q0ZP348a9asYfLkyezZs4cxY8awdu1aypUrR69evZgzZw79+/fn5MmTNG/enPHjx7uNcdVVVzF58mQvi1h6ejrLlxs3g4MHDzJ8+HAAHn74Yd566y3uvvtur3PLz8/nxx9/ZN68eTz22GMsXLjQq8+vv/7K4sWLOX78OI0bN+b2229nw4YNfPzxx/z000/k5+fTtm1b2rVrF9T7mQB8DtwlIh8CHYGjSimvZUlN9HBd0dlz5DQT5//K7sOnmHJDeypmpJkecybfyvxf/sZaoFidfZjV2Ye9+tz1wU9ur8d9sZkLm1Rxa+vx9BLHdiQsOXfP+InU5CRue9fcoHr4ZC4lUi1+laZw6PeSt+IYCg0qZ7DNh5LjyXVvrvJqs7mb0f5x72uOP+bceT6z1/7Fez/8GdJxoVC/Yim2HzD3EzueEzm/uIRXxIzM+loV0xSe5cuXM3jwYCwWC1WqVKFbt26sXr2arKwsbr75ZvLy8ujfvz+tW7emfv36bN++nbvvvpu+ffvSq1cvv2OvXr2a7t27U6lSJQCGDBnC0qVL6d+/PxaLhYEDBwYt56BBgxzbGzdu5OGHH+bIkSOcOHGC3r17mx5z5ZVXAtCuXTuys7NN+/Tt25e0tDTS0tKoXLky+/btY/ny5VxxxRWUKGEs/V922WVByxnviMgMoDtQUUR2AY8CKQBKqdeAeUAfYBtwCrgpNpImDiv/OMhz32xl1sjOpvv/PHiKnHwrjapkeik/rpab3s8t5eDJXBbd1436lTLc+o3/YjPvrwr95v3feVtCPiZUfClh4PTh8kXXRpUY2+9cej77nVt7Vt1ytK1Tjte/22563GYXC1uqJYlcm4XKFykWIc9q3FN7nVuFzPSUoBUxM9btPMw3m/37gW0Y24syJQ0L1OlcK0lJkJZsoXWtsozq1ZjW44335pM7zqNNrbLMWb+bzvUrUrVMOrdOW83CLf+4jfefPk347zzPiosGOyb24djpfBSKsiVTGTP7Z2auMVxBL2hYkaGd6jDi3bUccwkoKCwJrYgZzvq6xFGiEKzlKlr4Uui7du3K0qVLmTt3LkOHDmX06NHccMMNbNiwgQULFvDyyy8za9Ys3n777ZDHBsPCFYqvV6lSpRzbw4YNY86cObRq1YqpU6eyZMkS02PS0gzrgcViIT/f/J+evY9rv0T+k6OUGhxgvwLuLCJxzgoGv/EDAL/+fYwmVUu77Zvx4588+InhyL3ovm5+xzloi2q78JnveGJAc4Z0dDqEh6OEAXy16e+Q+rerU461O72tbXaeGNCchz7d6OX0Hg52J/y9R0872n57/BIEwZIkFCjlUxGzs/bhnhw6mcvFzy013b9l/CXkFxSQmZ7iUIIbVsng8CmnQvJIv3OZ8OVmsuqWM7U0ujK0Ux3e/WEnt053XxqcelMWe47kUDEjlRHvriUzLdmhhAGUSHW/FpYtmeqwyrWtXQ6AAW1qOvZf1a6WQxEb3KEW4y5vxp4jOV6K2MQrW3DeOUbOStf5/ndVS/57ZQusBYrU5CQKChSLR3Wncqa5xTUcEtpHTHuIaSJJ165dmTlzJlarlf3797N06VI6dOjAzp07qVy5MsOHD+eWW25h3bp1HDhwgIKCAgYOHMiECRNYt26d37E7duzId999x4EDB7BarcyYMYNu3fzfbAAyMzM5fvy4z/3Hjx+nWrVq5OXl8f7774d8zoHo0qULX3zxBTk5OZw4cYK5c6PnbKtJLE6eyeefYzls2XuMIW/+4OYjdPiku7VhTfYhhxIGhoIVLA99upF8m5XHVzqCSFO+VCof334eGx41t4Rve+JShnSsQ/akvtx9YYOw5xnaqQ7fje7ueF0ixVBSqpdJJy3ZQmpyEpYkIcXifasf3KGW2/Jt6RIpNKySSfakvmRP6sv/BrZw7BvYtiYlUi1kprv7RV3eqgYpSc477S1d6pE9qS8fjTwvoOzNqpc2be/euDLXdaxNr2ZV+fLuLnw7KvB18KPbOjPnzvNN96UlO8/9slbVSUu2eClRV7eryeAOtalToZTn4QBYkoRU2zhJSUK9iqUcAQaRIKEtYnZn/QT+064pQgYMGMDKlStp1aoVIsKTTz5J1apVmTZtGk899RQpKSlkZGQwffp0du/ezU033URBgXEDmDhxot+xq1WrxsSJE+nRowdKKfr06cMVV1wRUKZhw4YxcuRISpQo4ebIb2fChAl07NiROnXq0KJFC79KWzhkZWVx+eWX06pVK+rUqUP79u0pU6ZMROfQJCbNHl3g9trVR+hMvjPnlVKKq17z/m6HwpvLd1CzXAkv369A/LtnI55buNWtrde5VRzpImqULcHuI6e9jhtzSWMA0lPMbR3JLopRyVTv2/DwC+rxxrIdPuV6pN+5ZNUt55U+oUyJFO67uBF9Wnqns3t9aDvH0ueGsb14csGvbsqvp7I2KKs2Yz42lN/kJHOzRuOqmTSrbvzeR/du7FNeTyb0b06SyZjrx17s9rp5jeCuJeVKpVLOI+LUTqqLIpZk0wlKpSWzeFR3SqVZqJxZuGjbSJDY6Ste6MbmA/nUuecbalcoGWXJNNEgHtJXaPxz4sQJMjIyOHXqFF27dmXKlCm0bRt+tZ94Sl8RDc729BUzfvyTd1fudPNNMsO+3DZ77S5GfeQd7edKxYw0v1F313eq7dOp+7Xr2zHyvbU80u9cOtYrj4gRAXn4VB7Zk/py6QvL3CIVtz5+KWfyrRw6mUvNciU55z/zAOOGn5tv/PHaMbGPIy+Ypy9bhVKprH3EXeHw7PPl3V18OtGnpyTx64RLfZ6rL07nWmk69ivOb1CB92/txNjPNjJ95U7Hfvv77crCzfu4dfoaJl3Zgms7OPPf2eXNntQXpRSb9hzzUppunrqaRb+6+2a5zvXx2l3c95F3Ko1Iszr7EFfbFPkNj/aKaLRjsAS6fiW0RUw5nouXsqnRFCdGjBjB5s2bycnJ4cYbbyyUEqZJXPzlqvKFtUD5VcLs+bmynvCOuNswthetxn8N4Dey7pLmVb0UgKX39+B0nmGVy3RZgvr8rvNJTU4iNTnJa5lu3SMX0/7xb8jJKzBNzlqnQkl2HjzF7Nv9L9s1rpJJs+ql6d64EoM71PZy4DdbZgyGEqkWlt3fw5HzLCmI9E49z63Covu6Ua+i+5LdO8OyKG1TaETE1HL1ypC2NHnESGybVbcceVbF+r+O0KVBRWP+InKMsi9N1ipfIiZKWDAktCJm/5oVM6OfRlOs+OCDD2ItgiZO+OvQKY7n5DN77S7GXNqYtGTDZ2neL3t5asFvIY/37Rb/0XT25aifH+3F699t564LGzhu/q4O157Yncpdl61cyUxPcShamenO26S/TOoZacmsefhiCjxuOL9OuISTZ/IpXSKFo6fzTNNq2B3XAa7vbGSzn3pTB8Db2tegcobX8cFSq7xzZchVD+veuJLPYzyjTgF6NKkccK70FKdTfZkSKbx5Y5bb/mAUwUhg/4wL/AeDxpSEVsQQW/qKWMuh0Wg0Cc7hk7lc8ORix+tzq5fmqnY1+ftoDne87z9YxRePfbE5qH6Z6SmMsvkoLbqvG/uO+U8OOiirFhO+3ExBQeC7Q3qq/4jlW7vUo3HVTADTcj7pKRaHUuIrt9lDfZs6FLFUi7uC8ukd5zHm458d5Xseuzwy0eOuitAVrUMrkRYqD1zaxKvN4sPvLNKk2iyI1iA+61iR0FGT9tMrbn5wGo1GU9zwdGof9dEG8qwFdHtqsY8jAuPpCD/1JqdVxbVGoSv1K2XQ+ZwKfsctlWrhpvPrBlwmBGhU2VCyXvJR+ujhfucGXerGF+kpFscSmufSY63yJd0sVpGK1nPVgyKdKNuTc0ysahYPi9jwC+pFZW67Rcwax3pAYlvEwJZJTKPRaDTR4tDJXDfHbztvLNvOmfzIrAltGX8JJVItZE/qy9Kt+wMqWwDf3teNi0xSXYhI0HkJ7+xxDq1rl6VbI9/Ld5HA/j6Z+YC5Wq9KBrDQBYvrmK551iLJB7d25Ke/jpj6zHlGTT7U99yoyGA/z3hOZ5XYilg8v/MajUZTjNh//AzLft9Pj8aV2bTnGOkpSbSvWx6Aa6eYp5d48qvQ/cJ84ZrIs2uQSpGZJSbUyLxkS1LUlTBXzBQx12U8s3QX4eCqHJUraZ76obCc16Ai59mc8z0pKh8x+7ndf4n38mi8kOBLk/YSR7GWQlOcycgI3znWH/v376djx460adPGUWg72gwbNozZs2cDcOutt7J5s7cPztSpU7nrrrv8jrNkyRJWrFjheP3aa68xffr0yAqriSuynljIvbM20GbCN1z/1ipHbq8DJ86wdV94JW4mX2e+3AeQk2f1uS+RSU32VlDcFbFIWcSc22k+8p1FE1d9891bOkRtHrsV9ap2NQN3jhGJbRFzxk3GVAqNxoxvv/2WJk2aMG3atKCPsVqtIZU78sebb74Z9rFLliwhIyOD884zfGxGjhwZEZk08cn6v46Ytu86fMp06S8Ynr66Ff1aVic92ULFzDSqlk6nQClHzUh79OPZRqClyXDTV/gbMzVCY4aCq0XOnhT2bCXhLWKg01doIoNSitGjR9O8eXNatGjBzJkzAdi7dy9du3aldevWNG/enGXLlmG1Whk2bJij73PPPec21vr167n//vuZN28erVu35vTp08yYMYMWLVrQvHlzxowZ4+ibkZHB2LFj6dixo1v2/C1bttChg/OfZHZ2Ni1btgRg/PjxZGVl0bx5c0aMGGEasNK9e3fsiUXfeecdGjVqRLdu3fj+++8dfb744guH1a5nz57s27eP7OxsXnvtNZ577jlat27NsmXLGDduHE8//bTj3Dp16kTLli0ZMGAAhw8fdsw3ZswYOnToQKNGjYrMCqgpPP1f/t60vcv/FgflA3ZXD/cyPhc1qcyVbWoARq6q1rXKUrVMOtXLluBfhSj5kwgkmyTYisYynt0iVr5UqmmW+2jjOmOajzQiZwsJbhHTzvoJxfwH4O9fAvcLhaot4NJJQXX95JNPWL9+PRs2bODAgQNkZWXRtWtXPvjgA3r37s1DDz2E1Wrl1KlTrF+/nt27d7Nx40YAjhxxtyi0bt2a8ePHs2bNGiZPnsyePXsYM2YMa9eupVy5cvTq1Ys5c+bQv39/Tp48SfPmzRk/frzbGE2bNiU3N5ft27dTv359Zs6cyTXXXAPAXXfdxdixYwEYOnQoX375JZdddpnpee3du5dHH32UtWvXUqZMGXr06EGbNsaSUZcuXfjhhx8QEd58802efPJJnnnmGUaOHElGRgajRo0CDOuenRtuuIGXXnqJbt26MXbsWB577DGef/55APLz8/nxxx+ZN28ejz32GAsXeifi1BR/Jl3ZggdcakN6JhGaeGULnzf/SFl87HSsV55VOw4BhhUu3ilXyjv/WTQMVnaL1ACbQlzUuFrESqRExspfXElsNVTXmtREkOXLlzN48GAsFgtVqlShW7durF69mqysLN555x3GjRvHL7/8QmZmJvXr12f79u3cfffdfPXVV5QubV7g1s7q1avp3r07lSpVIjk5mSFDhrB06VIALBYLAwcOND3ummuuYdasWQDMnDmTQYMGAbB48WI6duxIixYtWLRoEZs2bfI596pVqxxzp6amOsYA2LVrF71796ZFixY89dRTfscBOHr0KEeOHHEULL/xxhsd5wFw5ZVXAtCuXTuys7P9jqWJD46eygvcyYMSHn5MXg7mfgwwyRHWOuxKGBjZ7eMds1xjdotYpPzDXMdMtsQmqs1VD4+FRS6eSGiLmAAiyivTsaaYEqTlKlr4ykfXtWtXli5dyty5cxk6dCijR4/mhhtuYMOGDSxYsICXX36ZWbNm8fbbb4c8NkB6erpPv7BBgwZx9dVXc+WVVyIiNGzYkJycHO644w7WrFlDrVq1GDduHDk5OX7PzSy8HODuu+/m3nvv5fLLL2fJkiWMGzfO7ziBSEszbjIWi4X8/PxCjaWJHtYCxcj31jKy2zkMfHVF4ANcqF4m3ev7lJacRPakvry8eBuvf/cHlXwkNoXoWH/sxNxyakUAACAASURBVPP9fnCH2sz48U/TCEa7s34kxbd/RL4Kekcb0WkNHCS4RSz+M+pqig9du3Zl5syZWK1W9u/fz9KlS+nQoQM7d+6kcuXKDB8+nFtuuYV169Zx4MABCgoKGDhwIBMmTGDdOv+ZxTt27Mh3333HgQMHsFqtzJgxw2FV8sc555yDxWJhwoQJDkuWXemqWLEiJ06ccERJ+pt7yZIlHDx4kLy8PD766CPHvqNHj1KjhrF04RpUkJmZyfHjx73GKlOmDOXKlXP4f7377rtBnYcmvth79DTfbN4XshLWtFppvvp3V69b7K9/G9+VO3s04OdxvX0q/hDdtAYNKmVGbezCMvHKFmRP6muacT4aWejzbfdFM5+0oqCIslcUCxLeIgZoi5gmIgwYMICVK1fSqlUrRIQnn3ySqlWrMm3aNJ566ilSUlLIyMhg+vTp7N69m5tuuokCW4GziRMn+h27WrVqTJw4kR49eqCUok+fPlxxxRVByTVo0CBGjx7Njh07AChbtizDhw+nRYsW1K1bl6ysLL/HV6tWjXHjxtG5c2eqVatG27ZtsVqN1AHjxo3j6quvpkaNGnTq1Mkxx2WXXcZVV13FZ599xksvveQ23rRp0xg5ciSnTp2ifv36vPPOO0GdhyZ+8Kco+WLBPV2pW7EkackWr5tsqRCW1MKZ2x+jejXi6a+38uClTfzWn4xnoqGc5tqCLPKssSnCqPUwJxKt8j8iUguYDlQFCoApSqkXPPp0Bz4DdtiaPlFKuXske9C+fXtlj/QKxOFXLmbr38dIufUr2tYuF+IZaOKBLVu20LRp01iLoSlCzD5zEVmrlGofI5EiSijXsKLmyKlcWo//xm+fnk0r83j/FnSa+K1bu2ui1Lk/7+XOD5xW4JvPr8fYy4LLnP7O9ztMa0yGmojVTm5+AW8s286tF9RzFCEvbsz7ZS93vL+OkqkWNo+/JCJjnj9pkaOEVLjvbWFY8ccBrntjVczmL0oCXb+iaRHLB+5TSq0TkUxgrYh8o5Ty/IUtU0r1i4YA9jXoYAq7ajQazdnOS4u2BewztHNdqpZJd2tzrQEJhVt28rT+3Na1PtdkhV/LMTU5iTt7FO+UGNEo03MmP7YJc4sqs35xIGqLw0qpvUqpdbbt48AWoEjjZO2fs/YR02g0Gv88v3Arby3f4XN/h3pGOSO7u9KoXo0c+7o3ruzWtzC3WE93qD4tqpmWKjqbcDjrR1B5aV2rLAB3xyhvm1bDnBSJl56I1AXaAKtMdncWkQ0iMl9EgqvCGvTE2lk/EYjW8rkm/tCfdex4fuHvfvcPam9YpRpUNpQifykHCqMweB4b6bxixRH7WxBJ5cW+TNu4amwCGCLtC1icibqzvohkAB8D9yiljnnsXgfUUUqdEJE+wBygockYI4ARALVr1w5+boxak1Z9cS+2pKenc/DgQSpUqKB/uAmOUoqDBw+Snp4euLMmojwx19sny5U+LaoysF1NBrrU67P4+T16LnvVqxh8/i7PJatY5bmKJxzvSRTeiljdHu2nVLaYBlBEkqgqYiKSgqGEva+U+sRzv6tippSaJyKviEhFpdQBj35TgClgOLqGIADgDNPVFD9q1qzJrl272L9/f6xF0RQB6enp1KwZv8V5E5G1Ow/zxjLfS5IArwxp59V2UdMqTJz/q2n/77c5L+EzR3RyLGsGg6ehTf+Pjm4esVi9vfZzKZ2uFbGoKWJimC/eArYopZ710acqsE8ppUSkA8ZS6cGIyWB71s76xZeUlBTq1asXazE0moTlWE7omfPBuUSZYmKxclWeOtavENK4nhaxjPSEzrIUFHbrYyRXBexjxcodQBznFJPp44pofsPPB4YCv4jIelvbf4DaAEqp14CrgNtFJB84DVyrIvitELEtTWpFTKPRaEyxWsO/Pj45sCVZJtauzudU4KO1u8Ia035jrluhJG8Py6JG2RJhy5co2JWWSOZ1jbX+E8XV1mJH1BQxpdRyArzHSqnJwORoySDaWV+j0Wh80ueFZew8eNKrPatuOVZnH2bRfd2olOm7HJGvtBID2tTgwIkzXNq8Wsgy2S1iliSh/lkeLWnHroBFMuWDfahYJTy3n4lOY5HgmfXB+LC1s75Go9G4s2nPUTbv9YyfgmuzanH/JU34YfvBsBUhEWFE13PCOtZecUffoJ0kRSF9xZk8I6N+rJLcijaJOUhoRcz+QWuLmEaj0bjT98Xlpu2TBrYEjPxdsSBJ+w554bSIRW7Mh/s1JS0liQubVA7cOQokaT3MQYIrYtpHTKPRaDzxFcDkbxmyqHD6Q+lbtJ1oOLbXLFeSF65tE7kBQ8Re+UanJSqihK6xwv5Ba0VMo9FonHzw45+m7asf6lnEknjjsJToG7SDpARUTiUKVr7iSkIrYuilSY1Go/Hi4TkbYy2CT5KiECFY3ImGs368kIjnFCoJrYg5lia1s75Go9E4aFTF2wn/2WtaxUASbxJZ6QiXRPSbS6RzKSwJrogZn7RO6KrRaDRO6lUs5dV2ResaMZDEm2jkzCruSAIqp3b7iF6CTnRFzPbQJY40Go3GyYJN+7za4kXxcdZVjBOB4oBEXq5NwFMKmYSOmkQndNVoNBoH1gLls6RNvFgmHEW+tUuJA+fSZHx8RpHA/vEmJbQ5KDgS+i0wLGIqZpmDNZpiy96f4edZsZYipojIJSLym4hsE5EHTPbXFpHFIvKTiPwsIn1iIWconPOfeTR4aH6sxfBLqsW4LemrtpNEzLllvy9LQp1VeCS0RUySkhCUXprUaELl9QuM55bXxFaOGCEiFuBl4GJgF7BaRD5XSm126fYwMEsp9aqInAvMA+oWubARoHQcFdZOtmkd+g+0NwlkEHMo2om43BoqiW0Rs6SQjFU762s0mlDpAGxTSm1XSuUCHwJXePRRQGnbdhlgTxHKFxFqly8JxNeSV0qycVsqKIixIHGEU2mJn8+psBQ4vfVjK0gckNiKWFIyFgq0RUyj0YRKDeAvl9e7bG2ujAOuF5FdGNawu4tGtPDY9s8Jr7ZP7jgPMApsxwspNqchbRFz4ljGi5+PqdDYDSSWBDqncElsRcySTAr52iKm0WhCxez24HkhGQxMVUrVBPoA74qI6TVVREaIyBoRWbN///4IixqYo6fy6Pnsd17t9mtjPFla4kkpjBescfg5FRb7OenPO8EVMZJSSJYCndBVo4knTvwDe9ZDgTXWkvhjF1DL5XVNvJcebwFmASilVgLpQEWzwZRSU5RS7ZVS7StVqhQFcf1z/Eyeabv92miJozuBPYpOW8Sc2JdpE0lpsX/3Ekm5DJc4+vlFgaRkLFj10qRGE0/8PAumdIPck7GWxB+rgYYiUk9EUoFrgc89+vwJXAQgIk0xFLGiN3cFgVkKn/FXNKNKZjpXt6vJmzdkxUAqc+xRdFoPc1KQgEqL/fNN1muTiR01SZKFFO2sr9HEGfH/e1RK5YvIXcACwAK8rZTaJCLjgTVKqc+B+4A3ROTfGCc1TPlK0hVj8qzeYpVMTSYpSXjq6vgobWTHbvSJyzcyRtiDKoZ2rhNjSSJHx3rlua5jbe7s0SDWosScBFfEkkmWAo6dzo+1JBqNxk4xiZZSSs3DcMJ3bRvrsr0ZOL+o5QoHM4tYnjU+wxLtXwu9NOmkXKlUsif1jbUYESXZksR/B7SItRhxQWIvTVpSSKaAU3lx7Yui0cQvUc0hEN+KWCKxfNsBr7Z49TeqUdaw/vy7Z6MYS6LRFA2JrYglJZOMlbz8+Pznp9HEPSoaf2K0paMo+X7bASZ8udmtbVSvRlzZJj6KfHtSItVC9qS+XNaqeqxF0ZhhzYM5d8ChHbGWJGFIcEXMggVr3JrgNZq4JxqRjcVkaTJRGPLmKq+2uy5sSHI8hUoCnDwAW76ItRSxYe/PsGttrKUIjp3fw/r34fMw0+bt+cl4aBzE2S8xwiSlYMFKrlbENJrwiIpFzI5WxDQuTO0HM6+H04cjM96JfyA/NzJjRZvXL4A3L4y1FMFh/3OWZHFvP7Ef8s8EPn5Kd+OhcZDYitjOFaSQj8o7HWtJNJriSVRyfemlyaLis/W7Yy1C8OzfYjxbIxBcVVAATzeEOSMLP5bGHYci5hHr93QDmDm06OVJABJbEfvrBwAq5e6KsSAaTTFFRcGarJcmi4zZa4vjtS8CinqBLYHtxo8LP1akKLDC41Vh7bTIjHd0N4wrYzx2rnBuT6xd+CRsz54LK18235efYzwnp3vv+32BscQcDN9OMG8fX9E4j9NHghsnAUhsRezSpwAosOqoSY0mLEJRxM4cD3EpSCti0Sbixbx/fAP+/MF8384VsPrN4MbJOw3zx0DOMe9934wNLlr31CGY/4D7d+73b2BCZd838aVPwb7N5vuizcZPIP80fPVAZMbb4VKyaunTzu0zR8P/A5VzDObdD8d2w4L/uLfPH2N8bvblx1+/hON/w4KHYN27zr671kDOUVv/HN9zLbPJfPxv+Pphp6XNrkQf+dP3sZs+hdk3w6Y5wZ+bUrDwMeNzWDIJ3h0AXz1otK98xff3ughI7DxilYzw51Srd7FbjUYTBE/Wg6vehuYDA/edWBNqdoBbvwnQUS9NFhUHjrv77Ey+rg3lSqYGd3BejuGY3eAiZ9u8Ucbzw/9Acpp7/3cuNZ4b94ET+6B6G+e+rV+D9QykZUKN9rBuOqx6zRij3U3GDd7OhhnQ4ipo0NO/fIsmwJq3oWQFaHgxVG8N719l7Huti3f/3FOw6HH4/kV48C/v/f7451ewpECFc5xt1jz4YzE06hX4+L0b4JNbjW2lDD+4f7YY1SWCcZ1Z/SbUaOf+noqLj1b2cvf++zZBtZbe4+Tnwo6l0LCnoZCmlIDy9eDANkN5W/8+/Pi6s//OlVCqIiyZaFgXy9aBE38798+/HzZ/5j5HQZ6h6Kx6zVCIez9hjAHmrg6f3QnbFkLDXlCvq7P9kxHQ/QFo1t+9/7aF8NEwY3vjx9DsqMvcVudYnn9CDm6D5c+6t/2xCM6/BxY8aLz+9yYoU9O9z6EdxmdU5Vxv2SNEYlvE0kobT1oR02jCZ807wffd9WPgPnppssjYvNfd4tSvZXXOb2BaDtObrx+C96406oLmnnRf7vryXt8lql5oZThjW/MM68mfP8AHVxuO+NOvgNk3gdVmxco9BS+2hlc7u4/x0c3ur83mslt9Fj9ulMza7mIhOvmPczsvx7hB25XIM8cM2Xxx5rjheO465ysd4aW2hiKTe9J4XvyEcV5bvvQ9lp3XXRSM/NPw3kBDcX3/Kpjlx68qPxcO/A5z73N3cM89Ca715a0eTvKvX+DczjnqtDwuHAfvD4Rf5xnv+YutjfbJ7eDlLPjLI8L2nUsMy5F9iTfnCHz/gsu8Ju+jNdf53v38IUzpYXwP8s94K0J5OcZ3AAylzVVR278FPrrR/Xu3a43x3vlixYvwwTXw61wPmfLg1EHzY/JdFOHZN3vvN/t+RpjEtoillwEgzRrXNe00mrMUrYgVJR+NDPFmcnCb8bxnnaHo9HW5ia5/z3jc8wuUre1+nF3JmtId9m30Hvf3r6G2TZbVb5jPfcbFyrH5M5h1A4z4zrB62fF0Fp9+uflYT1Txbnv7Ehj+rXf7P78aSpedh/8xrFmuY6kCyKwGx/cabTOHwH1bIdNkHl/sDjJVxeMmBeKP/w3PNDYsZP7IPQmppWCS7fO5ZSEc+M3Y/nCw+TF/rvRuO+piPfzuf+77PD8DgL9WO78DAEf/hMcrm8/3RBXDig6GQtr0MpPzOGFYUsHc/+zMCUjLMLaP2GS1fzZ2Xu4Ih/4wl2FyBxfZvVO9FAVRU8REpBYwHagKFABTlFIvePQR4AWgD3AKo1bbuogJYfvw0rUiptHEEXppMha0rFkmtAPsFpf9W41nTysDwME/vBUxO2ZKmJ1tJkqQJzlHnUoEGMqLqyK24cPAY/hi9xrjef4DsOpVaHEN7F0PB7a693u+pftSnN0K53mj//A655h2ek+EjiPhjR7By3VivxF96IsVLxn+VBBYmdv7M3x6m/P1Wz6Wepc9E7x8nmz53Ltt1avBuTLYcbWim+WR27Me6tksfGbpdCbWgHE2xd2eUsNuWftltmHR96WEgbc1EQzL27xRMKRogj2iuTSZD9ynlGoKdALuFBHPRdZLgYa2xwjg1YhKYFuaLFGgFTGNJqq4OkAHilSz62F6abJISQ0lgeupQ4b/DBj+XAB/mChP66Y794fCzuWB+/ztocjNvRcm1TGUlRP7DUtJYdj0qaE0APwyy1sJA3clzB+eShgYfkd5pwwFL1j8KWHgVMKC4Z1L4MjOwP2+HR/8mMESybQ30/oZ15c3LnRffjbD/ufBrjB/fEtw3zVPZl5vJJ3dtdrZ9v0LxjJp9nL4bX7oY/ohaoqYUmqv3bqllDoObAE8a2pcAUxXBj8AZUWkWsSESE7DKsmkF5yK2JAaTUw5ttfcMhFrXH0odposb7jh0MSiJY3GhKAjKA9nw7NNna/z/PyR3fRJ+BnWA7HpU++2nCPwXDPjRllY7A7f0eTvn6M/RzxyODuy473a2bAAugYSuLLuXSPdhl1RK8wS4zdjvS2e9vY962BqX5hxbfjjm1AkPmIiUhdoA3i+OzUA1/CVXbY2t3dBREZgWMyoXduHGdx8Ys5YMiiplyY1icLUPnBoO4w9DElxGmuTF+CPz++2qEptEYsaFz6zhO37w7zuvdAq9GOikfjXl/+Y9YwjR2TcY48kPdsIxQoYCT6/y/315jlG9GQ4uAYjzBjkvs81OOGPxXBOCMvOfoj6lVxEMoCPgXuUUp5JY8yuxF4OJEqpKUqp9kqp9pUqmTgv+iHXUoqS6hSqsAnuNJp4wFFoN4Lf5+N/G5FikSLnqP/9wURWasLGWqDCU8JWTDYSaYbD+PLhHafRRAt/0ZXh8nZv57Y9mCUCRFURE5EUDCXsfaXUJyZddgG1XF7XBPZEUoZ8SwlKcgZrgVbENMWAkwfhm0d9l3mxW5Ei+cfimcbu4fX+QvtdWT8DfnrPu90sksqOm9zaIhYNPt/gXdboqatMckrZWfmKoYB9/VAUpdIUCVVbxFqCs4dAlv8QiJoiZouIfAvYopR61ke3z4EbxKATcFQpZbI4Gz75ySUpwRld+FtTPJh/P3z/PGz9KkDHCP+xOLTdRYYxHlP5mGvOSCMZoycVG3m3WfNh2uWQvczZppcmo8KJM+7LhC8MasXVSUvMs9iDM5mlJvqUCcG1JhQqNIBOd0L97tEZP1FIKwPV20ZmrAj+GY6mRex8YChwoYistz36iMhIEbFXYp0HbAe2AW8Ad0RaCGtyCUpJDnn52iKW8BzbG6Ui1UWIvY6bKjCyXc8d5VHuRZz7o0W4vhV2ynjG5GBEn+34DmbfUrixNQF5ZI57tGHmgfWGD83ce707B5PVXRM5araH8ucE7hcqd6+FS/4bfP9xR6HF1ZGVwTUrfrQxq3MZDL3Gw3WzzPfVvcC83Rf+LP8hEs2oyeVKKVFKtVRKtbY95imlXlNKvWbro5RSdyqlzlFKtVBKmcQAF44Cm0XsjK43mdic+AeebQILH421JIXDNev8zCGGw/L+X537o7E06cofi71D3kO1XBWYLavaxnBN9KgtYkVCUq7N/88sGeasG4pWmFjieqNt6iP5K0CbCERk+kIEmg2I7JjXu3r9BPGbuthWbDuQIvHoESOhrSvpZZw5uzyp1w0saeb7QuEKH8XGXek4MnAfM1oPgYxKznPIdEnSMCyICgmuFAdFLF44lJdCKXLYuDuAA7GmeGMvX/F7oDqHAcg9ZeRQMiMvx8hfFFVcUjuYlgISj34hsvpN77p0rrzb37stVKXPzCppP4ccH8WYNREh38QFw2r/E5pkgen9jSViMErX/P51EUoXBI37hHdcxcaB+yS51Gb09yegfH3n9r9+gvt3wL1bIKWk0XbvFt/H3rUW/rUeSptYhcG48V/4MCSXcLZdPB7u+8352uJRC1QC3KZDsY7f9BWcZ0s3YqZI/HuTy7xi1AK9aw3cNN94H1z3ux23GbrcC5dMDF4WX+QG4XuV4SNTfyAsKc7t+3cY5xaI4YvN2wN9LiGQ8IrY70eTyJDTfLUxyMR8mrOb17oYha7NeHdA4ISLhcWv8uXSHs7SZH6uUbNuat+wxXOwy09W7yN/Qvb3MLGWUdwY0I750efHHYdo8NB8LFgZlzyVahh/TiqWst1wty2E7YuNJeJNc5wJW+OFKi2glElU/OAPjYc/bl0Ine6Ayh45wy+4z7mdlALNrjS2k9NhmI98fK7LXuXrQ8nyULo63PGDkWm9dHWnUnalS4qNpBSo2MAoop1e1nzsCx8xfsP2QuENekLnuyCzqrPPqK3Qcxz0fxWu/cBQBut7pEm4fLJTTjMLdKUmcMkk7/Y6nZ3XEFelxE6ZmsZ8rla2ig2hznnG+2AvNXTzArh9hZFB/84fDXeEpCR3ZTcQvZ4wbxcxFMY7/KQo6Xh76EuJnpQs7yyNZOdWk99EDR8+ZamlCje/C4ldaxLILF2aEodzqV62RODOGo2/Uhh/rnB/vfFjKFvH8PuIGHZFLAm/Vq9wliY3f+bd9mcQiQ/NrAdvXui7/8rJxgNg2bOG875rhmpNVLjnw58AyEr6jWHJX9NQdjMk7yFapf/j3fmjG6MrTP9XYc7t/vvU62bc2H+1LQk17GncYNdNc/ZpOQgaXwr7bJaYSk2hVEUj6CNrOJx7uWHVSy9tWGOO7TWCD1oNNvpcNNYomr3lc0PxqNbKSEKbUcVRi9iLpBQ450LjD4Ur5eoYD4Abv4QNHxh+Vn8sNrZvdgmwMfvNXDsDUmzKU++JxjxXTPZWXkqUgy7/9nivLjCUaDtth8Jv84yH65+yLv82akNe9oJxfl894HyvPZdcxWPeFtcYz+Xru1sFzajdyXi+6m339mD/IP7rJ9ixzPf+On7qomZWM5S+1te5B//cu8U9EXGwDJ4Jf/9ibNcMUL/TlQj66Sa8Raxl3aqUkFxa1QgzP45G44vZN8ObF0V2TOWyNGknUkuTFpP/XUufCnycr6VaO64Rl56seNFwFHe9uWqiQq7V+E6UxSj9Y5ECynACvnmk6IXxrD/5r5+8+9z4OVz7Plxk8+tUyiicfanLd9JuCSpjy3LU8Tbn0nez/oaDeK/Hnf1LV4Orp0Kj3s723jYn9rY3GAoWwLlXOGXMcLFGgfE7Gfqp4QDvi5rtoO8zxm9zwKuGz5HrH7LOJtHEDVyuFWVqwFVvQUqQBoJvJ7iMfZfzfACquqQmKVneOH9PJbPjSGh5jXub59JajwhEz9bp4tw+5yLo87T7/tK2upDl6/te2nN1+u/5mHP7HlsQiv041zQ7o3539/cyle188/bGl0C30c7XlZtBzSxjO+tW3+NFUBFLeIuY2P6BFOSbFPbUaOKBAqsRvZaWgdMiJuZWr8IsTXouDyoF24LwqQs014tt/ExpMS/Uq4k4B06cobVs47XU5wHolLSFDekjojvpuKPmSWBdlxiHznFul63jOxjE/j1zrRhhb0sv7XSwPrTdsE57KlC+KFvL3cHcc3vTHMNCmFLSyA2VZLJkFyqtrzMeAO9fA78vKKRPkcu1oLdtSa/xpb4d5z05sBXw8L/ztNqlZoYtnYNKjQwF7I9vDeWvUS/oMNxwUfhfXXdlyPX96HIv9DQJtOpyjzMAy74UaFfU7NHZ/Z53+ozZ3495o+HHKcZ3xF4vNNj3/44VgfuAVsRCwuYUqXSYtiZa5J6C1JLhHz9vFKx5G8YeMreImW2HszTpeeE1i6Izw1OR2rE0+DmbDYCNs4PvrykU76Q+Gd0JbltmpFh562Jn2+0r4NXzjO3rPzZyNVVqbFgwDv1h5LY6YqtkV6qityJm9zuyP7veMJNNovAuehRaXWv4Y0WCZv2h/FL46X2jlmFaBBQSV66eCsd2m/tkFRUnTYKMTuxzbl/3kRFNGAlK2qospLj42pUoZ/iSla3jbHP9nC/0U8x89B/Gd65kebhjleGDB4Z/3W1L3S2CduzXxxJlnYpYKP5rwaAVseCRFK2IaaLMD69A11HhH//T+8azNQ83i5grq9+y/au1owwr2pkT4V9APf8hrnvXvN/xfe6vZ4XgX6SVsKjy+77jXPzcUh68tAkjLF9QTk5EdoKe4wyn6LxTgEA1202v9fVQu6Ox7eq03KCnc7tsLeNh3+77LDTpC3s3GAqZnbbDIPckdLjNeO36vTRL9WBJhirNCndenlRrZeT3KlvLf2qLcEgtaTi8B+L6jyE1I3C/ULjnFyM4p+c4731bXSJm7cEDkaDP01C9jbczfSWPyFb755xcwr+S5PpdqdzEfV81H3VR7SXbWg02rHM7lnr7xIXCbcuMPxCuxea1IhY8kmL7R6WXJjXRorBJZJMsYMWIfjK1iOFMxmmP1lIKpvaD3Wv8L08oBQvHGcsYnmN6KnuehXPtnPEcXydHjheeX/g76ZzhxwXv81bqjMgOXqGht9O4nf4uuZ5K14BanfxbNQCybMl8M018ss7/P+dr+w262YDIWzH8kZbhTO0QC1yV2EhRtrahjBUlJcqa+8h5Yv+c86NgJPnZFmW75QvoNsamiBUicrtaS+efEDCU9rTShZPRhYRXxOxLkxu27+ViH756Gk2hKGw+GfvxBfk4LWI4t5e4Zsx2WZrcvca57esik3vSKJm0/n3DIuFKuElhI5G0UVNopq/MZu4ve5mc8jr9LH5C/UMhWJ8jVywpcMuCyMwPxs0TYNOnxrKexrjpn/FRoipcTBMvFyHRTOhcpTns22jzVfTx5zZcml4Og3ysHoRJwkdN5mDcNJZu/jNAT40mTAr7+7abzH1ZxDZ96tLXJGpy/Qfm4276FCbaHFpP7vdOFBmuad1zHE1MGPuZkdKhhfiJWi2OHN4ZuM/ZRjAWKx7OUAAAIABJREFUplCxxniVyF7OLRrYra+R8nuz88hBuDryEeAJr4hlZBrmwxLkBuipSQiiVfrHL4XUxNyixgJETZrVmjRzxAX46T3zeUpW8B4jEK7Z+JP9KGIRLPsRKUTkYxHpKxLBVNhxQCP5i4Wpo6iTZJInLFTq94D7tgbuVxToKFtvinsNXTNyT0ZxcNu1zleuuHCxJLtH9UaIhLowmVG+rJHheGDLcjGWRJOwRMrErpRT+Xp3ABzd5XuuYBROz4u3/R+oPTz/2O7gZfvCxYfHn0XsnAjnVYsMrwLXAb+LyCQRaRLogLimwAorX+Gx5Gk0SNoT/jiuflmd7zJyeMUDrvmhNAZ25TTSgQTgO8N9tIlmFGmrwXDevwz/MDtxXNs24RUxe8K8ZKuOmjwriMmPLQoWMfCxdGBiEfNl6PH0AbEnhbRbrd7wKJvij4PbnNv+LqCR9mOJAEqphUqpIUBbIBv4RkRWiMhNIhLDnAJhsvFjWPAgnS2bCzfOxeOd2/F0j0pE609haTXY+AN18WOB+4ZKGR91MaNOFL90KenQa4KRiqRmluEr7ivwJA5IfEXMVo9r9e97YyyIJmGJmEWsIPil1VyXNAW+5vdUxA7+bjzb8/yEil2R82URu/CR8IvxRhkRqQAMA24FfgJewFDMClklPgbkhpGiIi3QEk0caWJ6adKbig1h7IHApYfCGrtR5MeMJ0qWh4f/NuplximJr4jZLGJ1rNmxlUNTNBRHHzH78Sf3B5bfPtVkl3Iqvo7xFRWVEmby2WW2ciW+/MAyqkQmK3mEEZFPgGVASeAypdTlSqmZSqm7gQgnbopTSlWEMdkwapt7+7n9jedI+9IUBnsuLR0UEl0q2vJ6eRZKLzJ0Ghw78edZG2lsFrGRyV/GWBBNwhIpH/Ap3XzXQ3NO5t3ky4Lga4mn0A71PhTPJIv7smWpynAyAo7khWeyUmqR2Q6lVCQrtscvdbsY2c3ByCtl9z/s/yo0vzLChesLSe1OhvW2938D99WEz7AvjWLXcew7dbaQ+Bax5PTAfTSawrDqtciNFY5Fz1f0Y4EPp2dVAMueNd8XDL4UT0lyV8RaXRv+HJGlqYiUtb8QkXIickcsBQqb/VvdM6IHi72wNhhJPu3LNKkljQLY8UTPx6DNUGhzfeC+mvDJqOxeiLyoicnqRXyiFTFNYhHpf3cfDPKdp8uOWfShUvDZnfDXj4HncAvjDnBxyjliMpcvRcyHRezkfvi2EE6/vqLrxOK+NBnuEmjkGa6UcrxxSqnDwPBAB4nIJSLym4hsE5EHfPS5RkQ2i8gmEQnwRYkAL2fB1vmhHXPTV1CqQnTkiQalKsAVkx1uJRpNopP4iphLzg+lNXBNqGz9CubcHvpxuSeMPF7vDoD9v8G4MoY1w5Ul/zPaXaMjw7KImbT9/YtvHzG70344uKbY8CQpyb0kTWEKoUeWJBGnhi4iFsCvA5Ktz8vApcC5wGAROdejT0PgQeB8pVQz4J5ICx4R6nSOtQQajQn6fmwn8RUxF6wF+oNPeGKlbH9ym285fvnIeHbNkH98n0fpIseBoc9tPQNH/nK+3rEMXuviUSQ8RMZkm7eveMm3T5okQZlaztfxYxFbAMwSkYtE5EJgBvBVgGM6ANuUUtuVUrnAh4DnGt5w4GWbhQ2lVEwd4madZ/ODDRghqdHEAfZrZItrYitHHHBWKWL5WhE7O7Hmw6lD0Z3j5w+NebwQ2LbQ2HRdQvzD1Hc8PEVy6VPwfHPn68M7Qh/Dk9RM8/Zf5/pe8hSLeykWf4pY0SZ+HQMsAm4H7gS+Be4PcEwNwEW7ZZetzZVGQCMR+V5EfhCRS3wNJiIjRGSNiKzZv99HJYRAFHgvQX9i7eLYTqtc3yi/MnJpeONrNLEgzce15izirFLEtEXsLMDMR2zeffBkPciLYm0zgCfru9wsXb5re36yNbncSOeMNB9jVxA+ZUVBkgWu/8S7/a8f3MsduSLivjSZ4sc/c6jJ2FFCKVWglHpVKXWVUmqgUup1pQImqzJzNvS8gCQDDYHuwGDgTdegAA8Zpiil2iul2leqFGb9ux+neDXVudVZxuryVtWhWX8oVze88TUaTUw4uxQx7SN2dvLLx8aztRD1RncsC9znzFHnHMfMEggX1fevkAELd601lCpfEVVmAQPg9Elr2Mt4tqQVTo4IISINRWS2zal+u/0R4LBdgMs6KzUBz3pCu4DPlFJ5SqkdwG8Yill0+GqMV1Pb2k69T1z/hARMg6LRxBj7/VinzzjLFDGrVsQSHjNl226JCvUH7zrWtH7BHWNPGfFKR99yxDsVG4R3nH3J8pp34d+b46kA+DsY9SbzgR7AdODdAMesBhqKSD0RSQWuBT736DPHNh4iUhFjqTKQghdRxNd3+qZ5RSmGRlMItCIWlCImIv8nIqXF4C0RWScivaItXKQ4lGlkENY+YgmM3efq0B8mO+2fe6iKWBiK0/cvur92vVEWhSI2bzR8flf05/FHSrpRv851mTK2lFBKfQuIUmqnUmoccKG/A5RS+cBdGI7+W4BZSqlNIjJeROyVlxcAB0VkM7AYGK2UOhi1s9BoEolkW+CyTlMSdGb9m5VSL4hIb6AScBPGv8wwMgsWPXsqd6XMsa3aRyyR+Xmm8WyWsiGslBAqPMVp6ZPGw4zlz0HLa6FS49DHDRYTPyK/JKdDfpR85yJVcaDw5IhIEvC7iNwF7AYCFsVUSs0D5nm0jXXZVsC9tkf8MWCKUaNQo4lHWg8xXDjO/1esJYk5wV4p7X/r+wDvKKU2UIzsiZJkwSIKq0nUkSaGnDkOiyf6iDYMEdfP9pDn6pBNEdu+xMjbdSiIqEJV4Ds6MBTOHHN//UpH+OGVwo8bKuLDOhXIl+j8EFJjeSq8Vh+Z/YueezDqTP4LaAdcD9wYU4lCJZxAk1aDoEbbyMui0UQCSwr0eBBSS8VakpgTrCK2VkS+xlDEFohIJuBXqxGRt0XkHxHZ6GN/dxE5KiLrbY+xZv0iwck8Q2d8f0WRum9oAvHtBPhuEmycXfixXIPgXmzjnrPLriCsf994/mtV4PFyTzrr8UWaBf+JzDgXPhJ83xFLoO8z3u1XuljQbl8JIz0iIi8uRAb+/NPhHxshbIlZr1FKnVBK7VJK3WSLnPwh1rKFxBPOagZ1c4wE/qdKembT0Gg0xZFgFbFbgAeALKXUKSAFY3nSH1MBn3l1bCxTSrW2PcYHKUvIbPjHsLi8t2xTtKbQhMNftnthfg6sej24ckC+8FyS3L0OVr9ly+FlU8TsFi5Xy82xPbBhpvd4k2rB5Ha+53v70vBljRSdPXzBtvgpbF+pCWTd6t7W6Q4oVREGvQeXTIIq50LVFuHL4+k4Hu10IUFgS1PRTnx6tRdPGudMZdklIZY60mg0cUmwPmKdgfVKqZMicj3QFnjB3wFKqaUiUrdw4kUGlVYaTkFpif0/dI0LezcYz0rBfFt+zXFHwxvLcxmxwApzPVx3fl/gfdy0y42SP036QlpGcHMpBX+uCE/OSOIZlThzSPB9wenD1fSyyMjjtTRZiHQhkeUn4DMR+QhwFPZUShVdMrPCUqkJ7P+VPwuMHGRnSI2b9CAajaZwBGsRexU4JSKtMDJS78QIAS8snUVkg4jMF5FmvjoVNiv1wA7nANBWClFjTxNFIhBE4WkR++Hl4I47Zk8NpSLjE1ZkSGhRiUkmP/VIRzV6jmcWOFG/O/R5OrLzBqY8cBAjUvIy2yPIfCRxwv5fAVivnKlFkhLLyKfRnLUEaxHLV0opEbkCeEEp9ZaIFNbZdR1QRyl1QkT6YOTkMQ3xUUpNAaYAtG/fPuS7doV045AbkxcAT4QrryZaBBvVeORPo5ah5w3o9GE4sjPcyZ0yLH0qyEPiIPo2yVL4RIh1L4iMLHYa9HR/3ag3zPXo06QfdBge2XkDoJQK5EZRbJia39uxXSIlbtKDaDSaQhCsRey4iDwIDAXm2hxgUwozsVLqmFLqhG17HpBiS4oYeVpcDcC31jZRGV4TBjtdlvaCSRPxwSB4vgV84nET370W/lc3xMlNFKkPr4MlE4M7/Eh2iPNFAUnitnfXhH/8g7ug4cXB9R2+OHCfrFshvbR7W5maMPYwjHFRkmt1CF7GCCEi79iCh9weRS5IBFinGjm2z29QIYaSaDSaSBGsIjYIOIORT+zv/2/vvMOjrLI//rmZBAKE3gXpsIACAQJYAFFRVBRQVHBtCMKyirq4gqCoCKuoq671p2JZRFFBRNeCItJRSiIGgnSQEqSEToCQZOb+/njfacl0ZjKTyfk8T5685b73PfNO5ubMued+D0bx2wDDB55RStWzJ9AqpbqatkRGDDHRyKUYmzSLs4WlafopTji83ZCN2LfOeey/Lsnu85/y38eWH4zfWZ8bv+0lhN71qcvpmW8egpfbwup3oeC0cWxnACWM7OSFmMcWTpSFeb8fCKxtTZdA8/WvGJGrYArt1gxAaf8SL1pACQlQwaX8Yv0Ogd83fHyLEZv7DqPgdxUgNxqGhIst/7rWu6q+IAilioAcMdP5mgFUVUpdD+RprX3miCmlPgVWAH9RSmUrpYYppUYqpezVjm8G1iul1gKvAYNNgcSI8sP6/ZG+hVCUzaYm5ncPe9YMKzjl3P7xCVhj/ml9+lfDgdtdRG7i59fg5daw8ZvQ7LHmw4m9MPeR0K6f2iu064rSP8A8NjCmZF0JKj/MJQMh7R6444vAr4Xika6iNO4O1RsH12cJorX+wuVnBnArcGG07QqVDg2rUi7RZegevgjuXRg9gwRBOCcCyhFTSt2KEQFbjCHk+rpSaozW2qsAlNb6Nl99aq3fAN4I3NTwcCIvDOKhgm8KzsCsu6HPs2bdQvObe3Y6LHoGevuIgP1ilgjqdBdsNhOMPihSTWu+qZ/164dhNbvECWbVW7EViEFEQyKtXG2JmZqSgdISaBRtI0LlsxEXux8Q0VZBKNUEOoI+jqEhdhBAKVUb+AkIgxJnyTDf2omrLGs4nHs22qbEP38sNaUiNNz+ufu5A2HUcts2P3x9RQOLmWZZo3nxGpmW8mB1+VttfCn87qK2kH8y8Puk/jV0GwOhz7P+2wz+BGo0i6wdXlBKncQ9MXA/8GhUjAmFIhMFFcpJkr4gxBOB5ogl2J0wk8NBXBsTdLVsBiAxZ2OULSmDFM1liYVVh7GAxSx662nxwMD3vLQNkpS6/tsEwsjl0PJqz+fqelWecdK6L9RpEx5bgkRrXVlrXcXlp5XWOsj52SgSqVqggiDEBIE6Uz8opeYppYYopYZgJL3O9XNNTFHV1HFsbgugzqAQZoo4YunveW7myrafImNKLOFJZBWM0kWtr4fOQ5zHQknMvvJJuHdBSKYVo147+OssGFOkTFjr2JfjUkrdqJSq6rJfTSk1IJo2BUWBIUQ9rdCLIywIQqkm0GT9MRg6Xu2BDsBUrXXpCe0DuoUxiFk3SVmQsHH6iFFKKFiW/8d/m48HBt9vrHPPD9Cmn3PfnltVNEJ44U3GasPeE4O/x2XjnNutroFq53tvGyxKQaWahnP3j/VGgrhrrcrY5SmttWOpq9b6GBDAUt0Y4eAGALboML6XgiDEDAFPL5orjh7WWo/WWn/p/4rYQvU11Lyvt5SuWr8xi80Gr3eCdy/339YtmqONFYvxzt9XFD9WoRr0cCm7VPk8SKwAlz7kPPbQWmculQph9r9iDadchYpQLlHDNMPBa9gZylWKzD3Ci6cHWXpWGEzrC8D56qCfhoIglEZ8jvRKqZNKqRMefk4qpU6UlJFhoWrDaFsQX6x43VC0L8rZXPjk1pK3J5r8c4sRIXrwN+exum2Lt9PaPfqVXBUe32dIStip3sTlAv/Tke8U9nU/kJAI10yBKg2K9FWmyVBKvayUaq6UaqaU+g/wa7SNCpY9ug5jr/lLtM0QBCHM+HTEPCS52n8qa639iAvFGC66S6t2REY3tkyx3Yva+vE97vub5sIP4zy3jRcq1zUiRDWaQb83YEjRuj4mFarhlpifkOg798vtnMv2eR1h0AwAvrEWkTKwJBmK+Q9vgKTkoF5GHPMAkA/MBGYBZ4D7o2pRCJzVSVQqV3oCeYIgBEapWvkYLv4+9UdKQDs2+mTNhiMRWpzguopv4TOGbhjA90VSBz8rIidXGp77dS8aiemh0OlOaNK9+PFRGVDlPPdjrqKsf18BNxVdxODifF1wo3N7xGJoYyTJWyhSHqpSnWAtjnu01qe01uO01mnmz2Na61P+r4wt8kvRbKogCIFTJh2xNckj2bw/BsrURJovhsE7PUO/vuCMoWz/28fFzyW6OGJLX4ANX8H/7ofD25zHt/4Y+r2jSdowo2B1OKll5m0luJRodXXE6raF9re4X+OIiClo5XnFnJsj1vfl8NsdByil5iulqrnsV1dKzYumTaFgI6FsfIEUhDJGmXTEACqsmw4FZUCf5+w5pPLlmrUMFz9f/Jyn8kK/few/Ed9e2zGWSTA/Fue5KJaff1F4+q7XzuU+AUY4fKx83KZdomxdhoUmcxH/1DJXSgKgtT4KlLrQoQ1F+SQRcxWEeKNMxbr3nnc1Df40ojSNVzwBHIA+z0TXqEiQNfvcpgBzNkPFWgRVRidQdv0c/j4jxdB5YCsEbTNKL+1ZCal3QKaHCGGguDpK/lY1lqvkLNLthROkhG5L2cGmlGqktd4NoJRqgkcV3djGRgI3d5ZFR4IQb5QpRyzhhlfgHZfVbLlxuhz8i2HBX3Nin1FI++YPYPZQqFTbKQZ6yuU5bV8ImZ+Ex85YJ7EcYE7B2vO7Ktdzb9MsAPmONC/vRyCFu11XVA77CVJKXSAnFngcWK6UWmLu9wRGRNGekLChSLKU2UkMQYhbytSnun79Bm77h0+dhfzT5L/cjryti6NjVKxw0KwBac8HO5XjEJKkMA+OmIrqH90IWZ8Xvz5eqFjL8/GOd8GtH7mr3QPc9ZX/Posm6dsJdGrSzvldoHrj4K4R0Fr/AKQBmzFWTv4TY+VkqcIWiQi1IAhRp0w5YkVZv/cYZw9sptyJ3RycNTra5kSPZS95VrJf+qJz+7WOxu9gnQdfdPu77/OVagfXX++JMOQcK29ZCzwfT0iAtv3cjyUGKg/hZRZM8rlKBKXUvcACDAfsn8BHwMRo2hQUTXoAsNTWIcqGCIIQCcq0I6a0psCcnS3MPxtla6JI+vvObdfcsr0Z7u1OHQqvI3bNFN/nPclA2EmpV/xY99HQ5FJ4MNOYYrXT+FL3ds2vLH7t6A1GtOuO2b5tKu+SkzXhgO+23U3nvtRlI8UdDwFdgF1a68uBjkBOdE0KgpQ6bLfVd4xVgiDEF2XOERtbcbJj22azOvJ0EorqMcUr1gLYk+79/KlD3s+92S28jpi/iJCt0Pu5B1yE0dvdCq2ude7XaAq1WxvbddpC1+GuN3WuQrzaZaFG1QZww6twflffNlWo7vu8K45k/KK1JAdCucqB9yOcK3la6zwApVR5rfUmoNRI1GtrPoXIaklBiFfKnCM2abRTULtX4c+Oen6qrIQtFkyC93vD/iznMVfJiQNZxa+xc/pQ+OsXXnSf93OXT3Dfv/Qfzm2Lix5X2lD462fe+3GVnlAJYLMa2+Uj7AyVq2j8LjqFefMH8Fh2ZO8tuJJt6oh9BcxXSv0P+DPKNgWMLiyggESGXNIk2qYIghABypwjllxEh0fbDAcsoaw4YnYHLOO/oV1/NsxCuN5ysjreAXVaw1WTPJ93FUZt5Efjq0p9eHiTsa0SnNOvCRYY+D4MjtAq0Ivuh16PQbe/RaZ/ISC01jdqrY9prScCTwDvAwOia1Xg2CNi9apKySpBiEfKfNKBzWZMSap4npo8thuqNTK27dOBGe8bpXyerR89uwBspiPW6S7DQfrtI2O/0MzZcxNSda3TmACP7wdL+cCS3u1SEUqBNiNiygLtbj4n832SlAy9HvXfLhzIVGdAaK2X+G8VWyTs+ply1BPpCkGIU8r8J1vbp6mKRsS0hm0/lY7aiP54pR2cOWruuDgtq6ca0hTRpM0Nxu+L7of+bziP2x0x12nTykWcxqQKThV8f7jmtrUzSwk16hacrXbGbId/bgnt2kjw6C7456ZoWyFEgrzjKOtZ2ibsIskiq2wFIR4pk47Yfy5yqrtXnW4IchbLEfvtI0PSYd3MkjStOLtXwtFdns/tWwt//hZYP79/afw+8Lvz2PL/hG7XX/qGfq0rLXrDxOPGNKQr9twx12hX1xFw5ZOQXDX4+9hzyrSGFlca96zRLDSbK9WCynVDuzYSVKjmvppTiB+szgUrEhEThPikTH6ybS75RcpcmddAHYbCfGej42Yy9dGdJWiZBz7oA6+2N4pvr/nI/dw7PWFqL2PbZoOMD5yRpKJ8a0op5O53HnPdDhZbofuqw9u9yD48tNbzcU/yE640vtj4XfdC57EEC/T4J4zbHbidjmvt73kcRDiFsoPLF5F4CM4LglCcMumIXdG6Dlm2JsVPuK4YtK8O9CWhEElW/B+8mup+zFdpofWzDWdr9tDI2mWnXEVo29+537Sn53bVmxQ/1n4Q3Ds/sPvUbAFtB8Bd/wvOvupNILECXGGuvLSYpYq6jQyuH0GIJo7UCdiekxtFQwRBiBRlMlm/Y6Pq9Ch4iGXli6jpL30RLnnQiMYkRNkRmze++LHdv8C6WdD+Vvfj2xfCHFMra9O33vvcGcaC231fdo++JZYv3mbMdud2xZpGEe1yKcYqRm9Ub+IehVQKbv0wePvKVYIJLhG/hAR44nBg9R0FIVbQTkcsUXLEBCEuKZOOGMB1PS6C1UUObp5r/Ew8DlZzmjJajpg35gwv7oh9dGNg1067Lnx2VKzhvWj6g78ZMhGVzLqNY3YYBbQD0e26b2XknrmlzP65C6UVl4hYOckRE4S4pMx+su+/oiUvFNzq+eS+dbDkeWPbFmFZiwO/Q8EZ4z6uOWq++M+FcGhrZO0KBG/irjWauU9JVqoZuHhqUoXIC60KQmnBJSImyfqCEJ+U2RBBleQkdmsvK9++dBHgdBkIw86+dfBOD/djE4/DjFt8X3d8D8y6O3J2BYp9pV7Ty4zf96c7KhUIQmlHKXUN8CpgAd7TWj/npd3NwOdAF611hqc2IWNGxF4quJmK4ogJQlwSsU+2UuoDpdRBpdR6L+eVUuo1pdQ2pdQ6pVSnSNnijR9sXTyfOLjBsVm4cS5kh3dsdVDUCQP45XXY+qP/aw/+7r9NpEksD8MXOXO4areCWi2ia5MghAGllAV4E7gWaAvcppRq66FdZeBBYFVEDNFGRH6PrkNiguSICUI8EsmvWNOAa3ycvxZoaf6MAN6KoC0eKSSRDnlTueXsk17bJJ7YDe9dCSdKqDTdjxP8t4klGnQKrhC2IJQOugLbtNY7tNb5wGdAfw/tJgMvAJFRRjYjYjYSuP2iRhG5hSAI0SVijpjWeilwxEeT/sB0bbASqKaUKtF6O6sfu5LjpJCuW9PnrMdZBydH/vDfobUQTh8JLK+spBy7QPnHeuh8T/Hjgz+F4QtL3h5BiC4NgD0u+9nmMQdKqY7A+VprH0uVHW1HKKUylFIZOTk5gVthpkYkJiZSsVyZzSQRhLgmmkkHfgc6OyEPYn6oU8VZRHezbsSlea96bzztOiOx3mYzZC72mEsutYacLVCQB5NrwgtN4SOznvCWHyH/dPG+bLbAph9Lkmrnw/Wm0v5F90OdC4ztWq2Ki68OnVeytglCyeNpHtAhqaqUSgD+A/wzkM601lO11mla67TatWsHboW5gtgmsiuCELdE8yuWz4HO7aDWU4GpAGlpaRHTl96LnwHyrUug699g9TvG/pC5niUh/lhiKOHbefyAUQB60RTYv86QyIgmN7wK3zxkbN/8ATQzyjyhlLFYAAxn8US2s1h4nylQtYHhjDa6qHifghBfZAPnu+w3BFzD2JWBC4HFylC/rwd8rZTqF9aEfXNqUnlboSwIQqknmhExfwNdifDW7Z3o0NDpNHXJe9P3BXYnDALX5XqmLsz5Gyx5LvpOGBjFs+/62ljh2OxyQxOsKAkJTicM4OL7DCX9yx8rOTsFIXqkAy2VUk2VUuWAwcDX9pNa6+Na61pa6yZa6ybASiC8ThhAoZF6psURE4S4JZqO2NfAXebqyYuA41rrfSVtxLXt6vO/Ud0d+7lUiMyN1n0WmX5DQSVAs8vgqaOenTBBKONorQuBUcA8YCMwS2v9u1JqklKqX4kZMu9xANqxrcRuKQhCyRKxqUml1KdAL6CWUiobeApIAtBavw3MBa4DtgGnAQ+Z4iVHvw7n8fXaPzlDMi8V3MwG3ZiWai/jkmLIgQoXlYLIURGEMorWei7GOOV6zOMSa611r4gYcdgQbq6sPOSaCoIQF0TMEdNa3+bnvAbuj9T9g+W12zry9VpjZvR1600ALKFDbDtiD2+Cl1t7PpeY7JjWAOCqyUbB6z2r4LxUz9cIghBbaDMlVoSSBSFukU+3DwpJ5Jm0X+BK80twvXbRNagoVeob0hI9x7of7zUehnxnbF812ZCmuPRBo95jUw8isoIgxCamoKs4YoIQv8in24VJ/S8oduzd5Tuhxz/hyaPwt2XOE08cMiJSkaJCDWO1pT+x1Aad4YrH3Y/1GgcN0+CRbXDJA4Y0hSAIpQ9z1WSiRZL1BSFeEUfMhbsubsK2Z671fDIhwZB3uP4VuGUaWJKMiFTj7tD6ehg2/9xuft2Lzu370+GhtYbkxbkUwE6pbdgsCELppGlPALZVbB9lQwRBiBQi1VyERH+FddOKrCm45zvn9lPHDMX8/xQrSeebhzcZqxfnPmLs127lctKDI9VzLDTr5X7sji9gTzpYzwZ3b6HUY7NpEqQOYXzS/HLYOo+dFWIsLUIQhLAhjpgHzquazJ/HQygdp5QhetqwC2SnO493HQG/fwWnDha/5oFz71HIAAAgAElEQVQ1RmTNW1kke0Tr7yugXCVYNxN6PGJE6Fxp0dv4EcocOw6dokWdlGibIUQCc2oyKUmGakGIV2Rq0gNjrvmL277WQYr5D/0RRiyGFlfBmB1w3b9hzFYYv9e9XbVGULO5sV3UsbIz4G1o0gNqtYTqjeGysd7bCmUSi0TD4hez1mTm3twoGyIIQqSQr1keKBqcWrnjCBc3rxl4BwkJcF5HuGO2+/HyKc4SQrtXQo3mxa+t5e4E0vhiGOK3prBQhrFIHmD8Ytaa3H+yIMqGCIIQKSS04oGWdd2neW57dyWFVi9Th6HS6CIjmd6V4Ytg6A/hvY8Q94gfFseY3wpTGwXxRVAQhFKFOGIeaN+wGqseu5KerZyO0rKthyJ/4wadpOSQEDSSqB/HmFOT3ZpLNQxBiFfEEfNC3SrJtKnnlI6Yv/FAFK0RBO+IHxa/aHNqUiWIjpggxCviiPlg7DXO8kELxBETYpRg15IIpQdtLaRAW2RBhiDEMeKI+cCSoBjcxVClP3BC9LlKM+k7j/DKT1uibUZEsIknFrdomxUbCRTa5D0WhHhFHDE/PHujU0hx3/EzUbQkOMK+uKCUc8vbK3jlp63BS5GUAuLwJQkmR3PzsJLAawu2RtsUQRAihDhifnBNhL54ysIoWhI4s9L30OLx79l7rPQ4jiXFmQJrtE0QhIDR2opVhmlBiGvkEx4C67KPkV8YuxGnOb9lA7Dr0KkoWxI7lE80/tRP5hVG2ZLwI1OT8Uu5BI2VBJ658cJomyIIQoQQRyxIFm06SL83fmbytxuibYpX8goMJ7Fcory9duzJzvGYayN+WBxjK8RKAinlRXtbEOIV+U8dJPdMM2pIpu88EmVLvJO55xgASf4KmJchEkzVU6s1/ryW+HtFggMzWT9BVHsFIW6R/9QB8OPonsWObdp/MgqWBIc4Yk7sqX7WOAwfxeMCBMFA24wcMXHEBCF+kf/UAdCqbmX/jWIQGbudKHtELA6nJuPwJQl2bFYKsSDfqQQhfpGPd4B4cmpsMfofsGqFJECSuF2xR8Ti85nE42sSALAVYtPK8UVCEIT4QxyxAKlUrniy7Oxfszl+uoDvs/ZFwSL/xKXPESKOZP14zBGLv5ck2NE2rCRgEUdMEOIWccQCZPbfL6ZrU/eC3GO/WMc/P1/L32esYWcMSkXEZ/QnNOwRhXh8JjEamBXCga0QKxYSZKQWhLhFPt4B0rpeFaYP7Vrs+O4jhgN2/ExBSZvkF/kH7cQeT4jHHDEtU5NxS7U/vqNFwp8yNSkIcYw4YkFQ3oMu15YDuUBsZenYV9HFY/QnVOyrzsKpIzZ16Xbmb4h+MXh5m+MfmZoUhPhFVAKDwNe30lgcJkXWwIk9RyyczumzczcBsPO5vmHrMxTE4Y5vTuoKIl8hCHGMRMTCxNz1sZewH4ezcOdMXE5Nxt9LEkwKy1Xhc+tlJIgfJghxizhiQbJhUh82Tb6m2PF3luyIgjW+iVV5jWhgT3aOR0dMiGNshRRiIUE8MUGIWyLqiCmlrlFKbVZKbVNKjfNwfohSKkcplWn+3BtJe8JBxXKJJCdZom1GQIjP4cQS14Ku8feaBANrQQFWEth/PC/apgiCECEi5ogppSzAm8C1QFvgNqVUWw9NZ2qtU82f9yJlT0mwOUbKHtn/LUuOmBOHsn4cPpM4fEmCiQVDWf/k2cJomyIIQoSIZLJ+V2Cb1noHgFLqM6A/sCGC94wqfV5ZyuQBF3LnRY0Dan86v5C5WfsZ2KlBRJanx2HwJ2Tsjzcep2vj7xUJAGhNojIEXZvVqhRta4QQKCgoIDs7m7w8iWiWBZKTk2nYsCFJSUlBXRdJR6wBsMdlPxvo5qHdQKVUT2ALMFprvcdDm1LDE1+tD9gRe+a7jcxYtZvzqiZzSYtaYbdFpqycWCIgXxErSOQzTrFZASjUFjqcXy3KxgihkJ2dTeXKlWnSpIlowcU5WmsOHz5MdnY2TZs2DeraSOaIefqrK/of4xugida6PfAT8KHHjpQaoZTKUEpl5OTkhNnM0Ph1Qm9+GXcF91zaJOQ+ck6eBeBEXnEx2NyzhcxYteuc/smKI+bEvvw/HiNicfiSBACbMR1pxRKT8jiCf/Ly8qhZs6Y4YWUApRQ1a9YMKfoZSUcsGzjfZb8h8KdrA631Ya31WXP3XaCzp4601lO11mla67TatWtHxNhgqZlSnvOqVeCpGy4odu5kXgHTV+zk67V/Fr/QhUSLPYG8+LkHP/2Nx79cz4odh0O2cf6GAxw7nR/y9fGEfRyMRI7Y2UJr2PsMDvHE4hLTESvAIjpipRhxwsoOob7XkZyaTAdaKqWaAnuBwcBfXRsopeprre0CXP2AjRG0p8RoN/FHx3a/Dud5bedUey/uiS3cdBCAvILg/8nb+52xajdbD+Yy628XB91HvJEQwVWT0V6JKRGxOMUREUtA/pcLQvwSsYiY1roQGAXMw3CwZmmtf1dKTVJK9TObPaiU+l0ptRZ4EBgSKXtikcQE/85BKB626yW7D58O+vp4JJI6YtHOOyvwFFIVSj+FxmRBIRZxxISQSUlJiUi/OTk5dOvWjY4dO7Js2bKQ+pg2bRp//ul75sgTb7/9NtOnT/fZJiMjgwcffDAku0qaiJY40lrPBeYWOfaky/Z4YHwkbYg2zR+by9Kxl9OgWoVi5xICcMTmbzjA5X+pE9Q9XacxZAA3UEQwImaNriOWXyiOWCRQSl0DvApYgPe01s8VOf8wcC9QCOQAQ7XWu8JmwMJJALRRu2RqUog5FixYQOvWrfnwQ4+p3R6xWq1YLE4dzmnTpnHhhRdy3nnFZ46KtnVl5MiRfu+VlpZGWlpawLZFE6k1GQZqpZTjUK7nXCyrTXPpcws91iOcs2avo403lmwOfnGCiHAXx/5MIuGIFXiYWi5JzoojFnZcdBCvwsh3TVdKfa21dpXf+Q1I01qfVkr9HXgBGBQ2I/7MBKCWOiGOWBzw9De/s+HPE2Hts+15VTzmKXtCa83YsWP5/vvvUUoxYcIEBg0axL59+xg0aBAnTpygsLCQt956i0suuYRhw4aRkZGBUoqhQ4cyevRoR1+ZmZmMHTuWM2fOkJqayooVK/jqq6949tln0VrTt29fnn/+ecCIyD388MPMmzePl156ie7duwMwe/ZsMjIyuP3226lQoQIrVqygTZs2DB06lB9//JFRo0Zx8uRJpk6dSn5+Pi1atOCjjz6iYsWKTJw4kZSUFB555BF69epFt27dWLRoEceOHeP999+nR48eLF68mBdffJFvv/2WiRMnsnv3bnbs2MHu3bv5xz/+4YiWTZ48mRkzZnD++edTq1YtOnfuzCOPPBLW98kfUuIoDMx9qAf1qyaHfL2vqaWEkN4h56C9rwwpck/8+nfmbzjg8Zwj+hiBZP1o54hJRCwiOHQQtdb5gF0H0YHWepHW2j73vxJjQVL4sMtXyKpJIQzMmTOHzMxM1q5dy08//cSYMWPYt28fn3zyCX369HGcS01NJTMzk71797J+/XqysrK455573PpKTU1l0qRJDBo0iMzMTI4ePcqjjz7KwoULyczMJD09na+++gqAU6dOceGFF7Jq1SqHEwZw8803k5aWxowZM8jMzKRCBWPWKDk5meXLlzN48GBuuukm0tPTWbt2LW3atOH999/3+NoKCwtZvXo1r7zyCk8//bTHNps2bWLevHmsXr2ap59+moKCAjIyMvjiiy/47bffmDNnDhkZGeF41EEjEbEwUKdyMv93eyfu+mA1J/O8K2DvyMnFatO0rFvZbaXdGR8J+TUqlQ/annBHxI6fLuDYmXwa14xtUclpv+xk2i87PUYfI5msXxjlqUmJiEWEQHUQ7QwDvvd2Uik1AhgB0KhRo8As0Ma4YJNk/bgg0MhVpFi+fDm33XYbFouFunXrctlll5Genk6XLl0YOnQoBQUFDBgwgNTUVJo1a8aOHTt44IEH6Nu3L1dffbXPvtPT0+nVqxd2VYPbb7+dpUuXMmDAACwWCwMHDgzYzkGDnEHl9evXM2HCBI4dO0Zubi59+vTxeM1NN90EQOfOndm5c6fHNn379qV8+fKUL1+eOnXqcODAAZYvX07//v0dTuANN9wQsJ3hRCJiYaJjo+pkTfT8RwIwfk4WV7y0hKv+sxSAD5bvdJzLKyj+j7RVXSPBcu2eY0FriYV7GuPqV5Zw2b8Xh7XPksYh6BoBpynayfrRl8+ISwLRQTQaKnUHkAb821tnIUnw2OyOmBIJBOGc8fZ/pGfPnixdupQGDRpw5513Mn36dKpXr87atWvp1asXb775Jvfe67sMtK//UcnJyV5zvTxRqZLzC/+QIUN44403yMrK4qmnnvKq0VW+vBGwsFgsFBZ6DobY27i2ixUxbHHEwsyQS5p4PP7p6t1u+0dd9L3qVSk+rVk52VkiYei09KBsCPeYfeDEWf+NYhxLgnepkHOlMMqrFmVqMiL41UEEUEr1Bh4H+rloIoYHMyJWPshyKYLgiZ49ezJz5kysVis5OTksXbqUrl27smvXLurUqcPw4cMZNmwYa9as4dChQ9hsNgYOHMjkyZNZs2aNz767devGkiVLOHToEFarlU8//ZTLLrvMr02VK1fm5EnvNZpPnjxJ/fr1KSgoYMaMGUG/Zn90796db775hry8PHJzc/nuu+/Cfo9AkKnJMDOx3wU8dl0bFm0+yN8++tVjG6tNu/3zHPvFOm7ocB4Vyjm/NSS6zC8uCjJhX747F8cunlsQlxGxyDpi6/ce58ipfHq2ig0x5RIiEB3EjsA7wDVa64Nht8CMiCmLDNPCuXPjjTeyYsUKOnTogFKKF154gXr16vHhhx/y73//m6SkJFJSUpg+fTp79+7lnnvuwWZ+cZ0yZYrPvuvXr8+UKVO4/PLL0Vpz3XXX0b9/f5/XgBHxGjlypCNZvyiTJ0+mW7duNG7cmHbt2vl02kKhS5cu9OvXjw4dOtC4cWPS0tKoWrVqWO8RCCpWQnOBkpaWpqOVUBcMZ/KtDH53JWv3HAuofd/29Xnzr50c+0OnpTtEXQF2PteXQquNgW/9wuirWtHLh6RFl2d+cpRPsl97LjQZ911Y+okkB0/m0fWZBYBnO+/+YDVLtuQwuncrHurdMiz3tD+Xb0Z1p13Dkv/w2u//8FWtePDK8LwmX/eJ5vuvlPpVa12ia9GVUtcBr2DIV3ygtX5GKTUJyNBaf62U+gloB9hFqXdrrft56c5BwGPYi60g9wBzy/Xhusdmhfw6hOixceNG2rRpE20zBB/k5uaSkpLC6dOn6dmzJ1OnTqVTp07+L/SCp/fc3/glU5MRokI5C/+7/1I+uddXfq+T79bto8m473jsyyzyC20e566PnM5nbfZxHvl8nc++alYq57bfZNx3bD0Q3m8SsYa/7xP209YITE1+vDJ80lGh8PL8LVG9f7yitZ6rtW6ltW6utX7GPPak1vprc7u31rqu1jrV/PHrhAWF1UhfsCqJiAlCpBgxYgSpqal06tSJgQMHnpMTFiriiEWYS1rUYtsz1wbc/pNVu1m0+WCxrOACq429R88AcCjXdyqKp6myBz/LDNgGb8Ry9NTfaki7A1YQgWnEmRl7/DcShGCp1w6ANeW7RtkQQYhfPvnkEzIzM9m0aRPjx0dHX14csRIg0RLcY9ZaF6sf2PLx77nx/34J6HpPyePHQyz+7Vq4vPfLS0LqoyTw54jZV0tGO7E+UuSe9S6bIpRS/jBWWK8rDFDuQhCEUok4YjGIJSHhnKJPniJiNVOC1yMDWLPrqGN7e84p1u89HrJdkcTm53nZHbVQk/XzCqycyCsI6dpI0d4lL+2XbYeiaIkQSXYfORNtEwRBiCDiiJUQw7o3Dbjt8OkZLNvq/x9rodXGta8uY9Hmg0WOF3c2erepG/D9XbEUUYfNNBcfdJo8n8nfbvB0SVTwt3Kx0OGIhRYRu/715bSf+KNjP5anaSNFWXzNsYCWddCCENeII1ZCDPXgiJ3LKrQjp/I5lJvPxn0neHS2e/K+J6fEtbSP1aaxBZgrZZd9sGPXKDtyKp/3l/8RsL2FVhu//+k/mrZw04GQIk/+Xo9dPyxUR2zbwVz3+5VBn8RXBQghcpTBPzVBKFOII1ZCNKhWgZ3P9WXnc315ut8FfDLcWE259ZlraVCtQtD9dZo831HKSAN7j53hm7V/suvwKQ7lnuWuixvzrwEXOtq7rhZs/thcrn99eUD3UUW+jYeq2v/8D5vo+9pytufkem2z99gZhk7LYHQICwv81ZC0RwnDJX4aC9EhVxMOnIy86O6JM5KHFg3Kl5NVk0LopKSkRKTfnJwcunXrRseOHVm2bFlE7lGUIUOGMHv2bADuvfdeNmwoPiszbdo0Ro0a5bOfxYsX88svzpzrt99+m+nTp4fX2CCQT3gUuNtFfT/JksDP465waDUV5Z07O3sVhj12xogcaa25/rVlHD3tjCQlJiRwXjWnYn/RQNCGfScCsjVccg/2Kc1DJ8/SvLbngeFMvvGPfufhU0H373/VZGA5YqfOFpJXYPWbUxdrEbEnvlrPnRc1jug9TuQVUO8citsLwWHTigSlOZ0fnwtMhNLNggULaN26NR9++GHA11it1qDKHfnivffeC/naxYsXk5KSwiWXXALAyJEjw2JTqIgjFiO8eEsHFm8+yLfr9rkd73NBPa/XXG3WrTyUW3xFZIKCci5/8KE6VO8uc59+HD8ni8Y1KwbdT9HImifszk0oUTd/L8/uiOX7mZq85tWl7Dlyxu+0sS6hCaN12ceoVzWZOpWj7wDJysySJUGZf7NIiaO44PtxsD8rvH3WawfXPhdQU601Y8eO5fvvv0cpxYQJExg0aBD79u1j0KBBnDhxgsLCQt566y0uueQShg0bRkZGBkophg4dyujRox19ZWZmMnbsWM6cOUNqaiorVqzgq6++4tlnn0VrTd++fXn++ecBIyL38MMPM2/ePF566SW6d+8OGMKnd999N6tXrwZg586d9OvXj3Xr1jFp0iS++eYbzpw5wyWXXMI777xTrN5qr169ePHFF0lLS+O///0vU6ZMoX79+rRq1cpRV/Kbb77hX//6F/n5+dSsWZMZM2Zw5swZ3n77bSwWCx9//DGvv/46CxYsICUlhUceeYTMzExGjhzJ6dOnad68OR988AHVq1enV69edOvWjUWLFnHs2DHef/99evTocc5vIcjUZMxwc+eGvPHXTrx9R2fHsRkBisF6It9q4/gZZ4Ts3WV/0Pc19/Cxt+m1lTsO+5x6++estUHZsvXASTJ2HfHbzr7yMZTZT39TkwWmp+ZvanJPgCvUIjUzOX/DAd5btsOx3++Nnx0VA/yxw8e077lgL7cVaF6hEF7OiiMmhIE5c+aQmZnJ2rVr+emnnxgzZgz79u3jk08+oU+fPo5zqampZGZmsnfvXtavX09WVhb33HOPW1+pqalMmjSJQYMGkZmZydGjR3n00UdZuHAhmZmZpKen89VXXwFw6tQpLrzwQlatWuVwwgDatGlDfn4+O3YY493MmTO59dZbARg1ahTp6emsX7+eM2fO8O2333p9Xfv27eOpp57i559/Zv78+W7Tld27d2flypX89ttvDB48mBdeeIEmTZowcuRIRo8eTWZmZjFn6q677uL5559n3bp1tGvXjqefftpxrrCwkNWrV/PKK6+4HT9XJCIWY1xzoTMCdmmLWgD8++b22LTm0S8C/zaVV2ClVV33KcDf/zzh5mCdzrdSqbz7n8D8DQcYPj2DiTe05Q4vU10Hg8xHuu3dlY5oV9F/5av/OML2nFxu69rI4dwUjZ69u3QHOblneew676VCNu/3PdVqtZ7bqsmiuDpitUKUBvHE8OlG6Zt7ezQrdu6H9ftpW78KjbxEJLfnnKKZl2nfc0G55CIKJceYghE8aPmS2lUqRdsUIRwEGLmKFMuXL+e2227DYrFQt25dLrvsMtLT0+nSpQtDhw6loKCAAQMGkJqaSrNmzdixYwcPPPAAffv25eqrr/bZd3p6Or169aJ2baMe7e23387SpUsZMGAAFouFgQMHerzu1ltvZdasWYwbN46ZM2cyc+ZMABYtWsQLL7zA6dOnOXLkCBdccAE33HCDxz5WrVrldu9BgwaxZYtRbSQ7O9sR9cvPz6dpU9/qBcePH+fYsWOOguV33303t9xyi+P8TTfdBEDnzp3ZuXOnz76CQSJiMcgjV7vXDrwl7XwGdQlO1PFMgY2WdSsXO95x8nzHtqdi0fuPGxGhrQdzWe5Fm8pfPlZRXKdOXQViAW59ZwXj52Rhs2mnI1YkIvbM3I1MXboDX/hzUu0rSX/ZfjhAqz3T/43l5BVYi+iWeX4eWuuw6q6N/PhX+ryy1OWu7veN1AIC+5TAumz31zLt5z/4rshUuhA+Prf2okf+q1ze2ntdWUEIFG/jQ8+ePVm6dCkNGjTgzjvvZPr06VSvXp21a9fSq1cv3nzzTe69996Q+gZITk72mhc2aNAgZs2axZYtW1BK0bJlS/Ly8rjvvvuYPXs2WVlZDB8+nLy8PJ/3LzptaeeBBx5g1KhRZGVl8c477/jtxx/2KU+LxUJhYfhSNcQRi0FGXdGSh69q5fX8B0PSuKlTA599nDWlBuY+6B52PeaS0P/N2j8ZPycLrTVNxn1H88fmkmRWAThwIo9TZ51yBc1qe/5Wvnl/cDUsP1m12+Pxe6alO5wbbzli6TuPFJORgOJq+VprNu8/yYvzNqO15uOVu9yieOfisKzNPs62g7luLtCh3HzyPEg7zEzfw/WvL3ebavTEjpxcfvUwdetqp31a0JeERLAOsut1d7y3yqsorP3dKKobN/GbDdz/yRqPr10IHyN6Fo+OCkKw9OzZk5kzZ2K1WsnJyWHp0qV07dqVXbt2UadOHYYPH86wYcNYs2YNhw4dwmazMXDgQCZPnsyaNWt89t2tWzeWLFnCoUOHsFqtfPrpp46oki+aN2+OxWJh8uTJDBo0CMDhLNWqVYvc3FzHKklf9168eDGHDx+moKCAzz//3HHu+PHjNGhg/K90XVRQuXJlTp4s/r+ratWqVK9e3bEK9KOPPgrodZwr4oiVIh6+qhV929fnitZ1ad+gqs+21SoaeSVtz6vCj6N7emzz1Ne/8+nq3Yw1dcisNs0rP20F4KeNB6lU3vkt5vp29T32URimVZVLtuQ4pg0TXP4qH/vSGem65e0VHsssfbEm221/5+HT3PbuSt5YtI3jZwqY8NV6t/On8s/NcThyKr+Ykr89H+/IqXyHXtq4OYbt//puo5uzeLbQyle/7XXsX/HSEga+taLYfVz9KtdI5mEvtUYXbjro8bg/Dp86y/Jth3jws99Cur7D0z/6byQETcPqhqxNlWTJIBHOnRtvvJH27dvToUMHrrjiCl544QXq1avH4sWLSU1NpWPHjnzxxRc89NBD7N27l169epGamsqQIUOYMmWKz77r16/PlClTuPzyy+nQoQOdOnWif//+Adk1aNAgPv74Y0d+WLVq1Rg+fDjt2rVjwIABdOnSxe+9J06cyMUXX0zv3r3dinZPnDiRW265hR49elCrVi3H8RtuuIEvv/yS1NTUYtIbH374IWPGjKF9+/ZkZmby5JNPBvQ6zgUVC3pIwZCWlqYzMjKibUbUOZNvpc2TP3g9nzXxaionG85YgdVGy8e/D/oelgTliLJM7n8BT/zv92JtmtWqRN/29WlUoyJXtqmL1aapXbm8VzkOMLTTXl+4jQPH89wKZr97VxrDp2egFPwxxVi16KmfoisaP1j+B5NcojWt6qaw5YAROXthYHvGfuEueDtzxEW0rleFqqazqh2LBJTjfq7Pr6gd1SomsWTM5W4OyNejLqV9w2r0eGGhY9Wl6zW1UsqTMaE3AHd/sJolW3K4rWsjptzUztHup4d70vvlpY7XmLHzCDe/XdxBs5+//vVl5BXYHFHCyuUTyXq6j8f2vjh4Mo+uzyygVko5MiZcVez8XyZ875jGtj/7uVn7uG+G81vyuYgTB4JS6letdVpEb1JCBDqG7T58mtm/7mH0Va28Tr0Isc3GjRtp08Z7bqsQf3h6z/2NXxIRK6VUKGdh6ZjL+cRlZeW7dznfZ1cnIinIouN2XKe6bkk732ObHYdO8frCbYyZvY5Ok+fT5ZmfeHPRNp/9tnz8e15bsNXNCQN4w7zO33eDP4/5Xtlod8KAYk4YwKCpK+kw6Uc+W21MkzYdP5eHi6wE3XX4tNf+j50ucES0LjivCmCsbgTvqy4PuUSxlmzJAWBmuvs07WNfOiN32UdPe3XCAFbtMHLdGtVwJu6fPFtI7tlCOk+eH1TtSX/SIp7qeD7z3Ua3/eyj3p9XUfYcOc3cLMkt80ejmhV5+Oq/iBMmCHGOOGKlmEY1K3JJi1oOxf6r2tZlQt82HmUvdj7Xl6l3dvbQS2AkJ1nYNPkavn/Iv27Kv+dtDukea03RVzCEVb2xbGsOYz5f65jKzAoxIX7cnCz+M99YXfOly1Qh+JfQeOprIzq477gz+XP2r84p0o0eBHOLRp+LpnSt/sOZJ9b9+UU+7z9o6kq0ppgL9euuoxw+lc+4OVks25pT7LpPV+9mylx3J8qe9O9p8QZ4FsEt6pzd9H+/FGvjjYFv/cJ9M9bERHUCQRCEaCOOWJxxb49mDtmLolx9QT1Wjr+Sbc9cG1SfT93QFjCcsTb1q5yzjYEw7MN0r+ce/SKLz3/N5vv1+9mek8vSLcUdjkB5dcFWj8dX7vCvewZGTpidRz53RtWufbV4yQ9Pjs65OCNn8q3FHMa7PzDEEXcfOc2d768u1v/4OVm8s3SHuyaYuXkyr5C1e46xYvthVu7wvLrUvgrU1QEFQ9Lk09WeF2LYySuw8unq3Y6FE94cP0GIJ+QLR9kh1PdaHLEyRr2qySRaElj9+JUMcd6v7W4AAA1+SURBVCm15ErXpjXY+Vxftj5zLSvGX8E9l7prr2RM6E3l8pFNIF654whtfeTAATz46W9c+dISLKbgaHMvKzvtNKvl+/yizc5k98nfbmDE9AyajPvOTRi3KKnnV/PZpyutn/iBO99fRcs6Tq2v//68M+Dri7Lj0CnOFtr47xDvyaxNx89ll4eSUW8v3e7YXuUSiev/5s/c9u5KBk9dyawiU8dgaMJ5Y/ycLDJ2Ovs6ciqfh2dmsnDTAR6elUnf15Yxfo5z8YW/KgeCUNpJTk7m8GHfAtlCfKC15vDhwyQnB18FRZL1yzj2RP4Jfdtwb49mWG3a4dj4442FW3nxxy1UrZDk01kBuOOiRny3bp9bPUxXrm9fv1h5p2DZ+Vxf3lu2g38VyV+yk/nkVaROmu/xXKhMHnAhTxRZlVnS7HyuLy0em+vQSvNGtYpJDvmS/qnn8ergjoDnBRF2eraqXSzi2KJOikcZEYBef6nN4s3+I5SVkxPJmhj4woKymKwvlH4KCgrIzs4+Z/0qoXSQnJxMw4YNSUpyr4bhb/ySddFlnCRLgtuKt0CdMID7erWgVkp5burUkDW7jzJ4qhEtqZBk4dIWNflpoxFhurBBFcZd24an+11I88fmAsZ057zf9zumAN/4aye+XefdIQiUe3s041BuPm8v2e52/JVBqVSrWI7/3X8p/d/8+ZzvY+fbIgK10eLH0T254qXi0h6uuGrI/S/zT/6X6d92T9O+3pwwICAnDIxpUEGId5KSkvyquQtCRCNiSqlrgFcBC/Ce1vq5IufLA9OBzsBhYJDWeqevPuXbZOzyy/ZDtG9YjRRz2vJ0fiHr956ga9MajjZ7jpymWsUkx6rONxdt47aujahRqRxaa5RSvLZgKy+bSfRg6Cj9Mv5KrFZNh0meNav+0bsl/+htiOBqrTl6uoCzhVYunrIQgD+mXOdYfWa1aW55+xfW7D7msS9fzLi3G7e/t8qxf1vXRkzufwEtvMiDTLyhLQs2HWTZ1sBXMRbltds68ujsdTSsXoF37uxczOGyO9J2GYqSpGalchw+VbzofCAEI3khETFBEEor/saviDliSikLsAW4CsgG0oHbtNYbXNrcB7TXWo9USg0GbtRaD/LVrwxi8Y/Npmn22Fya1qrE9w/1IDnJvTzG8TMFJCgjIf3rtX9yfbvzaNfQt8CtJ06dLWT1H0fIyT3rELUtSrsGVd1WZe58ri8HT+TR9VnD4dnyr2spl5jA6j+OcOs7htzEfb2a83+Lt3NV27q8e1caWmuajp9brO8Jfds4plErl0/kpLlSdPbIix3SFTuevY6EIlHKtXuOOaJ6s0deTFoTp6NbdJoxMUH5nbIMpN1Hw7py7HQBD3zqLvq687m+HDmVT6fJwU/5iiMmCEJZIJqO2MXARK11H3N/PIDWeopLm3lmmxVKqURgP1Bb+zBKBrGyQX6hjcQEVcwJiRRHTuWzds8x/vvLTkb0aEb3ls6Vp8u3HuKPw6e4sWMDR7Rv//E8aqaUc9No+/H3/dSoVM7NMbKTV2Bl2dZDNKhWgdb1Kjte1+RvN/D+8j/YNPkakpMsnM4vpGK5RI6fKaBKcmLQGlKn8wspKNQOsVqAbQdP8n3WflKSE2leOwWrTVM+KYENf57gX99tJGNCb2qllGfnoVOUT0pgR84p2tSvQo1K5dh3/Ax1Kyc77J2zJpvXF27jj0On+HncFTSoZqi/n8grcKy43Hc8jx05uazNPk6DahX4bMRF7Dh0ivKJCTSoVoFfdx2leqVyXNaqdsCvSxwxQRBKK9F0xG4GrtFa32vu3wl001qPcmmz3myTbe5vN9scKtLXCGCEufsXIBihqlpA6PNCkSNW7YLYtU3sCp5YtS1YuxprrQP33GIYpVQOsCvA5rH6/kHs2iZ2BU+s2hardkFwtvkcvyKZrO/pq3xRry+QNmitpwJTQzJCqYxY/CYdq3ZB7NomdgVPrNoWq3aVBME4lLH8nGLVNrEreGLVtli1C8JrWyR1xLIB17o4DYGiy7QcbcypyapAYEqagiAIgiAIpZxIOmLpQEulVFOlVDlgMPB1kTZfA3eb2zcDC33lhwmCIAiCIMQTEZua1FoXKqVGAfMw5Cs+0Fr/rpSaBGRorb8G3gc+Ukptw4iEDY6AKSFNaZYAsWoXxK5tYlfwxKptsWpXrBHLzylWbRO7gidWbYtVuyCMtpU6ZX1BEARBEIR4QWpNCoIgCIIgRAlxxARBEARBEKJE3DpiSqlrlFKblVLblFLjomTDTqVUllIqUymVYR6roZSar5Taav6ubh5XSqnXTHvXKaU6hdGOD5RSB03dNvuxoO1QSt1ttt+qlLrb073CZNtEpdRe87llKqWuczk33rRts1Kqj8vxsL7fSqnzlVKLlFIblVK/K6UeMo9H9bn5sCsWnlmyUmq1UmqtadvT5vGmSqlV5uufaS7eQSlV3tzfZp5v4s/mskS0x7BYGb/M/mNyDJPxK2x2xcIzi974pbWOux+MxQHbgWZAOWAt0DYKduwEahU59gIwztweBzxvbl8HfI+hrXYRsCqMdvQEOgHrQ7UDqAHsMH9XN7erR8i2icAjHtq2Nd/L8kBT8z22ROL9BuoDncztyhjlutpG+7n5sCsWnpkCUsztJGCV+SxmAYPN428Dfze37wPeNrcHAzN92Rzuz2cs/0Ti/QnBhp3EwPhl9h+TY5gXu2LhsyjjV/C2RW38iteIWFdgm9Z6h9Y6H/gM6B9lm+z0Bz40tz8EBrgcn64NVgLVlFL1w3FDrfVSiuuzBWtHH2C+1vqI1vooMB+4JkK2eaM/8JnW+qzW+g9gG8Z7Hfb3W2u9T2u9xtw+CWwEGhDl5+bDLm+U5DPTWutcczfJ/NHAFcBs83jRZ2Z/lrOBK5VSyofNZYlYHcNKfPyC2B3DZPwKm13eKBPjV7w6Yg2APS772fh+syOFBn5USv2qjDJNAHW11vvA+KME6pjHS9rmYO0oaftGmSHyD+zh82jZZoacO2J8Q4qZ51bELoiBZ6aUsiilMoGDGIP2duCY1rrQw30cNpjnjwM1I2VbKSMWnkEsj1+h2FKSNkb9s2hHxq+gbIrK+BWvjlhApZNKgEu11p2Aa4H7lVI9fbSNFZu92VGS9r0FNAdSgX3AS+bxErdNKZUCfAH8Q2t9wlfTkrTNg10x8cy01latdSpGJY2uQBsf94mFv7VYJRaeQWkcvyD6f1cx8VkEGb+CJVrjV7w6YoGUV4o4Wus/zd8HgS8x3tgD9pC9+fug2bykbQ7WjhKzT2t9wPxA2IB3cYZ1S9Q2pVQSxmAxQ2s9xzwc9efmya5YeWZ2tNbHgMUYORbVlFHCrOh9vJU4i4nPb5SJ+jOI8fGLEGwpERtj5bMo41folPT4Fa+OWCDllSKKUqqSUqqyfRu4GliPe1mnu4H/mdtfA3eZq1cuAo7bQ8gRIlg75gFXK6Wqm2Hjq81jYadIbsmNGM/Nbttgc7VKU6AlsJoIvN/mXP/7wEat9csup6L63LzZFSPPrLZSqpq5XQHojZEDsgijhBkUf2aeSpx5s7ksEdUxrBSMX/Z7xtwYFiOfRRm/grcteuOXDuPKllj6wVgFsgVjjvfxKNy/GcbKibXA73YbMOaQFwBbzd81tHPFxpumvVlAWhht+RQj3FuA4a0PC8UOYChG4uE24J4I2vaRee915h91fZf2j5u2bQaujdT7DXTHCCevAzLNn+ui/dx82BULz6w98Jtpw3rgSZfPwmrz9X8OlDePJ5v728zzzfzZXJZ+wv3+BHnvmBm/zP5jcgzzYlcsfBZl/AretqiNX1LiSBAEQRAEIUrE69SkIAiCIAhCzCOOmCAIgiAIQpQQR0wQBEEQBCFKiCMmCIIgCIIQJcQREwRBEARBiBLiiAlxg1Kql1Lq22jbIQiCEAoyhpVNxBETBEEQBEGIEuKICSWOUuoOpdRqpVSmUuods9BqrlLqJaXUGqXUAqVUbbNtqlJqpTKKwX5pqjujlGqhlPpJKbXWvKa52X2KUmq2UmqTUmqGqeQsCIIQNmQME8KJOGJCiaKUagMMwigonApYgduBSsAabRQZXgI8ZV4yHXhUa90eQ3nZfnwG8KbWugNwCYa6NUBH4B9AWwxF5Esj/qIEQSgzyBgmhJtE/00EIaxcCXQG0s0vehUwCs/agJlmm4+BOUqpqkA1rfUS8/iHwOdmDbwGWusvAbTWeQBmf6u11tnmfibQBFge+ZclCEIZQcYwIayIIyaUNAr4UGs93u2gUk8Uaeer9pavUP1Zl20r8jcuCEJ4kTFMCCsyNSmUNAuAm5VSdQCUUjWUUo0x/hbtFe7/CizXWh8HjiqlepjH7wSWaK1PANlKqQFmH+WVUhVL9FUIglBWkTFMCCviaQslitZ6g1JqAvCjUioBKADuB04BFyilfgWOY+RgANwNvG0OUjuAe8zjdwLvKKUmmX3cUoIvQxCEMoqMYUK4UVr7ip4KQsmglMrVWqdE2w5BEIRQkDFMCBWZmhQEQRAEQYgSEhETBEEQBEGIEhIREwRBEARBiBLiiAmCIAiCIEQJccQEQRAEQRCihDhigiAIgiAIUUIcMUEQBEEQhCjx/+HIVhG21XbRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "# loss\n",
    "def plot_history_loss(fit):\n",
    "    # Plot the loss in the history\n",
    "    axL.plot(fit.history['loss'],label=\"loss for training\")\n",
    "    axL.plot(fit.history['val_loss'],label=\"loss for validation\")\n",
    "    axL.set_title('model loss')\n",
    "    axL.set_xlabel('epoch')\n",
    "    axL.set_ylabel('loss')\n",
    "    axL.set_ylim([0,3])\n",
    "    axL.legend(loc='upper right')\n",
    "\n",
    "# acc\n",
    "def plot_history_acc(fit):\n",
    "    # Plot the loss in the history\n",
    "    axR.plot(fit.history['acc'],label=\"loss for training\")\n",
    "    axR.plot(fit.history['val_acc'],label=\"loss for validation\")\n",
    "    axR.set_title('model accuracy')\n",
    "    axR.set_xlabel('epoch')\n",
    "    axR.set_ylabel('accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "plot_history_loss(history)\n",
    "plot_history_acc(history)\n",
    "plt.show()\n",
    "fig.savefig('../logs/'+IN_DIR_PATH+'/loss_acc.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0AAAAKOCAYAAACP9nkyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdd7xdZZXw8d9KQhAISC9JwNARIiAkKHUCCNIRhQEEDSrEgqKOOmAZiW1E5AVBnEFQBEYFREQ6JIMgRWrovQgMSeggEErKZb1/nB08ZJ9bgHvPPsn+ff3sT85+dltnea/myXqeZ0dmIkmSJEl1MKjqACRJkiSpXewASZIkSaoNO0CSJEmSasMOkCRJkqTasAMkSZIkqTbsAEmSJEmqDTtAkrQAiYhFIuL8iHghIs56B/fZLyIm9WdsVYmILSPivqrjkCR1hvA9QJLUfhHxceDfgHWAl4BbgR9l5tXv8L6fAL4EbJaZc95xoB0uIhJYMzMfrDoWSdL8wQqQJLVZRPwb8DPgP4EVgFWA/wJ274fbvwe4vw6dn76IiCFVxyBJ6ix2gCSpjSLi3cD3gYMz80+Z+XJmzs7M8zPzG8U5C0fEzyJierH9LCIWLo6Ni4ipEfG1iHgqIh6PiE8Vx74HfBfYOyJmRMRnImJiRPy26fmjIiLndgwi4oCI+HtEvBQRD0fEfk3tVzddt1lE3FgMrbsxIjZrOnZFRPwgIq4p7jMpIpbt5vvPjf/fm+L/SETsFBH3R8RzEfGtpvM3iYhrI+IfxbnHR8TQ4tiVxWm3Fd9376b7HxoRTwC/mdtWXLN68YyNiv3hEfFMRIx7R//FSpLmG3aAJKm9NgXeBZzTwznfBj4IbAhsAGwCfKfp+IrAu4ERwGeAX0TEUpl5OI2q0pmZOSwzf91TIBGxGHAcsGNmLg5sRmMo3rznLQ1cWJy7DHA0cGFELNN02seBTwHLA0OBr/fw6BVp5GAEjQ7bScD+wMbAlsB3I2K14twu4KvAsjRyty3wBYDM3Ko4Z4Pi+57ZdP+laVTDJjQ/ODMfAg4FfhcRiwK/AU7JzCt6iFeStACxAyRJ7bUM8EwvQ9T2A76fmU9l5tPA94BPNB2fXRyfnZkXATOAtd9mPK8DoyNikcx8PDPvanHOzsADmfk/mTknM08H7gV2bTrnN5l5f2a+CvyBRuetO7NpzHeaDZxBo3NzbGa+VDz/LmB9gMyckpnXFc99BPgl8C99+E6HZ+bMIp43ycyTgAeA64GVaHQ4JUk1YQdIktrrWWDZXuamDAcebdp/tGh74x7zdKBeAYa91UAy82Vgb+BzwOMRcWFErNOHeObGNKJp/4m3EM+zmdlVfJ7bQXmy6firc6+PiLUi4oKIeCIiXqRR4Wo5vK7J05n5Wi/nnASMBn6emTN7OVeStACxAyRJ7XUt8BrwkR7OmU5j+NZcqxRtb8fLwKJN+ys2H8zMSzNzOxqVkHtpdAx6i2duTNPeZkxvxX/TiGvNzFwC+BYQvVzT4/KmETGMxiIUvwYmFkP8JEk1YQeoJiLiyxFxZ0TcFRFfqTqeThERO0TEfRHxYEQcVnU8ncCclPVnTjLzBRrzXn5RTP5fNCIWiogdI+LI4rTTge9ExHLFYgLfBX7b3T17cSuwVUSsUizA8M25ByJihYjYrZgLNJPGULquFve4CFgrIj4eEUMiYm8aQ9S+ExEP0ugMDZTFgReBGUV16vPzHH8SWK10Vc+OBaZk5oE05jad8I6jxN+dVsxJmTlpzbyonewA1UBEjAYOojGRegNgl4hYs9qoqhcRg4FfADsC6wL7RsS61UZVLXNSNhA5ycyjabwD6DvA08BjwBeBPxen/BC4CbgduAO4uWh7O8+aDJxZ3GsKcEHT4UHA12hUeJ6jMbfmCy3u8SywS3Hus8C/Ay8A29PIyfI0KkgD4es0Flh4iUZ16sx5jk8ETi1WifvX3m4WEbsDO9AY9geN/x42mrv63dvl706ZOSkzJ62ZF7WbL0KtgYjYC/hw8a+dRMR/ADMz88ier1ywRcSmwMTM/HCx/02AzPxxpYFVyJyUmZMyc1JmTsrMSZk5ac28qN2sANXDnTSGwCxTLPu6E7ByxTF1ghE0/uV9rqm8eVJ3HZmTMnNSZk7KzEmZOSkzJ62ZF7VVpR2giNgjGi/ka7XqEBGxZER8oWl/VETc2Q/PHRdvfonfKRGx5zu9b6fKzHuAnwCTgUuA2wDfEt96InXdS6LmpMyclJmTMnNSZk7KzElr5kVtVXUFaF/gamCfeQ8U40GXpMV49H4wjsYL/96xaKg6j73KzF9n5kbFiwOfo/EOjLqbypsrYSN5+yttLSjMSZk5KTMnZeakzJyUmZPWzIvaqrK/uBfLkG5O4y3m+xRt4yLi8oj4PY2Jv0cAq0fErRHx0x7utXpEXBIRUyLiqrkVpYjYNSKuj4hbIuJ/ixWPRtGY/PrV4r5bFrfZKiL+FhF/b64GRcQ3IuLGiLg9Ir5XtI2KiHsi4r9oTE7u+OFkEbF88ecqwEdprDJVdzcCa0bEqhExlMbP4XkVx1Q1c1JmTsrMSZk5KTMnZeakNfOitqpsEYSI2B/YOjM/ExF/o7EC0hI0liQdnZkPF52VCzJzdHHNm/ab7nUZ8LnMfCAiPgD8ODO3iYilgH9kZkbEgcB7M/NrETERmJGZRxXXnwIsRuOFgOsA52XmGhGxPbAn8Fka5dnzgCOB/wP+DmyWmdd18/0mABMAhr5r0Y2XW+WtrtLav55+7O+83tUFBO9efkXetehbfmdiv1ph2MKVPn+uF154galTHyMzWWaZZVlppYFayGr+YU7KzEmZOSkzJ2XmpMyctNZpebn55inPZOZylQYxgAYv8Z7MOa/2fmI/yVefvjQzd2jbA3vR05vIB9q+NF5EB3BGsX8hcENmPtzXmxSVpM2AsyLeGEI692/XI4EzI2IlYCjQ033/nJmvA3dHxApF2/bFdkuxPwxYk0YH6NHuOj8AmXkicCLAyLXfl1864c/dnVpLX95y9apDkCRJammRheLRqmMYSDnnVRZeu9c3B/Sb1279xbJte1gfVNIBiohlgG2A0RGRwGAak90uovHW8rdiEI0qz4Ytjv0cODozz4uIcTTeF9Gdmc0hNv3548z85Tzxj3obcUqSJEmqWFVzgPYETsvM92TmqMxcmUZ1Zot5znuJxlvAu5WZLwIPF++6mbsowQbF4XcD04rP49/KfQuXAp8uqkxExIi5c2kkSZKk+VNADGrf1mGqimhf4Jx52s6m8bbvNxRvH78mIu5sWgRh7YiY2rTtBewHfCYibgPuAnYvzp1IY2jcVcAzTbc+H9hjnkUQSjJzEvB74NqIuAP4I33rOEmSJEnqQJUMgcvMcS3ajgOOa9H+8XmaFurmtqWJVZl5LnBui/b7gfWbmq6a5/iwps/HAse2eN7oFm2SJElSZwsgWr1+qR46ryYlSZIkSQOkylXgJEmSJFWhA+fmtEt9v7kkSZKk2rEDJEmSJKk2HAInSZIk1Y2LIEiSJEnSgs8KkCRJklQr4SIIkiRJklQHVoAkSZKkunEOkCRJkiQt+KwASZIkSXUSOAdIkiRJkurACpAkSZJUK+EcIEmSJEmqAytAkiRJUt04B0iSJEmSFnx2gCRJkiTVhkPgJEmSpLpxEQRJkiRJar+IWDkiLo+IeyLiroj4ctG+dERMjogHij+X6ub68cU5D0TE+N6eZwdIkiRJqpVoLILQrq13c4CvZeZ7gQ8CB0fEusBhwGWZuSZwWbH/5m8SsTRwOPABYBPg8O46SnPZAZIkSZJUmcx8PDNvLj6/BNwDjAB2B04tTjsV+EiLyz8MTM7M5zLzeWAysENPz3MOkCRJklQnQbvnAC0bETc17Z+YmSe2OjEiRgHvB64HVsjMx6HRSYqI5VtcMgJ4rGl/atHWLTtAkiRJkgbSM5k5preTImIYcDbwlcx8MfrWSWt1UvZ0gUPgJEmSpLrprDlARMRCNDo/v8vMPxXNT0bESsXxlYCnWlw6FVi5aX8kML2nZ9kBkiRJklSZaJR6fg3ck5lHNx06D5i7qtt44NwWl18KbB8RSxWLH2xftHXLIXCSJElSrUSfKzNtsjnwCeCOiLi1aPsWcATwh4j4DPB/wF4AETEG+FxmHpiZz0XED4Abi+u+n5nP9fQwO0CSJEmSKpOZV9N6Lg/Ati3Ovwk4sGn/ZODkvj7PDlAbrDBsYb685epVh9FRVp5wZtUhdJz7fv6xqkPoOIsu7P9ESZI0IAa1dRW4jtJRtS9JkiRJGkj+86okSZJUJ0GnzQFqq/p+c0mSJEm1YwdIkiRJUm04BE6SJEmqm3ARBEmSJEla4FkBkiRJkmql416E2lb1/eaSJEmSascKkCRJklQ3zgGSJEmSpAWfFSBJkiSpbpwDJEmSJEkLPitAkiRJUp1EOAdIkiRJkurACpAkSZJUN84BkiRJkqQFnx0gSZIkSbXhEDhJkiSpblwEQZIkSZIWfFaAJEmSpFoJF0GQJEmSpDqwAiRJkiTVjXOAJEmSJGnBZwVIkiRJqpPAOUCSJEmSVAdWgCRJkqRacRU4SZIkSaoFK0CSJElS3bgKnOpg0qWXsP56a7PeOmvw0yOPqDqcShz7qbHc/bPdufL7O7zRdtLnNuXyidtz+cTtmXLkLlw+cfsKI6zWlz5/IGuPGs7mYzesOpSO4u9OmTkpMydl5qTMnLRmXtROdoBqoquri68ccjDnnn8xt9x+N2edcTr33H131WG13RnXPMI+R1/5praDTriWrSdOYuuJk7hgylQumDK1ouiqt+9+4/nDny+oOoyO4u9OmTkpMydl5qTMnLRmXtRudoBq4sYbbmD11ddg1dVWY+jQoey19z5ccP65VYfVdtfe/zTPvzyz2+O7j12Zc67/vzZG1Fk222JLllpq6arD6Cj+7pSZkzJzUmZOysxJa+alIjGofVuH6byINCCmT5/GyJErv7E/YsRIpk2bVmFEnWfTtZbj6Rdf4+9Pzag6FHUQf3fKzEmZOSkzJ2XmpDXzonbruEUQImIF4Bjgg8DzwCzgyMw8Z4Ce921gr2L3fcAdxeeTi2e/kpmnRcQpwAWZ+ceBiGOgZWapLWo8+a2VPT6wCn+qcfVHrfm7U2ZOysxJmTkpMyetmZeK1DjHHdUBisZP+5+BUzPz40Xbe4Dd5jlvSGbO6Y9nZuaPgB8V952RmQvk7O8RI0Yydepjb+xPmzaV4cOHVxhRZxk8KNh5o5F86PuTqg5FHcbfnTJzUmZOysxJmTlpzbyo3TptCNw2wKzMPGFuQ2Y+mpk/j4gDIuKsiDgfmAQQEd+IiBsj4vaI+N7cayJi/4i4ISJujYhfRsTgon1GRPwoIm6LiOuKalO3ImJiRHy9RfvGEfHXiJgSEZdGxEr9lYCBMmbsWB588AEeefhhZs2axVlnnsHOu+zW+4U18S/rrsCDT7zI48+/WnUo6jD+7pSZkzJzUmZOysxJa+alAhHOAeog6wE393B8U2B8Zm4TEdsDawKbABsCG0fEVhHxXmBvYPOimtMF7FdcvxhwXWZuAFwJHPRWA4yIhYCfA3tm5sY0hsr96K3ep92GDBnCMccez647f5gN3/dePrbXv7LueutVHVbb/fKzH+Tib3+INVZcnNuO2pX9tlwVgD02cfgbwEEH7M8O22zJgw/cx+i1RvHbU0+uOqTK+btTZk7KzEmZOSkzJ62ZF7VbtBp3WZWIOARYNTO/Wuz/AtiCxlycXwD/kpmfKo4dBewJ/KO4fBjwY2AR4FvAU0X7IsDpmTkxImYC78rMjIi9ge0y88Cm58/IzGFN+xOBGZl51Nw5QMC9wN+AvxenDQYez8w3vTwmIiYAEwBWXmWVje9/6NF3mp4FysoTzqw6hI5z388/VnUIHWfRhTtqlK4kqSYWWSimZOaYquMYKIOWGpULb/0fbXvea+cc2FH57LS/XdwFvPG3wMw8OCKWBW4qml5uOjeAH2fmL5tvEBFfojGH6Jst7j87/9nj6+Ltff8A7srMTXs6KTNPBE4E2HjjMZ3Ty5QkSZJqrNOGwP0FeFdEfL6pbdFuzr0U+HREDAOIiBERsTxwGbBn8ZmIWLpYSKG/3AcsFxGbFvdfKCKs00qSJGm+ERFt2zpNR1WAiqFpHwGOiYh/B56mUfU5lMZQtuZzJxXzfa4tEjsD2D8z746I7wCTImIQMBs4GOiXMWiZOSsi9gSOi4h308jhz2hUryRJkiR1sI7qAAFk5uPAPt0cPmWec48Fjm1xjzOB0iST5vk9xft8/tjd8WJ/YtPnA5o+3wps1e2XkCRJkjpUUO93LXXaEDhJkiRJGjB2gCRJkiTVRscNgZMkSZI0gKLYasoKkCRJkqTasAIkSZIk1UpnLk/dLlaAJEmSJNWGFSBJkiSpZqwASZIkSVINWAGSJEmSasYKkCRJkiTVgBUgSZIkqWasAEmSJElSDVgBkiRJkuokiq2mrABJkiRJqg0rQJIkSVKNBNFRc4Ai4mRgF+CpzBxdtJ0JrF2csiTwj8zcsMW1jwAvAV3AnMwc09vz7ABJkiRJqtIpwPHAaXMbMnPvuZ8j4v8BL/Rw/daZ+UxfH2YHSJIkSVJlMvPKiBjV6lg0SlX/CmzTX8+zAyRJkiTVTCcNgevFlsCTmflAN8cTmBQRCfwyM0/s7YZ2gCRJkiQNpGUj4qam/RP70lEp7Auc3sPxzTNzekQsD0yOiHsz88qebmgHSJIkSaqZNleAnunL4gTzioghwEeBjbs7JzOnF38+FRHnAJsAPXaAXAZbkiRJUif6EHBvZk5tdTAiFouIxed+BrYH7uztpnaAJEmSpJqJiLZtfYjldOBaYO2ImBoRnykO7cM8w98iYnhEXFTsrgBcHRG3ATcAF2bmJb09zyFwkiRJkiqTmft2035Ai7bpwE7F578DG7zV59kBkiRJkuokiq2mHAInSZIkqTasAEmSJEk1Mx+9B6jfWQGSJEmSVBtWgCRJkqQaCfq2OtuCyg6QKvHYiXtXHULHWWrsF6sOoeM8f+PxVYcgSZIWMA6BkyRJklQbVoAkSZKkmqnzEDgrQJIkSZJqwwqQJEmSVDf1LQBZAZIkSZJUH1aAJEmSpDoJ5wBJkiRJUi1YAZIkSZJqxgqQJEmSJNWAFSBJkiSpZqwASZIkSVINWAGSJEmSaiQIK0CSJEmSVAd2gCRJkiTVhkPgJEmSpLqp7wg4K0CSJEmS6sMKkCRJklQn4TLYkiRJklQLVoAkSZKkmrECJEmSJEk1YAVIkiRJqhkrQJIkSZJUA1aAJEmSpLqpbwHICpAkSZKk+rACJEmSJNWMc4AkSZIkqQbsAEmSJEmqDTtANTLp0ktYf721WW+dNfjpkUdUHU5HMCcwcoUlueTEQ7jl7O8w5Y/f5uB9xwHw0Q+9nyl//DYvTzmOjdZdpdogK+bPSZk5KTMnZeakzJy0Zl7aKyLaunUaO0A10dXVxVcOOZhzz7+YW26/m7POOJ177r676rAqZU4a5nS9zmFH/4n3f+yH/Msnj+Kze2/FOqutyF0PTWefr53E1Tc/VHWIlfLnpMyclJmTMnNSZk5aMy9qNztANXHjDTew+uprsOpqqzF06FD22nsfLjj/3KrDqpQ5aXjimRe59d6pAMx4ZSb3PvwEw5dbkvsefpIHHn2q4uiq589JmTkpMydl5qTMnLRmXqphBUgLvOnTpzFy5Mpv7I8YMZJp06ZVGFH1zEnZKistzYZrj+TGOx+pOpSO4c9JmTkpMydl5qTMnLRmXtRulXSAImKPiMiIWKeb40tGxBfmaVszIi6IiIciYkpEXB4RWw1gjEtHxOSIeKD4c6mifZ2IuDYiZkbE1wfq+f0tM0ttndgjbydz8maLLTKU0486kG8cdTYvvfxa1eF0DH9OysxJmTkpMydl5qQ181INK0Dtty9wNbDPvAciYjCwJPCFprZ3ARcCJ2bm6pm5MfAlYLUW1/fXu40OAy7LzDWBy4p9gOeAQ4Cj+uk5bTFixEimTn3sjf1p06YyfPjwCiOqnjn5pyFDBnH6UQdx5sU3ce5fbqs6nI7iz0mZOSkzJ2XmpMyctGZe1G5t7wBFxDBgc+AzFB2giBhXVHR+D9wBHAGsHhG3RsRPgf2AazPzvLn3ycw7M/OU4vqJEXFiREwCTouIURFxVUTcXGybFeetFBFXFve9MyK2jIjBEXFKsX9HRHy1eMTuwKnF51OBjxTPfSozbwRmD2ii+tmYsWN58MEHeOThh5k1axZnnXkGO++yW9VhVcqc/NMJh+/HfQ8/wXG//UvVoXQcf07KzEmZOSkzJ2XmpDXzUpFo49Zh+qta8lZ8BLgkM++PiOciYqOifRNgdGY+HBGjis8bAkTE0cDNvdx3Y2CLzHw1IhYFtsvM1yJiTeB0YAzwceDSzPxRUWlaFNgQGJGZo4tnLVncb4XMfBwgMx+PiOX75+tXY8iQIRxz7PHsuvOH6erqYvwBn2bd9darOqxKmZOGzTZcjf12+QB33D+N685oFDoPP/48Fl5oCEcfuhfLLjWMPx33OW6/bxq7HfyLiqNtP39OysxJmTkpMydl5qQ186J2i1bjLgf0gREXAj/LzMkRcQiwMo3hbYdn5tbFOaOAC5o6JUcDj2bmscX+OcCawP2Z+dGImAhkZn6vOP5u4HganZsuYK3MXLSYM3Qy8Fvgz5l5azG35ybgoiKOSZn5ekT8IzPndoaIiOczc6mm/YnAjMxsORQuIiYAEwBWXmWVje9/6NF3mjot4JYa+8WqQ+g4z994fNUhSJJqaJGFYkpmjqk6joGy8Apr5oj9jm3b8x4+ZueOymdbh8BFxDLANsCvIuIR4BvA3jSKYy/3cOldwNxKEZm5B3AAsHTTOc3XfxV4EtiARuVnaHHdlcBWwDTgfyLik5n5fHHeFcDBwK+KezwZESsVca8EvKX1gDPzxMwck5ljllt2ubdyqSRJkqQB0u45QHsCp2XmezJzVGauDDwMbDHPeS8Bizft/x7YPCKaB4Qu2sNz3g08npmvA58ABgNExHuApzLzJODXwEYRsSwwKDPPBv6Df3a0zgPGF5/HAy5IL0mSpPlf1HsVuHbPAdqXxgIHzc4GPg+88br5zHw2Iq6JiDuBizPzGxGxC3B0RPyMRnXnJeCH3Tznv4CzI2Iv4HL+WR0aB3wjImYDM4BPAiOA30TE3M7gN4s/jwD+EBGfAf4P2AsgIlakMWRuCeD1iPgKsG5mvviWsyFJkiSprdraAcrMcS3ajgOOa9H+8Xn27wV26ua+E+fZfwBYv6npm0X7qfxzZbdmG83bkJnPAtu2aH8CGNkqDkmSJKnTBdCBhZm2qeo9QJIkSZLUdnaAJEmSJNVGFe8BkiRJklSZzlycoF2sAEmSJEmqDStAkiRJUs3UuABkBUiSJElSfVgBkiRJkmrGOUCSJEmSVANWgCRJkqQ6CecASZIkSVItWAGSJEmSaiSAQYPqWwKyAiRJkiSpMhFxckQ8FRF3NrVNjIhpEXFrse3UzbU7RMR9EfFgRBzWl+fZAZIkSZJqJqJ9Wx+cAuzQov2YzNyw2C4qf4cYDPwC2BFYF9g3Itbt7WF2gCRJkiRVJjOvBJ57G5duAjyYmX/PzFnAGcDuvV1kB0iSJEnSQFo2Im5q2ib08bovRsTtxRC5pVocHwE81rQ/tWjrkYsgSJIkSTXT5hehPpOZY97iNf8N/ADI4s//B3x6nnNafYns7cZWgCRJkiR1lMx8MjO7MvN14CQaw93mNRVYuWl/JDC9t3vbAZIkSZLqpI0LILzdQlNErNS0uwdwZ4vTbgTWjIhVI2IosA9wXm/3dgicJEmSpMpExOnAOBpzhaYChwPjImJDGkPaHgE+W5w7HPhVZu6UmXMi4ovApcBg4OTMvKu359kBkiRJkmokaPscoB5l5r4tmn/dzbnTgZ2a9i8CSktk98QhcJIkSZJqwwqQJEmSVCvRURWgdrMCJEmSJKk2rABJkiRJNVPjApAVIEmSJEn1YQVIkiRJqhnnAEmSJElSDdgBkiRJklQbDoGTOsTzNx5fdQgd54I7p1cdQkfaZfTwqkOQtAB5ZeacqkNQu4WLIEiSJElSLVgBkiRJkmokcBEESZIkSaoFK0CSJElSzdS4AGQFSJIkSVJ9WAGSJEmSasY5QJIkSZJUA1aAJEmSpJqpcQHICpAkSZKk+rACJEmSJNVJOAdIkiRJkmrBDpAkSZKk2nAInCRJklQjgYsgSJIkSVItWAGSJEmSaiVcBEGSJEmS6sAKkCRJklQzNS4AWQGSJEmSVB9WgCRJkqSacQ6QJEmSJNWAFSBJkiSpTsI5QJIkSZJUC1aAJEmSpBoJnAMkSZIkSbVgBUiSJEmqGStAkiRJklQDdoAkSZIk1YZD4CRJkqSaqfEIOCtAdTLp0ktYf721WW+dNfjpkUdUHU5HMCdl5uTNpj3yIF/fe7s3tk9usTYX/u6kqsOqnD8nZeakzJyUmZOyL33+QNYeNZzNx25YdSiqCTtANdHV1cVXDjmYc8+/mFtuv5uzzjide+6+u+qwKmVOysxJ2YhRa3DUmZM56szJ/OT3lzD0XYuwydY7Vh1Wpfw5KTMnZeakzJy0tu9+4/nDny+oOozaiYi2bZ3GDlBN3HjDDay++hqsutpqDB06lL323ocLzj+36rAqZU7KzEnP7rzhalYc+R6WGz6y6lAq5c9JmTkpMydl5qS1zbbYkqWWWrrqMFQjdoBqYvr0aYwcufIb+yNGjGTatGkVRlQ9c1JmTnp2zaXnsvkOH6k6jMr5c1JmTsrMSZk5UceIxhygdm2dpvIOUESsGBFnRMRDEXF3RFwUEWv18dpREXFni/ZFI+J3EXFHRNwZEVdHxLDuzn+bcY+LiM36417tkJmltk4sSbaTOSkzJ92bPXsWN/11Eptut0vVoVTOn5Myc1JmTsrMidQZKl0FLhq/9ecAp2bmPkXbhsAKwP29XDu4h8NfBp7MzPcV564NzO6XoP9pHDAD+Fs/33dAjBgxkqlTH3tjf9q0qQwfPrzCiKpnTsrMSfduvfpyVl3nfSy5zHJVh1I5f07KzEmZOSkzJ+oUQWfOzSgOoeUAACAASURBVGmXqitAWwOzM/OEuQ2ZeStwdUT8tKje3BERe8MbVZfLI+L3wB3NN4qI1SLilogYC6wETGu6532ZObPYHRwRJ0XEXRExKSIWKa5fPSIuiYgpEXFVRKxTtC8XEWdHxI3FtnlEjAI+B3w1Im6NiC0HKkH9ZczYsTz44AM88vDDzJo1i7POPIOdd9mt6rAqZU7KzEn3rr7kz2zh8DfAn5NWzEmZOSkzJ1JnqPo9QKOBKS3aPwpsCGwALAvcGBFXFsc2AUZn5sNFR2RuhecM4FOZeWtEzAYmRcSewGU0KkwPFNevCeybmQdFxB+AjwG/BU4EPpeZD0TEB4D/ArYBjgWOycyrI2IV4NLMfG9EnADMyMyjWn2xiJgATABYeZVV3m5++s2QIUM45tjj2XXnD9PV1cX4Az7NuuutV3VYlTInZeaktZmvvsrt11/JhO/8pOpQOoI/J2XmpMyclJmT1g46YH+uueqvPPvsM4xeaxSHffu77D/+01WHtcCrcQGIaDUetW0PjzgEWDUzvzpP+zHAHZl5crH/P8BZwIvA4Zm5ddE+CrgeeB74WGbe1XSPYcD2wIeAjwObAq8CkzNzzeKcQ4GFgJ8BTwP3NYWxcNHReQqY3tS+HLAO8DV66AA123jjMXnN9Tf1JSWSmlxw5/TeT6qhXUY7ZEZS/3ll5pyqQ+g4ywxbaEpmjqk6joGyxCrvzbHfOLltz/vLIZt1VD6rrgDdBezZor2nPunL8+y/ADwGbF7cD4DMnAH8CfhTRLwO7AScDcxsurYLWITGUMB/ZGarN3ANAjbNzFffFGCdu82SJEmarw2q8d9lq54D9Bdg4Yg4aG5DMYfneWDviBgcEcsBWwE3dHOPWcBHgE9GxMeLe2weEUsVn4cC6wKPdhdEZr4IPBwRexXXRERsUByeBHyxKb65naSXgMXf4veVJEmSVKFKO0DZGH+3B7BdsQz2XcBE4PfA7cBtNDpJ/56ZT/Rwn5eBXWgsSrA7sDrw14i4A7gFuIlG9acn+wGfiYjbaFSSdi/aDwHGRMTtEXE3jcUPAM4H9phfFkGQJEmSVP0QODJzOvCvLQ59o9iaz70CuKJp/xEaCymQmf8AxjadflqLe75xfnHNUU2fHwZ2aBHfM8DeLdrvB9Zv8QxJkiSpo9V4BFzlQ+AkSZIkqW0qrwBJkiRJap+Iei/oZQVIkiRJUm1YAZIkSZJqZlB9C0BWgCRJkiTVhxUgSZIkqWacAyRJkiRJNWAFSJIkSaqZTioARcTJwC7AU5k5umj7KbArMAt4CPhU8d7Pea99BHgJ6ALmZOaY3p5nBUiSJElSlU4BdpinbTIwOjPXB+4HvtnD9Vtn5oZ96fyAHSBJkiSpVgKINv6nN5l5JfDcPG2TMnNOsXsdMLK/vr8dIEmSJEkDadmIuKlpm/AWr/80cHE3xxKYFBFT+npf5wBJkiRJGkjP9HV42rwi4tvAHOB33ZyyeWZOj4jlgckRcW9RUeqWHSBJkiSpZuaHF6FGxHgaiyNsm5nZ6pzMnF78+VREnANsAvTYAXIInCRJkqSOEhE7AIcCu2XmK92cs1hELD73M7A9cGdv97YCJEmSJNVJREe9CDUiTgfG0ZgrNBU4nMaqbwvTGNYGcF1mfi4ihgO/ysydgBWAc4rjQ4DfZ+YlvT3PDpAkSZKkymTmvi2af93NudOBnYrPfwc2eKvPswMkSZIk1UwHFYDazjlAkiRJkmrDCpAkSZJUIwEMqnEJyAqQJEmSpNqwAiRJkiTVTI0LQFaAJEmSJNWHFSBJkiSpZjrpPUDtZgVIkiRJUm3YAZIkSZJUGw6BkyRJkmokwkUQJEmSJKkWrAC1waw5r/PYs69UHUZHWWbY0KpD6DiLLuyv47x2GT286hAkaYE39blXqw5BFfBFqJIkSZJUA/6TsyRJklQz9a3/WAGSJEmSVCNWgCRJkqSa8UWokiRJklQDVoAkSZKkGglgUH0LQFaAJEmSJNWHFSBJkiSpTiKcAyRJkiRJdWAFSJIkSaqZGheArABJkiRJqg87QJIkSZJqo9shcBGxRE8XZuaL/R+OJEmSpIFW50UQepoDdBeQNJYKn2vufgKrDGBckiRJktTvuu0AZebK7QxEkiRJ0sDzRah9EBH7RMS3is8jI2LjgQ1LkiRJkvpfrx2giDge2Br4RNH0CnDCQAYlSZIkaeBE8TLUdmydpi/vAdosMzeKiFsAMvO5iBg6wHFJkiRJUr/rSwdodkQMorHwARGxDPD6gEYlSZIkacB0Xl2mffoyB+gXwNnAchHxPeBq4CcDGpUkSZIkDYBeK0CZeVpETAE+VDTtlZl3DmxYkiRJkgZCBAzqwLk57dKXIXAAg4HZNIbB9WnlOEmSJEnqNH1ZBe7bwOnAcGAk8PuI+OZAByZJkiRpYES0b+s0fakA7Q9snJmvAETEj4ApwI8HMjBJkiRJ6m99Gc72KG/uKA0B/j4w4UiSJEnSwOm2AhQRx9CY8/MKcFdEXFrsb09jJThJkiRJ86FOfEFpu/Q0BG7uSm93ARc2tV83cOFIkiRJ0sDptgOUmb9uZyCSJEmS2qPGBaA+rQK3ekScERG3R8T9c7d2BKf+M/O11/jYDlux6zYfYKetxnDskT+sOqTKfenzB7L2qOFsPnbDqkPpKJMuvYT111ub9dZZg58eeUTV4XQEc1JmTsrMSZk5KTMnrXV1dfGvO27BFw/Yq+pQVAN9WQThFOA3QAA7An8AzhjAmDQAhi68MKedfRHn/+V6zr3sWq66fDK3Trmh6rAqte9+4/nDny+oOoyO0tXVxVcOOZhzz7+YW26/m7POOJ177r676rAqZU7KzEmZOSkzJ2XmpHu/O/m/WW2NtaoOozaCYFC0b+s0fekALZqZlwJk5kOZ+R1g64ENS/0tIlhssWEAzJk9mzlzZtd68hvAZltsyVJLLV11GB3lxhtuYPXV12DV1VZj6NCh7LX3Plxw/rlVh1Upc1JmTsrMSZk5KTMnrT35+DSuuuxS9thnfNWhqCb60gGaGY2/KT8UEZ+LiF2B5Qc4Lg2Arq4udtv2g2w6ehSbb7UNG2w0tuqQ1GGmT5/GyJErv7E/YsRIpk2bVmFE1TMnZeakzJyUmZMyc9LakRMP46vf+j6DBvXlr6XqF218CWon/nt7X37SvgoMAw4BNgcOAj7dHw+PiK6IuLVpO6yX8ydGxNdbtA+PiD8Wn8dFxAsRcUtE3BMRh/dHrE3PGhURd/Z+ZucZPHgw5112HVfecj+33zKF+++5q+qQ1GEys9RW90qhOSkzJ2XmpMyclJmTsr/+78UsveyyrLv++6sORTXS0zLYAGTm9cXHl4BP9PPzX83MdzwDPTOnA3s2NV2VmbtExGLArRFxQWZOeafPWVAs8e4l2WSzLbnq8sms9d71qg5HHWTEiJFMnfrYG/vTpk1l+PDhFUZUPXNSZk7KzEmZOSkzJ2W33nQ9V0y+mKsvn8zMma/x8ksv8c0vH8iPj/1V1aEt8Orc+e62AhQR50TEn7rbBjKoiHgkIr4XETdHxB0RsU7T4Q0i4i8R8UBEHFSc37Iqk5kvA1OA1SPigIg4vukZF0TEuOLzjIj4UUTcFhHXRcQKRfsKRR5uK7bNissHR8RJEXFXREyKiEUGKBX95rlnnubFF/4BwGuvvsrfrrqc1dZYu+Ko1GnGjB3Lgw8+wCMPP8ysWbM468wz2HmX3aoOq1LmpMyclJmTMnNSZk7KvnzYRCbfcC8X/+1OfnL8bxi72VZ2fjTgeqoAHd/Dsf6ySETc2rT/48w8s/j8TGZuFBFfAL4OHFi0rw98EFgMuCUiml/S+iYRsUxx7g+Ania8LAZcl5nfjogjaQzz+yFwHPDXzNwjIgbTGAq4FLAmsG9mHhQRfwA+Bvx2nmdPACYADG8a71uVp556gkMPmcDrXV28/vrr7Ljbx9h6+x2rDqtSBx2wP9dc9VeeffYZRq81isO+/V32H98vozvnW0OGDOGYY49n150/TFdXF+MP+DTrrlfvKqE5KTMnZeakzJyUmRN1kjrPuIpW41Hb9vCIGZk5rEX7I8DmmTktIj4A/CgzPxQRE4FBmfnd4rzTgD8BtwIXZObooqpzLvB34HXgpMw8ISIOAMZk5heLay8AjsrMKyJiJvCuzMyI2BvYLjMPjIingZGZObMptlHA5Mxcs9g/FFgoM7t9sc77Ntgo/zTp6neQqQXPMsOGVh1Cx1l04V5HpEqS1O/uf/ylqkPoOBusssSUzBxTdRwDZfk1RufePz2rbc87/qPrdlQ+O/lvXHM7HV28Oc55e2ytenBXZeYu87TN4c2d3Xc1fZ6d/+wJzvu8nmKbe37HD4GTJEmSNH9Wv3aPiHcVw9vGATf28bpHgA0jYlBErAxs0odrLgM+DxARgyNiibcRryRJktQxgsYiCO3aOk2fO0ARsfAAPH+ReZbBPqIP19wAXAhcB/ygWAGuL64BHgbuAI4Cbu7DNV8Gto6IO2gspuBAXUmSJGk+1usQuIjYBPg18G5glYjYADgwM7/0Th+emYO7aR/V9PkmGpUeMnNiN+c/AowuPl8BXNHinAT26+b6YU2f/wj8sfj8JLB7i0tGN51/VKt7SpIkSZ1qUOcVZtqmLxWg44BdgGcBMvM2YOuBDEqSJEmSBkJfFkEYlJmPzjN+r2uA4pEkSZI0wOpcAepLB+ixYhhcFu/C+RJw/8CGJUmSJEn9ry8doM/TGAa3CvAk8L9FmyRJkqT5TAQduTpbu/Q6Bygzn8rMfTJz2WLbJzOfaUdwkiRJkhZsEXFyRDwVEXc2tS0dEZMj4oHiz6W6uXZ8cc4DETG+L8/ryypwJ9HiZaOZOaEvD5AkSZLUWTpsDtApwPHAaU1thwGXZeYREXFYsX9o80URsTRwODCGRn9lSkScl5nP9/SwvqwC9780Xgh6GY136SwPzOzTV5EkSZKkHmTmlcBz8zTvDpxafD4V+EiLSz8MTM7M54pOz2Rgh96e12sFKDPPbN6PiP8pbi5JkiRpPtTmKUDLRsRNTfsnZuaJvVyzQmY+DpCZj0fE8i3OGQE81rQ/tWjrUV8WQZjXqsB73sZ1kiRJkurnmcwcMwD3bdWNK03dmVdf5gA933SjQTTKU4e9pdAkSZIkqe+ejIiViurPSsBTLc6ZCoxr2h8JXNHbjXvsAEVjfbwNgGlF0+uZ2WuvSpIkSVJnCmBQ5y+DfR4wHjii+PPcFudcCvxn0wpx2wPf7O3GPS6CUHR2zsnMrmKz8yNJkiSp30TE6cC1wNoRMTUiPkOj47NdRDwAbFfsExFjIuJXAJn5HPAD4MZi+37R1qO+zAG6ISI2ysyb39Y3kiRJktRR+rIUdLtk5r7dHNq2xbk3AQc27Z8MnPxWntdtBygihmTmHGAL4KCIeAh4mUbVLDNzo7fyIEmSJEmqWk8VoBuAjWi95rYkSZKk+VTnTwEaOD11gAIgMx9qUyySJEmSNKB66gAtFxH/1t3BzDx6AOKRJEmSNIAiYn5YBW7A9NQBGgwMo/ULhiRJkiRpvtNTB+jxzPx+2yKRJEmS1BY1LgD1uAJejdMiSZIkaUHUUwWotO62JEmSpPnfoBqXOrqtAPXlLaqSJEmSND/pqQIkSZIkaQETUOtV4HqaAyRJkiRJCxQ7QJIkSZJqwyFwkiRJUs3UeAScFSBJkiRJ9WEFqA2GDhnEysssWnUYkhYQr8ycU3UIHefVWV1Vh9Bxlll84apD0HxikaGDqw5B7RYugy1JkiRJtWAFSJIkSaqZoL4lICtAkiRJkmrDCpAkSZJUI40XoVYdRXWsAEmSJEmqDStAkiRJUs1YAZIkSZKkGrACJEmSJNVMRH1LQFaAJEmSJNWGHSBJkiRJteEQOEmSJKlGXAZbkiRJkmrCCpAkSZJUJwE1XgPBCpAkSZKk+rACJEmSJNXMoBqXgKwASZIkSaoNK0CSJElSjbgKnCRJkiTVhBUgSZIkqWZqPAXICpAkSZKk+rACJEmSJNVKMIj6loCsAEmSJEmqDTtAkiRJkmrDIXCSJElSjQQugiBJkiRJtWAFSJIkSaqT8EWokiRJklQLVoAkSZKkmhlU40lAVoBqZNKll7D+emuz3jpr8NMjj6g6nI5gTsrMSZk5ebMvff5A1h41nM3Hblh1KB1j+tTH2Gu37Rn3gQ3YZtP386sTjq86pI7g706ZOSmb+dprfGyHrdh1mw+w01ZjOPbIH1YdkhZwdoBqoquri68ccjDnnn8xt9x+N2edcTr33H131WFVypyUmZMyc1K2737j+cOfL6g6jI4yeMgQvvuDn3DF9bdx3qQrOfXXJ3D/vfdUHVal/N0pMyetDV14YU47+yLO/8v1nHvZtVx1+WRunXJD1WEt0OauAteurdPYAaqJG2+4gdVXX4NVV1uNoUOHstfe+3DB+edWHValzEmZOSkzJ2WbbbElSy21dNVhdJQVVlyJ923wfgCGLb44a661Dk88Pq3iqKrl706ZOWktIlhssWEAzJk9mzlzZhOd+LdmLTDsANXE9OnTGDly5Tf2R4wYybRp9f4/Z3NSZk7KzIneqsf+7xHuvP1W3r/xJlWHUil/d8rMSfe6urrYbdsPsunoUWy+1TZssNHYqkNa4A2KaNvWaQasAxQRMwbw3gdExIAOsI6G4yLiwYi4PSI2ajp2SUT8IyLmmzEgmVlqq/u/rpiTMnNSZk70Vrw8YwYTxu/LxP88isWXWKLqcCrl706ZOene4MGDOe+y67jylvu5/ZYp3H/PXVWHpAWYFaDu7QisWWwTgP9uOvZT4BNVBPV2jRgxkqlTH3tjf9q0qQwfPrzCiKpnTsrMSZk5UV/Nnj2bCeP3YY8992GnXT9SdTiV83enzJz0bol3L8kmm23JVZdPrjqUBZ5zgNokIk6JiD2b9mcUf46LiCsi4o8RcW9E/C6KfxKJiLER8beIuC0iboiIxYvLhxeVmAci4sime24fEddGxM0RcVZEDCvat42IWyLijog4OSIWLtofiYjvFeffERHrFLfaHTgtG64DloyIlQAy8zLgpQFOV78aM3YsDz74AI88/DCzZs3irDPPYOdddqs6rEqZkzJzUmZO1BeZydcP+SxrrLUOEw7+ctXhdAR/d8rMSWvPPfM0L77wDwBee/VV/nbV5ay2xtoVR6UFWSe9B+j9wHrAdOAaYPOIuAE4E9g7M2+MiCWAV4vzNyyumQncFxE/L459B/hQZr4cEYcC/1Z0kE4Bts3M+yPiNODzwM+Kez2TmRtFxBeArwMHAiOAf/4zDUwt2h7vy5eJiAk0KkesvMoqbzkZ/W3IkCEcc+zx7Lrzh+nq6mL8AZ9m3fXWqzqsSpmTMnNSZk7KDjpgf6656q88++wzjF5rFId9+7vsP/7TVYdVqRuv/xtnn/l71ll3NNtv1Zj7c+h/fJ9tt9uh4siq4+9OmTlp7amnnuDQQybwelcXr7/+Ojvu9jG23n7HqsPSAixajUftlxtHzMjMYfO0nQJckJl/bD4nIsYB387M7Yr2/6bRCboNOCEzN5/nPgcAm2fmQcX+xcCPgCVpdHSmFqcOBa4FjgN+nplbFedvCxycmR+NiEeKe02LiA8AP8rMD0XEhcCPM/Pq4prLgH/PzCnF/jjg65m5S2+52HjjMXnN9Tf1LXGS1ItXZs6pOoSO8+qsrqpD6DjLLL5w1SFoPvHYs69UHULHWWvFxaZk5piq4xgoq753/Tz8tPZNZf/UJu/pqHy2uwI0h2LYXTHEbWjTsZlNn7toxBZAdz207s6fnJn7Np8YEb29rW/uvebeBxqdqJWbzhlJozolSZIkaT7V7kUQHgE2Lj7vDizUy/n30pjrMxYgIhaPiJ46bdfRGDq3RnH+ohGxVnGfUXPbaSxg8Ndenn0e8MliNbgPAi9kZp+Gv0mSJEkdKxorELZr6zQDWQFaNCKmNu0fDZwEnFvM7bkMeLmnG2TmrIjYG/h5RCxCY47Ph3o4/+lieNzpcxc5AL5TzPv5FHBW0YG6ETihl/gvAnYCHgReAT4190BEXAWsAwwrvuNnMvPSXu4nSZIkqWIDNgdI/+QcIEn9yTlAZc4BKnMOkPrKOUBlC/wcoHXXz++ddmHbnjd+7CodlU/fAyRJkiSpNuwASZIkSTUSwKCItm29xhOxdkTc2rS9GBFfmeeccRHxQtM5332737+T3gMkSZIkqWYy8z4a7/gkIgYD04BzWpx6VV9eQdMbO0CSJElSzXTe2mxv2BZ4KDMfHagHOAROkiRJ0kBaNiJuatom9HDuPsDp3RzbNCJui4iLI2K9txuMFSBJkiSpZtr8ep5n+rIKXEQMBXYDvtni8M3AezJzRkTsBPwZWPPtBGMFSJIkSVIn2BG4OTOfnPdAZr6YmTOKzxcBC0XEsm/nIVaAJEmSpFoJos0loD7al26Gv0XEisCTmZkRsQmNQs6zb+chdoAkSZIkVSoiFgW2Az7b1PY5gMw8AdgT+HxEzAFeBfbJzHw7z7IDJEmSJKlSmfkKsMw8bSc0fT4eOL4/nmUHSJIkSaqRoN4LAdT5u0uSJEmqGStAkiRJUs106CIIbWEFSJIkSVJtWAGSJEmSaqa+9R8rQJIkSZJqxAqQ9P/bu+94uYr6/+OvNwkJBEIHgdB7E0IJIJEivdcAoSihCtJRFBD4KTaKIggKXzqKSpVOKEZBeu81UUBSAAGBhJJA/Pz+mFlY7rkpBLLnZuf95HEfuXv23N25H/bunpnPzGfMzMzMSiKvATIzMzMzMyuCM0BmZmZmZgXxPkBmZmZmZmaFcAbIzMzMzKwwXgNkZmZmZmZWAHeAzMzMzMysGJ4CZ2ZmZmZWmHInwDkDZGZmZmZmBXEGyMzMzMysMAXXQHAGyMzMzMzMyuEMkJmZmZlZQdJGqOWmgNwBMjObxvTq6bfujhyTquNvfr7uJnQ5J2y6dN1N6JIWnLNX3U0wayl/YpiZmZmZFcZrgMzMzMzMzArgDJCZmZmZWVGECl4D5AyQmZmZmZkVwxkgMzMzM7PCeA2QmZmZmZlZAdwBMjMzMzOzYngKnJmZmZlZQUrfCNUZIDMzMzMzK4YzQGZmZmZmJZGLIJiZmZmZmRXBGSAzMzMzs8I4A2RmZmZmZlYAZ4DMzMzMzAojV4EzMzMzMzNrf84AmZmZmZkVRMB05SaAnAEyMzMzM7NyOANkZmZmZlYYrwEyMzMzMzMrgDtAZmZmZmZWDE+BMzMzMzMrjDdCNTMzMzMzK4AzQGZmZmZmhXERBDMzMzMzswI4A2RmZmZmVhBvhGpmZmZmZlYIZ4DMzMzMzIoirwGyMtx6y82suPzSLL/MEpxy8ol1N6dLcEyqHJMqx6TKMalyTJLBpx/Db3dfiwsP3OqTYx+MfpvLj9uLc/fbhMuP24sPx7xTYwvr5ddJ5xwXayV3gAoxfvx4DjvkQK69fjCPPvEMV1z6Z5595pm6m1Urx6TKMalyTKockyrH5FMrbLAdA3507meO3X/luSy84prse84tLLzimtx/5bkT+On25tdJ5xyXGijtA9Sqr67GHaBCPPjAAyy++BIsuthi9OjRgx13HsgN119bd7Nq5ZhUOSZVjkmVY1LlmHxqwRX6MUPvWT9zbNj9Q1h+g20BWH6DbRl631/raFrt/DrpnONireYOUCFGjhzBAgss+MntPn0WYMSIETW2qH6OSZVjUuWYVDkmVY7JxL3/9pvMPMc8AMw8xzy8//ZbNbeoHn6ddM5xqYda+NXVTLUOkKQxU/GxB0k6c2o9fn4OSfqNpGGSnpC0Sj6+sKSHJT0m6WlJ+0/NdnxZIqJyTF0xJ9lCjkmVY1LlmFQ5JlWOiU0Ov04657hYq7kK3IRtBiyZv9YAzsr/jgLWioixkmYGnpJ0XUSMrK+pk9anzwIMH/7KJ7dHjBjO/PPPX2OL6ueYVDkmVY5JlWNS5ZhMXK/Z5mTMW68z8xzzMOat1+k12xx1N6kWfp10znGxVmvpFDhJF0ka0HR7TP53PUm3S7pS0nOS/qjc9ZfUT9I9kh6X9ICk3vnH55d0s6Shkk5uesyNJd0r6RFJV+ROCpI2kPSopCclXSCpZz7+kqQf5/OflLRMfqhtgN9Hch8wm6T5ImJcRIzN5/RkGplGuFq/fgwbNpSXXnyRcePGccVll7LFllvX3axaOSZVjkmVY1LlmFQ5JhO3xOrr8/SQawB4esg1LLHGBjW3qB5+nXTOcWm9tBGqWvbV1XSlDNDKwPLASOBuoL+kB4DLgJ0j4kFJswAf5PP75p8ZCzwv6Yx837HAhhHxnqQfAEfkDtJFwAYR8YKk3wMHAKflx3ojIlaR9B3ge8A+QB/g0+EIGJ6PjZK0IHAjsARwZGfZH0n7AfsBLLjQQl88Ol9Q9+7d+fXpZ7LVFpswfvx49hi0F8stv3zdzaqVY1LlmFQ5JlWOSZVj8qnrTzmCV558kA/e/S9nDVqX/rsezBoD9uW6kw7niduuYpa552Pro06b9AO1Ib9OOue4WKups3mXX8oDS2MiYuYOxy4CboiIK5vPkbQe8MOI2CgfP4vUCXocODsi+nd4nEFA/4jYN98eDPwMmI3U0RmeT+0B3Av8BjgjItbJ528AHBgR20t6KT/WCElrAD+LiA0l3Qj8IiLuyj8zBPh+RDzc1I75gWuArSLitQnFYtVVV4u7739o8oNnZmb2BR1/8/N1N6HLOWHTpetugk0jZpxeD0fEanW3Y2pZ9qsrx4VX/71lz/e1JWfvUvFsdQboY/KUsTzFrUfTfWObvh9PapuACfXQJnT+bRGxS/OJkvpOol2Nx2o8DqRO1IJN5yxAyk59IiJGSnoaWBu4chLPYWZmZmZmNWv1+pWXgFXz99sA00/i/OdIa336AUjqLWlinbb7SFPnlsjn95K0VH6cRRrHgW8Cd0ziua8DvpWrwa0JvBMRoyQtIGnG4rz5dgAAIABJREFU/PizA/0BD7OZmZmZ2bSj4DrYUzMD1EvS8KbbpwLnAtfmtT1DgPcm9gARMU7SzsAZudPxAbDhRM7/T54e9+dGkQPg2LzuZ0/gityBehA4exLtvwnYHBgGvA/smY8vC/xKUpD+l/4yIp6cxGOZmZmZmdkE5GUpo0kzsj7uOGUuzx47nXR9/j4wKCIemZLnmmodoIiYUHZpzabvj87n3g7c3vSzBzV9/2CHn4G0zueipnO2bPr+b0C/TtozhFQ0oePxRZq+fwhYL38fwIGdnH8bsGL11zIzMzMzmzaoK6Zm4BsR8cYE7pvQFjWf2zRRwtnMzMzMzIrW6RY1U/JA7gCZmZmZmRVGat0XMJekh5q+9uukSQHcKunhCdw/oS1qPreutA+QmZmZmZm1nzcmowx2/1xheR7gNknPRcQ/mu7vbM7eFO3n4wyQmZmZmVlhuloRuIgYmf99HbgaWL3DKZPcomZyuQNkZmZmZma1kTSTpN6N74GNgac6nNbpFjVT8nyeAmdmZmZmVpquVQTuK8DVqdI13YE/RcTNkvYHiIizmfAWNZ+bO0BmZmZmZlabiPgXsFInx89u+r7TLWqmhKfAmZmZmZlZMZwBMjMzMzMrSCpO0LXmwLWSM0BmZmZmZlYMZ4DMzMzMzEry6QalRXIGyMzMzMzMiuEMkJmZmZlZYQpOADkDZGZmZmZm5XAGyMzMzMysNAWngJwBMjMzMzOzYjgDZGZmZmZWFHkfIDMzMzMzsxI4A2RmZmZmVhjvA2RmZmZmZlYAd4DMzMzMzKwYngJnZmZmZlYQUXQVbGeAzMzMzMysHM4AmZmZmZmVpuAUkDNAZmZmZmZWDGeAzMzMzMwK441QzczMzMzMCuAMkJmZmZlZYUreCNUdoBYYH8H7Yz+uuxnWxfXq6T9Hmzxvjh5bdxO6nBl7dKu7CV3OCZsuXXcTbBrxwqjRdTfBrKV8xWVmZmZmVpiCE0BeA2RmZmZmZuVwBsjMzMzMrCSi6BSQM0BmZmZmZlYMd4DMzMzMzKwYngJnZmZmZlYYb4RqZmZmZmZWAGeAzMzMzMwKIsreCNUZIDMzMzMzK4YzQGZmZmZmhSk4AeQMkJmZmZmZlcMZIDMzMzOz0hScAnIGyMzMzMzMiuEMkJmZmZlZYbwPkJmZmZmZWQGcATIzMzMzK4z3ATIzMzMzMyuAO0BmZmZmZlYMT4EzMzMzMytMwTPgnAEyMzMzM7NyOANkZmZmZlaaglNAzgCZmZmZmVkxnAEyMzMzMyuI8EaoZmZmZmZmRXAGyMzMzMysJPJGqGZmZmZmZkVwBsjMzMzMrDAFJ4CcASrFwQfsw9KLzE//fn3rbkqX4Zh07tZbbmbF5Zdm+WWW4JSTT6y7OV2CY/JZI4e/wo5bb8x6a6zE+l9bmfPOPrPuJnUJfk+p8t9OlWPSufHjx7PTZl/noEE71t0UK4A7QIXYZbc9uPyaG+puRpfimFSNHz+eww45kGuvH8yjTzzDFZf+mWefeabuZtXKManq1r07x//kJG6//3Guu/UfXHz+2bzw3LN1N6t2fk/5LP/tVDkmE/bHC85isSWWqrsZZVELv7oYd4AKsdbX12b22eeouxldimNS9eADD7D44kuw6GKL0aNHD3bceSA3XH9t3c2qlWNS9ZV55+OrK60MwMy9e7PkUsvw6qgRNbeqfn5P+Sz/7VQ5Jp17bdQI7hxyC9sN3KPuplgh3AEys0+MHDmCBRZY8JPbffoswIgRZV/YOiYT98q/X+KpJx5j5VVXr7sp1sX4b6fKMencyT86isOPOYHppvNlaeuopf91NVPtlSZpzFR87EGSpuqkcyW/kTRM0hOSVsnH+0q6V9LT+fjOU7MdZq0UEZVjKrlOJo7JxLw3Zgz77bELP/r5L+k9yyx1N8e6GP/tVDkmVXf8dTBzzDUXy624ct1NsYK4CtyEbQYsmb/WAM7K/74PfCsihkqaH3hY0i0R8XZ9TTX7cvTpswDDh7/yye0RI4Yz//zz19ii+jkmnfvoo4/Yb4+BbDdgIJtvtW3dzbEuyH87VY5J1WMP3c/ttw3mrr/fxtixH/Le6NEcfeg+/OL08+pumrWxluYaJV0kaUDT7TH53/Uk3S7pSknPSfqj8pCIpH6S7pH0uKQHJPXOPz6/pJslDZV0ctNjbpwzNI9IukLSzPn4BpIelfSkpAsk9czHX5L043z+k5KWyQ+1DfD7SO4DZpM0X0S8EBFDASJiJPA6MPfUjZxZa6zWrx/Dhg3lpRdfZNy4cVxx2aVsseXWdTerVo5JVUTwvUO+zRJLLcN+Bx5ad3Osi/LfTpVjUnXoUT/itgeeY/A9T3HSmRfSb6113PlpEal1X11NV5psuTJwGLAcsBjQX1IP4DLg0IhYCdgQ+CCf3xfYGfgqsLOkBSXNBRwLbBgRqwAPAUdImgG4CNg5Ir5Kynwd0PTcb+TzzwK+l4/1AV5pOmd4PvYJSasDPYB/dvxlJO0n6SFJD735xhtTEo8v1b6DdmfT9ddm2NDnWWGpRbjk4gvqblLtHJOq7t278+vTz2SrLTah71eXZYcdd2K55Zevu1m1ckyqHrz/Hq667E/cfeftbLzO6my8zuoMue3muptVO7+nfJb/dqocE7PO5ev4v0t6Ni8zqYyu5YTJO5Iey1/HT/HzdTYf9csgaUxEzNzh2EXADRFxZfM5ktYDfhgRG+XjZwF3A48DZ0dE/w6PMwjoHxH75tuDgZ8Bs5E6OsPzqT2Ae4HfAGdExDr5/A2AAyNie0kv5ccaIWkN4GcRsaGkG4FfRMRd+WeGAN+PiIfz7fmA24E9coZogvqusmr87c77Jzt2VqZePT0j1SbPm6PH1t2ELmfGHt3qbkKX4/cUm1wvjBpddxO6nJUWmuXhiFit7nZMLSv2XTWu++vdLXu+ReeecaLxzNfV80XEI3m218PAthHxTNM56wHfi4gtv2h7Wv3u+DE565SnuPVouq/5E308qW0CJtRDm9D5t0XELs0nSprUrnSNx2o8DqRO1IJN5ywAjMyPNwtwI3DspDo/ZmZmZmY2YRExChiVvx8t6VnSzKupslFWq6fAvQSsmr/fBph+Euc/R1rr0w9AUm9JE+u03UeaOrdEPr+XpKXy4yzSOA58E7hjEs99HfCtXA1uTeCdiBiVp+VdTVofdMUkHsPMzMzMrOvpohuhSlqEtDSms+lTX8t1AQZLmuL5o1MzA9RL0vCm26cC5wLXSnoAGAK8N7EHiIhxucz0GZJmJK3/2XAi5/8nT4/7c6PIASlL84KkPYErcgfqQeDsSbT/JmBzYBip8tue+fhOwDrAnPm5AAZFxGOTeDwzMzMzsxLNJemhptvnRMQ5HU/KxcuuAg6LiHc73P0IsHBEjJG0OXANqVrz5zbV1gDZp7wGyCaH5+vb5PIaoCqvAarye4pNLq8BqiphDdD1Q+5p2fMtMtcMk4ynpOmBG4BbIuLUST1mXse/WkR87mpjXakKnJmZmZmZFSbXBjgfeHZCnR9J8zZtk7M6qR/z5pQ8n4eHzMzMzMwK08X25+lPWqP/pKTGspJjgIUAIuJsYABwgKSPSctiBsYUTmVzB8jMzMzMzGqTt52ZaJcsIs4Ezvwyns8dIDMzMzOzwnStBFBreQ2QmZmZmZkVwx0gMzMzMzMrhqfAmZmZmZmVRF2uCEJLOQNkZmZmZmbFcAbIzMzMzKw45aaAnAEyMzMzM7NiOANkZmZmZlYQ4TVAZmZmZmZmRXAGyMzMzMysMAUngJwBMjMzMzOzcjgDZGZmZmZWGK8BMjMzMzMzK4AzQGZmZmZmhVHBq4CcATIzMzMzs2K4A2RmZmZmZsXwFDgzMzMzs9KUOwPOGSAzMzMzMyuHM0BmZmZmZoUpOAHkDJCZmZmZmZXDGSAzMzMzs4JI3gjVzMzMzMysCM4AmZmZmZkVxhuhmpmZmZmZFcAZIDMzMzOz0pSbAHIHqBW6SfTq6VCb2Zdjzt49626CmbWRwcNer7sJZi3lq3IzMzMzs8IUnADyGiAzMzMzMyuHO0BmZmZmZlYMT4EzMzMzMyuMN0I1MzMzMzMrgDNAZmZmZmZFkTdCNTMzMzMzK4EzQGZmZmZmBRFeA2RmZmZmZlYEd4DMzMzMzKwY7gCZmZmZmVkxvAbIzMzMzKwwXgNkZmZmZmZWAGeAzMzMzMwK432AzMzMzMzMCuAMkJmZmZlZSeQ1QGZmZmZmZkVwB8jMzMzMzIrhKXBmZmZmZgVR/iqVM0BmZmZmZlYMZ4DMzMzMzEpTcArIGSAzMzMzMyuGM0BmZmZmZoXxRqhmZmZmZmYFcAbIzMzMzKww3gjVzMzMzMysAM4AmZmZmZkVpuAEkDNAJbn1lptZcfmlWX6ZJTjl5BPrbk6X4JhUOSZVjkmVY1LlmFQ5JlWOSdVdV13Er/fajFP33JS7rryw7uZYAdwBKsT48eM57JADufb6wTz6xDNccemfefaZZ+puVq0ckyrHpMoxqXJMqhyTKsekyjGpevXFF3jwxss48Hd/4dDzbuC5+/7OG8NfqrtZZVALv7oYd4AK8eADD7D44kuw6GKL0aNHD3bceSA3XH9t3c2qlWNS5ZhUOSZVjkmVY1LlmFQ5JlWvvzyMBZfrS48ZZqRbt+4sutLqPH3XrXU3y9qcO0CFGDlyBAsssOAnt/v0WYARI0bU2KL6OSZVjkmVY1LlmFQ5JlWOSZVjUjXvokvx0hMP8t47/2Xchx/w/P238/bro+pulrW5LtsBkjSvpEsl/VPSM5JukrTUVHy+bSUt13R7Dkm3SRqa/509H19G0r2Sxkr63tRqz5ctIirHVHL9QxyTzjgmVY5JlWNS5ZhUOSZVjknVPAsvwboD9+P8I/fggh/sxXyLL8t03brV3awiqIX/TbIt0qaSnpc0TNJRndzfU9Jl+f77JS3yRX73LtkBUno3uBq4PSIWj4jlgGOAr0zOz0qakt9rW2C5pttHAUMiYklgSL4N8BZwCPDLKXiO2vTpswDDh7/yye0RI4Yz//zz19ii+jkmVY5JlWNS5ZhUOSZVjkmVY9K5fpvvxCHnXMf+p/+ZGXvPylwLLFJ3k6yFJHUDfgtsRroW36U5KZHtDfw3IpYAfg2c9EWes0t2gIBvAB9FxNmNAxHxGPCopCGSHpH0pKRtACQtIulZSb8DHgEWlDRG0q/yuUMkzZ3PXVzSzZIelnRnzuisBWwNnCLpMUmLA9sAF+env5jUQSIiXo+IB4GPWhSLL8Vq/foxbNhQXnrxRcaNG8cVl13KFltuXXezauWYVDkmVY5JlWNS5ZhUOSZVjknnxvz3TQDefm0kT995Kyutv1XNLWp/Im2E2qqvSVgdGBYR/4qIccClpOvwZs3X5VcCG+gLpE/VWTq2bpIOARaNiMM7HO8O9IqIdyXNBdwHLAksDPwLWCsi7svnBrB7RPxR0vHAPBFxkKQhwP4RMVTSGsAvImJ9SRcBN0TElfnn346I2Zqe+78RMXvT7R8BYyKi00yQpP2A/fLNpYHnv2hcvgSzAosA44E3gFdrbU3X4JhUOSZVjknVrMCCQDfgNRwTcEw645hUOSZVS5P2puwGvAiMrrc5ACwcEXPX3YipRdLNwFwtfMoZgA+bbp8TEefktgwANo2IffLtbwJrRMRBTe19Kp8zPN/+Zz7njSlpzLS2EaqAn0taB/gf0IdPp8W93Oj8ZP8DLsvfXwL8RdLMwFrAFU2dxp5To6H5f+o5U+OxvwhJD0XEanW3oytxTKockyrHpHOOS5VjUuWYVDkmVY5J60TEpnW3oUlnmZyOGZrJOWeyddUO0NPAgE6O7wbMDawaER9JeonUowR4bxKPGaQpf29HRN/JaMNrkuaLiFGS5gNen7ymm5mZmZnZZBpOyoo2LACMnMA5w/OMsFlJ6/KnSFddA/Q3oKekfRsHJPUjTXV7PXd+vpFvT8h0fNqJ2hW4KyLeBV6UtGN+TElaKZ8zGujd9PPXAXvk7/cAyi7Ub2ZmZmb25XsQWFLSopJ6AANJ1+HNmq/LBwB/iy+wjqdLdoDyL7QdsFEug/008CPgJmA1SQ+RskHPTeRh3gOWl/QwsD5wQj6+G7C3pMdJmabGIqtLgSMlPZqLIJyYn38osFG+3SjPPRw4AjhW0nBJs3xZv3sLdLlpeV2AY1LlmFQ5Jp1zXKockyrHpMoxqXJMChQRHwMHAbcAzwKXR8TTkk6Q1KgScj4wp6RhpGvwSqnsz6NLFkH4MkgaExEz190OMzMzMzPrOrpkBsjMzMzMzGxqaNsMkJmZmZmZWUfOAJmZmZmZWTHcAbKJ+iK77JqVzn8/NqX82rHJ5deK2efnDpBNkCQ1SgxK2kfSmnW3qW6dfdD4wyeRtIykpetuR1chaVbS7uZI6itpzpqb1CU0/l78d9M5SfNAqobqGCWSVpa0jaTF6m5LVyFpeUnrSZr3i5QCnlZJ6lZ3G2za5g6QTVBT52dTYGPglXpbVK8OHcIVJM0taWZfqEAuU3k/sK+kletuT93y62E5YCtJFwC/Y9KbNZei8bfSe6JnFUjSVsBtkvYDd4Lgk8+fS4HNgAckLZWPFxsXSZsBVwN7A3dKWjUfLyIm+TVwRN6yxGyKuANkEyVpCeBi4N8RMULS9HW3qS5NnZ8DSRe0hwF/kjRTiSNwDZJmIl2cnAe8CWwnqW+9rapXfj08BvQDdgAuiogPoZyLlI4krS5pnoj4n6RDgcGSfihp3brb1hXk99ozgMHA+o2NwHMnqMjP6nxhfyawX0TsD/wJ+Kqk3qW+5+YBpl8De0XEN4FzgbMldS8hJpIWAu4DtgR2cCfIplSRb6o2YR0vziJiGHAcMEjSehHxUWkXcJJ6Nn2/IWkH4i2BWYGPgPdralpX8T7wc+BI0sVbD2B7Sas0n1TC66b5d4yID4BfAicDi0oakI+HpF41NbFOA4CbJO0EfJ30mpkZ2FrS5rW2rAvI77UHk14zfwC2aMoE/a/OttVoOLBLRNyRL3z3ArYHrpO0h6QZ6m1eLT4EjouIu/Lt04CRQClTwuYA/h/wI+ArwE7NnaASPmfsy+Ey2PaJDlO8dgIWBh4FHgE2Je26e0hE3N58bjuT9FXgG8A1EfFvSf1J6zp6AtsBW0XEWEnrA3dGxEc1NrelcmzGAP+LiJebjq8MDATGkkZvVwUejIg3amloi3RcMwfMSIrNbyUdDixE6iDOne+7MCLG19bgFmnEJWcxjiFN2zk+Iv6Q13RsC/Qh/f1cU2db6yBpZoCIGNN0bEZgPWB/4KaI+L8cq9ebz2tXOSYfd8iaDgAWjIhTJW1Hei3tGhFDa2xqy+SYjIuIcZJmi4i3m/627gd2ioiXJfUBXm3n95acARwtaT1gK9LMgysj4oVSrk3si+tedwOs62i6eDuIdAF7EfB/pNGmP+UPoUsk7RIRd9bX0tbIiyx7AesCH0u6ChgFXEW6EFkxn7c3sD7wICkj1PYkbUEaebwLmFfSdRFxFkBEPJoH4TYB/gj0B1YC2roD1PT3sz+wK3AI8Iik/wAXAoOAnYENgM3b+QKlocPFyNzAqcC8wI8l3RwR/5J0JfAtYHVJt0VEMWul8oX84cDrkp4ETojkA0l3ktZL7ZrXfMxLmmra1nJMjgBek/QE8JP8GrqicU5EXC1pe1LHue07QE0xeV3SoxHx03zXdJK6A9MD70jalfTeM5A0ONUWlIrr7A48CQyLiEcAGoOxwNakaaNrA/0l7e1OkE2KO0D2GUoViPqSLuh3A/4FXJbnF/9R0nhgRJ1tbIX8hrtGRPxe0q+A75A+ZC4BDiQtwNwXmIn0xrxnRIyurcEtkj9sZga+DxwWETdK6gdcI6lXRPwKPukEfQtYBuhXwihtjk0vYA3Sup8BwC2k7OE44DRJswAzRsRr9bW0dZo6hd8FlidNkzyENG3yL5J2zJnVi4H3Cuv8LEGaXnwwaQT7PKCXpDMiYnjO9NwkaUtSFnrziPhvfS2e+iYQk5kaMWk6b1fS59SwWhraQp3FRFJv4DcRMQIYL+kh4IfAWsAB7ZQlzJnPvwA3kT5PfiLpqIi4GiAi/i5pJHASabDyO+782OTwGqDCqcPi2oh4nTRSfxOwc0RslEeq95bUNyIujYh/1dHWVpG0LLAmcKOkJUmjTr8mLWgfSJoSeBSwOjAf8K2IeLKm5rZUHp0eTVqE+m4+9iCpw3x4Y81CnsKzALBNRDxdV3untg5rfgL4gDTyegop07N9nrJypKRNIuLdUjo/DZIGkjqDx0bEm3k9y9HAHcBflQojvBIRb9Xa0Nb7H/A28GxEPAdsAywIHNQ4QdIapOnHm0bE47W0srUmGhNJM+XOz/HAwOZOURubUEwObjpnBVKGec+IeKLlLZy6VgAejogjI+IE0u99fs4ANsxAygLtHhF/9jogmxzuABVM0vSNxbWSltKne7gMJa1xOTHf15jO824tDW0hpcXY5wD3kD54fkD68H2WNOXra8AWwOMRsW9E/CAinqmrva3U4UNlNGkOPgAR8TywE7C5pPkjFQDYpTFVoR11WPPzdUkrkEo730eahnJwnsq0Eymb2vZZsAnoA/wxIkYqVQxs+ClwGSlrVqJXgceBr+fs6Zuki7v1JB0DEBH3A18vZYCFicfk6JwhfAnYrJ0HVjqYWEyOz+ecAnwtIl6oq5FT0X+AyFPSiYhbSQORv9SnexN+SHpN3OjOj00ud4AKpbSAfbv8/SGkFPMlkk6IiPOB24HDJF1HmraycwGZn01InZy9I2Jonm7yF9LahQNJnaBfk/ZEGpjnXhehw8X+QnkO+keS/to4JyLuIWVAGrc/bn1LW6cpHgeSKr1tS8oO/hn4LXCFpD+SyqXv3u5/P1DNKGfdgFUAmqa4bQksHRE/iYiXWtS82jXHJyLeB54GvgmspFRO/01gP2Ap5eqTETGylsa2yOeIyTL5nHsi4sVaGtsinyMmjepn10aqItgWJE2vTytlPk2q9nZu4/7cCTobWDnffj4ibmm631PgbJKKuYCzitWBTSTNAaxDKkvbE3hQ0kcRcZykr5Aqwf07Il6tsa1TXV5k/FtgEVKFLgAi4qa87mlz4Nuk7NDxpCIIbX2B36zpYv9wYA1J+0bE1pKulzQY+A2wKLBine1sNUmrkaakbAQcCryUs6qHK23WNx3wbrtfxDY0ZZR3AcYDL5AqAT6c19KdTVqncBywYV3trEtTfNYAhkfEefli9wjgckl3kC7q+gBFXMR9jpjML6lHXk/X1j5nTHpGxNgam/ulygOL6wNjJS1K2lB6J+AuSRdExF751PfzfZ/hzo9NLneACiNphoj4MCLOzynl9Umvg24R8Vpe0H6vpPki4jtA269XUNqv5gRS1bJFSDtr7xARtwFExC2SglTBa0/gzBLfZPNF7Y7AFpFKkPaIiK1yMYivk8pd79zOF/vNmbDsXeBWUoZwXdL0SCTtCNwaEe+0vpX1krQt6e9pCGle/hWk9XMXkqaULgRsXVjmpzmD+h3S9NEhkj4mFVh5lVTo4DukKYHfbvcLfcekagpj0jadH0gzByR9QCpqMB/w/Yh4Nw823Srp96RpcVuRsutmU8QdoIIoVaBaXdK/SaNHQ0kLtncC1pX0j9wJ6g/ckjNArxdwsS/S4tGhwNC8kP8qSdtHxF8hpdxzJuipAuIBdHqxvzBpauQikg4mlR19BxgQaYPcthqJnACR5qPPHKnS0n9IneLeEbEQgKRvkioD/r2+ZtYjd4b7Af0j4nVJ25A6zd0jYsd8Tu8ooGJis6aL2g1JHcD+wMekEtgXA/tGxHWSFgbej4j/1NbYFnFMqkqPSeMzJyL+Iekx0pTqsZIWjIhXSOueNgfmBG6IVAHO+/7YFPFGqAWRNDtp8eAupApdK0fEO0qbNq4NXA/8I1+4dIs236dE0nTRtMN68++sVLnqHGDbiPhbXW2sS4eRyPWBZ4C5gDNIxSEuIe17dDhwYkQMbecPopwZfSH/vRxK2o3+ZtIasY9JG5xeQ9oHahNSh7qUheufkHQU8HNgvXwRMztpiu0ewI0589y2r5MJydn22YEnSH9Lm5OmCM5Nmta0CqmqWVvvldXMMakqOSaN9wVJy5Gmt40jrf35Pmk7gYtJ1y3kzpDZF+IiCAWQUlWUSIv6h5IWTt5KGmEiIs4jjVbvAnwtzzX+X+eP1j6a5llvly/yv9p036XAPqQyvevW1MTaNHV+DiYVfpg+Ip4irXXZMCIuBJYk7XnzbvPPtKk9gJuVSq+uQ4rJHMD+pAuWdYHX89eupXV+JG0iae2IOBE4FrhS0qL5Pecu4HxSaf12f518ovG+25AvWr9Buqj7dkSMz2srTwPuJpXybWuOSZVjkuTOz2bA5aQqmneRKv5dQdpS4JekangL1NVGay/OALW5DiP50+epSguT1rPMTRqVvV3SnKS9Oq6LiFE1Nnmq6xCTnUlzjRvTlW6MiCubzt0eeDpSmeeiSNqAVF51vaY52CJV5dkU+Blp+lvblqOVtFxEPJNHZn9MGiQ4OiIuVypysC1pnvrVEfGPOtvaSk2jtY1/fwEsS8oG3ifph6TO4TciYljHbGtJJO1B6iD/i9QJfJM0ov3biDgjn9P2GfdmjklV6TFRKnjwJ9KWAWuQ1j+tExH/zZ89y5IKQhQ3tdimDneACpEXVK5JGlG5EniFNH2pJ2kEe37SyHXb7/XTIGkvUhWZXwBjSdW8NgWuiYir6mxbHTpOTZK0DCkLFqRpGBuTqnpdTioJ/kG08UL2PBr5G1L1v78DPfi0ilm/3ClcjFSetifws/i0zHMRJC0Wuby3pONImxaeFhH3Svo5sAOwPDC+lMxPM0l7k/YRO4lUJXEtUvbw36T9oo6OiHMn/AjtxzGpKjkmTYMoc5PWU74IfI+0dcDQ/D58d/O1SYnTaO3L5w5QAZT2KRnm3XrbAAAVnUlEQVRAmkv7E2Bm0jSVR0hVmjYCfhXtt4P0Z3Qyan0R8C1gyYj4p6T5SKn2AcCFEXFtne1tpQ5ZsdlJnZ73gb1IC3HPIE2f/AGpEMQldbW1FSR9AzidtJnpHR3uO5VU8W7bPDq5CDA60t4cbU3SQsB0EfGSpGWBo0l7kFyV7/8JqTrTwRFxp6Q5S4jLhORO4X0RcZuk3qSqm9tExF6SVia9btpm/5bJ4ZhUlRiTps/hXhHxvqQZgDtImZ7ZI2K80kanJwH7RCpSZPal8RqgNpents1K6uisRbqwvZjUEeqXL2T3KqXzk2821j4NIu39c6NSBbNRpJH+P5MW+BejqfPTqDZ0GekD+OyI+GZEPEDat2VD2jg2TfPxtyZ1gu+QNIuk5SUdJmkl0gfyP4C/SZo1Il4q4SJf0hbAVcC1kn5Mmot/H7CxUulrIuK4fPouSiX32z4uDR3XcmTdgB/m95/RwKPA3ErbDDzabhe1HTkmVY5Jkjs/mwPXSTqNVKBpK1LW62eSDgLOAk5158emBpfBbjMdU8MR8aakU4ClSRd1WwK9gUHAdyXdD7T9tJ2mC/yDgE0lDQWGRsTBki4EHpC0RkSMkHRlu86znhhJB5DWtGxG2nX7svwB/BulsqzfJlU3K2E91AvAfJI2IU1x607K+qwM3EZa/9QDmA1o+71+8oXKyaT3j+lJ+/y8Q9rbJ4AtlHZufwd4irQW6MOamttyHTKou5GmFb9G+juaBbhA0v6k11BvUuXAtuaYVDkmn1Kq9rYbqdrqh6SNpBckrYP6LjATcGRE/NVT3mxqcAaozTS9ue4v6WhJJ5EWro8BRpMuVtYDHgIGRcSYUt5YJG1F2vNoF2BFoC9AROxJKjnamOpUxGLt5pFISfOTFt/uROrodCNVOztF0r6R9kPaKdq0upmypr+FoaTM6emkKne/i4glgcdIe9x8GBE/iIiXa2pyyyhVhdyK9B4yMiJeIK2FWjtSlbergb+SppMeC/wiIv5dV3vr0PS+ewRp2uho4EjSxrinkTrL1+Rjh0Sb7d/SGcekqvSYND5zlAoePAK8GhGXR8R1pIGmrwPzRcQxEXFSfLoPXxHXKNZazgC1obzmZwfSiMoNwDsR8XOl3aQvJS1U3jYiXq+xmS3TdGE7CymlPoC0qP+gfP8iEbFLXgNUxJtth5HIvUmdncNIi/k3AQ7M66JuBH4s6dKIeKu+Fk913SLiY/gkHmOAo4CfR9ocuDFY9A7QO89XH9vurxVJK0fEo0pV3vYBLlTa7HRt4M38OnqVlC38C2lD2HZ+nUxQXruxTERsIOlIUhWvc0mbwO4maSbSutsxtTa0hRyTqpJjkqe9bUTKep0GHCjppIh4PSJGShpOKv/9VK0NtSK4A9Rm8gjLkqQR22+TNlQ7FSAiBkiaJ33bXiNLHXUYzRcp8zUM+APwRkSslc87GFhK0uHR5uW/mzV1ftYkTWsaFGmTz1mAl4E18gfVS6TO0OjaGjuVKZWzvkDS9nlQYC5gVER8IOlDSHtG5Qv/75CqE7X99C5J3YGdJA2PiH9L+hWpNO3jwMsRsUE+T6QLto+AYjo/qpb2DmBOSTeQpi5tky/4dpf0dF5H19YckyrH5FNKBR32B06OiKMkTQ88pVQC/DXS9Lfz62yjlcNT4NpA0+g0pE7tPKRF7P2AHSPiQ0mHSNohj7S0decHPnOB/03gBEk7kqZ1XQncK2mz/KY7CPi/xuh/u5PUV9LakubLnZ3dSOvDVgGIVGr0GdJrZ3/ggnbuGOaL93+SLuovVyoaMh1pk9PGiOUMeb7+LsA3o433PWqW/yaOARaVdFNEvENa+zQYeEPSjPlURWF7/DRf1EpaTlKfPGJ/JWkdw28jYpykQaTqm6/V19rWcEyqHJNP5ffWPwAfRsT9ABHxXeAC4EZgd2CLiLinvlZaSVwGexomaS3Sm8kjHd5oNyO9wQ6IiMGSdieVq906Iv5ZY5NbSmkx6e6kql2/BX5IutBdBdiONPXg1+26rqUjpQX9PwfOA56IiLslfZW098IY0maej+ZzpwNmavPMT/M0wD6k3cc3Ia3/eYZUiekd0tTJj4BhJUzv6jhinTvKF5Gm/O0iaTbSGoVVSEUxXq2npfVQKv+9VUScnN9jvg30Av4faerOWsARwL3AasDAdu80OyZVjklVHkg6k1TW+qqm48cB+wIrRNpfzUUPbKpzB2gaJukQ4BBSludRpd3qI0/XGUgarb0d+CrpQqXd31ybL2inJy1gP55U1WwQsFFTJ7Eb6fVfSuZnE9KmnoMi4t6m4zMCcwMHk6oB3hgRbVvmujNKG+LuRer8HEva6+hBUlGMxUjx2bmEC33lPTny9ysDPSPivtzpOQ3oFRE7Ke0VdShwbkSMqLHJLZUzhluSqiW+Scqebkd6j9kXuAK4lpSF70lafzmynta2hmNS5Zh8Zp+fVUkFZf4ZES9L2h74MXB8RFzddP6ZpAJNfSl042RrLa8BmoZFKk/8Pmlh8l45E9RD0scRcamkf5HWcBBtXvCgQ+dnF2AIaS3LtcCYpvUKBwPPRq4uU5CtgWM7dH5+RhqFPBD4FXAcsKGkJyJibD3NbC1JmwLbkyrcvSfpBFJlpk1Iu6+Pl9QjIsbV2tAWkLQ0sIOkc0gXawcDH0t6gjRN5bvAaXk63OaSflzSRUojMybpVtLaje2B2fKgyo3pmpf9SKXRL442XMTekWNSVXpMJPUAPs4x2Iw0EPl7UqGUHSLiL0oFmU6T1C0irgSIiIMkzVPKoKTVz2uApjGS5s5T3xoXb7eTKsicL2m1iBiX33gOIE3/eruAzk+3ps7PtqRqVT1IZZ3fJ6XcyVmx/UgbrRVByXTA4jTtKZGnZKxBKrl6QT78O+D8du785JHZxvc9SRme9YH+ABHxAfBL4DngeqVCAKXsCdWHtLnpwcDmwKoRsQppSuAOpNfPd4BReS1DSZ2f5nVOK5IGWK4A3pJ0pKTuEXEjaV+kdUjrDduaY1JVekzyIMqZwEaS+pOmXG9Gyqh/DJwnadNIZa+/S17zpE/XMbf9+mTrOjwFbhqjVKr5XNIbZzfSvOG3lDb43AfYlHRheyZp/vFjtTW2BSR9HVgKeID0Bvtb4PKI+D9Js5KyG8uRphrMCuwdEcWV2MyZr8WAEyLiv5LmBf6TMxxnAJc0Fqa2qw5ZwlmBcZEqvR1Aqpp4aiMzmKdQzt7ugwdQics3SJ2db5Cq3T2aO4qDgcERcUrJ8/Pza+VgUnz+C2xMyhb+m7Se8GNJM0VE228u3eCYVJUYE6WNTS8jXZ9cGhGvS1ocmBc4PSJWk/R94KfAJhHx9/xzxb6fWL08BW4a0XiTiIhRkv5GWoR8YWNRdkScmQe3G1mP9SPiifpaPPXlDNgvSGsT5iDNtR4GDJR0f0Q8JumXwMyk0sZvRcQbtTW4hSTNDSwZEffk9T8fATMA20i6obGeJU8XXBEYXl9rW6PpIv97pH1sFlLa3+Ze0vqng/MI7c2RSjoX1fkBiIi/SxpF2oV9U0ljI+IZSYOB7iVfrOQR7X2ATSOiMXJ9K6ms8Y6kfcVOI73/FsExqSoxJkqFUs4gde4uaGTaI+0ltzZpgBLgfuBu4IPGz5b6fmL1cwdoGtBhhPYA0kjSHqQNKn8QESflUy8hpZCfbvcsh6R1SVmu3ZozF5JuI+0wvbekc3Mn8C0K2p8k6w4c0zSFayAwDliTtMfPnaSRuX2AHdp5IbvSItxuwAukymU7ARuSpmasD/QmVU2cDdhT0j+AD9r9g1mfrRx5KGnx8b9ImwX/ilQG/awcj81JJcDbOibNOunsTQc8EGlPpOlJ127jJA0hXdA9A+19QeeYVDkmQPq9RgCNym7dJDUKGbxEGkw5jTTV+KB2n21g0wavAZoGNHV+tgK+BuwREbeSqjDtIOlgSZsDPyKVMm7rzk+2MnBGh87PiaRKZ/2Ah4DD85zkYjSNvI0C/gasBDwZEaMj4gJSJ/l50pSvBUgVBJ+tq71Tm6QtSGucliVlAucGno+IdyPiMtIH9vdJGcLzgW9HxPttdnHSqabOT39gAGm9wsykOLxO2kD5DVLmcPuIeKamprZch0Gn6fPh4cA3JG0XER/laUwHAPtHxN+izasEOiZVjsknZiJ9Jn8dPtlDrLHecihwJ/A2cJw7P9ZVOAPUhUn6SlMKfVbSRoyrNd5AI+IBSfuQFq93J61vaetqVU0fOIuT9mhpHN8MmJ9UveoSYBRpz593OnucdjSJTOHREfGLiLgLuKvOdrZKzhKeTlOWUNLzwOaS1oiI+yPiNkl3A1+JQvbIyhmx6SOVt96ZtE7upIi4IU+dPAg4BziAVBb8zShgLVRDh7+j/YCNJd0A3EDat+VIScuTLugGkfbRamuOSZVj8qmIeFtpLekOkkZEWnvc6ACtDKxKyvy8X/I0WutaXAShi5K0DClVfhppStv5SosMfwK8GhEHNp07IzBjFLBJY4OkDYCjgB9EKv89Pen1PE7S0aRRpxsi4sNaG1qDnCncEfh+RLwqaXXSdME/AC8CG5A2xh3bzh9Eko4g7Sdxel7b83EeSPgBaUrcf0gZjuOA9SLilRqb2xJ5oOCnpH3BnpC0CHAdcH9E7JvPmZP0t7UgsGs0bYpaEqX9Sg4ALidNAbwHuBmYnlRN8m3gD9Hm+6s1c0yqHJMkD54cBsxJisXfSdssnAt8LyJuqrF5ZhXuAHVRkhYELiVdnGxASqtfS1rLsjlpg8Ij6mthvSTNRCoE0Qu4MiIeyMd3IW0Ou2tEvFhjE1umk0zhWaRM4VJN56xIyhR2I+3C3bYfxo0Rxjwi+U5EHJunBipSifjZSVmPRUijlKe2czwalIqGHAf8OCJulTQPMIZ0wXIzqXLTT/K5cwDTRSFFQ6Ayor8GaSrggRFxh6Q1gW8Br5AuZtu+aAg4Jp1xTCZM0ldIayy/Q1qLuzhwYkRc48yPdTXuAHVhkn5Nmta1G+lNZft8+wLSG8xl8WkBhOJI6gPsTVrI/ihpIeYAYNtS1is4UzhhktYHjiFlCR9W2mtiupwJOhy4EXg52njfo4bcoXmDtJbnGqXytBcD/y8ihkhaFLgauCUiflBnW7uCHI/zSNUTt42IDyWtRlp3+ShwZrtPN+7IMalyTDqXO0L/Iw3UDnfnx7oid4C6oKYR7B6kHZQPA5YhbZ52I7AwqWLV3hHxQn0trV++qF8F2IhUheb2iBhab6tax5nCCeuQJbwsIh7OxweSCh9sU8K0twalghA/Ia1H+BVwc0T8Smkj4fF5OtwfgO1Kyvw0k7QjsFdEbCZpfuB4UrGgQyPtGbUKMLJNF7J3yjGpckzMpn3uAHVRecpOD9KUlcVIF/lH5dHbRUlTe4oYybeJc6ZwwpqyhBuQdiP/kJQlHBBlVEv8jDwN7ibgmIg4sanzsyWpBPZzJa356WxkWtLjwLMRMTAPMBwNzE5aN9X2awodkyrHxKz9uAx2FxXJWNKI7IbAHyPimnzfi+78WO4kQ1rUH6RSziNJFXceAbYhbfB5dS0N7AIi7W90CvBD0nqXV4CtS+z8AETEzaTqgIMkzZY7P4NII9gfltT5gc9sMbCspIXzsZWAxSRdnTOEpwCvki5u255jUuWYmLUfZ4CmAZL2JE17Ozki2mb3aPvinCm0KZGrwZ1MKoyxK2mPkrYvBAHV0fx8Qftr4BbgxsbCdUn/Bh6NiG0aVQTrafHU55hUOSZm7c0ZoGnDvaRRfbPPcKbQpkREDCZN2Tmdgjo/WTcASd0BIuJl0vrKr5H2clkon/cbYDlJ8xVwUeuYVDkmZm3MGaBphKRezv7YxDhTaJ9Xae8rkuYCHgJWiYi3JPVoVOnK66AGkPbK6knKqB4aucR8u3JMqhwTs/bnDNA0oqSLFJtizhTa51La+0qubncwcI+k2SNtnNwj33cDcDZpHceiwE9LuKh1TKocE7P25wyQWRspbUTfbErkNVBnkjYM/q+knhExVlJ/Ugn55zpW/Wp3jkmVY2LWvpwBMmsj7vyYTVpeA3UQ8FAe4R8r6UDS5rDvl3hR65hUOSZm7csZIDMzK1Ie4T8JuAjYF9glIh6rtVE1c0yqHBOz9uMOkJmZFUvSFsD1wMoR8Xjd7ekKHJMqx8SsvbgDZGZmRfPauSrHpMoxMWsf7gCZmZmZmVkxXATBzMzMzMyK4Q6QmZmZmZkVwx0gMzMzMzMrhjtAZmZmZmZWDHeAzMymEZLGS3pM0lOSrpDU6ws81nqSbsjfby3pqImcO5uk70zBc/xI0vcm93iHcy6SNOBzPNcikp76vG00M7PyuANkZjbt+CAi+kbECsA4YP/mO5V87vf1iLguIk6cyCmzAZ+7A2RmZtYVuQNkZjZtuhNYImc+npX0O+ARYEFJG0u6V9IjOVM0M4CkTSU9J+kuYPvGA0kaJOnM/P1XJF0t6fH8tRZwIrB4zj6dks87UtKDkp6Q9OOmx/qhpOcl/RVYelK/hKR98+M8LumqDlmtDSXdKekFSVvm87tJOqXpub/9RQNpZmZlcQfIzGwaI6k7sBnwZD60NPD7iFgZeA84FtgwIlYBHgKOkDQDcC6wFbA2MO8EHv43wB0RsRKwCvA0cBTwz5x9OlLSxsCSwOpAX2BVSetIWhUYCKxM6mD1m4xf5y8R0S8/37PA3k33LQKsC2wBnJ1/h72BdyKiX378fSUtOhnPY2ZmBkD3uhtgZmaTbUZJj+Xv7wTOB+YHXo6I+/LxNYHlgLslAfQA7gWWAV6MiKEAki4B9uvkOdYHvgUQEeOBdyTN3uGcjfPXo/n2zKQOUW/g6oh4Pz/HdZPxO60g6aekaXYzA7c03Xd5RPwPGCrpX/l32BhYsWl90Kz5uV+YjOcyMzNzB8jMbBryQUT0bT6QOznvNR8CbouIXTqc1xeIL6kdAn4REf/X4TkOm4LnuAjYNiIelzQIWK/pvo6PFfm5D46I5o4Skhb5nM9rZmaF8hQ4M7P2ch/QX9ISAJJ6SVoKeA5YVNLi+bxdJvDzQ4AD8s92kzQLMJqU3Wm4BdiraW1RH0nzAP8AtpM0o6TepOl2k9IbGCVpemC3DvftKGm63ObFgOfzcx+Qz0fSUpJmmoznMTMzA5wBMjNrKxHxn5xJ+bOknvnwsRHxgqT9gBslvQHcBazQyUMcCpwjaW9gPHBARNwr6e5cZnpwXge0LHBvzkCNAXaPiEckXQY8BrxMmqY3KccB9+fzn+SzHa3ngTuArwD7R8SHks4jrQ16ROnJ/wNsO3nRMTMzA0V8WTMizMzMzMzMujZPgTMzMzMzs2K4A2RmZmZmZsVwB8jMzMzMzIrhDpCZmZmZmRXDHSAzMzMzMyuGO0BmZmZmZlYMd4DMzMzMzKwY/x9epfX10zc2bAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1152x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/numpy/lib/arraysetops.py:564: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n",
      "/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/m-shiota/.pyenv/versions/miniconda3-4.3.30/envs/micresearch36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "72061"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y = model.predict(valX)\n",
    "pred_y_c = np.argmax(pred_y,axis=1)\n",
    "# pred_y_one_hot = np.identity(len(class_list))[pred_y_c]\n",
    "true_y = np.argmax(valY,axis=1)\n",
    "confusion_mtx = confusion_matrix(true_y, pred_y_c)\n",
    "np.savetxt('../logs/'+IN_DIR_PATH+'/cm.csv', confusion_mtx)\n",
    "plot_confusion_matrix(confusion_mtx, target_names=class_list)\n",
    "plt.show()\n",
    "cr = classification_report(true_y,pred_y_c,labels=class_list)\n",
    "f = open('../logs/'+IN_DIR_PATH+'/cr.txt','w')\n",
    "f.write(cr)\n",
    "f.close()\n",
    "f = open('../logs/'+IN_DIR_PATH+'/trainingtime.txt','w')\n",
    "f.write(str(end_time))\n",
    "f.close()\n",
    "\n",
    "keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.3.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
